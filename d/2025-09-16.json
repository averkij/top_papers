{
    "date": {
        "ru": "15 сентября",
        "en": "September 15",
        "zh": "9月15日"
    },
    "time_utc": "2025-09-15 23:09",
    "weekday": 0,
    "issue_id": 5904,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.06652",
            "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
            "url": "https://huggingface.co/papers/2509.06652",
            "abstract": "IntrEx, a large dataset annotated for interestingness in educational conversations, shows that fine-tuned LLMs can predict human judgments of interestingness better than larger proprietary models, highlighting the role of linguistic and cognitive factors in engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.",
            "score": 21,
            "issue_id": 5890,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "50c590f2ee4baa42",
            "authors": [
                "Xingwei Tan",
                "Mahathi Parvatham",
                "Chiara Gambi",
                "Gabriele Pergola"
            ],
            "affiliations": [
                "Department of Computer Science, University of Warwick, UK",
                "Department of Psychology, University of Warwick, UK",
                "School of Computer Science, University of Sheffield, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06652.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#rlhf",
                    "#science",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Интересность диалогов: ключ к эффективному обучению",
                    "desc": "Исследователи представили IntrEx - крупный датасет, аннотированный на предмет интересности в образовательных диалогах. Обученные на этих данных языковые модели лучше предсказывают оценки интересности, чем более крупные проприетарные модели. Анализ выявил влияние лингвистических и когнитивных факторов на вовлеченность учащихся. Результаты подчеркивают важность специализированных данных для моделирования вовлеченности в образовательных контекстах."
                },
                "en": {
                    "title": "Unlocking Engagement: Fine-Tuning LLMs for Interesting Educational Conversations",
                    "desc": "The paper introduces IntrEx, a novel dataset designed to assess interestingness in educational conversations, particularly in teacher-student interactions. It demonstrates that fine-tuned large language models (LLMs) can more accurately predict human judgments of interestingness compared to larger proprietary models. The study emphasizes the importance of linguistic features, such as concreteness and comprehensibility, in maintaining learner engagement during dialogues. By utilizing a rigorous annotation process and a comparison-based rating approach, the research highlights how specialized datasets can enhance the modeling of engagement in educational contexts."
                },
                "zh": {
                    "title": "趣味性驱动学习者参与的关键",
                    "desc": "IntrEx是一个大型数据集，专门标注了教育对话中的趣味性，旨在研究语言和认知因素如何影响学习者的参与感。研究表明，经过微调的大型语言模型（LLMs）在预测人类对趣味性的判断方面，表现优于更大的专有模型，如GPT-4o。这一发现强调了专门数据集在教育环境中建模参与感的潜力。通过分析语言特征，如具体性和可理解性，研究揭示了这些因素如何影响教育对话中的学习者兴趣。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10441",
            "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
            "url": "https://huggingface.co/papers/2509.10441",
            "abstract": "InfGen, a one-step generator replacing the VAE decoder, enables arbitrary high-resolution image generation from a fixed-size latent, significantly reducing computational complexity and generation time.  \t\t\t\t\tAI-generated summary \t\t\t\t Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the InfGen, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
            "score": 18,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "c6a96819cd15917d",
            "authors": [
                "Tao Han",
                "Wanghan Xu",
                "Junchao Gong",
                "Xiaoyu Yue",
                "Song Guo",
                "Luping Zhou",
                "Lei Bai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10441.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "InfGen: мгновенная генерация изображений любого разрешения",
                    "desc": "InfGen - это новый генератор изображений, заменяющий декодер VAE в моделях латентной диффузии. Он позволяет создавать изображения произвольного высокого разрешения из латентного представления фиксированного размера. InfGen значительно снижает вычислительную сложность и время генерации по сравнению с обычными диффузионными моделями. Эксперименты показывают, что InfGen способен улучшить многие модели для создания изображений сверхвысокого разрешения, сократив время генерации 4K-изображений до менее 10 секунд."
                },
                "en": {
                    "title": "Revolutionizing High-Resolution Image Generation with InfGen",
                    "desc": "InfGen is a novel one-step generator that replaces the traditional VAE decoder, allowing for high-resolution image generation from a fixed-size latent representation. This approach significantly reduces the computational complexity and generation time associated with creating images, particularly at 4K resolution. By utilizing a compact generated latent from diffusion models, InfGen enables arbitrary resolution outputs without the need for retraining existing models. Experiments demonstrate that InfGen can enhance various models, achieving 4K image generation in under 10 seconds, thus streamlining the image generation process."
                },
                "zh": {
                    "title": "InfGen：高效生成任意分辨率图像的创新解决方案",
                    "desc": "InfGen是一种新型生成器，取代了变分自编码器（VAE）的解码器，能够从固定大小的潜在空间生成任意高分辨率的图像。这种方法显著降低了计算复杂性和生成时间，使得生成4K图像的时间缩短到10秒以内。通过将扩散模型生成的固定潜在视为内容表示，InfGen能够在不重新训练扩散模型的情况下，解码任意分辨率的图像。实验表明，InfGen可以提升多种模型的性能，推动高分辨率图像生成的进程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09677",
            "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
            "url": "https://huggingface.co/papers/2509.09677",
            "abstract": "Scaling large language models improves their ability to execute longer tasks by isolating execution capability and mitigating self-conditioning effects, despite diminishing single-step accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
            "score": 18,
            "issue_id": 5883,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "03dddc4470e66eb9",
            "authors": [
                "Akshit Sinha",
                "Arvindh Arun",
                "Shashwat Goel",
                "Steffen Staab",
                "Jonas Geiping"
            ],
            "affiliations": [
                "ELLIS Institute Tübingen",
                "Institute for AI, University of Stuttgart",
                "Max Planck Institute for Intelligent Systems",
                "Tübingen AI Center",
                "University of Cambridge",
                "University of Southampton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09677.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Масштабирование LLM: путь к длительным задачам",
                    "desc": "Данная статья исследует влияние масштабирования больших языковых моделей (LLM) на их способность выполнять более длительные задачи. Авторы обнаружили, что увеличение размера модели может экспоненциально улучшить длину успешно выполняемых задач, несмотря на уменьшение точности отдельных шагов. Исследование показало, что ошибки в выполнении длительных задач связаны с проблемами в исполнении, а не с неспособностью рассуждать. Кроме того, было выявлено явление самообусловливания, когда модели становятся более склонны к ошибкам при наличии их предыдущих ошибок в контексте."
                },
                "en": {
                    "title": "Scaling Models for Better Long-Task Execution",
                    "desc": "This paper explores how scaling large language models (LLMs) enhances their performance on longer tasks, despite a decrease in accuracy for individual steps. The authors argue that the challenges LLMs face with extended tasks stem from execution errors rather than reasoning limitations. They propose a method to isolate execution capability by providing explicit knowledge and planning for long-horizon tasks. The findings suggest that larger models can handle more complex tasks effectively, even when smaller models achieve perfect accuracy on single-step tasks, highlighting the importance of model size and compute resources for task execution."
                },
                "zh": {
                    "title": "扩大模型规模，提升长任务执行能力",
                    "desc": "本论文探讨了大型语言模型（LLMs）在执行长任务时的能力提升，尽管单步准确率可能下降。我们发现，单步准确率的微小提升可以在任务长度上带来指数级的改善。我们提出通过明确提供解决长时间任务所需的知识和计划来隔离执行能力。研究表明，尽管小模型在单步任务中表现完美，但大型模型在执行多轮任务时的表现显著更好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08643",
            "title": "X-Part: high fidelity and structure coherent shape decomposition",
            "url": "https://huggingface.co/papers/2509.08643",
            "abstract": "X-Part is a generative model that decomposes 3D objects into semantically meaningful parts with high fidelity, using bounding boxes and point-wise semantic features, and supports interactive editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.",
            "score": 18,
            "issue_id": 5887,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "39ccecef3b91dc4b",
            "authors": [
                "Xinhao Yan",
                "Jiachen Xu",
                "Yang Li",
                "Changfeng Ma",
                "Yunhan Yang",
                "Chunshi Wang",
                "Zibo Zhao",
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Zhuo Chen",
                "Chunchao Guo"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "NJU",
                "ShanghaiTech",
                "Tencent Hunyuan",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08643.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Умная декомпозиция 3D-объектов на редактируемые части",
                    "desc": "X-Part - это генеративная модель для декомпозиции 3D-объектов на семантически значимые части с высокой точностью. Модель использует ограничивающие рамки и поточечные семантические признаки для генерации частей объекта. X-Part обеспечивает возможность интерактивного редактирования сгенерированных частей. Эксперименты показывают, что X-Part достигает передовых результатов в генерации форм на уровне отдельных частей."
                },
                "en": {
                    "title": "Revolutionizing 3D Object Generation with X-Part",
                    "desc": "X-Part is a generative model that effectively breaks down 3D objects into meaningful parts while maintaining high geometric fidelity. It utilizes bounding boxes as prompts and incorporates point-wise semantic features to ensure that the parts are both semantically relevant and structurally coherent. This model allows for interactive editing, making it easier for users to manipulate and generate 3D shapes at the part level. The results demonstrate that X-Part outperforms existing methods, paving the way for improved applications in areas like mesh retopology and 3D printing."
                },
                "zh": {
                    "title": "X-Part：可控的3D物体分解与编辑",
                    "desc": "X-Part是一种生成模型，能够将3D物体分解为具有语义意义的部分，并保持高保真度。它利用边界框和逐点语义特征来支持可控的部分生成。X-Part还设计了一个可编辑的管道，允许用户进行交互式编辑。实验结果表明，X-Part在部分级形状生成方面达到了最先进的性能，开创了创建可编辑和结构合理的3D资产的新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09713",
            "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented\n  Generation for Multi-hop Question Answering",
            "url": "https://huggingface.co/papers/2509.09713",
            "abstract": "HANRAG, a heuristic-based framework, improves question-answering systems by efficiently handling multi-hop queries and reducing noise through query decomposition and filtering.  \t\t\t\t\tAI-generated summary \t\t\t\t The Retrieval-Augmented Generation (RAG) approach enhances question-answering systems and dialogue generation tasks by integrating information retrieval (IR) technologies with large language models (LLMs). This strategy, which retrieves information from external knowledge bases to bolster the response capabilities of generative models, has achieved certain successes. However, current RAG methods still face numerous challenges when dealing with multi-hop queries. For instance, some approaches overly rely on iterative retrieval, wasting too many retrieval steps on compound queries. Additionally, using the original complex query for retrieval may fail to capture content relevant to specific sub-queries, resulting in noisy retrieved content. If the noise is not managed, it can lead to the problem of noise accumulation. To address these issues, we introduce HANRAG, a novel heuristic-based framework designed to efficiently tackle problems of varying complexity. Driven by a powerful revelator, HANRAG routes queries, decomposes them into sub-queries, and filters noise from retrieved documents. This enhances the system's adaptability and noise resistance, making it highly capable of handling diverse queries. We compare the proposed framework against other leading industry methods across various benchmarks. The results demonstrate that our framework obtains superior performance in both single-hop and multi-hop question-answering tasks.",
            "score": 13,
            "issue_id": 5885,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "a0f4a95527c1fb76",
            "authors": [
                "Duolin Sun",
                "Dan Yang",
                "Yue Shen",
                "Yihan Jiao",
                "Zhehao Tan",
                "Jie Feng",
                "Lianzhen Zhong",
                "Jian Wang",
                "Peng Wei",
                "Jinjie Gu"
            ],
            "affiliations": [
                "Ant Group, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09713.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "HANRAG: Умное решение для сложных вопросов",
                    "desc": "HANRAG - это новая эвристическая система, улучшающая работу вопросно-ответных систем. Она эффективно обрабатывает многоэтапные запросы путем их декомпозиции и фильтрации шума. HANRAG использует мощный механизм для маршрутизации запросов, их разбиения на подзапросы и фильтрации шума из полученных документов. Система показывает превосходные результаты как в простых, так и в многоэтапных задачах вопросно-ответного поиска по сравнению с другими ведущими методами."
                },
                "en": {
                    "title": "HANRAG: Smart Query Handling for Better Answers",
                    "desc": "HANRAG is a new framework that improves question-answering systems by breaking down complex queries into simpler sub-queries. It uses a heuristic approach to filter out irrelevant information, reducing noise in the retrieval process. This method enhances the system's ability to handle multi-hop queries, which require information from multiple sources. By comparing HANRAG with existing methods, the results show it performs better in answering both single-hop and multi-hop questions."
                },
                "zh": {
                    "title": "HANRAG：提升问答系统的智能框架",
                    "desc": "HANRAG是一个基于启发式的方法框架，旨在提高问答系统的性能，特别是在处理多跳查询时。它通过查询分解和过滤来有效减少噪声，从而提升系统的适应性和抗噪声能力。该框架利用强大的揭示器，将复杂查询分解为子查询，并从检索的文档中去除无关内容。与其他主流方法相比，HANRAG在单跳和多跳问答任务中表现出更优越的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09716",
            "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions",
            "url": "https://huggingface.co/papers/2509.09716",
            "abstract": "Voice Style Adaptation (VSA) evaluates the ability of spoken language models to modify their speaking style based on spoken instructions, using a bilingual benchmark and a Large Audio Language Model as a Judge framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at https://junzhan2000.github.io/VStyle.github.io/{project's homepage}.",
            "score": 9,
            "issue_id": 5883,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "a1fdfdb8fb8b7486",
            "authors": [
                "Jun Zhan",
                "Mingyang Han",
                "Yuxuan Xie",
                "Chen Wang",
                "Dong Zhang",
                "Kexin Huang",
                "Haoxiang Shi",
                "DongXiao Wang",
                "Tengtao Song",
                "Qinyuan Cheng",
                "Shimin Li",
                "Jun Song",
                "Xipeng Qiu",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09716.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#audio",
                    "#dataset",
                    "#alignment",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Новый рубеж в разговорном ИИ: адаптация стиля речи по голосовым командам",
                    "desc": "Статья представляет новую задачу под названием Voice Style Adaptation (VSA), которая оценивает способность разговорных языковых моделей адаптировать свой стиль речи на основе устных инструкций. Авторы создали двуязычный бенчмарк VStyle для изучения этой задачи, охватывающий четыре категории генерации речи. Они также предложили фреймворк Large Audio Language Model as a Judge для объективной оценки результатов. Эксперименты показали, что существующие модели имеют ограничения в контролируемой адаптации стиля речи."
                },
                "en": {
                    "title": "Transforming Speech: Adapting Style with Voice Commands",
                    "desc": "Voice Style Adaptation (VSA) is a new task that tests how well spoken language models (SLMs) can change their speaking style based on spoken commands. This includes adjusting aspects like tone, rhythm, and character portrayal. The study introduces a bilingual benchmark called VStyle, which evaluates SLMs on various speech generation categories, including acoustic features and empathy. The research highlights the limitations of current models in adapting their style, aiming to improve human-machine interaction through better control of speech characteristics."
                },
                "zh": {
                    "title": "语音风格适应：让机器更懂人类的说话风格",
                    "desc": "语音风格适应（VSA）是一项新任务，旨在评估口语模型根据口头指令调整说话风格的能力。我们提出了VStyle，这是一个双语基准，涵盖了声学属性、自然语言指令、角色扮演和隐性共情等四个类别的语音生成。通过引入大型音频语言模型作为评估框架，我们能够客观地评估模型在文本忠实性、风格遵循和自然性方面的表现。实验结果表明，当前模型在可控风格适应方面存在明显局限，突显了这一任务的新颖性和挑战性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10147",
            "title": "Virtual Agent Economies",
            "url": "https://huggingface.co/papers/2509.10147",
            "abstract": "The sandbox economy framework analyzes the emerging AI agent economy, focusing on its origins and permeability, and discusses design choices for safe and steerable AI markets.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the \"sandbox economy\" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI \"mission economies\" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing.",
            "score": 8,
            "issue_id": 5884,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "2fd9f178e62d64b0",
            "authors": [
                "Nenad Tomasev",
                "Matija Franklin",
                "Joel Z. Leibo",
                "Julian Jacobs",
                "William A. Cunningham",
                "Iason Gabriel",
                "Simon Osindero"
            ],
            "affiliations": [
                "Google DeepMind",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10147.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Проектирование безопасной экономики ИИ-агентов для общего блага",
                    "desc": "Статья представляет концепцию 'экономики песочницы' для анализа зарождающейся экономики ИИ-агентов. Авторы рассматривают два ключевых аспекта: происхождение (спонтанное или намеренное) и степень отделенности от человеческой экономики. Обсуждаются возможности и риски, связанные с развитием экономики ИИ-агентов, включая системные экономические риски и усиление неравенства. Предлагаются механизмы для создания безопасных и управляемых рынков ИИ-агентов, такие как аукционы для справедливого распределения ресурсов и 'миссионерские экономики' для достижения коллективных целей."
                },
                "en": {
                    "title": "Navigating the Future: Designing Safe AI Agent Economies",
                    "desc": "The paper introduces the 'sandbox economy' framework to analyze the new economic landscape created by autonomous AI agents. It highlights two main aspects: the origins of these agents, whether they emerge spontaneously or are intentionally designed, and their connection to the existing human economy, which can be either permeable or impermeable. The authors emphasize the potential benefits of this AI agent economy, such as enhanced coordination, while also warning of risks like economic instability and inequality. They propose design strategies, including auction mechanisms and mission-oriented AI systems, to create safe and effective markets for AI agents that align with human values."
                },
                "zh": {
                    "title": "设计可控的AI代理市场，迎接经济新机遇",
                    "desc": "这篇论文分析了新兴的人工智能代理经济，提出了“沙盒经济”框架来理解这一系统。它主要关注人工智能代理的起源和与人类经济的关系，探讨了安全可控的AI市场设计选择。研究表明，AI代理经济的自发出现可能带来前所未有的协调机会，但也伴随系统性经济风险和不平等加剧的挑战。作者建议通过拍卖机制和AI“使命经济”的设计，确保资源分配的公平性和信任、安全、问责的社会技术基础设施。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04996",
            "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies",
            "url": "https://huggingface.co/papers/2509.04996",
            "abstract": "FLOWER, a 950 M-parameter VLA policy, achieves competitive performance with reduced computational costs through intermediate-modality fusion and action-specific Global-AdaLN conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to 50% of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by 20% through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance with bigger VLAs across 190 tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. In addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark. Demos, code and pretrained weights are available at https://intuitive-robots.github.io/flower_vla/.",
            "score": 7,
            "issue_id": 5889,
            "pub_date": "2025-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "06fa195b98a0804d",
            "authors": [
                "Moritz Reuss",
                "Hongyi Zhou",
                "Marcel Rühle",
                "Ömer Erdinç Yağmurlu",
                "Fabian Otto",
                "Rudolf Lioutikov"
            ],
            "affiliations": [
                "Intuitive Robots Lab, Karlsruhe Institute of Technology, Germany",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04996.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#training",
                    "#robotics",
                    "#optimization",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "🌸",
                "ru": {
                    "title": "FLOWER: Эффективная VLA-политика для интуитивной робототехники",
                    "desc": "FLOWER - это эффективная политика зрения-языка-действия (VLA) с 950 миллионами параметров. Она использует промежуточное слияние модальностей и специфическое для действий Global-AdaLN кондиционирование для сокращения вычислительных затрат. FLOWER демонстрирует конкурентоспособную производительность на 190 задачах в десяти симуляционных и реальных эталонных тестах. Модель достигает нового рекорда на бенчмарке CALVIN ABC, показывая устойчивость для различных роботизированных воплощений."
                },
                "en": {
                    "title": "Efficient Robotics with FLOWER: A 950M-Parameter VLA Policy",
                    "desc": "The paper presents FLOWER, a Vision-Language-Action (VLA) policy with 950 million parameters that enhances efficiency in robotics applications. It introduces intermediate-modality fusion to optimize model capacity by reducing the number of layers in large language models (LLMs) by up to 50%. Additionally, it employs action-specific Global-AdaLN conditioning, which decreases the parameter count by 20% through modular adaptation. FLOWER achieves competitive performance across 190 tasks while significantly reducing computational costs compared to existing multi-billion-parameter models."
                },
                "zh": {
                    "title": "FLOWER：高效的视觉-语言-动作策略",
                    "desc": "本文介绍了一种名为FLOWER的950M参数的视觉-语言-动作（VLA）策略，旨在降低计算成本并提高效率。通过中间模态融合和特定动作的全局自适应层归一化（Global-AdaLN）条件，FLOWER在保持竞争性能的同时，减少了模型的参数量。该模型在仅200小时的H100 GPU训练后，能够在190个任务中表现出色，并在CALVIN ABC基准测试中达到了新的最优状态。FLOWER的设计使其在多种机器人平台上都表现出良好的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10396",
            "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
            "url": "https://huggingface.co/papers/2509.10396",
            "abstract": "IGPO, an RL framework utilizing inpainting in masked diffusion large language models, enhances sample efficiency and achieves state-of-the-art results in mathematical benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.",
            "score": 6,
            "issue_id": 5886,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "aeadbfb97e57966c",
            "authors": [
                "Siyan Zhao",
                "Mengchen Liu",
                "Jing Huang",
                "Miao Liu",
                "Chenyu Wang",
                "Bo Liu",
                "Yuandong Tian",
                "Guan Pang",
                "Sean Bell",
                "Aditya Grover",
                "Feiyu Chen"
            ],
            "affiliations": [
                "MIT",
                "Meta Superintelligence Labs",
                "Tsinghua University, College of AI",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10396.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#diffusion",
                    "#synthetic",
                    "#games",
                    "#rl",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Инпейнтинг направляет исследование в обучении с подкреплением языковых моделей",
                    "desc": "IGPO - это новая система обучения с подкреплением для маскированных диффузионных языковых моделей. Она использует возможность инпейнтинга для улучшения исследования пространства решений. IGPO стратегически вставляет частичные правильные рассуждения во время онлайн-сэмплирования, что повышает эффективность обучения. Система достигает новых рекордных результатов на математических бенчмарках."
                },
                "en": {
                    "title": "Enhancing Reinforcement Learning with Inpainting in dLLMs",
                    "desc": "This paper introduces IGPO, a reinforcement learning (RL) framework that leverages inpainting techniques in masked diffusion large language models (dLLMs) to improve sample efficiency. By using inpainting, IGPO helps guide the exploration process in RL, addressing the challenge of sparse rewards and inefficient sampling. The framework strategically incorporates partial ground-truth reasoning during online sampling, which enhances the model's ability to discover effective solutions. The results demonstrate that IGPO achieves state-of-the-art performance on mathematical benchmarks, showcasing the potential of combining inpainting with RL in dLLMs."
                },
                "zh": {
                    "title": "利用图像修复提升强化学习效率的IGPO框架",
                    "desc": "IGPO是一种强化学习框架，利用掩蔽扩散大语言模型中的图像修复技术，提升样本效率并在数学基准测试中取得了最先进的结果。掩蔽扩散大语言模型（dLLMs）通过图像修复能力为强化学习算法设计提供了新的思路。IGPO通过在在线采样过程中插入部分真实推理轨迹，指导探索过程，避免了模型在寻找正确解决方案时的样本浪费。该方法在群体优化方法中应用，恢复了有意义的梯度，同时提高了样本效率，最终在多个数学基准测试中取得了显著的提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09995",
            "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
            "url": "https://huggingface.co/papers/2509.09995",
            "abstract": "QuantAgent, a multi-agent LLM framework, excels in high-frequency trading by leveraging specialized agents for technical indicators, chart patterns, trends, and risk, outperforming existing neural and rule-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets.",
            "score": 4,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "5964ddeaa46fcc92",
            "authors": [
                "Fei Xiong",
                "Xiang Zhang",
                "Aosong Feng",
                "Siqi Sun",
                "Chenyu You"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Stony Brook University",
                "University of British Columbia",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09995.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#science",
                    "#multimodal",
                    "#agents",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "QuantAgent: Революция в высокочастотной торговле с помощью мультиагентных языковых моделей",
                    "desc": "QuantAgent - это инновационная мультиагентная система на основе больших языковых моделей (LLM), разработанная специально для высокочастотной торговли. Система использует четыре специализированных агента: Indicator, Pattern, Trend и Risk, каждый из которых оснащен специфическими инструментами и возможностями структурированного рассуждения для анализа различных аспектов рыночной динамики в коротких временных окнах. В ходе оценки на десяти финансовых инструментах QuantAgent продемонстрировал превосходную производительность как в точности прогнозирования, так и в кумулятивной доходности за 4-часовые торговые интервалы, превзойдя сильные нейронные и основанные на правилах базовые модели. Результаты исследования показывают, что сочетание структурированных финансовых приоров с языковым рассуждением открывает новые возможности для создания прослеживаемых систем принятия решений в реальном времени на высокочастотных финансовых рынках."
                },
                "en": {
                    "title": "Revolutionizing High-Frequency Trading with Specialized Agents",
                    "desc": "QuantAgent is a multi-agent framework designed for high-frequency trading (HFT) that utilizes specialized agents to analyze technical indicators, chart patterns, trends, and risk. Unlike traditional large language models (LLMs) that focus on long-term investment strategies, QuantAgent is tailored for rapid decision-making in fast-paced trading environments. Each agent within the framework is equipped with specific tools to effectively interpret short-term market signals. In tests, QuantAgent outperformed existing neural and rule-based systems, demonstrating its effectiveness in achieving higher predictive accuracy and returns in HFT scenarios."
                },
                "zh": {
                    "title": "QuantAgent：高频交易的智能决策新工具",
                    "desc": "QuantAgent 是一个多智能体大语言模型框架，专门为高频交易设计。它通过四个专业代理（技术指标、图表模式、趋势和风险）来处理市场动态，能够快速做出基于短期信号的决策。与传统的金融大语言模型不同，QuantAgent 更加注重快速、精准的交易需求。实验结果显示，QuantAgent 在预测准确性和累计收益方面优于现有的神经网络和规则基础系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09926",
            "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised\n  Learning in Open-World Scenarios",
            "url": "https://huggingface.co/papers/2509.09926",
            "abstract": "LoFT, a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning, improves reliability of pseudolabels and discriminative ability in open-world scenarios, outperforming previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\\% of the unlabeled data compared with previous works.",
            "score": 4,
            "issue_id": 5884,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "e85c5f480d51fb5c",
            "authors": [
                "Jiahao Chen",
                "Zhiyuan Huang",
                "Yurou Liu",
                "Bing Su"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09926.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "🦚",
                "ru": {
                    "title": "Эффективная тонкая настройка для обучения с длинным хвостом",
                    "desc": "Статья представляет LoFT - новый фреймворк для эффективной тонкой настройки моделей машинного обучения в условиях несбалансированного полу-контролируемого обучения с длинным хвостом. LoFT улучшает качество псевдо-меток и способность к различению в сценариях открытого мира. Авторы также предлагают расширение LoFT-OW для работы с данными вне распределения. Эксперименты показывают превосходство LoFT над существующими методами даже при использовании только 1% немеченых данных."
                },
                "en": {
                    "title": "Enhancing Long-Tailed Learning with LoFT: Fine-Tuning for Better Pseudolabels",
                    "desc": "LoFT is a new framework designed for long-tailed semi-supervised learning that enhances the quality of pseudolabels and improves model performance in open-world scenarios. It builds on the foundation model fine-tuning approach, allowing for better utilization of unlabeled data alongside imbalanced labeled datasets. By addressing issues like overconfidence and low-quality pseudolabels, LoFT enables more reliable learning outcomes. The framework also includes a variant, LoFT-OW, which specifically tackles challenges posed by out-of-distribution samples, demonstrating superior results on various benchmarks with minimal unlabeled data usage."
                },
                "zh": {
                    "title": "LoFT：提升长尾半监督学习的可靠性与区分能力",
                    "desc": "LoFT是一种高效的参数微调框架，专为长尾半监督学习设计，旨在提高伪标签的可靠性和在开放世界场景中的区分能力。该方法通过将大量未标记数据与不平衡的标记数据集结合，克服了传统方法中常见的过度自信和低质量伪标签的问题。LoFT在基础模型微调的基础上进行扩展，能够生成更可靠的伪标签，从而促进不平衡学习的效果。实验结果表明，LoFT在多个基准测试中表现优于以往方法，即使只使用1%的未标记数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10058",
            "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
            "url": "https://huggingface.co/papers/2509.10058",
            "abstract": "A training-free framework uses a large language model to disambiguate color terms and refine text embeddings for improved color accuracy in text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.",
            "score": 3,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "e647125383aba6a5",
            "authors": [
                "Sung-Lin Tsai",
                "Bo-Lun Huang",
                "Yu Ting Shen",
                "Cheng Yu Yeo",
                "Chiang Tseng",
                "Bo-Kai Ruan",
                "Wen-Sheng Lien",
                "Hong-Han Shuai"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University Hsinchu, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10058.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точные цвета в генерации изображений без дообучения",
                    "desc": "Предложена система, использующая большую языковую модель для уточнения цветовых терминов в запросах для генерации изображений по тексту. Метод улучшает точность цветопередачи путем доработки текстовых эмбеддингов на основе пространственных отношений цветов в цветовом пространстве CIELAB. В отличие от существующих подходов, данный метод не требует дополнительного обучения или использования референсных изображений. Эксперименты показали, что система повышает соответствие цветов без ущерба для качества генерируемых изображений."
                },
                "en": {
                    "title": "Enhancing Color Accuracy in T2I with a Training-Free Framework",
                    "desc": "This paper presents a novel training-free framework that enhances color accuracy in text-to-image (T2I) generation by utilizing a large language model (LLM). The framework addresses the challenge of ambiguous color terms, which often lead to misaligned images in applications like fashion and product visualization. By disambiguating color prompts and refining text embeddings in the CIELAB color space, the method improves the fidelity of generated colors without the need for additional training or reference images. Experimental results show that this approach successfully aligns colors with human intent while maintaining high image quality."
                },
                "zh": {
                    "title": "无训练框架提升文本到图像生成中的颜色准确性",
                    "desc": "本文提出了一种无训练框架，利用大型语言模型（LLM）来消歧义颜色术语，并优化文本嵌入，以提高文本到图像生成中的颜色准确性。当前的扩散模型在处理复杂的颜色描述时表现不佳，常常导致生成的图像与人类意图不符。我们的方法通过解析文本提示中的模糊颜色术语，并在CIELAB颜色空间中根据颜色术语的空间关系来细化文本嵌入，从而实现更精确的颜色渲染。实验结果表明，该框架在不影响图像质量的情况下，显著改善了颜色对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09734",
            "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with\n  MCP-Mediated Tools",
            "url": "https://huggingface.co/papers/2509.09734",
            "abstract": "MCP-AgentBench is a benchmark designed to evaluate language agents in MCP-mediated tool interactions, providing a standardized framework for assessing real-world performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Model Context Protocol (MCP) is rapidly emerging as a pivotal open standard, designed to enhance agent-tool integration and interoperability, and is positioned to unlock a new era of powerful, interconnected, and genuinely utilitarian agentic AI. However, despite MCP's growing adoption, existing benchmarks often fail to capture real-world agent performance within this new paradigm, leading to a distorted perception of their true operational value and an inability to reliably differentiate proficiencies. To bridge this critical evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark specifically engineered to rigorously assess language agent capabilities in MCP-mediated tool interactions. Core contributions of MCP-AgentBench include: the establishment of a robust MCP testbed comprising 33 operational servers with 188 distinct tools; the development of a benchmark featuring 600 systematically designed queries distributed across 6 distinct categories of varying interaction complexity; and the introduction of MCP-Eval, a novel outcome-oriented evaluation methodology prioritizing real-world task success. Through extensive empirical evaluation of leading language agents, we provide foundational insights. MCP-AgentBench aims to equip the research community with a standardized and reliable framework to build, validate, and advance agents capable of fully leveraging MCP's transformative benefits, thereby accelerating progress toward truly capable and interoperable AI systems.",
            "score": 3,
            "issue_id": 5883,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "56883c4d7c401e99",
            "authors": [
                "Zikang Guo",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Wentao Hong",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology, Beijing, China",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09734.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый стандарт оценки ИИ-агентов в реальном мире",
                    "desc": "MCP-AgentBench - это новый бенчмарк для оценки языковых агентов в контексте взаимодействия с инструментами через протокол MCP. Он включает в себя тестовую среду с 33 серверами и 188 инструментами, а также 600 запросов разной сложности. Бенчмарк вводит новую методологию оценки MCP-Eval, ориентированную на успешность выполнения реальных задач. MCP-AgentBench призван обеспечить стандартизированную основу для разработки и валидации агентов, способных полноценно использовать преимущества MCP."
                },
                "en": {
                    "title": "Empowering Language Agents with MCP-AgentBench",
                    "desc": "MCP-AgentBench is a benchmark created to evaluate language agents that interact with tools using the Model Context Protocol (MCP). It addresses the shortcomings of existing benchmarks by providing a standardized framework that reflects real-world performance. The benchmark includes a testbed with 33 servers and 188 tools, along with 600 queries across various complexity levels. By introducing a new evaluation method focused on task success, MCP-AgentBench aims to enhance the development of more effective and interconnected AI systems."
                },
                "zh": {
                    "title": "MCP-AgentBench：评估语言代理的新标准",
                    "desc": "MCP-AgentBench是一个基准测试，旨在评估语言代理在MCP介导的工具交互中的表现。它提供了一个标准化的框架，以便更准确地评估代理在现实世界中的能力。该基准包括33个操作服务器和188种不同工具，设计了600个系统化的查询，涵盖6种不同复杂度的交互类别。通过这种方式，MCP-AgentBench帮助研究人员更好地理解和提升代理的性能，推动智能代理的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07966",
            "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
            "url": "https://huggingface.co/papers/2509.07966",
            "abstract": "Visual-TableQA is a large-scale, open-domain dataset for evaluating visual reasoning over complex tabular data, generated using a modular pipeline involving multiple reasoning LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.",
            "score": 3,
            "issue_id": 5900,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "afb635640cc1ffec",
            "authors": [
                "Boammani Aser Lompo",
                "Marc Haraoui"
            ],
            "affiliations": [
                "École de Technologie Supérieure Montreal, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07966.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#dataset",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Коллаборативные LLM создают датасет для визуального анализа таблиц",
                    "desc": "Visual-TableQA - это масштабный набор данных для оценки визуального анализа сложных табличных данных. Он был создан с использованием модульного конвейера, включающего несколько рассуждающих языковых моделей (LLM). Датасет содержит 2500 структурированных таблиц, отрендеренных в LaTeX, и 6000 пар вопросов-ответов, требующих глубокого рассуждения. Генерация данных осуществлялась путем коллаборативной работы нескольких моделей, выполняющих различные роли: генерацию, валидацию и вдохновение."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Visual-TableQA",
                    "desc": "Visual-TableQA is a new dataset designed to test how well machine learning models can understand and reason about complex tables. It includes 2,500 LaTeX-rendered tables and 6,000 question-answer pairs, created using a collaborative approach with multiple reasoning language models (LLMs). This dataset aims to improve the evaluation of visual reasoning in vision-language models by providing a diverse and rich set of examples. The results show that models trained on Visual-TableQA perform better on other benchmarks, demonstrating its effectiveness despite being generated synthetically."
                },
                "zh": {
                    "title": "视觉推理的新突破：Visual-TableQA",
                    "desc": "Visual-TableQA是一个大规模的开放领域数据集，旨在评估对复杂表格数据的视觉推理能力。该数据集通过一个模块化的生成管道创建，涉及多个推理大型语言模型（LLMs），以实现生成、验证和灵感激发等不同角色的协作。数据集中包含2500个丰富结构的LaTeX渲染表格和6000个推理密集的问答对，生成成本低于100美元。实证结果表明，基于Visual-TableQA微调的模型在外部基准测试中表现出色，超越了多个专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09524",
            "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning\n  and Label Distribution Learning",
            "url": "https://huggingface.co/papers/2509.09524",
            "abstract": "DeMeVa explores in-context learning and label distribution learning for predicting annotator-specific annotations and generating soft labels, demonstrating competitive performance and potential for further research.  \t\t\t\t\tAI-generated summary \t\t\t\t This system paper presents the DeMeVa team's approaches to the third edition of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et al., 2025). We explore two directions: in-context learning (ICL) with large language models, where we compare example sampling strategies; and label distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we evaluate several fine-tuning methods. Our contributions are twofold: (1) we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance; and (2) we argue that LDL methods are promising for soft label predictions and merit further exploration by the perspectivist community.",
            "score": 2,
            "issue_id": 5895,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "8eea111352153d33",
            "authors": [
                "Daniil Ignatev",
                "Nan Li",
                "Hugh Mee Wong",
                "Anh Dang",
                "Shane Kaszefski Yaschuk"
            ],
            "affiliations": [
                "Utrecht University, Utrecht, The Netherlands"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09524.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🏷️",
                "ru": {
                    "title": "Новые подходы к обучению с разногласиями: от контекста к распределению меток",
                    "desc": "Статья представляет подходы команды DeMeVa к задаче обучения с разногласиями (LeWiDi 2025). Исследуются два направления: обучение в контексте (ICL) с использованием больших языковых моделей и обучение распределению меток (LDL) с помощью RoBERTa. Показано, что ICL эффективно предсказывает аннотации, специфичные для разных аннотаторов, а агрегация этих предсказаний дает конкурентоспособные мягкие метки. Авторы утверждают, что методы LDL перспективны для предсказания мягких меток и заслуживают дальнейшего изучения."
                },
                "en": {
                    "title": "Harnessing ICL and LDL for Enhanced Annotation Predictions",
                    "desc": "The DeMeVa paper investigates two innovative approaches in machine learning: in-context learning (ICL) and label distribution learning (LDL). ICL utilizes large language models to predict specific annotations from different annotators by comparing various example sampling strategies. Additionally, the paper demonstrates that LDL methods, particularly with RoBERTa, can effectively generate soft labels through fine-tuning techniques. The findings suggest that both ICL and LDL hold significant potential for improving annotation processes and warrant further research in the field."
                },
                "zh": {
                    "title": "探索上下文学习与标签分布学习的潜力",
                    "desc": "DeMeVa研究了上下文学习和标签分布学习，以预测特定注释者的注释并生成软标签。我们比较了不同的示例采样策略，并评估了多种微调方法。研究表明，上下文学习能够有效预测注释者特定的注释，并将这些预测聚合成软标签，表现出竞争力。标签分布学习方法在软标签预测方面也显示出潜力，值得进一步研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01535",
            "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge\n  into Large Language Models",
            "url": "https://huggingface.co/papers/2509.01535",
            "abstract": "Causal Attention Tuning (CAT) enhances Large Language Models (LLMs) by injecting causal knowledge into the attention mechanism, improving prediction accuracy and robustness in out-of-distribution scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. Implementation details can be found at https://github.com/Kairong-Han/CAT.",
            "score": 2,
            "issue_id": 5897,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "915b9e9dacb125e5",
            "authors": [
                "Kairong Han",
                "Wenshuo Zhao",
                "Ziyu Zhao",
                "JunJian Ye",
                "Lujia Pan",
                "Kun Kuang"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "Noahs Ark Lab, Huawei Technologies"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01535.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Причинно-следственное внимание для усиления языковых моделей",
                    "desc": "Статья представляет новый метод под названием Causal Attention Tuning (CAT) для улучшения больших языковых моделей (LLM). CAT внедряет причинно-следственные знания в механизм внимания модели, что повышает точность предсказаний и устойчивость в сценариях вне распределения обучающих данных. Авторы предлагают автоматизированный конвейер для генерации причинно-следственных сигналов на уровне токенов и вводят механизм Re-Attention для улучшения обучения. Эксперименты показывают эффективность подхода на предложенном эталонном тесте Spurious Token Game и других задачах."
                },
                "en": {
                    "title": "Enhancing LLMs with Causal Knowledge for Better Predictions",
                    "desc": "Causal Attention Tuning (CAT) is a method designed to improve Large Language Models (LLMs) by incorporating causal knowledge into their attention mechanisms. This approach addresses the issue of LLMs often learning misleading correlations instead of genuine causal relationships, which can hinder their performance, especially in unfamiliar situations. CAT uses an automated process to generate causal signals at the token level and employs a Re-Attention mechanism to enhance training, allowing the model to concentrate on relevant causal structures. Experimental results show that CAT significantly boosts prediction accuracy and robustness in out-of-distribution scenarios, making LLMs more reliable."
                },
                "zh": {
                    "title": "因果知识提升语言模型的预测能力",
                    "desc": "因果注意力调优（CAT）通过将因果知识注入到注意力机制中，增强了大型语言模型（LLMs）的性能。研究表明，传统的LLMs在处理大规模数据时，往往捕捉到的是虚假的相关性，而非真实的因果关系，导致在分布外场景中的表现不佳。CAT提出了一种新的方法，通过自动生成令牌级因果信号，并引入再注意力机制，帮助模型关注因果结构，减少注意力分数中的噪声和偏差。实验结果表明，CAT在多个下游任务中有效利用因果知识进行预测，并在分布外场景中保持稳健性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09990",
            "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority\n  Languages in China",
            "url": "https://huggingface.co/papers/2509.09990",
            "abstract": "Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks.",
            "score": 1,
            "issue_id": 5886,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "83ad01064c3c6566",
            "authors": [
                "Guixian Xu",
                "Zeli Su",
                "Ziyin Zhang",
                "Jianing Liu",
                "XU Han",
                "Ting Zhang",
                "Yushuang Dong"
            ],
            "affiliations": [
                "Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE",
                "Minzu University of China",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09990.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#synthetic",
                    "#multilingual",
                    "#machine_translation",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "📰",
                "ru": {
                    "title": "Преодоление языкового барьера: новый датасет для генерации заголовков на языках меньшинств Китая",
                    "desc": "Статья представляет новый набор данных CMHG для генерации заголовков на языках меньшинств Китая: тибетском, уйгурском и монгольском. Датасет содержит 100 000 записей для тибетского и по 50 000 для уйгурского и монгольского языков. Авторы также предлагают тестовый набор, аннотированный носителями языка, в качестве эталона для будущих исследований. Этот ресурс призван способствовать развитию генерации заголовков и созданию соответствующих бенчмарков для языков меньшинств Китая."
                },
                "en": {
                    "title": "Empowering Minority Languages with Tailored Datasets for Headline Generation",
                    "desc": "This paper addresses the challenges faced by minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, particularly in the context of headline generation due to their unique writing systems. The authors introduce a new dataset called Chinese Minority Headline Generation (CMHG), which contains 100,000 entries for Tibetan and 50,000 entries each for Uyghur and Mongolian. This dataset is specifically designed for supervised learning tasks, providing a substantial resource for training models in headline generation. Furthermore, a high-quality test set annotated by native speakers is included to establish benchmarks for future research in this area."
                },
                "zh": {
                    "title": "推动中国少数民族语言标题生成的创新数据集",
                    "desc": "本论文介绍了一个新的数据集，名为中国少数民族标题生成数据集（CMHG），旨在解决藏语、维吾尔语和蒙古语在标题生成任务中的数据匮乏问题。该数据集包含10万个藏语条目，以及各5万个维吾尔语和蒙古语条目，专门用于标题生成。我们还提供了一个由母语者注释的高质量测试集，作为未来研究的基准。希望这个数据集能为中国少数民族语言的标题生成提供有价值的资源，并推动相关基准的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09737",
            "title": "World Modeling with Probabilistic Structure Integration",
            "url": "https://huggingface.co/papers/2509.09737",
            "abstract": "Probabilistic Structure Integration (PSI) learns richly controllable world models from data through probabilistic prediction, structure extraction, and integration, enhancing video prediction and understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful \"intermediate structures\", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.",
            "score": 1,
            "issue_id": 5899,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "c056ea95a829dc6a",
            "authors": [
                "Klemen Kotar",
                "Wanhee Lee",
                "Rahul Venkatesh",
                "Honglin Chen",
                "Daniel Bear",
                "Jared Watrous",
                "Simon Kim",
                "Khai Loong Aw",
                "Lilian Naing Chen",
                "Stefan Stojanov",
                "Kevin Feigelis",
                "Imran Thobani",
                "Alex Durango",
                "Khaled Jedoui",
                "Atlas Kazemian",
                "Dan Yamins"
            ],
            "affiliations": [
                "Stanford NeuroAI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09737.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#data"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "PSI: Циклическое обучение моделей мира с вероятностной структурой",
                    "desc": "Probabilistic Structure Integration (PSI) - это система для обучения гибко управляемых моделей мира на основе данных. PSI использует трехэтапный цикл: вероятностное предсказание, извлечение структуры и интеграция. Система строит вероятностную графическую модель данных, извлекает низкоразмерные свойства и интегрирует их обратно в процесс обучения. PSI была обучена на 1,4 триллиона токенов видеоданных из интернета и показала высокие результаты в задачах предсказания и понимания видео."
                },
                "en": {
                    "title": "Enhancing World Models with Probabilistic Structure Integration",
                    "desc": "Probabilistic Structure Integration (PSI) is a novel approach for creating advanced world models from data using a three-step process. First, it builds a probabilistic graphical model to predict relationships between variables in the data. Next, it extracts meaningful low-dimensional structures through causal inference, enabling the model to understand complex data patterns. Finally, these structures are integrated back into the training process, enhancing the model's predictive capabilities and allowing for flexible control over its outputs."
                },
                "zh": {
                    "title": "通过概率结构集成提升视频理解能力",
                    "desc": "概率结构集成（PSI）是一种通过概率预测、结构提取和集成来从数据中学习可控的世界模型的系统。它的第一步是构建一个概率图模型，支持描述数据中变量之间依赖关系的条件分布。第二步是通过因果推断提取数据中的低维特征，形成有意义的中间结构。最后一步是将这些结构转化为新的标记类型，持续增强模型的能力，从而实现更好的视频预测和理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08825",
            "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
            "url": "https://huggingface.co/papers/2509.08825",
            "abstract": "LLM hacking introduces significant variability and error in social science research, affecting statistical conclusions and requiring rigorous verification and human annotations to mitigate.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.   We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.   Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.",
            "score": 1,
            "issue_id": 5897,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "4f165e5bca7b20e0",
            "authors": [
                "Joachim Baumann",
                "Paul Röttger",
                "Aleksandra Urman",
                "Albert Wendsjö",
                "Flor Miriam Plaza-del-Arco",
                "Johannes B. Gruber",
                "Dirk Hovy"
            ],
            "affiliations": [
                "Bocconi University",
                "GESIS, Leibniz Institute for the Social Sciences",
                "LIACS, Leiden University",
                "University of Gothenburg",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08825.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#multimodal",
                    "#training",
                    "#ethics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ЛЛМ-хакинг: скрытая угроза достоверности исследований в социальных науках",
                    "desc": "Это исследование посвящено проблеме ЛЛМ-хакинга в социальных науках, где использование больших языковых моделей может привести к систематическим ошибкам и случайным погрешностям в анализе данных. Авторы провели масштабный эксперимент, воспроизведя 37 задач аннотации данных из 21 опубликованного исследования с использованием 18 различных моделей. Результаты показывают, что даже высокоточные модели не устраняют полностью риск ЛЛМ-хакинга, а общепринятые методы коррекции регрессионных оценок малоэффективны. Исследование подчеркивает важность человеческих аннотаций и тщательной проверки результатов, особенно вблизи порогов статистической значимости."
                },
                "en": {
                    "title": "Mitigating LLM Hacking: Ensuring Reliable Social Science Research",
                    "desc": "This paper discusses the concept of 'LLM hacking', which refers to the variability and errors introduced by large language models (LLMs) in social science research. The authors demonstrate that different choices made by researchers, such as model selection and prompting strategies, can lead to significant biases and errors in statistical conclusions. Through extensive analysis of 37 data annotation tasks, they find that incorrect conclusions arise in a substantial number of hypotheses, highlighting the need for rigorous verification and human annotations to mitigate these risks. The study also reveals that while higher-performing models reduce the risk of LLM hacking, they do not eliminate it entirely, emphasizing the importance of careful model selection and validation in research."
                },
                "zh": {
                    "title": "LLM黑客行为：社会科学研究中的隐患",
                    "desc": "本研究探讨了大型语言模型（LLM）在社会科学研究中的应用及其带来的风险，称之为LLM黑客行为。研究发现，LLM的输出结果因研究者的选择（如模型、提示策略等）而存在显著差异，这可能导致系统性偏差和随机错误。通过对37个数据标注任务的分析，发现约三分之一的假设得出了错误结论，尤其是在小型语言模型中更为明显。研究强调了人类标注在减少假阳性结果和改善模型选择中的重要性，并指出常见的回归估计修正技术在降低LLM黑客风险方面效果有限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.04500",
            "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under\n  Mixed and Inappropriate Contexts",
            "url": "https://huggingface.co/papers/2509.04500",
            "abstract": "LLMs process mixed contexts by prioritizing less prevalent information, which can degrade response quality; RW-Steering, a two-stage fine-tuning approach, improves LLM safety by identifying and ignoring inappropriate signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable context engineering solution for improving LLM safety in real-world use.",
            "score": 0,
            "issue_id": 5901,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "4e9c238755cf2613",
            "authors": [
                "Rushi Wang",
                "Jiateng Liu",
                "Cheng Qian",
                "Yifan Shen",
                "Yanzhou Pan",
                "Zhaozhuo Xu",
                "Ahmed Abbasi",
                "Heng Ji",
                "Denghui Zhang"
            ],
            "affiliations": [
                "Google LLC",
                "Stevens Institute of Technology",
                "University of Illinois Urbana-Champaign",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.04500.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#dataset",
                    "#training",
                    "#hallucinations",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Управление вниманием LLM для повышения безопасности и качества ответов",
                    "desc": "Статья представляет исследование того, как большие языковые модели (LLM) обрабатывают смешанный контекст, уделяя приоритетное внимание менее распространенной информации. Авторы адаптировали модель Рескорла-Вагнера из нейронауки для количественной оценки влияния конкурирующих контекстуальных сигналов на выходные данные LLM. Они обнаружили, что LLM склонны включать менее распространенную информацию, что может ухудшить качество ответов. Для решения этой проблемы авторы предлагают метод RW-Steering, двухэтапный подход к тонкой настройке, который позволяет модели идентифицировать и игнорировать неподходящие сигналы."
                },
                "en": {
                    "title": "Enhancing LLM Safety with RW-Steering",
                    "desc": "This paper discusses how Large Language Models (LLMs) struggle with mixed contexts that contain both relevant and inappropriate information, which can lead to poor response quality. The authors introduce the Poisoned Context Testbed to analyze how LLMs prioritize less prevalent signals, revealing a tendency to incorporate inappropriate content. To address this issue, they propose RW-Steering, a two-stage fine-tuning method that helps LLMs identify and disregard harmful signals. Their experiments demonstrate that RW-Steering significantly enhances response quality and provides a reliable solution for improving LLM safety in practical applications."
                },
                "zh": {
                    "title": "提升大型语言模型安全性的RW-Steering方法",
                    "desc": "大型语言模型（LLMs）在处理混合上下文时，往往会优先考虑不常见的信息，这可能会降低响应质量。为了解决这个问题，研究者们提出了一种名为RW-Steering的两阶段微调方法，可以帮助模型识别并忽略不适当的信号。通过引入“污染上下文测试平台”，研究者们发现LLMs在面对混合信息时，容易受到少量不当内容的影响，从而影响输出结果。RW-Steering方法的实验结果显示，经过微调的模型在响应质量上提高了39.8%，有效提升了LLM在现实应用中的安全性。"
                }
            }
        }
    ],
    "link_prev": "2025-09-12.html",
    "link_next": "2025-09-16.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9月12日"
    },
    "short_date_next": {
        "ru": "16.09",
        "en": "09/16",
        "zh": "9月16日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 4,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 5,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 1
    }
}