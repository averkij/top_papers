
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (14 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.x2e806ebbdf3cc22d { background: url("https://hfday.ru/img/19630117/2e806ebbdf3cc22d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x2e806ebbdf3cc22d:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x2e806ebbdf3cc22d { background: url("https://hfday.ru/img/19630117/2e806ebbdf3cc22d.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x2e806ebbdf3cc22d:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p>4 октября | 14 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-03.html">⬅️ 03.10</a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-07.html">➡️ 07.10</a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.00907', 'title': 'Addition is All You Need for Energy-efficient Language Models', 'url': 'https://huggingface.co/papers/2410.00907', 'abstract': 'Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.', 'score': 81, 'issue_id': 1, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'L-Mul: революция в эффективности нейронных вычислений', 'desc': 'Статья представляет новый алгоритм L-Mul для аппроксимации умножения чисел с плавающей запятой с помощью операций целочисленного сложения. Этот метод потребляет значительно меньше вычислительных ресурсов, чем 8-битное умножение с плавающей запятой, но обеспечивает более высокую точность. Применение L-Mul в оборудовании для тензорной обработки потенциально может снизить энергозатраты на 95% для поэлементных умножений тензоров с плавающей запятой и на 80% для скалярных произведений. Эксперименты показали, что прямое применение L-Mul к механизму внимания практически не приводит к потерям точности.'}, 'en': {'title': 'Efficient Precision: Revolutionizing Neural Computation with L-Mul', 'desc': 'This paper introduces the L-Mul algorithm, which approximates floating point multiplications using integer additions, significantly reducing computational resources while maintaining high precision. The method is particularly energy-efficient, potentially reducing energy costs by up to 95% for element-wise tensor multiplications. Theoretical and experimental evaluations demonstrate that L-Mul with a 4-bit mantissa achieves precision comparable to traditional 8-bit floating point operations. Applying L-Mul in transformer models shows almost no loss in precision, making it a promising alternative for efficient neural network computations.'}, 'zh': {'title': '用整数加法革新浮点数乘法', 'desc': '这篇论文提出了一种新的算法L-Mul，可以用整数加法来近似浮点数乘法，从而大大减少计算资源的消耗。与传统的8位浮点数乘法相比，L-Mul不仅计算精度更高，而且能显著降低能耗。实验表明，L-Mul在多种任务中表现出色，尤其是在自然语言理解和常识问答等领域。通过将L-Mul应用于注意力机制，几乎没有精度损失，甚至在某些情况下优于传统方法。'}}, 'hash': '2e806ebbdf3cc22d'}, {'id': 'https://huggingface.co/papers/2410.02613', 'title': 'NL-Eye: Abductive NLI for Images', 'url': 'https://huggingface.co/papers/2410.02613', 'abstract': "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.", 'score': 14, 'issue_id': 4, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'NL-Eye: раскрывая пробелы в визуальном мышлении ИИ', 'desc': 'Статья представляет NL-Eye - новый бенчмарк для оценки способностей визуальных языковых моделей (VLM) к абдуктивному рассуждению. NL-Eye адаптирует задачу абдуктивного естественно-языкового вывода к визуальной области, требуя от моделей оценивать правдоподобность гипотетических изображений на основе исходного изображения. Эксперименты показывают, что современные VLM значительно уступают людям в этой задаче, часто демонстрируя результаты на уровне случайного угадывания. NL-Eye представляет важный шаг к разработке VLM, способных к надежному мультимодальному рассуждению для реальных приложений.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Real-World Reasoning', 'desc': 'The paper introduces NL-Eye, a benchmark designed to test the visual abductive reasoning skills of Visual Language Models (VLMs). It adapts the Natural Language Inference task to the visual domain, requiring models to assess the plausibility of images based on a given premise. The study finds that current VLMs struggle with this task, often performing no better than random chance, while humans excel. This highlights a significant gap in the reasoning abilities of VLMs, suggesting the need for further development to enable real-world applications like accident prevention.'}, 'zh': {'title': '提升视觉语言模型的推理能力：NL-Eye的挑战', 'desc': '这篇论文探讨了视觉语言模型（VLM）在推理因果关系方面的能力。研究团队开发了一个名为NL-Eye的基准，用于评估VLM的视觉溯因推理能力。实验结果显示，当前的VLM在NL-Eye任务中表现不佳，常常只能达到随机基线水平，而人类在可行性预测和解释质量上表现优异。这表明现代VLM在溯因推理能力上存在不足。'}}, 'hash': '315844ac62249b1b'}, {'id': 'https://huggingface.co/papers/2410.02703', 'title': 'Selective Attention Improves Transformer', 'url': 'https://huggingface.co/papers/2410.02703', 'abstract': "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.", 'score': 12, 'issue_id': 1, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#architecture', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Избирательное внимание: повышение эффективности трансформеров без дополнительных параметров', 'desc': 'Статья представляет концепцию избирательного внимания (Selective Attention) для улучшения механизма внимания в трансформерах. Этот подход уменьшает внимание к ненужным элементам в контексте, что приводит к улучшению производительности языкового моделирования. Избирательное внимание позволяет уменьшить размер буфера контекста внимания, значительно снижая требования к памяти и вычислительным ресурсам при инференсе. Эксперименты показывают, что трансформеры с избирательным вниманием могут достичь такой же производительности, как и стандартные модели с вдвое большим количеством параметров.'}, 'en': {'title': 'Selective Attention: Streamlining Efficiency in Language Models', 'desc': "The paper introduces Selective Attention, a modification to the standard attention mechanism in machine learning models that filters out unnecessary elements, enhancing performance. This approach improves language modeling across various model sizes and context lengths, making models as effective as those with twice the number of attention heads and parameters. Selective Attention also significantly reduces memory and computational requirements during inference by decreasing the size of the attention's context buffer. This innovation allows models to maintain the same level of accuracy while using substantially less memory, making them more efficient."}, 'zh': {'title': '选择性注意力：提升性能，减少资源消耗', 'desc': '这篇论文介绍了一种名为选择性注意力的新方法，可以减少注意力机制中不必要元素的影响。通过这种方法，语言模型的性能在不同模型大小和上下文长度下都得到了提升。选择性注意力使得模型在相同的验证困惑度下，显著减少了内存和计算需求。实验表明，使用选择性注意力的变压器模型在注意力模块中需要的内存比传统方法少得多。'}}, 'hash': 'fd0e4fdfc820e20f'}, {'id': 'https://huggingface.co/papers/2410.01699', 'title': 'Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding', 'url': 'https://huggingface.co/papers/2410.01699', 'abstract': 'The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel decoding algorithm, has been used to accelerate the auto-regressive generation and can be executed without training. However, the Jacobi decoding relies on a deterministic criterion to determine the convergence of iterations. Thus, it works for greedy decoding but is incompatible with sampling-based decoding which is crucial for visual quality and diversity in the current auto-regressive text-to-image generation. In this paper, we propose a training-free probabilistic parallel decoding algorithm, Speculative Jacobi Decoding (SJD), to accelerate auto-regressive text-to-image generation. By introducing a probabilistic convergence criterion, our SJD accelerates the inference of auto-regressive text-to-image generation while maintaining the randomness in sampling-based token decoding and allowing the model to generate diverse images. Specifically, SJD facilitates the model to predict multiple tokens at each step and accepts tokens based on the probabilistic criterion, enabling the model to generate images with fewer steps than the conventional next-token-prediction paradigm. We also investigate the token initialization strategies that leverage the spatial locality of visual data to further improve the acceleration ratio under specific scenarios. We conduct experiments for our proposed SJD on multiple auto-regressive text-to-image generation models, showing the effectiveness of model acceleration without sacrificing the visual quality.', 'score': 11, 'issue_id': 7, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#inference', '#cv'], 'emoji': '🚀', 'ru': {'title': 'Ускорение генерации изображений без потери качества', 'desc': 'Статья представляет новый алгоритм параллельного декодирования - Speculative Jacobi Decoding (SJD) для ускорения авторегрессивной генерации изображений по тексту. SJD использует вероятностный критерий сходимости, что позволяет сохранить случайность при выборке токенов и разнообразие генерируемых изображений. Алгоритм предсказывает несколько токенов за шаг и принимает их на основе вероятностного критерия, что уменьшает количество шагов генерации. Эксперименты показали эффективность SJD в ускорении моделей без потери качества изображений.'}, 'en': {'title': '"Faster Images, Same Quality: Meet Speculative Jacobi Decoding!"', 'desc': 'The paper introduces Speculative Jacobi Decoding (SJD), a new algorithm to speed up the process of generating images from text using large auto-regressive models. Unlike previous methods, SJD uses a probabilistic approach to decide when to stop iterating, which allows it to work well with sampling-based decoding that is important for creating diverse and high-quality images. This method lets the model predict several tokens at once, reducing the number of steps needed to generate an image. Experiments show that SJD can make the image generation process faster without losing the quality of the images.'}, 'zh': {'title': '推测雅可比解码：加速自回归图像生成的新方法', 'desc': '当前的大型自回归模型可以生成高质量、高分辨率的图像，但需要大量的步骤进行推理，耗时较长。现有的雅可比解码算法可以加速自回归生成，但不适用于基于采样的解码。本文提出了一种新的推测雅可比解码算法，通过引入概率收敛标准，加速生成过程并保持图像的多样性。实验表明，该算法在不影响视觉质量的情况下有效提高了模型的生成速度。'}}, 'hash': 'e4be41b1e418b1b8'}, {'id': 'https://huggingface.co/papers/2410.03017', 'title': 'Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise', 'url': 'https://huggingface.co/papers/2410.03017', 'abstract': "Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.", 'score': 11, 'issue_id': 1, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#rlhf', '#multimodal', '#medicine'], 'emoji': '🤖👨\u200d🏫', 'ru': {'title': 'Tutor CoPilot: ИИ-помощник для демократизации качественного образования', 'desc': 'Исследование представляет систему Tutor CoPilot, которая использует языковые модели для поддержки репетиторов в реальном времени. В рандомизированном контролируемом исследовании с участием 900 репетиторов и 1800 учеников из недостаточно обслуживаемых сообществ было обнаружено, что ученики, работавшие с репетиторами, использующими Tutor CoPilot, на 4 процентных пункта чаще осваивали темы. Анализ 550 000+ сообщений показал, что репетиторы с доступом к Tutor CoPilot чаще использовали качественные педагогические стратегии. Исследование демонстрирует, как системы человек-ИИ могут масштабировать экспертные знания в реальных областях и сделать качественное образование доступным для всех учащихся.'}, 'en': {'title': 'Scaling Expertise: AI-Powered Tutoring for All', 'desc': 'The paper introduces Tutor CoPilot, a Human-AI system designed to provide expert-like guidance to tutors, enhancing the quality of education for students, especially in under-served communities. In a large-scale trial involving 900 tutors and 1,800 students, the system improved student mastery of topics by 4 percentage points, with the most significant gains seen in students taught by lower-rated tutors. The study highlights the cost-effectiveness of Tutor CoPilot, costing only $20 per tutor annually, and its ability to encourage tutors to use effective teaching strategies. Despite some issues with grade-level appropriateness, Tutor CoPilot demonstrates the potential of AI to scale expertise and improve educational outcomes.'}, 'zh': {'title': 'Tutor CoPilot：让优质教育触手可及', 'desc': '这篇论文介绍了一种名为Tutor CoPilot的创新人机协作系统，旨在通过模拟专家思维为辅导员提供类似专家的指导。研究表明，使用Tutor CoPilot的辅导员能更有效地帮助学生掌握知识，尤其是那些评分较低的辅导员，其学生的知识掌握率提高了9个百分点。Tutor CoPilot每年每位辅导员的成本仅为20美元，并且能促使辅导员采用更高质量的教学策略。尽管Tutor CoPilot在某些情况下会生成不适合年级的建议，但总体上它展示了人机协作系统在教育领域扩展专业知识的潜力。'}}, 'hash': '72b9d001759c0a5d'}, {'id': 'https://huggingface.co/papers/2409.19989', 'title': 'RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models', 'url': 'https://huggingface.co/papers/2409.19989', 'abstract': 'Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.', 'score': 8, 'issue_id': 2, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Бесшовные текстуры из текста: новый уровень реализма в 3D', 'desc': 'Предложен новый метод генерации текстур на основе текстовых описаний, решающий проблемы несогласованности видов, видимых швов и несоответствия текстур и трехмерных моделей. Метод использует современные 2D диффузионные модели, включая SDXL и несколько ControlNet, для захвата структурных особенностей и мелких деталей текстур. Применяется стратегия симметричного синтеза видов с региональными промптами для улучшения согласованности. Также введены новые техники смешивания текстур и мягкой инпайнтинга для уменьшения швов.'}, 'en': {'title': 'Seamless Textures, Perfectly Aligned: A New Era in Text-to-Texture Generation', 'desc': 'The paper introduces a new method for generating textures from text descriptions that align well with 3D meshes, addressing common issues like view inconsistencies and seams. It uses advanced 2D diffusion models, such as SDXL and ControlNets, to capture detailed structural features in textures. The approach includes a symmetrical view synthesis strategy and regional prompts to improve view consistency. Novel techniques like texture blending and soft-inpainting are employed to minimize seam regions, showing superior performance over existing methods.'}, 'zh': {'title': '无缝对齐：革新文本到纹理生成', 'desc': '这篇论文提出了一种新的文本到纹理生成方法，解决了现有方法中视图不一致、明显接缝和纹理与网格不对齐的问题。该方法利用最先进的2D扩散模型，如SDXL和多个ControlNets，来捕捉生成纹理中的结构特征和复杂细节。通过对称视图合成策略和区域提示，增强了视图一致性。此外，引入了新的纹理混合和软修补技术，显著减少了接缝区域。实验结果表明，该方法优于现有的最先进方法。'}}, 'hash': 'd0d2a72fcf8c7f62'}, {'id': 'https://huggingface.co/papers/2410.02362', 'title': 'A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond', 'url': 'https://huggingface.co/papers/2410.02362', 'abstract': 'Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.', 'score': 7, 'issue_id': 3, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#survey', '#medicine', '#architecture', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'Mamba: Революция в анализе медицинских изображений', 'desc': 'Эта статья рассматривает применение модели Mamba в анализе медицинских изображений. Mamba, являясь частным случаем модели пространства состояний (State Space Model), предлагает линейную временную сложность и эффективную обработку длинных последовательностей без механизмов внимания. В отличие от трансформеров, Mamba способна лучше справляться с долгосрочными зависимостями и объединением мультимодальных данных. Статья подробно описывает архитектуры Mamba, их оптимизации и применения в медицинской визуализации.'}, 'en': {'title': 'Mamba: Revolutionizing Medical Imaging with Efficiency and Precision', 'desc': 'Mamba is a type of State Space Model that offers a more efficient alternative to traditional deep learning methods like transformers in medical image analysis. Unlike transformers, which struggle with high computational demands and long-range dependencies, Mamba operates with linear time complexity, making it faster and more memory-efficient. It excels in processing long sequences without attention mechanisms and effectively merges multimodal data, enhancing diagnostic accuracy. The paper details various Mamba architectures and optimizations, highlighting its potential to revolutionize medical imaging by addressing current limitations.'}, 'zh': {'title': 'Mamba：医学图像分析的新突破', 'desc': 'Mamba是一种特殊的状态空间模型，在医学图像分析中逐渐成为模板化深度学习方法的替代方案。与变压器模型相比，Mamba具有线性时间复杂度，能够更高效地处理长序列数据，减少内存需求。Mamba在多模态数据融合中表现出色，提高了诊断准确性和患者治疗效果。本文详细介绍了Mamba在医学成像中的应用，包括其架构、优化技术及未来发展方向。'}}, 'hash': 'dd4bc5254b2493c9'}, {'id': 'https://huggingface.co/papers/2410.02760', 'title': 'Erasing Conceptual Knowledge from Language Models', 'url': 'https://huggingface.co/papers/2410.02760', 'abstract': "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info", 'score': 7, 'issue_id': 1, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#alignment', '#interpretability'], 'emoji': '🧹', 'ru': {'title': 'ELM: Новый подход к избирательному стиранию знаний в языковых моделях', 'desc': 'Статья представляет новый метод стирания концепций в языковых моделях - Erasure of Language Memory (ELM). Авторы предлагают комплексную систему оценки эффективности стирания, основанную на трех критериях: полнота удаления знаний, сохранение условной генерации и специфичность. ELM использует целевые обновления низкого ранга для изменения выходных распределений стираемых концепций, сохраняя при этом общие возможности модели.'}, 'en': {'title': "Erase with Grace: ELM's Breakthrough in Concept Erasure", 'desc': "The paper introduces a new evaluation framework for concept erasure in language models, focusing on innocence, seamlessness, and specificity. It presents Erasure of Language Memory (ELM), a method that uses low-rank updates to effectively erase concepts while maintaining the model's fluency and performance on unrelated tasks. ELM is tested on various domains, showing superior results in erasing specific topics without affecting other capabilities. The method also demonstrates robustness against adversarial attacks, ensuring reliable performance."}, 'zh': {'title': '语言模型中的概念消除新标准', 'desc': '这篇论文提出了一种新的评估框架，用于评估语言模型中概念消除的效果。该框架基于三个关键标准：无害性（完全知识移除）、无缝性（保持条件流畅生成）和特异性（保留无关任务性能）。研究者开发了一种名为语言记忆消除（ELM）的方法，通过低秩更新来改变被消除概念的输出分布，同时保持模型的整体能力。实验表明，ELM在生物安全、网络安全和文学领域的消除任务中表现优异。'}}, 'hash': '018d36385dbb4dea'}, {'id': 'https://huggingface.co/papers/2410.02241', 'title': 'MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction', 'url': 'https://huggingface.co/papers/2410.02241', 'abstract': 'Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing a single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, a single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, a novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose a novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As a result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct a comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research.', 'score': 5, 'issue_id': 1, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#architecture', '#optimization', '#math'], 'emoji': '📈', 'ru': {'title': 'MIGA: Прорыв в прогнозировании фондового рынка с помощью специализированных экспертов', 'desc': 'MIGA - это новый фреймворк для прогнозирования фондового рынка, основанный на смеси экспертов с групповой агрегацией. В отличие от существующих моделей, MIGA использует специализированных экспертов для разных стилей акций, что позволяет более точно учитывать особенности различных типов ценных бумаг. Фреймворк включает новую архитектуру внутригрупповой аттенции, улучшающую взаимодействие между экспертами. MIGA значительно превосходит другие модели на китайских фондовых индексах, достигая 24% годовой доходности на индексе CSI300.'}, 'en': {'title': '"MIGA: Tailored Expertise for Smarter Stock Predictions"', 'desc': 'The paper introduces MIGA, a new framework for stock market prediction that uses a Mixture of Expert with Group Aggregation approach. Unlike traditional models that apply a single model to all stocks, MIGA dynamically switches between specialized experts tailored to different stock styles. This method enhances prediction accuracy by allowing experts to share information through an inner group attention mechanism. MIGA demonstrates superior performance on Chinese Stock Index benchmarks, achieving a notable 24% excess annual return on the CSI300 benchmark.'}, 'zh': {'title': 'MIGA：股票预测的风格专家混合框架', 'desc': '股票市场预测一直是一个具有挑战性的问题，因为市场的高波动性和信息噪声比低。传统的机器学习和深度学习方法通常使用单一模型来预测所有类型的股票，但由于股票风格和市场趋势的差异，这种方法的准确性有限。本文提出了一种新的专家混合框架MIGA，通过动态切换不同风格的专家来生成针对不同风格股票的预测。MIGA在多个中国股票指数基准上表现优异，尤其是在CSI300基准上实现了24%的超额年回报率。'}}, 'hash': '9cfda67b2beb4586'}, {'id': 'https://huggingface.co/papers/2410.01273', 'title': 'CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction', 'url': 'https://huggingface.co/papers/2410.01273', 'abstract': 'Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.', 'score': 4, 'issue_id': 4, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#agents', '#dataset', '#rl', '#multimodal', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'CANVAS: навигация роботов с человеческим здравым смыслом', 'desc': 'CANVAS - это новая система для навигации роботов, использующая визуальные и языковые инструкции. Она основана на имитационном обучении, позволяющем роботу учиться на примерах человеческого поведения. Авторы создали датасет COMMAND с аннотированными человеком результатами навигации для обучения систем в симулированных средах. CANVAS превосходит базовую систему ROS NavStack во всех тестовых сценариях, особенно при работе с зашумленными инструкциями.'}, 'en': {'title': 'Guiding Robots with Human-Like Understanding', 'desc': 'The paper introduces CANVAS, a framework that helps robots navigate using both visual and linguistic instructions, making them more aligned with human expectations. It uses imitation learning to teach robots how to interpret abstract human commands by learning from human navigation behavior. The researchers created a dataset called COMMAND to train these systems, showing that CANVAS performs better than traditional rule-based systems, especially in challenging environments. The framework also demonstrates strong Sim2Real transfer, meaning it works well in real-world scenarios after being trained in simulations.'}, 'zh': {'title': 'CANVAS：让机器人导航更贴近人类常识', 'desc': '这篇论文介绍了一个名为CANVAS的新框架，它结合视觉和语言指令来实现常识感知的导航。通过模仿学习，机器人可以从人类的导航行为中学习。实验表明，CANVAS在所有环境中都优于传统的基于规则的系统，尤其是在噪声指令下表现出色。在真实世界的应用中，CANVAS展示了从模拟环境到现实环境的出色转移能力。'}}, 'hash': 'db64c057695506e6'}, {'id': 'https://huggingface.co/papers/2410.03535', 'title': 'NRGBoost: Energy-Based Generative Boosted Trees', 'url': 'https://huggingface.co/papers/2410.03535', 'abstract': 'Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling.', 'score': 3, 'issue_id': 4, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#math', '#optimization', '#dataset'], 'emoji': '🌳', 'ru': {'title': 'Генеративный бустинг: мощь деревьев решений в мире генеративных моделей', 'desc': 'Статья исследует генеративные расширения популярных древовидных алгоритмов, таких как Random Forests и Gradient Boosted Decision Trees, для работы с табличными данными. Авторы предлагают энергетический генеративный алгоритм бустинга, аналогичный бустингу второго порядка в XGBoost. Новый метод способен моделировать плотность данных и выполнять различные задачи вывода, сохраняя при этом высокую дискриминативную производительность. Эксперименты показывают, что предложенный подход конкурентоспособен как с традиционными древовидными методами, так и с нейросетевыми моделями для генерации выборок.'}, 'en': {'title': 'Boosting Trees Beyond Boundaries: Generative Power Unleashed', 'desc': 'This paper explores how tree-based methods like Random Forests and Gradient Boosted Decision Trees can be extended to generative models, which can model data density and perform tasks beyond just classification or regression. The authors introduce an energy-based generative boosting algorithm that mirrors the second-order boosting used in popular tools like XGBoost. Their proposed method not only matches the discriminative performance of traditional GBDT on real-world tabular data but also competes well with neural networks in generating samples. This approach offers a versatile model that can handle both inference and sampling tasks effectively.'}, 'zh': {'title': '树模型的生成式新突破：能量提升算法', 'desc': '这篇论文探讨了如何将随机森林和梯度提升决策树等树模型扩展为生成模型，重点在于显式建模数据密度。作者提出了一种基于能量的生成提升算法，与XGBoost中的二阶提升类似。实验表明，该算法在处理推理任务时，能够在多个真实数据集上达到与GBDT相似的判别性能，并优于其他生成方法。同时，它在采样任务中也能与神经网络模型竞争。'}}, 'hash': 'e71078964f133d48'}, {'id': 'https://huggingface.co/papers/2410.03103', 'title': 'Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning', 'url': 'https://huggingface.co/papers/2410.03103', 'abstract': 'Fill-in-the-Middle (FIM) has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm, which reorders original training sequences and then performs regular next-token prediction (NTP), often leads to models struggling to generate content that aligns smoothly with the surrounding context. Crucially, while existing works rely on rule-based post-processing to circumvent this weakness, such methods are not practically usable in open-domain code completion tasks as they depend on restrictive, dataset-specific assumptions (e.g., generating the same number of lines as in the ground truth). Moreover, model performance on FIM tasks deteriorates significantly without these unrealistic assumptions.   We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens (i.e., horizon length) at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different models and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level, and without resorting to unrealistic post-processing methods. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP only incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.', 'score': 2, 'issue_id': 7, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#plp', '#reasoning'], 'emoji': '🧩', 'ru': {'title': 'HLP: Умное заполнение пропусков в коде с прогнозированием горизонта', 'desc': 'Статья представляет новый метод обучения моделей для заполнения пропусков в коде - Horizon-Length Prediction (HLP). В отличие от стандартного подхода Fill-in-the-Middle (FIM), HLP учит модель предсказывать количество оставшихся токенов, что улучшает планирование и согласованность с контекстом. Эксперименты показывают, что HLP значительно повышает производительность на различных бенчмарках без необходимости в постобработке. Метод также улучшает способности модели к рассуждению о коде, при этом не требуя существенных дополнительных ресурсов.'}, 'en': {'title': '"Horizon-Length Prediction: Elevating Code Completion with Smarter Planning"', 'desc': "The paper discusses a new approach to improve code language models' ability to fill in missing code segments, called Fill-in-the-Middle (FIM). Traditional methods struggle because they rely on reordering sequences and predicting the next token, which doesn't always align well with the surrounding code context. The authors propose a new training method called Horizon-Length Prediction (HLP), which helps models predict how many tokens are needed to complete the middle section, improving their planning and infilling capabilities. This method significantly enhances model performance without needing complex post-processing, making it more practical for real-world applications."}, 'zh': {'title': '视界长度预测：提升代码填充的未来', 'desc': '这篇论文讨论了代码语言模型中的填充中间部分（FIM）问题，指出现有的训练方法在生成与上下文平滑衔接的内容时存在困难。作者提出了一种新的训练目标，称为视界长度预测（HLP），以改善模型在FIM任务中的表现。HLP通过预测剩余中间标记的数量，帮助模型在不依赖数据集特定后处理的情况下学习填充边界。实验结果表明，HLP显著提高了模型在多种基准测试中的表现，同时对训练和推理的开销影响很小。'}}, 'hash': '6d595cf08f21593b'}, {'id': 'https://huggingface.co/papers/2410.01999', 'title': 'CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs', 'url': 'https://huggingface.co/papers/2410.01999', 'abstract': "Recent advancements in Code Large Language Models (CodeLLMs) have predominantly focused on open-ended code generation tasks, often neglecting the critical aspect of code understanding and comprehension. To bridge this gap, we present CodeMMLU, a comprehensive multiple-choice question-answer benchmark designed to evaluate the depth of software and code understanding in LLMs. CodeMMLU includes over 10,000 questions sourced from diverse domains, encompassing tasks such as code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks, CodeMMLU assesses models's ability to reason about code rather than merely generate it, providing deeper insights into their grasp of complex software concepts and systems. Our extensive evaluation reveals that even state-of-the-art models face significant challenges with CodeMMLU, highlighting deficiencies in comprehension beyond code generation. By underscoring the crucial relationship between code understanding and effective generation, CodeMMLU serves as a vital resource for advancing AI-assisted software development, ultimately aiming to create more reliable and capable coding assistants.", 'score': 1, 'issue_id': 9, 'pub_date': '2024-10-02', 'pub_date_ru': '2 октября', 'data': {'categories': ['#benchmark', '#plp'], 'emoji': '🧠', 'ru': {'title': 'CodeMMLU: Переосмысление оценки понимания кода в ИИ', 'desc': 'CodeMMLU - новый бенчмарк для оценки понимания кода языковыми моделями. Он содержит более 10 000 вопросов с множественным выбором из различных областей программирования. В отличие от традиционных бенчмарков, CodeMMLU оценивает способность моделей рассуждать о коде, а не просто генерировать его. Результаты показывают, что даже современные модели сталкиваются со значительными трудностями при решении задач CodeMMLU.'}, 'en': {'title': '"From Code Generation to Code Comprehension: Elevating AI\'s Understanding"', 'desc': "The paper introduces CodeMMLU, a benchmark designed to test the code understanding capabilities of large language models (LLMs) through multiple-choice questions. It focuses on evaluating models' ability to comprehend and reason about code, rather than just generating it. The benchmark includes over 10,000 questions from various domains, highlighting the challenges even advanced models face in understanding complex software concepts. CodeMMLU aims to improve AI-assisted software development by emphasizing the importance of code comprehension in creating reliable coding assistants."}, 'zh': {'title': 'CodeMMLU：提升代码理解，超越生成', 'desc': '近年来，代码大语言模型（CodeLLMs）主要关注于开放式代码生成任务，往往忽视了代码理解的重要性。为了解决这一问题，我们提出了CodeMMLU，这是一个全面的多项选择题基准，用于评估大语言模型对软件和代码的理解深度。CodeMMLU包含超过10,000个问题，涵盖代码分析、缺陷检测和软件工程原理等任务。我们的评估显示，即使是最先进的模型在CodeMMLU上也面临重大挑战，强调了理解代码与生成代码之间的重要关系。'}}, 'hash': 'a61e37ff42c5cd79'}, {'id': 'https://huggingface.co/papers/2410.03645', 'title': 'GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs', 'url': 'https://huggingface.co/papers/2410.03645', 'abstract': 'Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated objects. To automatically generate demonstration data for these tasks at scale, we propose planning and RL solvers that generalize within object categories. The pipeline can generate data for up to 100 articulated tasks with 200 objects and reduce the required human efforts. To utilize such data, we propose an effective multi-task language-conditioned policy architecture, dubbed proprioceptive point-cloud transformer (PPT), that learns from the generated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the policy architecture, we show a promising usage of GenSim2 that the generated data can be used for zero-shot transfer or co-train with real-world collected data, which enhances the policy performance by 20% compared with training exclusively on limited real data.', 'score': 1, 'issue_id': 7, 'pub_date': '1963-01-17', 'pub_date_ru': 'надцатого мартобря', 'data': {'categories': ['#agents', '#rl', '#transfer_learning', '#robots'], 'emoji': '🤖', 'ru': {'title': 'GenSim2: ИИ-помощник для масштабного обучения роботов', 'desc': 'GenSim2 - это масштабируемая система для создания сложных симуляций роботов с использованием языковых моделей. Она автоматически генерирует разнообразные задачи и сцены, включая длительные последовательности действий с шарнирными объектами. Система использует планировщики и алгоритмы обучения с подкреплением для создания демонстрационных данных. Предложенная архитектура политики на основе трансформеров позволяет обучаться на сгенерированных данных и демонстрирует хороший перенос из симуляции в реальность.'}, 'en': {'title': 'Scaling Robotic Simulations with GenSim2: Automate, Learn, Transfer!', 'desc': 'The paper introduces GenSim2, a framework designed to automate the creation of complex simulation tasks using coding language models with multi-modal and reasoning capabilities. It addresses the scalability issues in robotic simulations by generating demonstration data for numerous tasks, reducing the need for human input. The framework includes a multi-task language-conditioned policy architecture called proprioceptive point-cloud transformer (PPT), which effectively learns from these demonstrations and shows strong sim-to-real zero-shot transfer capabilities. By combining GenSim2 with real-world data, the approach enhances policy performance significantly, demonstrating a 20% improvement over training with limited real data alone.'}, 'zh': {'title': 'GenSim2：自动化生成复杂模拟任务的未来', 'desc': '这篇论文提出了一种名为GenSim2的框架，旨在通过利用多模态和推理能力的编码大语言模型来创建复杂的模拟任务。该框架能够自动生成多达100个任务的数据，减少了人力投入。为了利用这些数据，研究者设计了一种多任务语言条件策略架构，称为本体点云变换器（PPT），可以实现从模拟到真实环境的零样本迁移。结合生成的数据和策略架构，GenSim2展示了在增强策略性能方面的潜力，比仅使用有限的真实数据训练提高了20%。'}}, 'hash': 'f956848db7fdb9bc'}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (2)', '#cv (1)', '#data', '#dataset (2)', '#diffusion (1)', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference (3)', '#interpretability (1)', '#math (2)', '#medicine (2)', '#multilingual', '#multimodal (4)', '#optimization (3)', '#plp (2)', '#quantum', '#rag', '#reasoning (1)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#robots', '#security', '#story_generation', '#survey (1)', '#training (2)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            let lang = 'ru'
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-06 20:26');
        } 
        function hideNextLink() {
            if (isToday('2024-10-06 20:26')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
    </script>
</body>
</html>
    