{
    "date": {
        "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 15",
        "zh": "4æœˆ15æ—¥"
    },
    "time_utc": "2025-04-15 02:25",
    "weekday": 1,
    "issue_id": 3236,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 5,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FUSION: Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "FUSION - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. FUSION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSIONï¼šæ·±åº¦é›†æˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†FUSIONï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨å®Œå…¨çš„è§†è§‰-è¯­è¨€å¯¹é½å’Œé›†æˆèŒƒå¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºLLMè§£ç è¿‡ç¨‹ä¸­çš„åæœŸæ¨¡æ€äº¤äº’ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä¸ªå¤„ç†æµç¨‹ä¸­å®ç°äº†æ·±åº¦ã€åŠ¨æ€çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¼•å¯¼çš„ç»Ÿä¸€è§†è§‰ç¼–ç ï¼Œå°†æ–‡æœ¬ä¿¡æ¯èå…¥è§†è§‰ç¼–ç ï¼Œå®ç°åƒç´ çº§çš„é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é€’å½’å¯¹é½è§£ç ï¼Œèƒ½å¤Ÿåœ¨è§£ç è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬ä¸Šä¸‹æ–‡é€’å½’èšåˆè§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„é—®é¢˜çº§è¯­ä¹‰é›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 3,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Upper Confidence Bound Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒçº§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçº§å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è®­ç»ƒæ•°æ®è§†ä¸ºç»Ÿä¸€æ•´ä½“ï¼Œå¿½è§†äº†æ•°æ®åˆ†å¸ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´ä¸åŒåˆ†å¸ƒçš„é‡‡æ ·æ¦‚ç‡ï¼Œä¼˜å…ˆè€ƒè™‘é«˜å¹³å‡ä¼˜åŠ¿æˆ–ä½æ ·æœ¬æ•°é‡çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå±•ç¤ºäº†åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥çš„ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 3,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GPT-4o: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç†è§£ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„å¤šæ¨¡æ€æ¨¡å‹GPT-4oåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨å…¨çƒæŒ‡ä»¤éµå¾ªã€ç²¾ç»†ç¼–è¾‘ç²¾åº¦å’Œç”Ÿæˆåæ¨ç†ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚å°½ç®¡ç°æœ‰åŸºå‡†æ˜¾ç¤ºGPT-4oåœ¨å›¾åƒå¤„ç†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å…¶åœ¨æŒ‡ä»¤ç†è§£å’ŒçŸ¥è¯†åº”ç”¨ä¸Šçš„å±€é™æ€§ã€‚æ¨¡å‹å¸¸å¸¸å¯¹æŒ‡ä»¤è¿›è¡Œå­—é¢è§£é‡Šï¼ŒçŸ¥è¯†çº¦æŸåº”ç”¨ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ¡ä»¶æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¯¹GPT-4oç»Ÿä¸€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„æ™®éå‡è®¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„åŸºå‡†å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œæ¨ç†åŸºç¡€çš„å¤šæ¨¡æ€ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 1,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ prima.cpp - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ´Ğ¾ 70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ CPU/GPU, Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº RAM/VRAM Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Wi-Fi Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Prima.cpp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ mmap Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ»ÑŒÑ†ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ğ´Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Halda Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "è®©å®¶åº­è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºprima.cppçš„åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ™®é€šå®¶åº­è®¾å¤‡ä¸Šè¿è¡Œ70Bè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ··åˆä½¿ç”¨CPUå’ŒGPUï¼Œä¼˜åŒ–å†…å­˜å’Œå¸¦å®½çš„ä½¿ç”¨ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ¡ˆå¯¹é«˜æ€§èƒ½ç¡¬ä»¶çš„ä¾èµ–ã€‚å®ƒé‡‡ç”¨äº†mmapç®¡ç†æ¨¡å‹æƒé‡ï¼Œå¹¶å¼•å…¥äº†ç®¡é“ç¯å¹¶è¡Œå’Œé¢„å–æŠ€æœ¯ï¼Œä»¥å‡å°‘ç£ç›˜åŠ è½½æ—¶é—´ã€‚é€šè¿‡ä¼˜åŒ–è®¡ç®—ã€é€šä¿¡å’Œå†…å­˜ç®¡ç†ï¼Œprima.cppæ˜¾è‘—é™ä½äº†å»¶è¿Ÿï¼Œä½¿å¾—å…ˆè¿›çš„AIæ¨¡å‹èƒ½å¤Ÿåœ¨å®¶åº­åŠ©æ‰‹ä¸­æ™®åŠã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4æœˆ14æ—¥"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†è®­ç»ƒè§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æˆæœ¬æ•ˆç›Šç­–ç•¥ã€‚æŠ¥å‘Šå±•ç¤ºäº†ä¸€ä¸ªåä¸º Seaweed-7B çš„ä¸­å‹ç ”ç©¶æ¨¡å‹ï¼Œæ‹¥æœ‰çº¦70äº¿å‚æ•°ï¼Œä½¿ç”¨665,000ä¸ªH100 GPUå°æ—¶ä»å¤´è®­ç»ƒã€‚å°½ç®¡ä½¿ç”¨çš„è®¡ç®—èµ„æºé€‚ä¸­ï¼ŒSeaweed-7B ä»è¡¨ç°å‡ºä¸æ›´å¤§æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è®¾è®¡é€‰æ‹©åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ã€‚æŠ¥å‘Šå¼ºè°ƒäº†æå‡ä¸­å‹æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„å…³é”®è®¾è®¡å†³ç­–ã€‚ç»éªŒä¸Šï¼Œæœ‰ä¸¤ä¸ªè§‚å¯Ÿç»“æœï¼š(1) Seaweed-7B çš„æ€§èƒ½ä¸æˆ–è¶…è¿‡ä½¿ç”¨æ›´å¤šGPUèµ„æºè®­ç»ƒçš„å¤§å‹æ¨¡å‹ï¼›(2) è¯¥æ¨¡å‹å±•ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯é€šè¿‡è½»é‡å¾®è°ƒæˆ–ç»§ç»­è®­ç»ƒæœ‰æ•ˆé€‚åº”å¤šç§ä¸‹æ¸¸åº”ç”¨ã€‚é¡¹ç›®é¡µé¢è§ https://seaweed.video/ã€‚",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "pinyin": "ZhÃ¨ piÄn jÃ¬shÃ¹ bÃ ogÃ o jiÃ¨shÃ o le xÃ¹nliÃ n shÃ¬pÇn shÄ“ngchÃ©ng jÄ«chÇ” mÃ³xÃ­ng de chÃ©ngbÄ›n xiÃ oyÃ¬ cÃ¨lÃ¼Ã¨. BÃ ogÃ o zhÇnshÃ¬ le yÄ«gÃ¨ mÃ­ngwÃ¨i Seaweed-7B de zhÅngxÃ­ng yÃ¡njiÅ« mÃ³xÃ­ng, yÇ’ngyÇ’u yuÄ“ 70 yÃ¬ cÄnshÃ¹, shÇyÃ²ng 665,000 gÃ¨ H100 GPU xiÇoshÃ­ cÃ³ngtÃ³u xÃ¹nliÃ n. JÄ«nrÃ¡n shÇyÃ²ng de jÃ¬suÃ n zÄ«yuÃ¡n shÃ¬guÃ¬ zhÅngdÄ›ng, Seaweed-7B rÃ©ng biÇoxiÃ n chÅ« yÇ” gÃ¨ng dÃ  mÃ³xÃ­ng xiÃ ng bÇmÄ›i de xÃ­ngnÃ©ng. ShÃ¨jÃ¬ xuÇnzÃ© zÃ i zÄ«yuÃ¡n shÃ²uxiÃ n de huÃ¡njÃ¬ng zhÅng yÃ³uwÃ¨i zhÃ²ngyÃ o. BÃ ogÃ o qiÃ¡ngdiÇo le tÃ­shÄ“ng zhÅngxÃ­ng kuÃ²sÃ n mÃ³xÃ­ng xÃ­ngnÃ©ng de guÇnjiÃ n shÃ¨jÃ¬ juÃ©cÃ¨. JÄ«ngyÃ n shÃ ng, yÇ’u liÇng gÃ¨ guÄnchÃ¡ jiÃ©guÇ’: (1) Seaweed-7B de xÃ­ngnÃ©ng yÇ” huÃ² chÄoguÃ² shÇyÃ²ng gÃ¨ng duÅ GPU zÄ«yuÃ¡n xÃ¹nliÃ n de dÃ xÃ­ng mÃ³xÃ­ng; (2) gÃ¨ mÃ³xÃ­ng zhÇnshÃ¬ chÅ« qiÃ¡ngdÃ  de fÃ nhuÃ  nÃ©nglÃ¬, kÄ› tÅngguÃ² qÄ«ngliÃ ng wÄ“itiÃ¡o huÃ² jÃ¬xÃ¹ xÃ¹nliÃ n yÇ’uxiÃ o shÃ¬yÃ¬ng duÅzhÇ’ng xiÃ yÃ³u yÃ¬ngyÃ²ng. XiÃ ngmÃ¹ yÃ¨miÃ n jiÃ n https://seaweed.video/ã€‚",
        "vocab": "[\n    {\"word\": \"æŠ€æœ¯æŠ¥å‘Š\", \"pinyin\": \"jÃ¬shÃ¹ bÃ ogÃ o\", \"trans\": \"technical report\"},\n    {\"word\": \"è§†é¢‘ç”Ÿæˆ\", \"pinyin\": \"shÃ¬pÃ­n shÄ“ngchÃ©ng\", \"trans\": \"video generation\"},\n    {\"word\": \"åŸºç¡€æ¨¡å‹\", \"pinyin\": \"jÄ«chÇ” mÃ³xÃ­ng\", \"trans\": \"foundational model\"},\n    {\"word\": \"æˆæœ¬æ•ˆç›Š\", \"pinyin\": \"chÃ©ngbÄ›n xiÃ oyÃ¬\", \"trans\": \"cost-effectiveness\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"ä¸­å‹\", \"pinyin\": \"zhÅngxÃ­ng\", \"trans\": \"medium-sized\"},\n    {\"word\": \"ç ”ç©¶æ¨¡å‹\", \"pinyin\": \"yÃ¡njiÅ« mÃ³xÃ­ng\", \"trans\": \"research model\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÇ”\", \"trans\": \"parameters\"},\n    {\"word\": \"ä»å¤´è®­ç»ƒ\", \"pinyin\": \"cÃ³ngtÃ³u xÃ¹nliÃ n\", \"trans\": \"train from scratch\"},\n    {\"word\": \"è®¡ç®—èµ„æº\", \"pinyin\": \"jÃ¬suÃ n zÄ«yuÃ¡n\", \"trans\": \"computational resources\"},\n    {\"word\": \"é€‚ä¸­\", \"pinyin\": \"shÃ¬zhÅng\", \"trans\": \"moderate\"},\n    {\"word\": \"åª²ç¾\", \"pinyin\": \"pÃ¬mÄ›i\", \"trans\": \"rival\"},\n    {\"word\": \"è®¾è®¡é€‰æ‹©\", \"pinyin\": \"shÃ¨jÃ¬ xuÇnzÃ©\", \"trans\": \"design choices\"},\n    {\"word\": \"å—é™\", \"pinyin\": \"shÃ²uxiÃ n\", \"trans\": \"constrained\"},\n    {\"word\": \"ç¯å¢ƒ\", \"pinyin\": \"huÃ¡njÃ¬ng\", \"trans\": \"environment\"},\n    {\"word\": \"å°¤ä¸º\", \"pinyin\": \"yÃ³uwÃ©i\", \"trans\": \"especially\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ngyÃ o\", \"trans\": \"important\"},\n    {\"word\": \"å¼ºè°ƒ\", \"pinyin\": \"qiÃ¡ngdiÃ o\", \"trans\": \"emphasize\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"enhance\"},\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ²sÃ n mÃ³xÃ­ng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ­ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"è®¾è®¡å†³ç­–\", \"pinyin\": \"shÃ¨jÃ¬ juÃ©cÃ¨\", \"trans\": \"design decisions\"},\n    {\"word\": \"ç»éªŒ\", \"pinyin\": \"jÄ«ngyÃ n\", \"trans\": \"experience\"},\n    {\"word\": \"è§‚å¯Ÿç»“æœ\", \"pinyin\": \"guÄnchÃ¡ jiÃ©guÇ’\", \"trans\": \"observation results\"},\n    {\"word\": \"æ³›åŒ–èƒ½åŠ›\", \"pinyin\": \"fÃ nhuÃ  nÃ©nglÃ¬\", \"trans\": \"generalization capability\"},\n    {\"word\": \"è½»é‡å¾®è°ƒ\", \"pinyin\": \"qÄ«ngliÃ ng wÄ“itiÃ¡o\", \"trans\": \"lightweight fine-tuning\"},\n    {\"word\": \"ç»§ç»­è®­ç»ƒ\", \"pinyin\": \"jÃ¬xÃ¹ xÃ¹nliÃ n\", \"trans\": \"continued training\"},\n    {\"word\": \"ä¸‹æ¸¸åº”ç”¨\", \"pinyin\": \"xiÃ yÃ³u yÃ¬ngyÃ²ng\", \"trans\": \"downstream applications\"},\n    {\"word\": \"é¡¹ç›®é¡µé¢\", \"pinyin\": \"xiÃ ngmÃ¹ yÃ¨miÃ n\", \"trans\": \"project page\"}\n]",
        "trans": "This technical report introduces cost-effective strategies for training foundational video generation models. The report presents a mid-sized research model named Seaweed-7B, which has approximately 7 billion parameters and was trained from scratch using 665,000 H100 GPU hours. Despite the moderate computational resources used, Seaweed-7B demonstrates performance comparable to larger models. Design choices are particularly important in resource-constrained environments. The report emphasizes key design decisions that enhance the performance of mid-sized diffusion models. Empirically, there are two observations: (1) Seaweed-7B's performance matches or exceeds that of larger models trained with more GPU resources; (2) The model demonstrates strong generalization capabilities and can effectively adapt to various downstream applications through lightweight fine-tuning or continued training. For more information, visit the project page at https://seaweed.video/.",
        "update_ts": "2025-04-14 09:12"
    }
}