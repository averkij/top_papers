{
    "date": {
        "ru": "15 апреля",
        "en": "April 15",
        "zh": "4月15日"
    },
    "time_utc": "2025-04-15 06:16",
    "weekday": 1,
    "issue_id": 3240,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 40,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "Продвинутые языковые модели теперь доступны на домашних устройствах",
                    "desc": "Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковые модели (до 70 миллиардов параметров) на обычных домашних устройствах. Система использует комбинацию CPU/GPU, низкие требования к RAM/VRAM и поддержку Wi-Fi для эффективной работы. Prima.cpp применяет технологию mmap для управления весами модели и вводит конвейерный кольцевой параллелизм с предвыборкой для скрытия загрузки с диска. Авторы предлагают алгоритм Halda для оптимального распределения слоев модели между устройствами, учитывая их гетерогенность."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "让家庭设备也能运行大型语言模型",
                    "desc": "本文介绍了一种名为prima.cpp的分布式推理系统，能够在普通家庭设备上运行70B规模的语言模型。该系统通过混合使用CPU和GPU，优化内存和带宽的使用，解决了传统方案对高性能硬件的依赖。它采用了mmap管理模型权重，并引入了管道环并行和预取技术，以减少磁盘加载时间。通过优化计算、通信和内存管理，prima.cpp显著降低了延迟，使得先进的AI模型能够在家庭助手中普及。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 25,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях",
                    "desc": "FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель использует текстово-управляемое унифицированное кодирование изображений и контекстно-зависимое рекурсивное декодирование для глубокой интеграции модальностей. Авторы разработали специальную функцию потерь и синтетический набор данных для оптимизации процесса обучения. FUSION превосходит существующие методы, демонстрируя эффективность подхода полной интеграции модальностей."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSION：深度集成的多模态语言模型",
                    "desc": "我们介绍了FUSION，这是一种多模态大型语言模型（MLLM），采用完全的视觉-语言对齐和集成范式。与现有方法主要依赖于LLM解码过程中的后期模态交互不同，我们的方法在整个处理流程中实现了深度、动态的集成。我们提出了文本引导的统一视觉编码，将文本信息融入视觉编码，实现像素级的集成。此外，我们设计了上下文感知的递归对齐解码，能够在解码过程中根据文本上下文递归聚合视觉特征，从而实现细粒度的问题级语义集成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10068",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
            "url": "https://huggingface.co/papers/2504.10068",
            "abstract": "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.",
            "score": 18,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "bbb7251e84f61649",
            "authors": [
                "Yang Shi",
                "Jiaheng Liu",
                "Yushuo Guan",
                "Zhenhua Wu",
                "Yuanxing Zhang",
                "Zihao Wang",
                "Weihong Lin",
                "Jingyun Hua",
                "Zekun Wang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Wentao Zhang",
                "Fuzheng Zhang",
                "Wenjing Yang",
                "Di Zhang"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10068.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#architecture",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Mavors: Эффективное понимание длинных видео с сохранением мелких деталей",
                    "desc": "Статья представляет новый подход Mavors для понимания длинных видео в мультимодальных больших языковых моделях. Mavors использует многоуровневое представление видео, сочетая внутрикадровое кодирование высокого разрешения и межкадровую агрегацию признаков. Этот метод позволяет сохранить как пространственные детали, так и временную динамику в видео. Эксперименты показывают превосходство Mavors над существующими методами в задачах, требующих детального пространственно-временного анализа."
                },
                "en": {
                    "title": "Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation",
                    "desc": "This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning."
                },
                "zh": {
                    "title": "Mavors：长视频理解的新突破",
                    "desc": "本论文提出了一种名为Mavors的新框架，旨在解决多模态大语言模型在长视频理解中的计算效率与细粒度时空模式保留之间的平衡问题。Mavors通过两个核心组件实现对原始视频内容的编码：一是使用3D卷积和视觉变换器的内部块视觉编码器（IVE），以保留高分辨率的空间特征；二是通过基于变换器的依赖建模和块级旋转位置编码的块间特征聚合器（IFA），建立块之间的时间一致性。该框架还通过子图像分解将图像视为单帧视频，从而统一了图像和视频的理解。实验结果表明，Mavors在保持空间保真度和时间连续性方面优于现有方法，特别是在需要细粒度时空推理的任务中表现突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08837",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.08837",
            "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.",
            "score": 17,
            "issue_id": 3237,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "e73823d36c951e4e",
            "authors": [
                "Haozhe Wang",
                "Chao Qu",
                "Zuming Huang",
                "Wei Chu",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "INF.AI",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08837.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#math",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Переосмысление зрения и языка: прорыв в медленном мышлении ИИ",
                    "desc": "Статья описывает новый подход к улучшению мультимодальных рассуждений в моделях машинного обучения с использованием медленного мышления. Авторы адаптируют алгоритм GRPO с техникой выборочного воспроизведения выборки (SSR) для решения проблемы исчезающих преимуществ. Они также вводят метод принудительного переосмысления для стимулирования самоанализа модели. Результатом является модель VL-Rethinker, которая достигает новых рекордных показателей на нескольких бенчмарках в области математики и междисциплинарных задач."
                },
                "en": {
                    "title": "Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning",
                    "desc": "This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "提升视觉-语言模型的慢思考能力",
                    "desc": "本文探讨了如何通过强化学习提升视觉-语言模型的慢思考能力。我们提出了一种新的技术，称为选择性样本重放（SSR），以解决优势消失问题，并结合强制重新思考的方法，增强模型的自我反思能力。通过这两种技术的结合，我们的模型VL-Rethinker在多个数学和科学基准测试中取得了显著的进展。最终，VL-Rethinker在多学科基准测试中也达到了开源的最新水平，缩小了与现有最佳模型的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10479",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "url": "https://huggingface.co/papers/2504.10479",
            "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
            "score": 16,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "51475893ef3c1d8b",
            "authors": [
                "Jinguo Zhu",
                "Weiyun Wang",
                "Zhe Chen",
                "Zhaoyang Liu",
                "Shenglong Ye",
                "Lixin Gu",
                "Yuchen Duan",
                "Hao Tian",
                "Weijie Su",
                "Jie Shao",
                "Zhangwei Gao",
                "Erfei Cui",
                "Yue Cao",
                "Yangzhou Liu",
                "Weiye Xu",
                "Hao Li",
                "Jiahao Wang",
                "Han Lv",
                "Dengnian Chen",
                "Songze Li",
                "Yinan He",
                "Tan Jiang",
                "Jiapeng Luo",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Wenqi Shao",
                "Junjun He",
                "Yingtong Xiong",
                "Wenwen Qu",
                "Peng Sun",
                "Penglong Jiao",
                "Lijun Wu",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Limin Wang",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10479.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "InternVL3: Революция в мультимодальном обучении языковых моделей",
                    "desc": "InternVL3 представляет собой значительный прогресс в области мультимодальных языковых моделей (MLLM). Эта модель объединяет обучение на мультимодальных и текстовых данных в едином процессе предобучения, что позволяет преодолеть сложности, возникающие при традиционном подходе адаптации текстовых моделей. InternVL3 использует ряд передовых техник, включая переменное визуальное позиционное кодирование (V2PE) и смешанную оптимизацию предпочтений (MPO). Модель демонстрирует превосходные результаты на различных мультимодальных задачах, достигая 72.2 балла на бенчмарке MMMU и конкурируя с ведущими проприетарными моделями."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning with InternVL3",
                    "desc": "InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community."
                },
                "zh": {
                    "title": "InternVL3：多模态预训练的新标杆",
                    "desc": "InternVL3是InternVL系列的重要进展，采用了原生的多模态预训练范式。与传统的将文本模型转变为多模态模型不同，InternVL3在单一预训练阶段同时学习多模态和语言能力。该模型通过引入可变视觉位置编码（V2PE）和先进的后训练技术，显著提升了性能和可扩展性。经过广泛的实证评估，InternVL3在多模态任务中表现优异，尤其在MMMU基准测试中取得了72.2的分数，成为开源多模态语言模型的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08942",
            "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
            "url": "https://huggingface.co/papers/2504.08942",
            "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
            "score": 13,
            "issue_id": 3237,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 апреля",
                "en": "April 11",
                "zh": "4月11日"
            },
            "hash": "d756012a0eceafb9",
            "authors": [
                "Xing Han Lù",
                "Amirhossein Kazemnejad",
                "Nicholas Meade",
                "Arkil Patel",
                "Dongchan Shin",
                "Alejandra Zambrano",
                "Karolina Stańczak",
                "Peter Shaw",
                "Christopher J. Pal",
                "Siva Reddy"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Google DeepMind",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique Montréal",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08942.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "AgentRewardBench: новый подход к оценке веб-агентов с помощью LLM",
                    "desc": "Статья представляет AgentRewardBench - первый бенчмарк для оценки эффективности языковых моделей (LLM) в оценке веб-агентов. Бенчмарк содержит 1302 траектории, охватывающие 5 наборов тестов и 4 LLM, каждая из которых проверена экспертом. Исследование показывает, что ни одна LLM не превосходит остальные во всех тестах, а правило-ориентированные методы оценки часто занижают успешность веб-агентов. Авторы подчеркивают необходимость разработки более гибких автоматических методов оценки веб-агентов."
                },
                "en": {
                    "title": "Revolutionizing Web Agent Evaluation with LLMs",
                    "desc": "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."
                },
                "zh": {
                    "title": "评估网络代理的新方法：AgentRewardBench",
                    "desc": "本论文探讨了如何评估网络代理的表现，特别是通过自然语言交互来完成任务的代理。传统的基于规则的方法在扩展新任务时存在困难，并且可能无法准确识别成功的轨迹。我们提出了AgentRewardBench，这是第一个基准测试，用于评估大型语言模型（LLM）在评估网络代理方面的有效性。通过对1302个轨迹的评估，我们发现没有单一的LLM在所有基准上表现优异，这表明需要开发更灵活的自动评估方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 10,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное постобучение LLM с учетом разнородности данных",
                    "desc": "Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают адаптивную стратегию обучения, учитывающую разнородность обучающих данных по сложности и источникам. Ключевая идея заключается в использовании величины преимущества политики для оценки пользы дальнейшего обучения на конкретном распределении данных. Метод применяет принцип Upper Confidence Bound для динамической корректировки вероятностей выборки из разных распределений, что позволяет оптимизировать процесс обучения."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "优化学习效率的分布级课程学习框架",
                    "desc": "本文提出了一种基于分布级学习能力的课程学习框架，旨在优化强化学习（RL）后训练的大型语言模型（LLM）。现有方法通常将训练数据视为统一整体，忽视了数据分布的多样性和复杂性。我们的方法通过动态调整不同分布的采样概率，优先考虑高平均优势或低样本数量的分布，从而提高学习效率。实验结果表明，该框架在逻辑推理数据集上显著提高了收敛速度和最终性能，展示了分布感知课程策略的价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 10,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации",
                    "desc": "Исследование оценивает способности мультимодальной модели GPT-4o от OpenAI в области семантического синтеза с использованием мировых знаний. Авторы анализируют три ключевых аспекта: глобальное следование инструкциям, точность детального редактирования и постгенерационное рассуждение. Результаты показывают, что модель часто интерпретирует инструкции буквально, непоследовательно применяет ограничения, основанные на знаниях, и испытывает трудности с задачами условного рассуждения. Исследование призывает к разработке более надежных методов оценки и стратегий обучения для улучшения мультимодальной генерации."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "提升多模态生成的上下文理解与推理能力",
                    "desc": "本研究评估了OpenAI的多模态模型GPT-4o在图像生成和编辑方面的能力，特别关注其在全球指令遵循、精细编辑精度和生成后推理三个维度的表现。尽管现有基准显示GPT-4o在图像处理上表现强劲，但我们的评估揭示了其在指令理解和知识应用上的局限性。模型常常对指令进行字面解释，知识约束应用不一致，并且在条件推理任务中表现不佳。这些发现挑战了对GPT-4o统一理解和生成能力的普遍假设，强调了需要更强大的基准和训练策略，以实现更具上下文意识和推理基础的多模态生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10368",
            "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10368",
            "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.",
            "score": 9,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "c5995fab2f284493",
            "authors": [
                "Wenyuan Zhang",
                "Shuaiyi Nie",
                "Xinghua Zhang",
                "Zefeng Zhang",
                "Tingwen Liu"
            ],
            "affiliations": [
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10368.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Большие модели рассуждений нуждаются в интуиции",
                    "desc": "S1-Bench - это новый бенчмарк для оценки производительности Больших Моделей Рассуждений (LRM) на простых задачах, требующих интуитивного мышления системы 1. Исследование показало, что LRM демонстрируют низкую эффективность на таких задачах, выдавая ответы в среднем в 15,5 раз длиннее, чем традиционные малые языковые модели. Модели часто находят правильные ответы рано, но продолжают ненужные рассуждения, иногда допуская множество ошибок. Результаты указывают на необходимость развития сбалансированных возможностей двойственного мышления в LRM."
                },
                "en": {
                    "title": "Evaluating Intuition: S1-Bench for Large Reasoning Models",
                    "desc": "The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking."
                },
                "zh": {
                    "title": "评估大型推理模型的直观思维能力",
                    "desc": "我们介绍了S1-Bench，这是一个新颖的基准测试，旨在评估大型推理模型（LRMs）在简单任务上的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。目前缺乏评估LRMs在需要这种能力的任务表现的基准。S1-Bench提供了一组简单、多样且自然清晰的问题，专门设计用于评估LRMs在这些任务中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10415",
            "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.10415",
            "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.",
            "score": 5,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "15b7c5f49cceef01",
            "authors": [
                "Parshin Shojaee",
                "Ngoc-Hieu Nguyen",
                "Kazem Meidani",
                "Amir Barati Farimani",
                "Khoa D Doan",
                "Chandan K Reddy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "VinUniversity",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10415.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "LLM-SRBench: Вызов искусственному интеллекту в открытии научных уравнений",
                    "desc": "Статья представляет LLM-SRBench - новый набор данных для оценки способностей больших языковых моделей (LLM) к открытию научных уравнений. Бенчмарк содержит 239 сложных задач из четырех научных областей, специально разработанных для предотвращения простого запоминания уравнений моделями. LLM-SRBench включает две категории: LSR-Transform с трансформированными физическими моделями и LSR-Synth с синтетическими задачами, требующими рассуждений на основе данных. Тестирование современных методов показало, что лучшая система достигает лишь 31.5% символьной точности, подчеркивая сложность задачи открытия научных уравнений."
                },
                "en": {
                    "title": "LLM-SRBench: A New Frontier in Scientific Equation Discovery",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies."
                },
                "zh": {
                    "title": "科学方程发现的新基准：LLM-SRBench",
                    "desc": "科学方程发现是科学进步中的一项基本任务，能够推导出自然现象的规律。最近，大型语言模型（LLMs）因其利用嵌入科学知识进行假设生成的潜力而受到关注。然而，评估这些方法的真实发现能力仍然具有挑战性，因为现有基准往往依赖于容易被LLMs记忆的常见方程，导致性能指标被夸大。本文介绍了LLM-SRBench，这是一个包含239个具有挑战性问题的综合基准，旨在评估基于LLM的科学方程发现方法，同时防止简单的记忆化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10157",
            "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
            "url": "https://huggingface.co/papers/2504.10157",
            "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "ba0402c49e397963",
            "authors": [
                "Xinnong Zhang",
                "Jiayu Lin",
                "Xinyi Mou",
                "Shiyue Yang",
                "Xiawei Liu",
                "Libo Sun",
                "Hanjia Lyu",
                "Yihang Yang",
                "Weihong Qi",
                "Yue Chen",
                "Guanying Li",
                "Ling Yan",
                "Yao Hu",
                "Siming Chen",
                "Yu Wang",
                "Jingxuan Huang",
                "Jiebo Luo",
                "Shiping Tang",
                "Libo Wu",
                "Baohua Zhou",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "Fudan University",
                "Indiana University",
                "Shanghai Innovation Institute",
                "University of Rochester",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10157.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#social_simulation",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "SocioVerse: виртуальный мир для моделирования общества",
                    "desc": "Статья представляет SocioVerse - модель социальной симуляции на основе больших языковых моделей (LLM) и агентов. Фреймворк включает четыре компонента выравнивания и базу из 10 миллионов реальных пользователей. Авторы провели масштабные эксперименты в трех областях: политика, новости и экономика. Результаты показывают, что SocioVerse способна отражать динамику населения, обеспечивая разнообразие и репрезентативность."
                },
                "en": {
                    "title": "SocioVerse: Enhancing Social Simulations with LLMs",
                    "desc": "This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains."
                },
                "zh": {
                    "title": "SocioVerse：社会模拟的新纪元",
                    "desc": "社会模拟正在通过模拟虚拟个体与环境之间的互动来改变传统社会科学研究。随着大型语言模型（LLMs）的进步，这种方法在捕捉个体差异和预测群体行为方面显示出越来越大的潜力。然而，现有方法在环境、目标用户、互动机制和行为模式方面面临对齐挑战。为此，我们提出了SocioVerse，这是一个基于LLM代理的社会模拟世界模型，具有强大的对齐组件和1000万真实个体的用户池。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09641",
            "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
            "url": "https://huggingface.co/papers/2504.09641",
            "abstract": "Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "b7c9f390686ff6ef",
            "authors": [
                "Xingjian Zhang",
                "Siwei Wen",
                "Wenjun Wu",
                "Lei Huang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University",
                "Hangzhou International Innovation Institute, Beihang University, Hangzhou, China",
                "SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09641.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#rl",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Маленькая модель - большие рассуждения: TinyLLaVA-Video-R1 для анализа видео",
                    "desc": "Статья представляет TinyLLaVA-Video-R1 - небольшую мультимодальную модель для рассуждений по видео. Модель обучена с помощью обучения с подкреплением на наборах данных для ответов на вопросы по видео. TinyLLaVA-Video-R1 демонстрирует улучшенные способности к рассуждениям и мышлению, а также проявляет эффект 'ага-момента'. Авторы делятся экспериментальными результатами для дальнейших исследований возможностей рассуждений по видео в небольших моделях."
                },
                "en": {
                    "title": "Empowering Small Models for Big Reasoning in Video Understanding",
                    "desc": "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."
                },
                "zh": {
                    "title": "小模型也能推理，TinyLLaVA-Video-R1助力视频理解！",
                    "desc": "最近，通过强化学习提高大型多模态模型（LMMs）的推理能力取得了显著进展。然而，大多数现有研究基于高度推理密集的数据集，如数学和代码，且通常选择大规模模型作为基础。我们认为，探索小规模模型的推理能力对计算资源有限的研究者仍然具有重要价值。此外，使模型能够解释其在一般问答数据集上的推理过程同样具有意义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09689",
            "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
            "url": "https://huggingface.co/papers/2504.09689",
            "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent",
            "score": 2,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "11b21970119f2c59",
            "authors": [
                "Jiahao Qiu",
                "Yinghui He",
                "Xinzhe Juan",
                "Yiming Wang",
                "Yuhan Liu",
                "Zixin Yao",
                "Yue Wu",
                "Xun Jiang",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Computer Science, Princeton University",
                "Department of Data Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University",
                "Department of Philosophy, Columbia University",
                "Theta Health Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09689.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#ethics",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Защита психического здоровья в эпоху ИИ-персонажей",
                    "desc": "Статья представляет EmoAgent - мультиагентную систему искусственного интеллекта для оценки и снижения рисков для психического здоровья при взаимодействии человека с ИИ-персонажами. EmoAgent состоит из двух компонентов: EmoEval, который симулирует виртуальных пользователей для оценки изменений психического состояния, и EmoGuard, который выступает посредником, отслеживающим психическое состояние пользователей. Эксперименты показали, что эмоционально вовлекающие диалоги могут привести к ухудшению психологического состояния уязвимых пользователей. EmoGuard значительно снижает эти риски, обеспечивая более безопасное взаимодействие человека с ИИ."
                },
                "en": {
                    "title": "Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent",
                    "desc": "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."
                },
                "zh": {
                    "title": "EmoAgent：保障人机交互心理安全的智能框架",
                    "desc": "随着大型语言模型驱动的人工智能角色的兴起，特别是对心理障碍的脆弱用户，安全问题引起了关注。为了解决这些风险，我们提出了EmoAgent，这是一个多智能体的人工智能框架，旨在评估和减轻人机交互中的心理健康危害。EmoAgent包括两个组件：EmoEval模拟虚拟用户，评估与AI角色交互前后的心理健康变化，并使用经过临床验证的心理评估工具进行评估。EmoGuard作为中介，监测用户的心理状态，预测潜在伤害，并提供纠正反馈，以降低风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09763",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
            "url": "https://huggingface.co/papers/2504.09763",
            "abstract": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.",
            "score": 1,
            "issue_id": 3240,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "c4ae0eadf040035f",
            "authors": [
                "Zaid Khan",
                "Elias Stengel-Eskin",
                "Archiki Prasad",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09763.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#data",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Автоматическая генерация абстракций для продвинутых математических задач",
                    "desc": "Исследователи представили концепцию EFA (Executable Functional Abstraction) - исполняемых функциональных абстракций для математических задач. Они разработали систему EFAGen, использующую большую языковую модель (LLM) для автоматического создания EFA на основе исходной задачи и ее пошагового решения. EFAGen способна генерировать EFA для сложных математических задач уровня олимпиад, сохраняя верность исходной проблеме. Созданные EFA могут использоваться для генерации вариаций задач различной сложности и тестирования моделей машинного обучения."
                },
                "en": {
                    "title": "Automating Advanced Math Problem Generation with EFAs",
                    "desc": "This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners."
                },
                "zh": {
                    "title": "自动生成高级数学问题的可执行功能抽象",
                    "desc": "本文介绍了一种名为可执行功能抽象（EFA）的程序，这些程序能够从特定的数学问题实例中推导出抽象过程，并生成新的相关实例。我们提出了一种自动构建高级数学问题的EFA的方法，称为EFAGen，它利用大型语言模型（LLM）根据种子数学问题及其逐步解决方案生成候选EFA程序。通过可执行单元测试，我们定义了有效EFA必须具备的属性，并展示了如何利用这些测试作为可验证的奖励来训练LLM，提高其EFA编写能力。最终，我们证明了EFAGen生成的EFA能够保持与种子问题的一致性，并能够生成适合学习者的不同难度的数学问题变体。"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4月16日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 7,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0,
        "#social_simulation": 1
    },
    "zh": {
        "text": "这篇技术报告介绍了训练视频生成基础模型的成本效益策略。报告展示了一个名为 Seaweed-7B 的中型研究模型，拥有约70亿参数，使用665,000个H100 GPU小时从头训练。尽管使用的计算资源适中，Seaweed-7B 仍表现出与更大模型相媲美的性能。设计选择在资源受限的环境中尤为重要。报告强调了提升中型扩散模型性能的关键设计决策。经验上，有两个观察结果：(1) Seaweed-7B 的性能与或超过使用更多GPU资源训练的大型模型；(2) 该模型展示出强大的泛化能力，可通过轻量微调或继续训练有效适应多种下游应用。项目页面见 https://seaweed.video/。",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "pinyin": "Zhè piān jìshù bàogào jièshào le xùnliàn shìpǐn shēngchéng jīchǔ móxíng de chéngběn xiàoyì cèlüè. Bàogào zhǎnshì le yīgè míngwèi Seaweed-7B de zhōngxíng yánjiū móxíng, yǒngyǒu yuē 70 yì cānshù, shǐyòng 665,000 gè H100 GPU xiǎoshí cóngtóu xùnliàn. Jīnrán shǐyòng de jìsuàn zīyuán shìguì zhōngděng, Seaweed-7B réng biǎoxiàn chū yǔ gèng dà móxíng xiàng bǐměi de xíngnéng. Shèjì xuǎnzé zài zīyuán shòuxiàn de huánjìng zhōng yóuwèi zhòngyào. Bàogào qiángdiǎo le tíshēng zhōngxíng kuòsàn móxíng xíngnéng de guǎnjiàn shèjì juécè. Jīngyàn shàng, yǒu liǎng gè guānchá jiéguǒ: (1) Seaweed-7B de xíngnéng yǔ huò chāoguò shǐyòng gèng duō GPU zīyuán xùnliàn de dàxíng móxíng; (2) gè móxíng zhǎnshì chū qiángdà de fànhuà nénglì, kě tōngguò qīngliàng wēitiáo huò jìxù xùnliàn yǒuxiào shìyìng duōzhǒng xiàyóu yìngyòng. Xiàngmù yèmiàn jiàn https://seaweed.video/。",
        "vocab": "[\n    {\"word\": \"技术报告\", \"pinyin\": \"jìshù bàogào\", \"trans\": \"technical report\"},\n    {\"word\": \"视频生成\", \"pinyin\": \"shìpín shēngchéng\", \"trans\": \"video generation\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jīchǔ móxíng\", \"trans\": \"foundational model\"},\n    {\"word\": \"成本效益\", \"pinyin\": \"chéngběn xiàoyì\", \"trans\": \"cost-effectiveness\"},\n    {\"word\": \"策略\", \"pinyin\": \"cèlüè\", \"trans\": \"strategy\"},\n    {\"word\": \"中型\", \"pinyin\": \"zhōngxíng\", \"trans\": \"medium-sized\"},\n    {\"word\": \"研究模型\", \"pinyin\": \"yánjiū móxíng\", \"trans\": \"research model\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"从头训练\", \"pinyin\": \"cóngtóu xùnliàn\", \"trans\": \"train from scratch\"},\n    {\"word\": \"计算资源\", \"pinyin\": \"jìsuàn zīyuán\", \"trans\": \"computational resources\"},\n    {\"word\": \"适中\", \"pinyin\": \"shìzhōng\", \"trans\": \"moderate\"},\n    {\"word\": \"媲美\", \"pinyin\": \"pìměi\", \"trans\": \"rival\"},\n    {\"word\": \"设计选择\", \"pinyin\": \"shèjì xuǎnzé\", \"trans\": \"design choices\"},\n    {\"word\": \"受限\", \"pinyin\": \"shòuxiàn\", \"trans\": \"constrained\"},\n    {\"word\": \"环境\", \"pinyin\": \"huánjìng\", \"trans\": \"environment\"},\n    {\"word\": \"尤为\", \"pinyin\": \"yóuwéi\", \"trans\": \"especially\"},\n    {\"word\": \"重要\", \"pinyin\": \"zhòngyào\", \"trans\": \"important\"},\n    {\"word\": \"强调\", \"pinyin\": \"qiángdiào\", \"trans\": \"emphasize\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"enhance\"},\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuòsàn móxíng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"性能\", \"pinyin\": \"xíngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"设计决策\", \"pinyin\": \"shèjì juécè\", \"trans\": \"design decisions\"},\n    {\"word\": \"经验\", \"pinyin\": \"jīngyàn\", \"trans\": \"experience\"},\n    {\"word\": \"观察结果\", \"pinyin\": \"guānchá jiéguǒ\", \"trans\": \"observation results\"},\n    {\"word\": \"泛化能力\", \"pinyin\": \"fànhuà nénglì\", \"trans\": \"generalization capability\"},\n    {\"word\": \"轻量微调\", \"pinyin\": \"qīngliàng wēitiáo\", \"trans\": \"lightweight fine-tuning\"},\n    {\"word\": \"继续训练\", \"pinyin\": \"jìxù xùnliàn\", \"trans\": \"continued training\"},\n    {\"word\": \"下游应用\", \"pinyin\": \"xiàyóu yìngyòng\", \"trans\": \"downstream applications\"},\n    {\"word\": \"项目页面\", \"pinyin\": \"xiàngmù yèmiàn\", \"trans\": \"project page\"}\n]",
        "trans": "This technical report introduces cost-effective strategies for training foundational video generation models. The report presents a mid-sized research model named Seaweed-7B, which has approximately 7 billion parameters and was trained from scratch using 665,000 H100 GPU hours. Despite the moderate computational resources used, Seaweed-7B demonstrates performance comparable to larger models. Design choices are particularly important in resource-constrained environments. The report emphasizes key design decisions that enhance the performance of mid-sized diffusion models. Empirically, there are two observations: (1) Seaweed-7B's performance matches or exceeds that of larger models trained with more GPU resources; (2) The model demonstrates strong generalization capabilities and can effectively adapt to various downstream applications through lightweight fine-tuning or continued training. For more information, visit the project page at https://seaweed.video/.",
        "update_ts": "2025-04-14 09:12"
    }
}