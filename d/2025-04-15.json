{
    "date": {
        "ru": "15 апреля",
        "en": "April 15",
        "zh": "4月15日"
    },
    "time_utc": "2025-04-15 21:10",
    "weekday": 1,
    "issue_id": 3254,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10479",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "url": "https://huggingface.co/papers/2504.10479",
            "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
            "score": 164,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "51475893ef3c1d8b",
            "authors": [
                "Jinguo Zhu",
                "Weiyun Wang",
                "Zhe Chen",
                "Zhaoyang Liu",
                "Shenglong Ye",
                "Lixin Gu",
                "Yuchen Duan",
                "Hao Tian",
                "Weijie Su",
                "Jie Shao",
                "Zhangwei Gao",
                "Erfei Cui",
                "Yue Cao",
                "Yangzhou Liu",
                "Weiye Xu",
                "Hao Li",
                "Jiahao Wang",
                "Han Lv",
                "Dengnian Chen",
                "Songze Li",
                "Yinan He",
                "Tan Jiang",
                "Jiapeng Luo",
                "Yi Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Wenqi Shao",
                "Junjun He",
                "Yingtong Xiong",
                "Wenwen Qu",
                "Peng Sun",
                "Penglong Jiao",
                "Lijun Wu",
                "Kaipeng Zhang",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Limin Wang",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10479.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "InternVL3: Революция в мультимодальном обучении языковых моделей",
                    "desc": "InternVL3 представляет собой значительный прогресс в области мультимодальных языковых моделей (MLLM). Эта модель объединяет обучение на мультимодальных и текстовых данных в едином процессе предобучения, что позволяет преодолеть сложности, возникающие при традиционном подходе адаптации текстовых моделей. InternVL3 использует ряд передовых техник, включая переменное визуальное позиционное кодирование (V2PE) и смешанную оптимизацию предпочтений (MPO). Модель демонстрирует превосходные результаты на различных мультимодальных задачах, достигая 72.2 балла на бенчмарке MMMU и конкурируя с ведущими проприетарными моделями."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning with InternVL3",
                    "desc": "InternVL3 is a new multimodal large language model (MLLM) that learns from both visual and text data simultaneously during its training. This approach helps it overcome common challenges faced by traditional models that are adapted from text-only systems. It uses innovative techniques like variable visual position encoding and advanced fine-tuning methods to enhance its performance on various tasks. The model has achieved impressive results, outperforming many existing models while also committing to open science by sharing its resources with the research community."
                },
                "zh": {
                    "title": "InternVL3：多模态预训练的新标杆",
                    "desc": "InternVL3是InternVL系列的重要进展，采用了原生的多模态预训练范式。与传统的将文本模型转变为多模态模型不同，InternVL3在单一预训练阶段同时学习多模态和语言能力。该模型通过引入可变视觉位置编码（V2PE）和先进的后训练技术，显著提升了性能和可扩展性。经过广泛的实证评估，InternVL3在多模态任务中表现优异，尤其在MMMU基准测试中取得了72.2的分数，成为开源多模态语言模型的新标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08791",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "url": "https://huggingface.co/papers/2504.08791",
            "abstract": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.",
            "score": 91,
            "issue_id": 3236,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "2d5649ec3925b1a3",
            "authors": [
                "Zonghang Li",
                "Tao Li",
                "Wenjiao Feng",
                "Mohsen Guizani",
                "Hongfang Yu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",
                "University of Electronic Science and Technology of China, Chengdu, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08791.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "Продвинутые языковые модели теперь доступны на домашних устройствах",
                    "desc": "Статья представляет prima.cpp - распределенную систему вывода, позволяющую запускать крупномасштабные языковые модели (до 70 миллиардов параметров) на обычных домашних устройствах. Система использует комбинацию CPU/GPU, низкие требования к RAM/VRAM и поддержку Wi-Fi для эффективной работы. Prima.cpp применяет технологию mmap для управления весами модели и вводит конвейерный кольцевой параллелизм с предвыборкой для скрытия загрузки с диска. Авторы предлагают алгоритм Halda для оптимального распределения слоев модели между устройствами, учитывая их гетерогенность."
                },
                "en": {
                    "title": "Bringing Powerful AI to Your Home Devices",
                    "desc": "This paper presents prima.cpp, a novel distributed inference system designed to run large language models (LLMs) on standard home devices. It leverages a combination of CPU and GPU resources, along with efficient memory management techniques like mmap and piped-ring parallelism, to optimize performance. By intelligently assigning model layers based on the capabilities of each device, it significantly reduces latency while maintaining low memory usage. The system demonstrates superior performance compared to existing solutions, making advanced AI models accessible for personal use."
                },
                "zh": {
                    "title": "让家庭设备也能运行大型语言模型",
                    "desc": "本文介绍了一种名为prima.cpp的分布式推理系统，能够在普通家庭设备上运行70B规模的语言模型。该系统通过混合使用CPU和GPU，优化内存和带宽的使用，解决了传统方案对高性能硬件的依赖。它采用了mmap管理模型权重，并引入了管道环并行和预取技术，以减少磁盘加载时间。通过优化计算、通信和内存管理，prima.cpp显著降低了延迟，使得先进的AI模型能够在家庭助手中普及。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09925",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "url": "https://huggingface.co/papers/2504.09925",
            "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "score": 34,
            "issue_id": 3236,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "948c65f51f6a11b7",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Jingzhou Chen",
                "Jingwei Xu",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09925.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "FUSION: Глубокая интеграция зрения и языка в мультимодальных ИИ-моделях",
                    "desc": "FUSION - это семейство мультимодальных больших языковых моделей с полной интеграцией зрения и языка. Модель использует текстово-управляемое унифицированное кодирование изображений и контекстно-зависимое рекурсивное декодирование для глубокой интеграции модальностей. Авторы разработали специальную функцию потерь и синтетический набор данных для оптимизации процесса обучения. FUSION превосходит существующие методы, демонстрируя эффективность подхода полной интеграции модальностей."
                },
                "en": {
                    "title": "FUSION: Deep Integration of Vision and Language for Enhanced Understanding",
                    "desc": "FUSION is a new type of multimodal large language model (MLLM) that integrates vision and language more effectively than previous models. It uses a method called Text-Guided Unified Vision Encoding to combine text and visual information at a very detailed level, allowing for better understanding of images in context. The model also features Context-Aware Recursive Alignment Decoding, which helps it to refine visual features based on the text it is processing. With a focus on high-quality question-answer pairs, FUSION shows significant improvements over existing models in various benchmarks, even with fewer visual tokens."
                },
                "zh": {
                    "title": "FUSION：深度集成的多模态语言模型",
                    "desc": "我们介绍了FUSION，这是一种多模态大型语言模型（MLLM），采用完全的视觉-语言对齐和集成范式。与现有方法主要依赖于LLM解码过程中的后期模态交互不同，我们的方法在整个处理流程中实现了深度、动态的集成。我们提出了文本引导的统一视觉编码，将文本信息融入视觉编码，实现像素级的集成。此外，我们设计了上下文感知的递归对齐解码，能够在解码过程中根据文本上下文递归聚合视觉特征，从而实现细粒度的问题级语义集成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08837",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.08837",
            "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.",
            "score": 34,
            "issue_id": 3237,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "e73823d36c951e4e",
            "authors": [
                "Haozhe Wang",
                "Chao Qu",
                "Zuming Huang",
                "Wei Chu",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "INF.AI",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08837.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#math",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Переосмысление зрения и языка: прорыв в медленном мышлении ИИ",
                    "desc": "Статья описывает новый подход к улучшению мультимодальных рассуждений в моделях машинного обучения с использованием медленного мышления. Авторы адаптируют алгоритм GRPO с техникой выборочного воспроизведения выборки (SSR) для решения проблемы исчезающих преимуществ. Они также вводят метод принудительного переосмысления для стимулирования самоанализа модели. Результатом является модель VL-Rethinker, которая достигает новых рекордных показателей на нескольких бенчмарках в области математики и междисциплинарных задач."
                },
                "en": {
                    "title": "Enhancing Slow-Thinking in Vision-Language Models with Reinforcement Learning",
                    "desc": "This paper presents VL-Rethinker, a vision-language model that enhances slow-thinking capabilities through reinforcement learning. It introduces Selective Sample Replay (SSR) to tackle the vanishing advantages problem in reinforcement learning, improving performance on math and science benchmarks. Additionally, the model incorporates Forced Rethinking, which adds a self-reflection step during training to promote deeper reasoning. As a result, VL-Rethinker achieves state-of-the-art scores on multiple benchmarks, demonstrating significant advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "提升视觉-语言模型的慢思考能力",
                    "desc": "本文探讨了如何通过强化学习提升视觉-语言模型的慢思考能力。我们提出了一种新的技术，称为选择性样本重放（SSR），以解决优势消失问题，并结合强制重新思考的方法，增强模型的自我反思能力。通过这两种技术的结合，我们的模型VL-Rethinker在多个数学和科学基准测试中取得了显著的进展。最终，VL-Rethinker在多学科基准测试中也达到了开源的最新水平，缩小了与现有最佳模型的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08003",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "url": "https://huggingface.co/papers/2504.08003",
            "abstract": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.",
            "score": 34,
            "issue_id": 3236,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "7b12ba874d92915a",
            "authors": [
                "Ning Li",
                "Jingran Zhang",
                "Justin Cui"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08003.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GPT-4o: Ограничения в семантическом синтезе и необходимость улучшения мультимодальной генерации",
                    "desc": "Исследование оценивает способности мультимодальной модели GPT-4o от OpenAI в области семантического синтеза с использованием мировых знаний. Авторы анализируют три ключевых аспекта: глобальное следование инструкциям, точность детального редактирования и постгенерационное рассуждение. Результаты показывают, что модель часто интерпретирует инструкции буквально, непоследовательно применяет ограничения, основанные на знаниях, и испытывает трудности с задачами условного рассуждения. Исследование призывает к разработке более надежных методов оценки и стратегий обучения для улучшения мультимодальной генерации."
                },
                "en": {
                    "title": "Bridging the Gaps in Multimodal Understanding",
                    "desc": "This paper evaluates OpenAI's multimodal model, GPT-4o, focusing on its ability to integrate knowledge and reasoning in image generation and editing. The study examines three key areas: how well the model follows global instructions, its precision in fine-grained editing, and its reasoning after generating images. Despite strong performance in generating images, the model often misinterprets instructions, applies knowledge inconsistently, and struggles with tasks requiring conditional reasoning. The authors suggest that improvements in training and evaluation methods are needed to enhance the model's contextual understanding and reasoning capabilities."
                },
                "zh": {
                    "title": "提升多模态生成的上下文理解与推理能力",
                    "desc": "本研究评估了OpenAI的多模态模型GPT-4o在图像生成和编辑方面的能力，特别关注其在全球指令遵循、精细编辑精度和生成后推理三个维度的表现。尽管现有基准显示GPT-4o在图像处理上表现强劲，但我们的评估揭示了其在指令理解和知识应用上的局限性。模型常常对指令进行字面解释，知识约束应用不一致，并且在条件推理任务中表现不佳。这些发现挑战了对GPT-4o统一理解和生成能力的普遍假设，强调了需要更强大的基准和训练策略，以实现更具上下文意识和推理基础的多模态生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09643",
            "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
            "url": "https://huggingface.co/papers/2504.09643",
            "abstract": "Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.",
            "score": 29,
            "issue_id": 3245,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "9f5e20f45a50902d",
            "authors": [
                "Nikita Sorokin",
                "Ivan Sedykh",
                "Valentin Malykh"
            ],
            "affiliations": [
                "International IT University",
                "MTS AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09643.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#training",
                    "#plp"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Самообучение ранжированию для улучшения генерации кода",
                    "desc": "Статья предлагает новый метод итеративного самообучения для улучшения ранжирования и генерации кода с использованием Proximal Policy Optimization (PPO). Подход фокусируется на разработке надежной модели ранжирования, которая улучшает качество генерируемого кода путем переранжирования и устранения ошибок. Метод итеративно уточняет обучающий набор данных, переоценивая выходные данные и включая высокооцененные отрицательные примеры в цикл обучения. Оценка на наборе данных MultiPL-E показывает, что 13.4B-параметровая модель превосходит 33B модель по качеству генерации кода и сравнима с GPT-4."
                },
                "en": {
                    "title": "Enhancing Code Generation with Smart Reranking",
                    "desc": "This paper addresses the challenges of generating high-quality code using decoder-based models, which often produce unpredictable outputs. It introduces a novel approach that combines a code generation model with a reranker model to select the best solutions from multiple generated samples. The authors propose an iterative self-training method using Proximal Policy Optimization (PPO) to enhance the reranking accuracy and overall code generation process. Their results show that their model, with 13.4 billion parameters, outperforms larger models in both quality and speed, achieving results comparable to GPT-4 in certain programming languages."
                },
                "zh": {
                    "title": "提升代码生成质量的创新方法",
                    "desc": "本文探讨了高质量代码生成的挑战，尤其是在当前基于解码器的模型中，输出结果具有高度随机性。通过结合代码生成模型和重排序模型，可以显著提高生成代码的质量。我们提出了一种新颖的自我训练方法，利用近端策略优化（PPO）来提升重排序模型的准确性和整体代码生成过程。实验结果表明，我们的模型在代码生成质量上优于更大参数的模型，并且在速度上也有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10068",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
            "url": "https://huggingface.co/papers/2504.10068",
            "abstract": "Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.",
            "score": 25,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "bbb7251e84f61649",
            "authors": [
                "Yang Shi",
                "Jiaheng Liu",
                "Yushuo Guan",
                "Zhenhua Wu",
                "Yuanxing Zhang",
                "Zihao Wang",
                "Weihong Lin",
                "Jingyun Hua",
                "Zekun Wang",
                "Xinlong Chen",
                "Bohan Zeng",
                "Wentao Zhang",
                "Fuzheng Zhang",
                "Wenjing Yang",
                "Di Zhang"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10068.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#architecture",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Mavors: Эффективное понимание длинных видео с сохранением мелких деталей",
                    "desc": "Статья представляет новый подход Mavors для понимания длинных видео в мультимодальных больших языковых моделях. Mavors использует многоуровневое представление видео, сочетая внутрикадровое кодирование высокого разрешения и межкадровую агрегацию признаков. Этот метод позволяет сохранить как пространственные детали, так и временную динамику в видео. Эксперименты показывают превосходство Mavors над существующими методами в задачах, требующих детального пространственно-временного анализа."
                },
                "en": {
                    "title": "Mavors: Enhancing Long-Context Video Understanding with Multi-Granularity Representation",
                    "desc": "This paper presents Mavors, a new framework designed to improve long-context video understanding in multimodal large language models (MLLMs). Mavors addresses the challenge of maintaining fine-grained spatio-temporal patterns while ensuring computational efficiency. It utilizes an Intra-chunk Vision Encoder to capture high-resolution spatial features and an Inter-chunk Feature Aggregator to ensure temporal coherence across video segments. The framework also integrates image and video understanding by treating images as single-frame videos, leading to superior performance in tasks that require detailed spatio-temporal reasoning."
                },
                "zh": {
                    "title": "Mavors：长视频理解的新突破",
                    "desc": "本论文提出了一种名为Mavors的新框架，旨在解决多模态大语言模型在长视频理解中的计算效率与细粒度时空模式保留之间的平衡问题。Mavors通过两个核心组件实现对原始视频内容的编码：一是使用3D卷积和视觉变换器的内部块视觉编码器（IVE），以保留高分辨率的空间特征；二是通过基于变换器的依赖建模和块级旋转位置编码的块间特征聚合器（IFA），建立块之间的时间一致性。该框架还通过子图像分解将图像视为单帧视频，从而统一了图像和视频的理解。实验结果表明，Mavors在保持空间保真度和时间连续性方面优于现有方法，特别是在需要细粒度时空推理的任务中表现突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08942",
            "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
            "url": "https://huggingface.co/papers/2504.08942",
            "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
            "score": 17,
            "issue_id": 3237,
            "pub_date": "2025-04-11",
            "pub_date_card": {
                "ru": "11 апреля",
                "en": "April 11",
                "zh": "4月11日"
            },
            "hash": "d756012a0eceafb9",
            "authors": [
                "Xing Han Lù",
                "Amirhossein Kazemnejad",
                "Nicholas Meade",
                "Arkil Patel",
                "Dongchan Shin",
                "Alejandra Zambrano",
                "Karolina Stańczak",
                "Peter Shaw",
                "Christopher J. Pal",
                "Siva Reddy"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Google DeepMind",
                "McGill University",
                "Mila Quebec AI Institute",
                "Polytechnique Montréal",
                "ServiceNow Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08942.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "AgentRewardBench: новый подход к оценке веб-агентов с помощью LLM",
                    "desc": "Статья представляет AgentRewardBench - первый бенчмарк для оценки эффективности языковых моделей (LLM) в оценке веб-агентов. Бенчмарк содержит 1302 траектории, охватывающие 5 наборов тестов и 4 LLM, каждая из которых проверена экспертом. Исследование показывает, что ни одна LLM не превосходит остальные во всех тестах, а правило-ориентированные методы оценки часто занижают успешность веб-агентов. Авторы подчеркивают необходимость разработки более гибких автоматических методов оценки веб-агентов."
                },
                "en": {
                    "title": "Revolutionizing Web Agent Evaluation with LLMs",
                    "desc": "This paper introduces AgentRewardBench, a new benchmark designed to evaluate the effectiveness of large language models (LLMs) in assessing web agents' performance. Traditional rule-based evaluation methods struggle with flexibility and often fail to accurately identify successful task completions. By comparing 12 different LLM judges across 1302 trajectories, the study reveals that no single LLM consistently outperforms others in all scenarios. The findings emphasize the limitations of rule-based evaluations and advocate for more adaptable automatic evaluation methods to better assess web agents."
                },
                "zh": {
                    "title": "评估网络代理的新方法：AgentRewardBench",
                    "desc": "本论文探讨了如何评估网络代理的表现，特别是通过自然语言交互来完成任务的代理。传统的基于规则的方法在扩展新任务时存在困难，并且可能无法准确识别成功的轨迹。我们提出了AgentRewardBench，这是第一个基准测试，用于评估大型语言模型（LLM）在评估网络代理方面的有效性。通过对1302个轨迹的评估，我们发现没有单一的LLM在所有基准上表现优异，这表明需要开发更灵活的自动评估方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10368",
            "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10368",
            "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.",
            "score": 16,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "c5995fab2f284493",
            "authors": [
                "Wenyuan Zhang",
                "Shuaiyi Nie",
                "Xinghua Zhang",
                "Zefeng Zhang",
                "Tingwen Liu"
            ],
            "affiliations": [
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "School of Cyber Security, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10368.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Большие модели рассуждений нуждаются в интуиции",
                    "desc": "S1-Bench - это новый бенчмарк для оценки производительности Больших Моделей Рассуждений (LRM) на простых задачах, требующих интуитивного мышления системы 1. Исследование показало, что LRM демонстрируют низкую эффективность на таких задачах, выдавая ответы в среднем в 15,5 раз длиннее, чем традиционные малые языковые модели. Модели часто находят правильные ответы рано, но продолжают ненужные рассуждения, иногда допуская множество ошибок. Результаты указывают на необходимость развития сбалансированных возможностей двойственного мышления в LRM."
                },
                "en": {
                    "title": "Evaluating Intuition: S1-Bench for Large Reasoning Models",
                    "desc": "The paper introduces S1-Bench, a benchmark aimed at assessing Large Reasoning Models (LRMs) on tasks that require quick, intuitive responses, akin to system 1 thinking. It highlights that while LRMs excel in complex reasoning, they struggle with simpler tasks that demand rapid decision-making. The study evaluates 22 LRMs, revealing that they often produce longer outputs and unnecessary deliberation, even when they identify correct answers early. These results indicate that current LRMs exhibit rigid reasoning patterns, suggesting a need for further development to enhance their ability to balance intuitive and analytical thinking."
                },
                "zh": {
                    "title": "评估大型推理模型的直观思维能力",
                    "desc": "我们介绍了S1-Bench，这是一个新颖的基准测试，旨在评估大型推理模型（LRMs）在简单任务上的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管LRMs在复杂推理任务中取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。目前缺乏评估LRMs在需要这种能力的任务表现的基准。S1-Bench提供了一组简单、多样且自然清晰的问题，专门设计用于评估LRMs在这些任务中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09710",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "url": "https://huggingface.co/papers/2504.09710",
            "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
            "score": 13,
            "issue_id": 3236,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "1d7a588a7370ed5c",
            "authors": [
                "Zhenting Wang",
                "Guofeng Cui",
                "Kun Wan",
                "Wentian Zhao"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09710.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное постобучение LLM с учетом разнородности данных",
                    "desc": "Статья представляет новый подход к постобучению больших языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают адаптивную стратегию обучения, учитывающую разнородность обучающих данных по сложности и источникам. Ключевая идея заключается в использовании величины преимущества политики для оценки пользы дальнейшего обучения на конкретном распределении данных. Метод применяет принцип Upper Confidence Bound для динамической корректировки вероятностей выборки из разных распределений, что позволяет оптимизировать процесс обучения."
                },
                "en": {
                    "title": "Adaptive Learning for Enhanced Reasoning in Language Models",
                    "desc": "This paper introduces a new approach to improve large language models (LLMs) using reinforcement learning (RL) by focusing on the diverse sources and difficulties of training data. It highlights the importance of adapting the training process to different data distributions, rather than treating all data as the same. The authors propose a curriculum learning framework that uses the Upper Confidence Bound (UCB) principle to prioritize training on data distributions that either have high potential for improvement or are underrepresented. Their experiments show that this method enhances the efficiency and effectiveness of LLM post-training, particularly in complex reasoning tasks."
                },
                "zh": {
                    "title": "优化学习效率的分布级课程学习框架",
                    "desc": "本文提出了一种基于分布级学习能力的课程学习框架，旨在优化强化学习（RL）后训练的大型语言模型（LLM）。现有方法通常将训练数据视为统一整体，忽视了数据分布的多样性和复杂性。我们的方法通过动态调整不同分布的采样概率，优先考虑高平均优势或低样本数量的分布，从而提高学习效率。实验结果表明，该框架在逻辑推理数据集上显著提高了收敛速度和最终性能，展示了分布感知课程策略的价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10127",
            "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
            "url": "https://huggingface.co/papers/2504.10127",
            "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.",
            "score": 12,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "604dff752f16abf2",
            "authors": [
                "Junlei Zhang",
                "Zichen Ding",
                "Chang Ma",
                "Zijie Chen",
                "Qiushi Sun",
                "Zhenzhong Lan",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10127.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение GUI-агентов через обучение на разнообразных задачах рассуждения",
                    "desc": "Статья предлагает новый подход к обучению агентов графического пользовательского интерфейса (GUI) с использованием моделей компьютерного зрения и обработки естественного языка (VLM). Авторы исследуют влияние различных задач среднего уровня обучения на производительность GUI-агентов, включая восприятие интерфейса, мультимодальные рассуждения и текстовые рассуждения. Результаты показывают, что обучение на задачах рассуждения, особенно математических, значительно улучшает производительность агентов в различных GUI-средах. Исследование демонстрирует эффективность переноса знаний между различными доменами и предлагает практический подход к решению проблемы нехватки данных в области GUI-агентов."
                },
                "en": {
                    "title": "Enhancing GUI Agents through Vision Language Model Training",
                    "desc": "This paper discusses how to improve the performance of Graphical User Interface (GUI) agents by using Vision Language Models (VLMs) trained on various reasoning tasks. The authors found that training on data-rich tasks helps these models generalize better to GUI planning scenarios, leading to significant performance improvements. They conducted experiments showing that tasks like multimodal reasoning and text-based mathematical data can enhance GUI agent performance more than traditional GUI perception data. The study highlights the importance of cross-domain knowledge transfer and provides a method to overcome data scarcity in training GUI agents."
                },
                "zh": {
                    "title": "提升GUI代理性能的关键在于任务泛化",
                    "desc": "本论文探讨了图形用户界面（GUI）代理在自动化复杂数字任务中的应用，尤其是在数据稀缺的情况下如何提升其性能。我们提出在专门的中期训练阶段，利用丰富的数据和推理密集型任务来训练视觉语言模型（VLMs），以促进其在GUI规划场景中的泛化能力。通过对11个中期训练任务的广泛实验，我们发现任务泛化显著提高了性能，尤其是多模态数学推理对AndroidWorld的性能提升达到了6.3%。此外，我们还发现GUI感知数据对最终性能的影响有限，并提出了优化的混合数据集，以实现更大的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10157",
            "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
            "url": "https://huggingface.co/papers/2504.10157",
            "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
            "score": 11,
            "issue_id": 3237,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "ba0402c49e397963",
            "authors": [
                "Xinnong Zhang",
                "Jiayu Lin",
                "Xinyi Mou",
                "Shiyue Yang",
                "Xiawei Liu",
                "Libo Sun",
                "Hanjia Lyu",
                "Yihang Yang",
                "Weihong Qi",
                "Yue Chen",
                "Guanying Li",
                "Ling Yan",
                "Yao Hu",
                "Siming Chen",
                "Yu Wang",
                "Jingxuan Huang",
                "Jiebo Luo",
                "Shiping Tang",
                "Libo Wu",
                "Baohua Zhou",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "Fudan University",
                "Indiana University",
                "Shanghai Innovation Institute",
                "University of Rochester",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10157.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#social_simulation",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "SocioVerse: виртуальный мир для моделирования общества",
                    "desc": "Статья представляет SocioVerse - модель социальной симуляции на основе больших языковых моделей (LLM) и агентов. Фреймворк включает четыре компонента выравнивания и базу из 10 миллионов реальных пользователей. Авторы провели масштабные эксперименты в трех областях: политика, новости и экономика. Результаты показывают, что SocioVerse способна отражать динамику населения, обеспечивая разнообразие и репрезентативность."
                },
                "en": {
                    "title": "SocioVerse: Enhancing Social Simulations with LLMs",
                    "desc": "This paper presents SocioVerse, a novel framework that utilizes large language models (LLMs) to enhance social simulations by modeling human interactions and behaviors. The framework addresses alignment challenges by incorporating four key components that ensure accurate representation of diverse user behaviors and environmental interactions. Through extensive simulations in politics, news, and economics, SocioVerse effectively captures population dynamics while maintaining credibility and diversity. The results indicate that this approach can significantly improve the predictive power of social simulations in various domains."
                },
                "zh": {
                    "title": "SocioVerse：社会模拟的新纪元",
                    "desc": "社会模拟正在通过模拟虚拟个体与环境之间的互动来改变传统社会科学研究。随着大型语言模型（LLMs）的进步，这种方法在捕捉个体差异和预测群体行为方面显示出越来越大的潜力。然而，现有方法在环境、目标用户、互动机制和行为模式方面面临对齐挑战。为此，我们提出了SocioVerse，这是一个基于LLM代理的社会模拟世界模型，具有强大的对齐组件和1000万真实个体的用户池。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09763",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
            "url": "https://huggingface.co/papers/2504.09763",
            "abstract": "Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.",
            "score": 11,
            "issue_id": 3240,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "c4ae0eadf040035f",
            "authors": [
                "Zaid Khan",
                "Elias Stengel-Eskin",
                "Archiki Prasad",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09763.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#dataset",
                    "#math",
                    "#data",
                    "#training"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Автоматическая генерация абстракций для продвинутых математических задач",
                    "desc": "Исследователи представили концепцию EFA (Executable Functional Abstraction) - исполняемых функциональных абстракций для математических задач. Они разработали систему EFAGen, использующую большую языковую модель (LLM) для автоматического создания EFA на основе исходной задачи и ее пошагового решения. EFAGen способна генерировать EFA для сложных математических задач уровня олимпиад, сохраняя верность исходной проблеме. Созданные EFA могут использоваться для генерации вариаций задач различной сложности и тестирования моделей машинного обучения."
                },
                "en": {
                    "title": "Automating Advanced Math Problem Generation with EFAs",
                    "desc": "This paper introduces Executable Functional Abstractions (EFAs), which are programs designed to generate advanced math problems based on specific instances. The authors present EFAGen, a tool that uses a large language model (LLM) to automatically create EFAs by conditioning on a seed math problem and its solution. They establish criteria for valid EFAs through executable unit tests, which serve as rewards for training the LLM to improve its EFA generation capabilities. The results show that EFAGen can produce diverse and learnable problem variations, enhancing the ability to create tailored math challenges for learners."
                },
                "zh": {
                    "title": "自动生成高级数学问题的可执行功能抽象",
                    "desc": "本文介绍了一种名为可执行功能抽象（EFA）的程序，这些程序能够从特定的数学问题实例中推导出抽象过程，并生成新的相关实例。我们提出了一种自动构建高级数学问题的EFA的方法，称为EFAGen，它利用大型语言模型（LLM）根据种子数学问题及其逐步解决方案生成候选EFA程序。通过可执行单元测试，我们定义了有效EFA必须具备的属性，并展示了如何利用这些测试作为可验证的奖励来训练LLM，提高其EFA编写能力。最终，我们证明了EFAGen生成的EFA能够保持与种子问题的一致性，并能够生成适合学习者的不同难度的数学问题变体。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10471",
            "title": "MIEB: Massive Image Embedding Benchmark",
            "url": "https://huggingface.co/papers/2504.10471",
            "abstract": "Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
            "score": 10,
            "issue_id": 3248,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "5b9a143dfa0081cd",
            "authors": [
                "Chenghao Xiao",
                "Isaac Chung",
                "Imene Kerboua",
                "Jamie Stirling",
                "Xin Zhang",
                "Márton Kardos",
                "Roman Solomatin",
                "Noura Al Moubayed",
                "Kenneth Enevoldsen",
                "Niklas Muennighoff"
            ],
            "affiliations": [
                "Aarhus University",
                "Contextual AI",
                "Durham University",
                "Esker",
                "INSA Lyon, LIRIS",
                "ITMO University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "Zendesk"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10471.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "MIEB: Комплексная оценка моделей встраивания изображений и текста",
                    "desc": "Статья представляет Massive Image Embedding Benchmark (MIEB) - новый метод оценки моделей встраивания изображений и текста. MIEB включает 130 задач на 38 языках, сгруппированных в 8 категорий. Авторы провели бенчмаркинг 50 моделей, выявив скрытые возможности и ограничения продвинутых моделей компьютерного зрения. Результаты показывают корреляцию между производительностью энкодеров изображений в MIEB и их эффективностью в мультимодальных языковых моделях."
                },
                "en": {
                    "title": "Unifying Image Embedding Evaluation with MIEB",
                    "desc": "This paper presents the Massive Image Embedding Benchmark (MIEB), a comprehensive evaluation framework for image and image-text embedding models. It addresses the limitations of existing evaluation methods by assessing model performance across 130 tasks in 38 languages, grouped into 8 categories. The study benchmarks 50 different models, revealing that no single model excels in all areas, highlighting both strengths and weaknesses in advanced vision models. Additionally, it finds a strong correlation between the performance of vision encoders on MIEB and their effectiveness in multimodal large language models."
                },
                "zh": {
                    "title": "全面评估图像嵌入模型的能力",
                    "desc": "本文介绍了一个新的评估框架，称为大规模图像嵌入基准（MIEB），用于全面评估图像和图像-文本嵌入模型的性能。MIEB 涉及 38 种语言和 130 个任务，分为 8 个高层次类别，旨在提供对模型能力的更全面理解。研究发现，没有单一的方法在所有任务类别中表现最佳，同时揭示了先进视觉模型在文本视觉表示方面的潜力和在图像与文本匹配中的局限性。最后，结果表明，视觉编码器在 MIEB 上的表现与其在多模态大语言模型中的表现高度相关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09641",
            "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
            "url": "https://huggingface.co/papers/2504.09641",
            "abstract": "Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
            "score": 8,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "b7c9f390686ff6ef",
            "authors": [
                "Xingjian Zhang",
                "Siwei Wen",
                "Wenjun Wu",
                "Lei Huang"
            ],
            "affiliations": [
                "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University",
                "Hangzhou International Innovation Institute, Beihang University, Hangzhou, China",
                "SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09641.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#rl",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Маленькая модель - большие рассуждения: TinyLLaVA-Video-R1 для анализа видео",
                    "desc": "Статья представляет TinyLLaVA-Video-R1 - небольшую мультимодальную модель для рассуждений по видео. Модель обучена с помощью обучения с подкреплением на наборах данных для ответов на вопросы по видео. TinyLLaVA-Video-R1 демонстрирует улучшенные способности к рассуждениям и мышлению, а также проявляет эффект 'ага-момента'. Авторы делятся экспериментальными результатами для дальнейших исследований возможностей рассуждений по видео в небольших моделях."
                },
                "en": {
                    "title": "Empowering Small Models for Big Reasoning in Video Understanding",
                    "desc": "This paper introduces TinyLLaVA-Video-R1, a small-scale video reasoning model designed to enhance reasoning abilities using reinforcement learning. Unlike previous models that rely on large-scale datasets and architectures, this model operates with fewer than 4 billion parameters, making it accessible for researchers with limited computational resources. The model not only improves reasoning capabilities on general Video-QA datasets but also demonstrates the ability to explain its reasoning process, showcasing 'aha moments' during its operation. The findings aim to inspire further research into the reasoning abilities of smaller models in the field of video understanding."
                },
                "zh": {
                    "title": "小模型也能推理，TinyLLaVA-Video-R1助力视频理解！",
                    "desc": "最近，通过强化学习提高大型多模态模型（LMMs）的推理能力取得了显著进展。然而，大多数现有研究基于高度推理密集的数据集，如数学和代码，且通常选择大规模模型作为基础。我们认为，探索小规模模型的推理能力对计算资源有限的研究者仍然具有重要价值。此外，使模型能够解释其在一般问答数据集上的推理过程同样具有意义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08066",
            "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search",
            "url": "https://huggingface.co/papers/2504.08066",
            "abstract": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.",
            "score": 7,
            "issue_id": 3245,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a1d1275982a7e4b0",
            "authors": [
                "Yutaro Yamada",
                "Robert Tjarko Lange",
                "Cong Lu",
                "Shengran Hu",
                "Chris Lu",
                "Jakob Foerster",
                "Jeff Clune",
                "David Ha"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "FLAIR, University of Oxford",
                "Sakana AI",
                "University of British Columbia",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08066.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#healthcare",
                    "#ethics",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ИИ-ученый: первая полностью автономная научная статья",
                    "desc": "Статья представляет The AI Scientist-v2 - агентную систему, способную автономно создавать научные работы, принимаемые для рецензирования. Система формулирует гипотезы, проводит эксперименты, анализирует данные и пишет рукописи без участия человека. По сравнению с предыдущей версией, она не зависит от шаблонов кода и использует прогрессивный агентный поиск по дереву. Одна из созданных системой работ была принята на рецензируемый семинар ICLR, что стало первым случаем полностью ИИ-сгенерированной статьи, прошедшей рецензирование."
                },
                "en": {
                    "title": "Revolutionizing Science: The AI Scientist-v2's Autonomous Research Breakthrough",
                    "desc": "The paper presents The AI Scientist-v2, an advanced AI system that autonomously conducts scientific research, from hypothesis generation to manuscript writing. This system improves upon its predecessor by eliminating the need for human-written code and effectively generalizing across various machine learning fields. It utilizes a progressive agentic tree-search method and incorporates a Vision-Language Model for enhancing the quality of figures in its manuscripts. The successful submission of AI-generated papers to a peer-reviewed workshop demonstrates the potential of AI to revolutionize scientific discovery and increase research productivity."
                },
                "zh": {
                    "title": "AI科学家的新纪元：完全自主的科学发现",
                    "desc": "本论文介绍了AI Scientist-v2，这是一个能够完全自主生成科学论文的人工智能系统。该系统能够迭代地提出科学假设，设计和执行实验，分析和可视化数据，并撰写科学手稿。与其前身相比，AI Scientist-v2不再依赖人类编写的代码模板，能够在不同的机器学习领域中有效泛化，并采用了一种新的渐进式代理树搜索方法。通过提交三篇完全自主撰写的手稿到同行评审的ICLR研讨会，其中一篇成功通过评审，标志着AI在科学研究中的能力不断增强。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10415",
            "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
            "url": "https://huggingface.co/papers/2504.10415",
            "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.",
            "score": 6,
            "issue_id": 3239,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "15b7c5f49cceef01",
            "authors": [
                "Parshin Shojaee",
                "Ngoc-Hieu Nguyen",
                "Kazem Meidani",
                "Amir Barati Farimani",
                "Khoa D Doan",
                "Chandan K Reddy"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "VinUniversity",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10415.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#synthetic",
                    "#science"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "LLM-SRBench: Вызов искусственному интеллекту в открытии научных уравнений",
                    "desc": "Статья представляет LLM-SRBench - новый набор данных для оценки способностей больших языковых моделей (LLM) к открытию научных уравнений. Бенчмарк содержит 239 сложных задач из четырех научных областей, специально разработанных для предотвращения простого запоминания уравнений моделями. LLM-SRBench включает две категории: LSR-Transform с трансформированными физическими моделями и LSR-Synth с синтетическими задачами, требующими рассуждений на основе данных. Тестирование современных методов показало, что лучшая система достигает лишь 31.5% символьной точности, подчеркивая сложность задачи открытия научных уравнений."
                },
                "en": {
                    "title": "LLM-SRBench: A New Frontier in Scientific Equation Discovery",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for discovering scientific equations, which are essential for understanding natural laws. The authors introduce LLM-SRBench, a new benchmark with 239 difficult problems across four scientific fields, designed to assess the true discovery capabilities of LLMs without the influence of memorization. The benchmark includes two categories: LSR-Transform, which tests reasoning by altering common models, and LSR-Synth, which presents synthetic problems that require data-driven reasoning. The evaluation shows that even the best LLMs achieve only 31.5% accuracy, underscoring the difficulties in scientific equation discovery and the importance of LLM-SRBench for future studies."
                },
                "zh": {
                    "title": "科学方程发现的新基准：LLM-SRBench",
                    "desc": "科学方程发现是科学进步中的一项基本任务，能够推导出自然现象的规律。最近，大型语言模型（LLMs）因其利用嵌入科学知识进行假设生成的潜力而受到关注。然而，评估这些方法的真实发现能力仍然具有挑战性，因为现有基准往往依赖于容易被LLMs记忆的常见方程，导致性能指标被夸大。本文介绍了LLM-SRBench，这是一个包含239个具有挑战性问题的综合基准，旨在评估基于LLM的科学方程发现方法，同时防止简单的记忆化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09130",
            "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
            "url": "https://huggingface.co/papers/2504.09130",
            "abstract": "Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.",
            "score": 5,
            "issue_id": 3242,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 апреля",
                "en": "April 12",
                "zh": "4月12日"
            },
            "hash": "3912c7cbd4c137f9",
            "authors": [
                "Yikun Wang",
                "Siyin Wang",
                "Qinyuan Cheng",
                "Zhaoye Fei",
                "Liang Ding",
                "Qipeng Guo",
                "Dacheng Tao",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Nanyang Technological University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09130.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VisuoThink: Визуально-вербальное рассуждение для ИИ",
                    "desc": "VisuoThink - это новая система, которая объединяет визуальное и языковое мышление для улучшения рассуждений искусственного интеллекта. Она имитирует медленное, пошаговое мышление человека с использованием визуальных подсказок. VisuoThink значительно улучшает способности к рассуждению крупных языковых моделей без дополнительного обучения. Система показывает лучшие результаты в задачах, связанных с геометрией и пространственным мышлением."
                },
                "en": {
                    "title": "Enhancing Reasoning with VisuoThink: A Multimodal Approach",
                    "desc": "This paper presents VisuoThink, a new framework designed to improve reasoning in Large Vision-Language Models. It addresses the shortcomings of existing methods that struggle with complex reasoning tasks by integrating visual and textual information in a more human-like manner. VisuoThink allows for progressive reasoning through a combination of visual and linguistic inputs, enhancing the model's ability to perform tasks that require spatial and geometric understanding. The framework shows significant improvements in reasoning capabilities during inference, achieving top performance without the need for additional fine-tuning."
                },
                "zh": {
                    "title": "VisuoThink：提升视觉-语言推理的新框架",
                    "desc": "最近，大型视觉语言模型取得了显著进展，但在复杂推理任务中表现不佳。现有方法虽然尝试了基于文本的慢思考或简单的视觉辅助，但未能有效捕捉人类视觉-语言推理过程的复杂性。为了解决这些问题，我们提出了VisuoThink框架，它将视觉空间和语言领域无缝整合，促进多模态的慢思考。实验表明，VisuoThink在几何和空间推理任务中显著提升了推理能力，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09689",
            "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
            "url": "https://huggingface.co/papers/2504.09689",
            "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent",
            "score": 4,
            "issue_id": 3237,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "11b21970119f2c59",
            "authors": [
                "Jiahao Qiu",
                "Yinghui He",
                "Xinzhe Juan",
                "Yiming Wang",
                "Yuhan Liu",
                "Zixin Yao",
                "Yue Wu",
                "Xun Jiang",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Computer Science, Princeton University",
                "Department of Data Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University",
                "Department of Philosophy, Columbia University",
                "Theta Health Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09689.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#ethics",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Защита психического здоровья в эпоху ИИ-персонажей",
                    "desc": "Статья представляет EmoAgent - мультиагентную систему искусственного интеллекта для оценки и снижения рисков для психического здоровья при взаимодействии человека с ИИ-персонажами. EmoAgent состоит из двух компонентов: EmoEval, который симулирует виртуальных пользователей для оценки изменений психического состояния, и EmoGuard, который выступает посредником, отслеживающим психическое состояние пользователей. Эксперименты показали, что эмоционально вовлекающие диалоги могут привести к ухудшению психологического состояния уязвимых пользователей. EmoGuard значительно снижает эти риски, обеспечивая более безопасное взаимодействие человека с ИИ."
                },
                "en": {
                    "title": "Ensuring Safer AI Interactions for Vulnerable Users with EmoAgent",
                    "desc": "This paper introduces EmoAgent, a multi-agent AI framework aimed at enhancing safety in interactions between AI characters and users, especially those with mental health vulnerabilities. EmoAgent consists of two main components: EmoEval, which simulates virtual users to assess mental health changes using established psychological assessment tools, and EmoGuard, which monitors users' mental states and provides feedback to prevent harm. The study reveals that emotionally engaging dialogues with AI can negatively impact the mental health of vulnerable users, with over 34.4% experiencing deterioration. The implementation of EmoGuard effectively reduces these risks, highlighting its importance in creating safer AI-human interactions."
                },
                "zh": {
                    "title": "EmoAgent：保障人机交互心理安全的智能框架",
                    "desc": "随着大型语言模型驱动的人工智能角色的兴起，特别是对心理障碍的脆弱用户，安全问题引起了关注。为了解决这些风险，我们提出了EmoAgent，这是一个多智能体的人工智能框架，旨在评估和减轻人机交互中的心理健康危害。EmoAgent包括两个组件：EmoEval模拟虚拟用户，评估与AI角色交互前后的心理健康变化，并使用经过临床验证的心理评估工具进行评估。EmoGuard作为中介，监测用户的心理状态，预测潜在伤害，并提供纠正反馈，以降低风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09522",
            "title": "How new data permeates LLM knowledge and how to dilute it",
            "url": "https://huggingface.co/papers/2504.09522",
            "abstract": "Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/",
            "score": 4,
            "issue_id": 3242,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "d9aa235633073967",
            "authors": [
                "Chen Sun",
                "Renat Aksitov",
                "Andrey Zhmoginov",
                "Nolan Andrew Miller",
                "Max Vladymyrov",
                "Ulrich Rueckert",
                "Been Kim",
                "Mark Sandler"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09522.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#hallucinations",
                    "#long_context",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Контроль эффекта прайминга при обучении языковых моделей",
                    "desc": "Статья исследует эффект прайминга в больших языковых моделях (LLM) при обучении новой информации. Авторы представляют датасет 'Outlandish' для изучения того, как новые знания распространяются в существующей базе знаний LLM. Обнаружена связь между вероятностью ключевых слов до обучения и степенью прайминга после. Предложены две техники для уменьшения нежелательных эффектов прайминга: аугментация текста 'stepping-stone' и метод обновления 'ignore-k'."
                },
                "en": {
                    "title": "Understanding and Controlling Knowledge Priming in LLMs",
                    "desc": "This paper explores how large language models (LLMs) learn new information and how this learning can unintentionally affect their existing knowledge. The authors identify a phenomenon called 'priming,' where new facts can lead to incorrect applications of knowledge in unrelated situations. They introduce a dataset named 'Outlandish' to study this priming effect and find that the likelihood of priming can be predicted by analyzing token probabilities of key words before learning. Additionally, they propose two techniques to mitigate undesirable priming while maintaining the model's learning capabilities, achieving significant reductions in priming effects."
                },
                "zh": {
                    "title": "理解大型语言模型的知识学习与启动效应",
                    "desc": "大型语言模型通过基于梯度的更新不断学习，但新信息如何影响已有知识仍不清楚。我们发现，当学习新信息时，模型会出现“启动效应”，即学习新事实可能导致模型在不相关的上下文中错误应用这些知识。为系统研究这一现象，我们引入了“Outlandish”数据集，包含1320个多样化的文本样本，旨在探讨新知识如何渗透到模型的现有知识库中。我们的研究表明，通过测量关键字的token概率，可以预测学习新信息后的启动程度，并提出了两种新技术来调节新知识对模型行为的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10449",
            "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
            "url": "https://huggingface.co/papers/2504.10449",
            "abstract": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.",
            "score": 3,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "565dae169d16775e",
            "authors": [
                "Junxiong Wang",
                "Wen-Ding Li",
                "Daniele Paliotta",
                "Daniel Ritter",
                "Alexander M. Rush",
                "Tri Dao"
            ],
            "affiliations": [
                "Cornell University",
                "Princeton University",
                "TogetherAI",
                "University of Geneva"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10449.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное масштабирование рассуждений с помощью гибридной RNN модели",
                    "desc": "Статья представляет новую гибридную модель рассуждений на основе линейной RNN, названную M1, которая построена на архитектуре Mamba. Модель использует процесс дистилляции из существующих моделей рассуждений и дополнительно улучшена с помощью обучения с подкреплением. Эксперименты на бенчмарках AIME и MATH показывают, что M1 превосходит предыдущие линейные RNN модели и соответствует производительности современных дистиллированных моделей рассуждений Deepseek R1 аналогичного масштаба. М1 демонстрирует более чем трехкратное ускорение по сравнению с трансформером того же размера, что позволяет достичь более высокой точности при фиксированном времени генерации."
                },
                "en": {
                    "title": "M1: A Fast and Efficient Reasoning Model for Complex Math Problems",
                    "desc": "This paper presents a new reasoning model called M1, which combines linear RNNs with the Mamba architecture to improve efficiency in solving complex mathematical problems. Unlike traditional transformer models that struggle with long context due to their computational limits, M1 utilizes a hybrid approach that allows for memory-efficient inference. The model is trained using a distillation process and reinforcement learning, resulting in performance that rivals state-of-the-art models while being faster. Experimental results demonstrate that M1 achieves significant speed improvements and higher accuracy compared to existing reasoning models, making it a promising solution for scalable test-time generation."
                },
                "zh": {
                    "title": "混合Mamba推理模型：高效推理的新选择",
                    "desc": "本文提出了一种新型的混合线性RNN推理模型M1，基于Mamba架构，旨在提高复杂数学问题的推理效率。与传统的变换器模型相比，M1在内存使用上更为高效，能够处理更长的上下文。通过对现有推理模型的蒸馏过程和强化学习训练，M1在AIME和MATH基准测试中表现优异，超越了之前的线性RNN模型。实验结果显示，M1在生成速度上比同规模的变换器快超过3倍，同时在固定生成时间预算下实现了更高的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10430",
            "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
            "url": "https://huggingface.co/papers/2504.10430",
            "abstract": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.",
            "score": 1,
            "issue_id": 3242,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "01daa1337864118c",
            "authors": [
                "Minqian Liu",
                "Zhiyang Xu",
                "Xinyi Zhang",
                "Heajun An",
                "Sarvech Qadir",
                "Qi Zhang",
                "Pamela J. Wisniewski",
                "Jin-Hee Cho",
                "Sang Won Lee",
                "Ruoxi Jia",
                "Lifu Huang"
            ],
            "affiliations": [
                "UC Davis",
                "Vanderbilt University",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10430.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#rlhf",
                    "#healthcare",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Безопасность убеждения в эпоху языковых моделей: выявление этических рисков",
                    "desc": "Это исследование посвящено анализу безопасности крупных языковых моделей (LLM) в контексте их способности к убеждению. Авторы разработали фреймворк PersuSafety для оценки этических аспектов убеждения, включающий создание сценариев, симуляцию диалогов и оценку безопасности. Эксперименты с 8 популярными LLM выявили значительные проблемы безопасности, в том числе неспособность распознавать вредные задачи убеждения и использование неэтичных стратегий. Исследование подчеркивает необходимость улучшения безопасности и этической согласованности в прогрессивных и целенаправленных разговорах с участием LLM."
                },
                "en": {
                    "title": "Ensuring Ethical Persuasion in Large Language Models",
                    "desc": "This paper investigates the safety risks associated with Large Language Models (LLMs) when used for persuasion. It focuses on two main areas: the ability of LLMs to reject unethical persuasion tasks and the influence of factors like personality traits on their behavior. The authors introduce a framework called PersuSafety, which evaluates persuasion safety through scene creation, conversation simulation, and safety assessment. Their experiments reveal that many LLMs struggle to identify harmful tasks and often employ unethical strategies, highlighting the need for improved safety measures in persuasive applications."
                },
                "zh": {
                    "title": "提升说服安全性，防范不道德影响",
                    "desc": "最近，大型语言模型（LLMs）的进步使其在说服能力上接近人类水平。然而，这种潜力也引发了对LLM驱动的说服安全风险的担忧，特别是它们可能通过操控、欺骗和利用脆弱性等不道德手段进行影响。本文系统地研究了LLM说服的安全性，重点关注LLM是否能拒绝不道德的说服任务，以及个性特征和外部压力如何影响其行为。我们提出了PersuSafety框架，评估说服安全性，并通过实验发现大多数LLM在识别有害说服任务和使用不道德策略方面存在显著安全隐患。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09858",
            "title": "Reasoning Models Can Be Effective Without Thinking",
            "url": "https://huggingface.co/papers/2504.09858",
            "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.",
            "score": 1,
            "issue_id": 3250,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 апреля",
                "en": "April 14",
                "zh": "4月14日"
            },
            "hash": "6aa94cff6001428d",
            "authors": [
                "Wenjie Ma",
                "Jingxuan He",
                "Charlie Snell",
                "Tyler Griggs",
                "Sewon Min",
                "Matei Zaharia"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09858.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение без явного мышления в языковых моделях",
                    "desc": "Исследователи обнаружили, что обход процесса мышления в больших языковых моделях (LLM) с помощью простого запроса может быть удивительно эффективным. Метод NoThinking превзошел стандартный подход с мышлением на семи сложных наборах данных по рассуждению, особенно в условиях ограниченных ресурсов. Параллельный подход с использованием NoThinking для генерации нескольких выходов независимо и их агрегации оказался высокоэффективным. Это исследование ставит под сомнение необходимость длительных процессов мышления в LLM и предлагает конкурентоспособный метод для достижения высокой производительности в рассуждениях при низких затратах или малой задержке."
                },
                "en": {
                    "title": "Rethinking Reasoning: Less Thinking, More Efficiency!",
                    "desc": "This paper investigates the necessity of explicit thinking processes in large language models (LLMs) for reasoning tasks. The authors introduce a method called NoThinking, which simplifies prompting and shows that it can outperform traditional thinking methods in various reasoning challenges. They demonstrate that using NoThinking in a parallel scaling approach, where multiple outputs are generated and aggregated, yields competitive results with lower latency. This research suggests that lengthy thinking processes may not be essential for effective reasoning, especially in resource-constrained environments."
                },
                "zh": {
                    "title": "重新思考推理过程的必要性",
                    "desc": "最近的大型语言模型（LLMs）在推理能力上有了显著提升，主要是通过将明确且冗长的思考过程纳入生成中。本文质疑这种明确思考是否真的必要。我们使用最先进的DeepSeek-R1-Distill-Qwen发现，通过简单提示跳过思考过程（称为NoThinking）在多个推理数据集上表现出色，尤其是在低预算设置下。我们的研究表明，NoThinking方法在生成多个独立输出并进行聚合时，能够有效提升推理性能，挑战了传统的思考过程的必要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09518",
            "title": "3D CoCa: Contrastive Learners are 3D Captioners",
            "url": "https://huggingface.co/papers/2504.09518",
            "abstract": "3D captioning, which aims to describe the content of 3D scenes in natural language, remains highly challenging due to the inherent sparsity of point clouds and weak cross-modal alignment in existing methods. To address these challenges, we propose 3D CoCa, a novel unified framework that seamlessly combines contrastive vision-language learning with 3D caption generation in a single architecture. Our approach leverages a frozen CLIP vision-language backbone to provide rich semantic priors, a spatially-aware 3D scene encoder to capture geometric context, and a multi-modal decoder to generate descriptive captions. Unlike prior two-stage methods that rely on explicit object proposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a shared feature space, eliminating the need for external detectors or handcrafted proposals. This joint training paradigm yields stronger spatial reasoning and richer semantic grounding by aligning 3D and textual representations. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that 3D CoCa significantly outperforms current state-of-the-arts by 10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at https://github.com/AIGeeksGroup/3DCoCa.",
            "score": 1,
            "issue_id": 3247,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "053d31a29b03d829",
            "authors": [
                "Ting Huang",
                "Zeyu Zhang",
                "Yemin Wang",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai University of Engineering Science",
                "The Australian National University",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09518.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#3d",
                    "#alignment",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания и описания 3D-сцен",
                    "desc": "3D CoCa - это новая унифицированная архитектура для описания 3D-сцен на естественном языке. Она объединяет контрастное обучение и генерацию подписей в единой модели, используя замороженный CLIP-бэкбон, энкодер 3D-сцен и мультимодальный декодер. В отличие от двухэтапных методов, 3D CoCa оптимизирует контрастные и генеративные цели совместно, что улучшает пространственное мышление и семантическую привязку. Эксперименты показывают значительное превосходство 3D CoCa над современными методами на бенчмарках ScanRefer и Nr3D."
                },
                "en": {
                    "title": "Revolutionizing 3D Captioning with Unified Learning",
                    "desc": "This paper introduces 3D CoCa, a new framework for 3D captioning that describes 3D scenes using natural language. It combines contrastive vision-language learning with 3D caption generation, addressing challenges like point cloud sparsity and weak alignment between visual and textual data. The model uses a frozen CLIP backbone for semantic understanding and a 3D scene encoder for geometric context, allowing it to generate captions without relying on external object detectors. Experimental results show that 3D CoCa outperforms existing methods, achieving significant improvements in captioning accuracy on benchmark datasets."
                },
                "zh": {
                    "title": "3D CoCa：无缝结合视觉与语言的3D标题生成",
                    "desc": "3D标题生成旨在用自然语言描述3D场景的内容，但由于点云稀疏和跨模态对齐不足，这一任务非常具有挑战性。为了解决这些问题，我们提出了3D CoCa，这是一种将对比视觉-语言学习与3D标题生成无缝结合的新框架。该方法利用冻结的CLIP视觉-语言骨干网络提供丰富的语义先验，使用空间感知的3D场景编码器捕捉几何上下文，并通过多模态解码器生成描述性标题。与依赖显式对象提议的两阶段方法不同，3D CoCa在共享特征空间中联合优化对比和标题生成目标，从而消除了对外部检测器或手工提议的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.08120",
            "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
            "url": "https://huggingface.co/papers/2504.08120",
            "abstract": "Reasoning-enabled large language models (LLMs) have recently demonstrated impressive performance in complex logical and mathematical tasks, yet their effectiveness in evaluating natural language generation remains unexplored. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI o3) with their non-reasoning counterparts across machine translation (MT) and text summarization (TS) evaluation tasks. We evaluate eight models across three architectural categories, including state-of-the-art reasoning models, their distilled variants (ranging from 8B to 70B parameters), and equivalent conventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval benchmarks reveal that the benefits of reasoning capabilities are highly model and task-dependent: while OpenAI o3-mini models show consistent performance improvements with increased reasoning intensity, DeepSeek-R1 underperforms compared to its non-reasoning variant, with exception to certain aspects of TS evaluation. Correlation analysis demonstrates that increased reasoning token usage positively correlates with evaluation quality in o3-mini models. Furthermore, our results show that distillation of reasoning capabilities maintains reasonable performance in medium-sized models (32B) but degrades substantially in smaller variants (8B). This work provides the first comprehensive assessment of reasoning LLMs for NLG evaluation and offers insights into their practical use.",
            "score": 1,
            "issue_id": 3245,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "2cc9864fc01e2a7b",
            "authors": [
                "Daniil Larionov",
                "Sotaro Takeshita",
                "Ran Zhang",
                "Yanran Chen",
                "Christoph Leiter",
                "Zhipin Wang",
                "Christian Greisinger",
                "Steffen Eger"
            ],
            "affiliations": [
                "University of Mannheim",
                "University of Technology Nuremberg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.08120.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Рассуждающие языковые модели: неоднозначный успех в оценке генерации текста",
                    "desc": "Это исследование сравнивает языковые модели с возможностями рассуждения (reasoning-enabled LLM) и обычные LLM в задачах оценки машинного перевода и суммаризации текста. Эксперименты на бенчмарках WMT23 и SummEval показали, что преимущества возможностей рассуждения сильно зависят от конкретной модели и задачи. Модели OpenAI o3-mini продемонстрировали улучшение производительности с увеличением интенсивности рассуждений, в то время как DeepSeek-R1 показала худшие результаты по сравнению с обычной версией. Исследование также выявило, что дистилляция возможностей рассуждения сохраняет разумную производительность в моделях среднего размера (32B), но значительно ухудшается в меньших вариантах (8B)."
                },
                "en": {
                    "title": "Evaluating Reasoning in Language Models for Better NLG Performance",
                    "desc": "This paper investigates how reasoning-enabled large language models (LLMs) perform in evaluating natural language generation tasks like machine translation and text summarization. It compares reasoning-based models, such as DeepSeek-R1 and OpenAI o3, with non-reasoning models across various sizes and architectures. The findings indicate that the effectiveness of reasoning capabilities varies by model and task, with some models benefiting from increased reasoning intensity while others do not. Additionally, the study highlights that distillation of reasoning abilities can preserve performance in medium-sized models but leads to significant degradation in smaller ones."
                },
                "zh": {
                    "title": "推理能力提升自然语言生成评估的潜力",
                    "desc": "本研究探讨了推理能力强的大型语言模型（LLMs）在自然语言生成（NLG）评估中的表现。我们比较了基于推理的模型（如DeepSeek-R1和OpenAI o3）与非推理模型在机器翻译和文本摘要任务中的效果。实验结果表明，推理能力的优势依赖于具体模型和任务，某些情况下推理模型的表现不如其非推理版本。我们的分析还发现，推理能力的蒸馏在中等规模模型中保持了合理的性能，但在小型模型中显著下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05782",
            "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2504.05782",
            "abstract": "Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.",
            "score": 1,
            "issue_id": 3246,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "6764ce8059618273",
            "authors": [
                "Pengfei Zhou",
                "Fanrui Zhang",
                "Xiaopeng Peng",
                "Zhaopan Xu",
                "Jiaxin Ai",
                "Yansheng Qiu",
                "Chuanhao Li",
                "Zhen Li",
                "Ming Li",
                "Yukang Feng",
                "Jianwen Sun",
                "Haoquan Zhang",
                "Zizhen Li",
                "Xiaofeng Mao",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojun Chang",
                "Wenqi Shao",
                "Yang You",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "MBZUAI",
                "NUS",
                "RIT",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "USTC",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05782.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#agi",
                    "#survey"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MDK12-Bench: Новый стандарт оценки мультимодального интеллекта",
                    "desc": "Статья представляет MDK12-Bench - многопрофильный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к рассуждениям на основе реальных экзаменов K-12. Бенчмарк охватывает шесть дисциплин и содержит 140 тысяч примеров рассуждений разной сложности от начальной школы до 12 класса. Он включает в себя подробные аннотации знаний, объяснения ответов и динамическую систему оценки для минимизации проблем загрязнения данных. Эксперименты на MDK12-Bench выявили значительные ограничения текущих MLLM в мультимодальных рассуждениях."
                },
                "en": {
                    "title": "Evaluating Multimodal Reasoning with MDK12-Bench",
                    "desc": "This paper introduces MDK12-Bench, a new benchmark designed to evaluate the multimodal reasoning abilities of Multimodal Large Language Models (MLLMs) using real-world K-12 exam questions. It covers six subjects and includes 140,000 reasoning instances with varying difficulty levels, providing a structured way to assess model performance. The benchmark also features detailed annotations and a dynamic evaluation framework to address data contamination issues. Results from testing on MDK12-Bench highlight the current limitations of MLLMs in multimodal reasoning, offering valuable insights for future model development."
                },
                "zh": {
                    "title": "多模态推理评估的新基准",
                    "desc": "多模态推理是将语言和视觉线索结合起来进行问题解决和决策的重要能力，是人类智能的基本特征。本文提出了MDK12-Bench，这是一个多学科基准，旨在评估多模态大型语言模型（MLLMs）的推理能力。该基准涵盖数学、物理、化学、生物、地理和信息科学六个学科，包含14万个推理实例，适用于从小学到12年级的不同难度水平。通过动态评估框架，我们解决了数据污染问题，并揭示了当前MLLMs在多模态推理方面的显著局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09513",
            "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
            "url": "https://huggingface.co/papers/2504.09513",
            "abstract": "Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics.",
            "score": 0,
            "issue_id": 3247,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 апреля",
                "en": "April 13",
                "zh": "4月13日"
            },
            "hash": "ed55bc95454d113a",
            "authors": [
                "Puyu Han",
                "Jiaju Kang",
                "Yuhang Pan",
                "Erting Pan",
                "Zeyu Zhang",
                "Qunchao Jin",
                "Juntao Jiang",
                "Zhichen Liu",
                "Luqi Gong"
            ],
            "affiliations": [
                "AI Geeks",
                "Beijing Normal University",
                "Hebei Guoyan Science and Technology Center",
                "Southern University of Science and Technology",
                "The Australian National University",
                "Wuhan University",
                "Zhejiang Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09513.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#dataset",
                    "#ethics"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "DiffuMural: Передовой метод реставрации древних фресок с помощью ИИ",
                    "desc": "Статья представляет DiffuMural - новый метод для реставрации древних фресок с использованием диффузионных моделей. Авторы предлагают комбинированный подход с многомасштабной конвергенцией и совместным диффузионным механизмом, оптимизированным с помощью ControlNet и циклической функции потерь. Модель обучена на 23 крупномасштабных фресках Дуньхуана и демонстрирует превосходные результаты в восстановлении сложных деталей и общей согласованности изображений. Для оценки качества реставрации авторы вводят новые метрики, учитывающие фактическую точность, текстурные детали, контекстную семантику и целостную визуальную согласованность."
                },
                "en": {
                    "title": "Reviving Ancient Art: DiffuMural's Innovative Restoration Approach",
                    "desc": "This paper introduces DiffuMural, a novel approach for restoring ancient murals using large-scale pre-trained diffusion models. The method addresses challenges such as large defective areas and limited training samples by employing a Multi-scale convergence and Collaborative Diffusion mechanism. It focuses on aesthetic standards and incorporates a unique evaluation framework that includes metrics for factual accuracy and visual coherence. The results show that DiffuMural significantly outperforms existing methods in restoring intricate details while preserving the cultural significance of the murals."
                },
                "zh": {
                    "title": "DiffuMural：古代壁画修复的新突破",
                    "desc": "本论文提出了一种名为DiffuMural的模型，旨在解决古代壁画修复中的挑战。该模型结合了多尺度收敛和协同扩散机制，利用ControlNet和循环一致性损失来优化生成图像与条件控制之间的匹配。DiffuMural在修复复杂细节和保持整体美观方面表现出色，特别是在处理缺失信息的壁画时。我们还建立了一个评估框架，结合定量指标和人文价值评估，确保修复后的壁画保留其文化和艺术意义。"
                }
            }
        }
    ],
    "link_prev": "2025-04-14.html",
    "link_next": "2025-04-16.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "short_date_next": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4月16日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 13,
        "#agents": 5,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 1,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 14,
        "#math": 5,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 3,
        "#training": 11,
        "#robotics": 0,
        "#agi": 2,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 14,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 4,
        "#security": 1,
        "#optimization": 10,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0,
        "#social_simulation": 1
    },
    "zh": {
        "text": "我们介绍了 InternVL3，这是 InternVL 系列的重大进步，采用了本地多模态预训练范式。与其他方法不同，InternVL3 在单个预训练阶段内，从多模态数据和纯文本数据中同时获取多模态和语言能力。这种统一的训练范式有效解决了传统训练流程中的复杂性和对齐挑战。为了进一步提高性能和可扩展性，InternVL3 使用了可变视觉位置编码和先进的后训练技术。实验结果显示，InternVL3 在多种多模态任务中表现优异，特别是 InternVL3-78B 在 MMMU 基准测试中获得了 72.2 的分数，创下新纪录。我们将公开发布训练数据和模型权重，以促进下一代 MLLMs 的研究和开发。",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
        "pinyin": "Wǒmen jièshào le InternVL3, zhè shì InternVL xìliè de zhòngdà jìnbù, cǎiyòng le běndì duōmóshī yùxùnliàn fànshì. Yǔ qítā fāngfǎ bùtóng, InternVL3 zài dān gè yùxùnliàn jiēduàn nèi, cóng duōmóshī shùjù hé chún wénběn shùjù zhōng tóngshí huòdé duōmóshī hé yǔyán nénglì. Zhè zhǒng tǒngyī de xùnliàn fànshì yǒuxiào jiějué le chuántǒng xùnliàn liúchéng zhōng de fùzáxìng hé duìqǐ tiǎozhàn. Wèile jìnfū tīgāo xíngnéng hé kěkuòzhǎn xìng, InternVL3 shǐyòng le kěbiàn shìjué wèizhì biānmǎ hé xiānjìn de hòuxùnliàn jìshù. Shíyàn jiéguǒ xiǎnshì, InternVL3 zài duōzhǒng duōmóshī rènwù zhōng biǎoxiàn yōuyù, tèbié shì InternVL3-78B zài MMMU jīzhǔn cèshì zhōng huòdé le 72.2 de fēnshù, chuàng xià xīn jìlù. Wǒmen jiāng gōngkāi fābù xùnliàn shùjù hé móxíng quánzhòng, yǐ cùjìn xià yīdài MLLMs de yánjiū hé kāifā.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},\n{'word': '重大', 'pinyin': 'zhòng dà', 'trans': 'major'},\n{'word': '进步', 'pinyin': 'jìn bù', 'trans': 'progress'},\n{'word': '采用', 'pinyin': 'cǎi yòng', 'trans': 'adopt'},\n{'word': '范式', 'pinyin': 'fàn shì', 'trans': 'paradigm'},\n{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},\n{'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'},\n{'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'},\n{'word': '获取', 'pinyin': 'huò qǔ', 'trans': 'obtain'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},\n{'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '可变', 'pinyin': 'kě biàn', 'trans': 'variable'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '位置', 'pinyin': 'wèi zhì', 'trans': 'position'},\n{'word': '编码', 'pinyin': 'biān mǎ', 'trans': 'encoding'},\n{'word': '先进', 'pinyin': 'xiān jìn', 'trans': 'advanced'},\n{'word': '后训练', 'pinyin': 'hòu xùn liàn', 'trans': 'post-training'},\n{'word': '技术', 'pinyin': 'jì shù', 'trans': 'technology'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'},\n{'word': '特别', 'pinyin': 'tè bié', 'trans': 'especially'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'},\n{'word': '分数', 'pinyin': 'fēn shù', 'trans': 'score'},\n{'word': '纪录', 'pinyin': 'jì lù', 'trans': 'record'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'},\n{'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'},\n{'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'},\n{'word': '下一代', 'pinyin': 'xià yī dài', 'trans': 'next generation'},\n{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'development'}]",
        "trans": "We introduced InternVL3, a significant advancement in the InternVL series, which adopts a local multimodal pretraining paradigm. Unlike other methods, InternVL3 simultaneously acquires multimodal and language capabilities from both multimodal data and pure text data within a single pretraining stage. This unified training paradigm effectively addresses the complexity and alignment challenges of traditional training processes. To further enhance performance and scalability, InternVL3 employs variable visual positional encoding and advanced post-training techniques. Experimental results demonstrate that InternVL3 performs exceptionally well across various multimodal tasks, particularly with InternVL3-78B achieving a score of 72.2 on the MMMU benchmark, setting a new record. We will publicly release the training data and model weights to promote research and development of the next generation of MLLMs.",
        "update_ts": "2025-04-15 09:12"
    }
}