{
    "date": {
        "ru": "2 Ð¼Ð°Ñ",
        "en": "May 2",
        "zh": "5æœˆ2æ—¥"
    },
    "time_utc": "2025-05-03 06:31",
    "weekday": 4,
    "issue_id": 3571,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.21853",
            "title": "A Survey of Interactive Generative Video",
            "url": "https://huggingface.co/papers/2504.21853",
            "abstract": "Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.",
            "score": 32,
            "issue_id": 3550,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "4e975f915f638955",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai",
                "Hao Chen",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou Technology, Shenzhen, China",
                "The Hong Kong University of Science and Technology (HKUST), Hong Kong",
                "The University of Hong Kong, Pok Fu Lam, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21853.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#interpretability",
                    "#video",
                    "#optimization",
                    "#survey",
                    "#agents",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ðŸŽ®",
                "ru": {
                    "title": "IGV: Ð‘ÑƒÐ´ÑƒÑ‰ÐµÐµ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð²Ð¸Ð´ÐµÐ¾ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¾Ð±Ð·Ð¾Ñ€ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸ Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð’Ð¸Ð´ÐµÐ¾ (IGV), ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÐ¾Ñ‡ÐµÑ‚Ð°ÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑÐ¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÑŽÑ‚ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ IGV Ð² Ñ‚Ñ€ÐµÑ… Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚ÑÑ…: Ð¸Ð³Ñ€Ñ‹, Ð²Ð¾Ð¿Ð»Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð˜Ð˜ Ð¸ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ð¾Ðµ Ð²Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°, Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÑŽÑ‰Ð°Ñ Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ IGV Ð½Ð° Ð¿ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ, ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ, ÐŸÐ°Ð¼ÑÑ‚ÑŒ, Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ð¸ Ð˜Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚. Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¸ Ð±ÑƒÐ´ÑƒÑ‰Ð¸Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð° IGV."
                },
                "en": {
                    "title": "Empowering Interactive Experiences with Generative Video",
                    "desc": "Interactive Generative Video (IGV) is a new technology that creates high-quality videos that users can interact with. It combines generative models to produce diverse video content and allows users to engage with it through control signals. The paper explores IGV applications in gaming, embodied AI, and autonomous driving, highlighting its potential for creating immersive experiences and training environments. A framework is proposed to break down IGV systems into five key modules, addressing challenges like real-time generation and accurate physics simulation to guide future advancements in the field."
                },
                "zh": {
                    "title": "æŽ¨åŠ¨äº’åŠ¨ç”Ÿæˆè§†é¢‘æŠ€æœ¯çš„æœªæ¥å‘å±•",
                    "desc": "äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰æ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œæ—¨åœ¨æ»¡è¶³å¯¹é«˜è´¨é‡äº’åŠ¨è§†é¢‘å†…å®¹çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†IGVå®šä¹‰ä¸ºä¸€ç§ç»“åˆç”Ÿæˆèƒ½åŠ›å’Œäº’åŠ¨ç‰¹æ€§çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿé€šè¿‡æŽ§åˆ¶ä¿¡å·å’Œåé¦ˆå®žçŽ°ç”¨æˆ·å‚ä¸Žã€‚æˆ‘ä»¬è°ƒæŸ¥äº†IGVåœ¨æ¸¸æˆã€å…·èº«äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ç­‰ä¸‰ä¸ªä¸»è¦é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æž¶ï¼Œå°†ç†æƒ³çš„IGVç³»ç»Ÿåˆ†è§£ä¸ºç”Ÿæˆã€æŽ§åˆ¶ã€è®°å¿†ã€åŠ¨æ€å’Œæ™ºèƒ½äº”ä¸ªæ¨¡å—ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æžæŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œæœ¬æ–‡æ—¨åœ¨æŽ¨åŠ¨IGVé¢†åŸŸçš„ç ”ç©¶ä¸Žå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00662",
            "title": "DeepCritic: Deliberate Critique with Large Language Models",
            "url": "https://huggingface.co/papers/2505.00662",
            "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.",
            "score": 28,
            "issue_id": 3549,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ð¼Ð°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "259dddc97137d27a",
            "authors": [
                "Wenkai Yang",
                "Jingwen Chen",
                "Yankai Lin",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "School of Computer Science and Technology, Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00662.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#rlhf",
                    "#math",
                    "#training"
                ],
                "emoji": "ðŸ§®",
                "ru": {
                    "title": "Ð£ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ LLM Ð´Ð»Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð¹ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¸ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ. ÐÐ° Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Qwen2.5-72B-Instruct Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ñ… Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ñ… ÐºÑ€Ð¸Ñ‚Ð¸Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ»ÑƒÐ¶Ð°Ñ‚ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸. Ð—Ð°Ñ‚ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐµÐ³Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸. Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ-ÐºÑ€Ð¸Ñ‚Ð¸Ðº Ð½Ð° Ð±Ð°Ð·Ðµ Qwen2.5-7B-Instruct Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ LLM-ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¸ Ð² Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð¸Ð¸ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸."
                },
                "en": {
                    "title": "Enhancing LLMs: From Shallow Critiques to Deep Insights in Math Solutions",
                    "desc": "This paper addresses the challenge of providing effective feedback for Large Language Models (LLMs) by enhancing their ability to critique mathematical solutions. The authors propose a two-stage framework where the first stage involves generating detailed critiques using a specific LLM, which serves as seed data for further training. In the second stage, reinforcement learning is applied to improve the critique model's performance using both human-labeled and automatically annotated data. The resulting critique model demonstrates superior performance in identifying errors and providing constructive feedback compared to existing LLM critics."
                },
                "zh": {
                    "title": "æå‡å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æ•°å­¦æ‰¹è¯„èƒ½åŠ›",
                    "desc": "éšç€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæä¾›å‡†ç¡®çš„åé¦ˆå’Œå¯æ‰©å±•çš„ç›‘ç£å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æž¶ï¼Œæ—¨åœ¨å¢žå¼ºLLMsåœ¨æ•°å­¦æ‰¹è¯„æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„é€æ­¥æ‰¹è¯„ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é”™è¯¯å¹¶æä¾›æ·±å…¥çš„åé¦ˆï¼Œå¸®åŠ©ç”Ÿæˆæ¨¡åž‹çº æ­£é”™è¯¯ã€‚æˆ‘ä»¬çš„å®žéªŒè¡¨æ˜Žï¼Œæ”¹è¿›åŽçš„æ‰¹è¯„æ¨¡åž‹åœ¨é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºŽçŽ°æœ‰çš„LLMæ‰¹è¯„è€…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00703",
            "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
            "url": "https://huggingface.co/papers/2505.00703",
            "abstract": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1",
            "score": 22,
            "issue_id": 3548,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ð¼Ð°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "ca564761ff71d15e",
            "authors": [
                "Dongzhi Jiang",
                "Ziyu Guo",
                "Renrui Zhang",
                "Zhuofan Zong",
                "Hao Li",
                "Le Zhuo",
                "Shilin Yan",
                "Pheng-Ann Heng",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "CUHK MiuLar Lab",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00703.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ðŸŽ¨",
                "ru": {
                    "title": "Ð Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð´Ð²ÑƒÑ… ÑƒÑ€Ð¾Ð²Ð½ÑÑ… ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÑŽÑ‚ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ",
                    "desc": "Ð”Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ T2I-R1 - Ð½Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ, ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½ÑƒÑŽ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð´Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹: ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð´Ð»Ñ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð½Ñ‹Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð¿Ð¸ÐºÑÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ð²Ð¾Ð´ÑÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ BiCoT-GRPO Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð±Ð¾Ð¸Ñ… ÑƒÑ€Ð¾Ð²Ð½ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹. ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¸Ñ… ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹ Ðº Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Janus-Pro Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ð»Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ð° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… T2I-CompBench Ð¸ WISE."
                },
                "en": {
                    "title": "Revolutionizing Text-to-Image Generation with Enhanced Reasoning",
                    "desc": "This paper introduces T2I-R1, a new model for generating images from text that uses advanced reasoning techniques. It employs a bi-level chain-of-thought (CoT) approach, which includes semantic-level reasoning for planning and token-level reasoning for detailed image generation. The model is enhanced by reinforcement learning (RL) and a novel reward system called BiCoT-GRPO, which optimizes both levels of reasoning simultaneously. As a result, T2I-R1 outperforms existing models, achieving significant improvements on benchmark tests."
                },
                "zh": {
                    "title": "åŒå±‚æ€ç»´é“¾æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹T2I-R1ï¼Œè¯¥æ¨¡åž‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’ŒåŒå±‚æ€ç»´é“¾æŽ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ€ç»´é“¾ï¼šè¯­ä¹‰å±‚æ¬¡çš„æ€ç»´é“¾ç”¨äºŽé«˜å±‚æ¬¡çš„æç¤ºè§„åˆ’ï¼Œä»¤ç”Ÿæˆè¿‡ç¨‹æ›´å…·é€»è¾‘æ€§ï¼›è€Œä»¤ç‰Œå±‚æ¬¡çš„æ€ç»´é“¾åˆ™ç”¨äºŽåœ¨é€å—ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œä½Žå±‚æ¬¡çš„åƒç´ å¤„ç†ã€‚é€šè¿‡å¼•å…¥BiCoT-GRPOï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­ä¼˜åŒ–è¿™ä¸¤ç§æ€ç»´é“¾ï¼Œä»Žè€Œæå‡ç”Ÿæˆæ•ˆæžœã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒT2I-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†çŽ°æœ‰çš„æœ€å…ˆè¿›æ¨¡åž‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00234",
            "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks",
            "url": "https://huggingface.co/papers/2505.00234",
            "abstract": "Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.",
            "score": 7,
            "issue_id": 3563,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ð¼Ð°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "9e975cfc8ecf9dea",
            "authors": [
                "Vishnu Sarukkai",
                "Zhiqiang Xie",
                "Kayvon Fatahalian"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00234.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#rl",
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "Ð¡Ð°Ð¼Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ LLM-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ Ð¾Ð¿Ñ‹Ñ‚Ðµ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹. Ð’Ð¼ÐµÑÑ‚Ð¾ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾Ð´ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸, Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ Ð¾Ð¿Ñ‹Ñ‚Ð° Ð°Ð³ÐµÐ½Ñ‚Ð° Ð½Ð° ÑÑ…Ð¾Ð¶Ð¸Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…. ÐœÐµÑ‚Ð¾Ð´ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð². Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ…, ÑÑ€Ð°Ð²Ð½Ð¸Ð¼Ð¾Ðµ Ñ Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°Ð¼Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Learning from Success: Automating Improvement in LLM Agents",
                    "desc": "This paper explores a new approach for improving Large Language Model (LLM) agents in sequential decision-making tasks without relying on extensive task-specific knowledge engineering. Instead of manually tuning prompts or creating curated examples, the authors propose that LLM agents can learn from their own successful experiences by building a database of self-generated examples. The study shows that simply accumulating successful task trajectories can significantly enhance performance on various benchmarks. Additionally, the paper introduces methods for selecting high-performing examples from this database, leading to even better results that rival more complex, knowledge-intensive methods."
                },
                "zh": {
                    "title": "è‡ªåŠ¨å­¦ä¹ æå‡LLMä»£ç†æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æŽ¢è®¨äº†å¦‚ä½•é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„æˆåŠŸç»éªŒæ¥è‡ªåŠ¨æå‡å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ä»£ç†åœ¨é¡ºåºå†³ç­–ä»»åŠ¡ä¸­çš„è¡¨çŽ°ï¼Œè€Œä¸æ˜¯ä¾èµ–äºŽç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†å·¥ç¨‹ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œç®€å•åœ°ç§¯ç´¯æˆåŠŸçš„è½¨è¿¹å¯ä»¥æ˜¾è‘—æé«˜åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨çŽ°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ‰©å±•æ–¹æ³•ï¼šé€šè¿‡åŸºäºŽç§ç¾¤çš„è®­ç»ƒé€‰æ‹©é«˜æ•ˆçš„ç¤ºä¾‹é›†åˆï¼Œä»¥åŠæ ¹æ®å®žè¯æ•ˆç”¨ä¿ç•™ä¸ªåˆ«è½¨è¿¹ã€‚è¿™äº›æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†è‡ªåŠ¨è½¨è¿¹æ•°æ®åº“æž„å»ºä½œä¸ºçŸ¥è¯†å·¥ç¨‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00497",
            "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
            "url": "https://huggingface.co/papers/2505.00497",
            "abstract": "Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.",
            "score": 6,
            "issue_id": 3554,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ð¼Ð°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "dc645b5c7e476cea",
            "authors": [
                "Antoni Bigata",
                "Rodrigo Mira",
                "Stella Bounareli",
                "MichaÅ‚ StypuÅ‚kowski",
                "Konstantinos Vougioukas",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "affiliations": [
                "Imperial College London",
                "University of WrocÅ‚aw"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00497.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#leakage",
                    "#video",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ðŸ—£ï¸",
                "ru": {
                    "title": "KeySync: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð³ÑƒÐ± Ð´Ð»Ñ Ð²Ð¸Ð´ÐµÐ¾",
                    "desc": "KeySync - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð³ÑƒÐ± Ð² Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð½Ð¾Ð²Ñ‹Ð¼ Ð°ÑƒÐ´Ð¸Ð¾. ÐžÐ½Ð° Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸, ÑƒÑ‚ÐµÑ‡ÐºÐ¸ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð¾ÐºÐºÐ»ÑŽÐ·Ð¸Ð¹ Ð»Ð¸Ñ†Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ. KeySync Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð³ÑƒÐ± Ð¸ ÐºÑ€Ð¾ÑÑ-ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, ÑƒÐ»ÑƒÑ‡ÑˆÐ°Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°Ñ ÑƒÑ‚ÐµÑ‡ÐºÑƒ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð½Ð¾Ð²Ð¾Ð¹ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐµ LipLeak. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ñ Ð¾ÐºÐºÐ»ÑŽÐ·Ð¸ÑÐ¼Ð¸ Ð¸ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÑ…, Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð´ÑƒÐ±Ð»ÑÐ¶."
                },
                "en": {
                    "title": "KeySync: Mastering Lip Synchronization with Precision",
                    "desc": "This paper introduces KeySync, a two-stage framework designed to improve lip synchronization in videos by aligning lip movements with new audio inputs. It addresses common challenges in talking head generation, such as maintaining temporal consistency and managing expression leakage and facial occlusions. KeySync employs a unique masking strategy to effectively tackle these issues, resulting in enhanced visual quality and reduced leakage as measured by a new metric called LipLeak. The framework demonstrates state-of-the-art performance in lip reconstruction and cross-synchronization, validated through various ablation studies."
                },
                "zh": {
                    "title": "KeySyncï¼šæå‡å”‡éƒ¨åŒæ­¥çš„åˆ›æ–°æ¡†æž¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKeySyncçš„åŒé˜¶æ®µæ¡†æž¶ï¼Œç”¨äºŽè§£å†³å”‡éƒ¨åŒæ­¥ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é®ç½©ç­–ç•¥ï¼Œè§£å†³äº†è¾“å…¥è§†é¢‘ä¸­çš„è¡¨æƒ…æ³„æ¼å’Œé¢éƒ¨é®æŒ¡ç­‰æ–°æŒ‘æˆ˜ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒKeySyncåœ¨å”‡éƒ¨é‡å»ºå’Œäº¤å‰åŒæ­¥æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæžœï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å¹¶å‡å°‘äº†è¡¨æƒ…æ³„æ¼ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¤šé¡¹æ¶ˆèžç ”ç©¶éªŒè¯äº†æ–°é®ç½©æ–¹æ³•åœ¨å¤„ç†é®æŒ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21659",
            "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
            "url": "https://huggingface.co/papers/2504.21659",
            "abstract": "Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
            "score": 5,
            "issue_id": 3549,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "6487e5a67faf07a5",
            "authors": [
                "Haotian Luo",
                "Haiying He",
                "Yibo Wang",
                "Jinluan Yang",
                "Rui Liu",
                "Naiqiang Tan",
                "Xiaochun Cao",
                "Dacheng Tao",
                "Li Shen"
            ],
            "affiliations": [
                "China Agricultural University",
                "Didichuxing Co. Ltd",
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21659.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#math",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ðŸ§ ",
                "ru": {
                    "title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ: ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ð¼Ñƒ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¼Ñƒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑŽ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ, Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‰ÑƒÑŽ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¸ Ð±Ð¸-ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸ÑÐ¼. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑÐ¾ÐºÑ€Ð°Ñ‰Ð°ÐµÑ‚ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹ Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸. ÐÐ° Ð¿ÑÑ‚Ð¸ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð½Ð°Ð±Ð¾Ñ€Ð°Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´Ð½ÑÑ Ð´Ð»Ð¸Ð½Ð° Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÑÐ¾ÐºÑ€Ð°Ñ‚Ð¸Ð»Ð°ÑÑŒ Ð±Ð¾Ð»ÐµÐµ Ñ‡ÐµÐ¼ Ð½Ð° 50%."
                },
                "en": {
                    "title": "Adaptive Reasoning for Efficient Inference in Complex Tasks",
                    "desc": "This paper addresses the challenge of efficiency in long-thought reasoning models used for complex tasks. It highlights that while some problems benefit from detailed reasoning, others do not, leading to the need for adaptive reasoning strategies. The authors propose a two-stage framework that combines long and short reasoning models to optimize reasoning depth based on the input. Their experiments show that this approach significantly reduces inference costs and reasoning length while maintaining performance across various mathematical datasets."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æŽ¨ç†ï¼Œæå‡æ•ˆçŽ‡ï¼",
                    "desc": "æœ€è¿‘ï¼Œé•¿æŽ¨ç†æ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å¯¼è‡´æŽ¨ç†å¼€é”€å¤§ï¼Œå› æ­¤æ•ˆçŽ‡æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„å®žè¯åˆ†æžæ˜¾ç¤ºï¼Œä½¿ç”¨é•¿é“¾æŽ¨ç†ï¼ˆLong-CoTï¼‰çš„å¥½å¤„åœ¨ä¸åŒé—®é¢˜ä¸Šå·®å¼‚å¾ˆå¤§ï¼šæœ‰äº›é—®é¢˜éœ€è¦å¤æ‚æŽ¨ç†ï¼Œè€Œå…¶ä»–é—®é¢˜åˆ™æ²¡æœ‰æ”¹å–„ï¼Œç”šè‡³å‡†ç¡®çŽ‡ä¸‹é™ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºè‡ªé€‚åº”æŽ¨ç†ç­–ç•¥ï¼Œæ ¹æ®è¾“å…¥è°ƒæ•´æŽ¨ç†æ·±åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æž¶ï¼Œé€šè¿‡æ··åˆé•¿çŸ­é“¾æŽ¨ç†æ¨¡åž‹å’ŒåŒå±‚åå¥½è®­ç»ƒï¼Œæ˜¾è‘—é™ä½ŽæŽ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19394",
            "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets",
            "url": "https://huggingface.co/papers/2504.19394",
            "abstract": "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.",
            "score": 5,
            "issue_id": 3558,
            "pub_date": "2025-04-27",
            "pub_date_card": {
                "ru": "27 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 27",
                "zh": "4æœˆ27æ—¥"
            },
            "hash": "5b65f8f309063f13",
            "authors": [
                "Toby Simonds"
            ],
            "affiliations": [
                "Tufa Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19394.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#agi",
                    "#benchmark",
                    "#games",
                    "#reasoning"
                ],
                "emoji": "ðŸš€",
                "ru": {
                    "title": "Ð˜Ð˜ Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ Ð»ÑŽÐ´ÐµÐ¹ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ñ€Ð°ÐºÐµÑ‚",
                    "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¼Ð¾Ñ‰Ð½Ñ‹Ñ… Ñ€Ð°ÐºÐµÑ‚ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ° RocketBench. ÐœÐ¾Ð´ÐµÐ»Ð¸ Ñ‚ÐµÑÑ‚Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð²Ñ‹ÑÐ¾Ñ‚Ñ‹ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹ Ð¿Ð¾ÑÐ°Ð´ÐºÐ¸. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ LLM Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð½Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ, Ð½Ð¾ Ð·Ð°Ñ‚Ñ€ÑƒÐ´Ð½ÑÑŽÑ‚ÑÑ Ñ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¹. ÐžÐ´Ð½Ð°ÐºÐ¾ Ð¿Ñ€Ð¸ ÑƒÑÐ¸Ð»ÐµÐ½Ð¸Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (RL) Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ 7 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð°Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ ÐºÐ°Ðº ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ñ‚Ð°Ðº Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð²-Ð»ÑŽÐ´ÐµÐ¹."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts LLMs for Rocket Design Challenges",
                    "desc": "This paper investigates the use of Large Language Models (LLMs) in the field of physical engineering, specifically in high-powered rocketry design. It introduces RocketBench, a benchmark that connects LLMs to advanced rocket simulations, and evaluates their performance on design tasks like optimizing target altitude and achieving precision landings. The results indicate that while LLMs possess a solid foundation in engineering knowledge, they struggle to improve their designs based on simulation feedback, often falling short of human capabilities. However, by incorporating reinforcement learning, a 7B parameter model surpasses both state-of-the-art models and human experts, highlighting the potential of RL-enhanced LLMs in complex engineering optimization tasks."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›ç«ç®­è®¾è®¡ï¼Œè¶…è¶Šäººç±»ä¸“å®¶ï¼",
                    "desc": "å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç‰©ç†å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨ä»ç„¶ä¸å¤Ÿæ·±å…¥ã€‚æœ¬æ–‡é€šè¿‡RocketBenchåŸºå‡†è¯„ä¼°LLMsåœ¨é«˜åŠŸçŽ‡ç«ç®­è®¾è®¡ä¸­çš„èƒ½åŠ›ï¼Œè¿žæŽ¥LLMsä¸Žé«˜ä¿çœŸç«ç®­æ¨¡æ‹Ÿã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸¤ä¸ªå¤æ‚çš„è®¾è®¡ä»»åŠ¡ï¼šç›®æ ‡é«˜åº¦ä¼˜åŒ–å’Œç²¾ç¡®ç€é™†æŒ‘æˆ˜ã€‚ç ”ç©¶å‘çŽ°ï¼Œå°½ç®¡æœ€å…ˆè¿›çš„LLMså±•çŽ°å‡ºå¼ºå¤§çš„å·¥ç¨‹çŸ¥è¯†ï¼Œä½†åœ¨æ ¹æ®æ¨¡æ‹Ÿç»“æžœè¿­ä»£è®¾è®¡æ—¶è¡¨çŽ°ä¸ä½³ï¼Œæœ€ç»ˆæœªèƒ½è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ï¼›ç„¶è€Œï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢žå¼ºåŽï¼Œ7Bå‚æ•°æ¨¡åž‹çš„è¡¨çŽ°è¶…è¿‡äº†çŽ°æœ‰åŸºç¡€æ¨¡åž‹å’Œäººç±»ä¸“å®¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20605",
            "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
            "url": "https://huggingface.co/papers/2504.20605",
            "abstract": "Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.",
            "score": 4,
            "issue_id": 3554,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "9637fef0c8d474ed",
            "authors": [
                "Mihai Nadas",
                "Laura Diosan",
                "Andrei Piscoran",
                "Andreea Tomescu"
            ],
            "affiliations": [
                "Babes-Bolyai University",
                "KlusAI Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20605.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#ethics",
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "ðŸ“š",
                "ru": {
                    "title": "ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð½Ð¾Ðµ Ð¼Ð¾Ñ€Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð²ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· Ð³Ð¸Ð³Ð°Ð½Ñ‚ÑÐºÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹",
                    "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ TF1-EN-3M - Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð· Ñ‚Ñ€ÐµÑ… Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² Ð°Ð½Ð³Ð»Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ñ… Ð±Ð°ÑÐµÐ½, ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ Ð´Ð¾ 8 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð¾Ð² Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð². ÐšÐ°Ð¶Ð´Ð°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ ÑÐ»ÐµÐ´ÑƒÐµÑ‚ ÑˆÐµÑÑ‚Ð¸ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð½Ð¾Ð¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ, Ð¾Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°Ñ ÑˆÐ¸Ñ€Ð¾ÐºÐ¸Ð¹ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑÐ¿ÐµÐºÑ‚Ñ€. ÐžÑ†ÐµÐ½ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°, ÑÐ¾Ñ‡ÐµÑ‚Ð°ÑŽÑ‰ÐµÐ³Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÑƒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ GPT Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ Ð¸ Ñ‡Ð¸Ñ‚Ð°Ð±ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸. Ð”Ð°Ñ‚Ð°ÑÐµÑ‚, ÐºÐ¾Ð´ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ ÑÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ñ‹Ð¿ÑƒÑ‰ÐµÐ½Ñ‹ Ð¿Ð¾Ð´ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð¹ Ð»Ð¸Ñ†ÐµÐ½Ð·Ð¸ÐµÐ¹, Ð¾Ñ‚ÐºÑ€Ñ‹Ð²Ð°Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼, Ð½Ð°Ñ€Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð° Ð¸ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð˜Ð˜."
                },
                "en": {
                    "title": "Empowering Moral Storytelling with TF1-EN-3M",
                    "desc": "This paper introduces TF1-EN-3M, a novel dataset containing three million English fables designed to teach moral lessons, generated by smaller instruction-tuned models. The stories are structured using a six-slot framework that ensures each narrative includes essential elements like characters, traits, and morals. A unique evaluation system combines assessments from a GPT-based critic and various metrics to ensure quality and diversity in the generated fables. The dataset and associated tools are made publicly available, promoting further research in areas like narrative intelligence and ethical AI development."
                },
                "zh": {
                    "title": "é“å¾·æ•…äº‹ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†TF1-EN-3Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«ä¸‰ç™¾ä¸‡ä¸ªè‹±è¯­å¯“è¨€çš„å¼€æ”¾æ•°æ®é›†ï¼Œä¸“é—¨ç”±æŒ‡ä»¤è°ƒä¼˜æ¨¡åž‹ç”Ÿæˆã€‚æ¯ä¸ªæ•…äº‹éµå¾ªå…­ä¸ªéƒ¨åˆ†çš„ç»“æž„ï¼ŒåŒ…æ‹¬è§’è‰²ã€ç‰¹å¾ã€èƒŒæ™¯ã€å†²çªã€è§£å†³æ–¹æ¡ˆå’Œé“å¾·ï¼Œç¡®ä¿äº†æ•…äº‹çš„è¿žè´¯æ€§å’Œä¸»é¢˜çš„å¤šæ ·æ€§ã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºŽGPTçš„è¯„åˆ†å’Œæ— å‚è€ƒçš„å¤šæ ·æ€§ä¸Žå¯è¯»æ€§æŒ‡æ ‡ã€‚TF1-EN-3Mä¸ºé“å¾·æ•…äº‹çš„ç”Ÿæˆå’Œæ•™è‚²AIçš„ç ”ç©¶æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œè¡¨æ˜Žå¤§è§„æ¨¡çš„é“å¾·å™äº‹ä¸å†éœ€è¦ä¸“æœ‰çš„å¤§åž‹æ¨¡åž‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18983",
            "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
            "url": "https://huggingface.co/papers/2504.18983",
            "abstract": "Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.",
            "score": 4,
            "issue_id": 3554,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 26",
                "zh": "4æœˆ26æ—¥"
            },
            "hash": "13143b6fb9ae9216",
            "authors": [
                "Xuyin Qi",
                "Zeyu Zhang",
                "Canxuan Gang",
                "Hao Zhang",
                "Lei Zhang",
                "Zhiwei Zhang",
                "Yang Zhao"
            ],
            "affiliations": [
                "AIML",
                "ANU",
                "La Trobe",
                "PSU",
                "UCAS",
                "UNSW"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18983.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#healthcare",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ðŸ©º",
                "ru": {
                    "title": "MediAug: ÐÐ¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð»Ð¾Ð½ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð¹ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ MediAug - ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð¹ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¸ÑÑÐ»ÐµÐ´ÑƒÑŽÑ‚ ÑˆÐµÑÑ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÑ Ð¸Ñ… Ðº Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¾Ð¿ÑƒÑ…Ð¾Ð»ÐµÐ¹ Ð¼Ð¾Ð·Ð³Ð° Ð¸ Ð³Ð»Ð°Ð·Ð½Ñ‹Ñ… Ð·Ð°Ð±Ð¾Ð»ÐµÐ²Ð°Ð½Ð¸Ð¹. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÑÑ‚ÑÑ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ÐºÐ°Ðº ÑÐ²ÐµÑ€Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹ (ResNet-50), Ñ‚Ð°Ðº Ð¸ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð² (ViT-B). Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð°ÑŽÑ‚ Ð½Ð°Ð¸Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€ Ð¸ Ð·Ð°Ð´Ð°Ñ‡, Ð¿Ð¾Ð´Ñ‡ÐµÑ€ÐºÐ¸Ð²Ð°Ñ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸."
                },
                "en": {
                    "title": "Enhancing Medical Imaging with Advanced Data Augmentation",
                    "desc": "This paper addresses the challenges of data augmentation in medical imaging, particularly the domain gap between natural images and medical scans. It introduces MediAug, a benchmark framework that evaluates six mix-based augmentation methods across different neural network architectures. The study finds that MixUp and SnapMix significantly enhance classification accuracy for brain tumors and eye diseases, respectively. By providing a systematic evaluation, the paper clarifies the effectiveness of various augmentation strategies in improving medical image analysis."
                },
                "zh": {
                    "title": "åŒ»å­¦å½±åƒæ•°æ®å¢žå¼ºçš„æ–°çªç ´",
                    "desc": "æ•°æ®å¢žå¼ºåœ¨åŒ»å­¦å½±åƒä¸­å¯¹äºŽæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€ç—…å˜æ£€æµ‹å’Œå™¨å®˜åˆ†å‰²è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼ŒåŒ»å­¦å½±åƒä¸Žè‡ªç„¶ç…§ç‰‡ä¹‹é—´çš„æ˜¾è‘—é¢†åŸŸå·®è·å¯èƒ½ä¼šæ‰­æ›²å…³é”®çš„ç–¾ç—…ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒçŽ°æœ‰çš„å¢žå¼ºç ”ç©¶å¾€å¾€å±€é™äºŽå•ä¸€ä»»åŠ¡æˆ–æž¶æž„ï¼Œå¯¼è‡´å…ˆè¿›çš„æ··åˆç­–ç•¥çš„ä¼˜åŠ¿ä¸æ˜Žç¡®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æž¶ï¼Œæ•´åˆäº†å…­ç§åŸºäºŽæ··åˆçš„æ•°æ®å¢žå¼ºæ–¹æ³•ï¼Œå¹¶åœ¨è„‘è‚¿ç˜¤MRIå’Œçœ¼ç—…è§†ç½‘è†œå›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00534",
            "title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System\n  for City Scale Traffic",
            "url": "https://huggingface.co/papers/2505.00534",
            "abstract": "Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.",
            "score": 2,
            "issue_id": 3560,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 Ð¼Ð°Ñ",
                "en": "May 1",
                "zh": "5æœˆ1æ—¥"
            },
            "hash": "d411bc0f9d34adf4",
            "authors": [
                "Muhammad Imran Zaman",
                "Usama Ijaz Bajwa",
                "Gulshan Saleem",
                "Rana Hammad Raza"
            ],
            "affiliations": [
                "Department of Computer Science, COMSATS University Islamabad, Lahore Campus, Lahore, Pakistan",
                "Pakistan Navy Engineering College, National University of Sciences and Technology (NUST), Pakistan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00534.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#video",
                    "#transfer_learning"
                ],
                "emoji": "ðŸš—",
                "ru": {
                    "title": "Ð£Ð¼Ð½Ð¾Ðµ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´ÑÑ‚Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÐºÐ°Ð¼ÐµÑ€ Ð² Ð³Ð¾Ñ€Ð¾Ð´ÑÐºÐ¸Ñ… ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÑŽÑ‚ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Mask R-CNN Ð´Ð»Ñ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¸ ResNet-152 Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð². Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¾ÐºÐºÐ»ÑŽÐ·Ð¸Ð¸, Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð¾ÑÐ²ÐµÑ‰ÐµÐ½Ð¸Ñ Ð¸ Ñ‚ÐµÐ½ÐµÐ¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ñ. Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð¿Ð¾ÐºÐ°Ð·Ð°Ð» Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð° Ð½Ð°Ð±Ð¾Ñ€Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… AI City Challenge, Ð´Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÐ² IDF1 0.8289."
                },
                "en": {
                    "title": "Revolutionizing Traffic Monitoring with Deep Learning Tracking",
                    "desc": "This paper presents a deep learning framework for Multi-Object Multi-Camera Tracking (MO-MCT) aimed at improving traffic monitoring in Intelligent Transportation Systems. The framework utilizes Mask R-CNN for detecting vehicles and applies Non-Maximum Suppression (NMS) to manage overlapping detections. It incorporates transfer learning for vehicle re-identification, allowing for the tracking of vehicles across different cameras despite challenges like occlusions and varying lighting conditions. The system is evaluated on a large dataset and shows strong performance metrics, indicating its potential for real-world traffic applications."
                },
                "zh": {
                    "title": "æ™ºèƒ½äº¤é€šä¸­çš„é«˜æ•ˆå¤šæ‘„åƒå¤´è·Ÿè¸ªè§£å†³æ–¹æ¡ˆ",
                    "desc": "éšç€ç½‘ç»œæ‘„åƒå¤´æ•°é‡çš„å¢žåŠ ï¼Œè§†è§‰ä¼ æ„Ÿå™¨åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢žå¼ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºŽæ·±åº¦å­¦ä¹ çš„å¤šç›®æ ‡å¤šæ‘„åƒå¤´è·Ÿè¸ªæ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³åŸŽå¸‚äº¤é€šåœºæ™¯ä¸­æ‰‹åŠ¨ç›®æ ‡è·Ÿè¸ªå’ŒåŒ¹é…çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æž¶åˆ©ç”¨Mask R-CNNè¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡éžæžå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰é€‰æ‹©é‡å æ£€æµ‹ä¸­çš„ç›®æ ‡ã€‚é€šè¿‡è¿ç§»å­¦ä¹ å®žçŽ°è½¦è¾†çš„é‡æ–°è¯†åˆ«ï¼Œç»“åˆé€‚å½“çš„æŸå¤±å‡½æ•°å’Œè·ç¦»åº¦é‡ï¼Œæœ€ç»ˆåœ¨AI City Challengeæ•°æ®é›†ä¸Šå–å¾—äº†ç«žäº‰åŠ›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18715",
            "title": "Spatial Speech Translation: Translating Across Space With Binaural\n  Hearables",
            "url": "https://huggingface.co/papers/2504.18715",
            "abstract": "Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.",
            "score": 2,
            "issue_id": 3566,
            "pub_date": "2025-04-25",
            "pub_date_card": {
                "ru": "25 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 25",
                "zh": "4æœˆ25æ—¥"
            },
            "hash": "979ba773cd3e90a0",
            "authors": [
                "Tuochao Chen",
                "Qirui Wang",
                "Runlin He",
                "Shyam Gollakota"
            ],
            "affiliations": [
                "Paul G. Allen School, University of Washington, Seattle, WA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18715.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "ðŸŽ§",
                "ru": {
                    "title": "ÐŸÑ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ Ñ€ÐµÑ‡Ð¸: ÑÐ»Ñ‹ÑˆÐ°Ñ‚ÑŒ Ð¼Ð¸Ñ€ Ð½Ð° ÑÐ²Ð¾ÐµÐ¼ ÑÐ·Ñ‹ÐºÐµ",
                    "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸ÑŽ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ñ€ÐµÑ‡Ð¸ Ð´Ð»Ñ ÑÐ»ÑƒÑ…Ð¾Ð²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð². Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ÑŒ Ñ€ÐµÑ‡ÑŒ Ð¾ÐºÑ€ÑƒÐ¶Ð°ÑŽÑ‰Ð¸Ñ… Ð»ÑŽÐ´ÐµÐ¹, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð³Ð¾Ð»Ð¾ÑÐ° ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰ÐµÐ³Ð¾ Ð² Ð±Ð¸Ð½Ð°ÑƒÑ€Ð°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ‹Ð²Ð¾Ð´Ðµ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€ÐµÑˆÐ°ÑŽÑ‚ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ ÑÐ»ÐµÐ¿Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð², Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð±Ð¸Ð½Ð°ÑƒÑ€Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ€ÐµÐ½Ð´ÐµÑ€Ð¸Ð½Ð³Ð°. ÐŸÑ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð² ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ… ÑÐ¸Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾Ð¼ÐµÑ… Ð¸ Ñ€ÐµÐ²ÐµÑ€Ð±ÐµÑ€Ð°Ñ†Ð¸Ð¸, Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ BLEU Ð´Ð¾ 22.01 Ð¿Ñ€Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ðµ Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ·Ñ‹ÐºÐ°Ð¼Ð¸."
                },
                "en": {
                    "title": "Transforming Speech Translation with Spatial Awareness",
                    "desc": "This paper presents a new approach to spatial speech translation using hearables, which are devices that can translate spoken language in real-time while preserving the spatial characteristics of the speakers. The authors address key challenges such as separating overlapping voices, accurately locating speakers, and ensuring that translations are expressive and timely, all while running efficiently on Apple M2 hardware. Their prototype demonstrates significant improvements over existing models, achieving a BLEU score of 22.01 in noisy environments, indicating high translation quality. User studies validate the system's ability to maintain the spatial cues of translated speech, marking a significant advancement in integrating spatial awareness into speech translation technology."
                },
                "zh": {
                    "title": "ç©ºé—´è¯­éŸ³ç¿»è¯‘ï¼šè®©äº¤æµæ— éšœç¢",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç©ºé—´è¯­éŸ³ç¿»è¯‘æ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡è€³æœºå°†çŽ¯å¢ƒä¸­è¯´è¯è€…çš„è¯­è¨€å®žæ—¶ç¿»è¯‘ä¸ºä½©æˆ´è€…çš„æ¯è¯­ï¼ŒåŒæ—¶ä¿ç•™æ¯ä½è¯´è¯è€…çš„æ–¹å‘å’Œç‹¬ç‰¹å£°éŸ³ç‰¹å¾ã€‚ä¸ºå®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶è§£å†³äº†å¤šä¸ªæŠ€æœ¯æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç›²æºåˆ†ç¦»ã€å®šä½ã€å®žæ—¶è¡¨è¾¾ç¿»è¯‘å’ŒåŒè€³æ¸²æŸ“ï¼Œä»¥ç¡®ä¿ç¿»è¯‘éŸ³é¢‘ä¸­çš„è¯´è¯è€…æ–¹å‘å¾—ä»¥ä¿ç•™ã€‚é€šè¿‡åŽŸåž‹åŒè€³è€³æœºçš„æ¦‚å¿µéªŒè¯è¯„ä¼°ï¼Œç ”ç©¶è¡¨æ˜Žè¯¥ç³»ç»Ÿåœ¨å¼ºå¹²æ‰°çŽ¯å¢ƒä¸­ä»èƒ½å®žçŽ°é«˜è¾¾22.01çš„BLEUåˆ†æ•°ï¼Œä¼˜äºŽçŽ°æœ‰æ¨¡åž‹ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥ç¡®è®¤äº†è¯¥ç³»ç»Ÿåœ¨çœŸå®žçŽ¯å¢ƒä¸­ç©ºé—´æ¸²æŸ“ç¿»è¯‘è¯­éŸ³çš„æœ‰æ•ˆæ€§ï¼Œæ ‡å¿—ç€å°†ç©ºé—´æ„ŸçŸ¥æ•´åˆåˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„ç¬¬ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20406",
            "title": "Skill Discovery for Software Scripting Automation via Offline\n  Simulations with LLMs",
            "url": "https://huggingface.co/papers/2504.20406",
            "abstract": "Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.",
            "score": 1,
            "issue_id": 3570,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ð°Ð¿Ñ€ÐµÐ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "995b07a7273b51fd",
            "authors": [
                "Paiheng Xu",
                "Gang Wu",
                "Xiang Chen",
                "Tong Yu",
                "Chang Xiao",
                "Franck Dernoncourt",
                "Tianyi Zhou",
                "Wei Ai",
                "Viswanathan Swaminathan"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20406.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#alignment",
                    "#graphs",
                    "#games",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ðŸ¤–",
                "ru": {
                    "title": "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±ÐµÐ· ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: LLM ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ ÑÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹",
                    "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÑŽ ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð´Ð°Ñ‡ Ð² Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð½Ð¾Ð¼ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ð±ÐµÐ· Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¾Ñ„Ð»Ð°Ð¹Ð½-ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð½Ð°Ð±Ð¾Ñ€Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐµÐ½Ð½Ñ‹Ñ… ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð¸ Ð¾Ð±Ñ‰ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð² Ð¿Ð¾ ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¸Ð½Ð³Ñƒ. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð², Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð³Ñ€Ð°Ñ„Ð¾Ð²Ñ‹Ñ… Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹ (GNN) Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÐ²ÑÐ·ÐµÐ¹ Ð¼ÐµÐ¶Ð´Ñƒ API. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ñ Adobe Illustrator Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ÑÑ‚Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸, ÑÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¾Ñ‚ÐºÐ»Ð¸ÐºÐ° Ð¸ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÑŽ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ ÐºÐ¾Ð´Ð° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ."
                },
                "en": {
                    "title": "Empowering Users with Automated Scripting through AI",
                    "desc": "This paper presents a framework that helps users automate tasks in software without needing deep programming skills. It uses Large Language Models (LLMs) to generate verified scripts by simulating tasks and refining them based on execution feedback. The framework includes a Graph Neural Network (GNN) to explore API synergies, which helps create a diverse set of useful scripts. Experiments show that this approach improves automation success rates and reduces costs compared to traditional methods."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæå‡è½¯ä»¶è„šæœ¬è‡ªåŠ¨åŒ–çš„åˆ›æ–°æ¡†æž¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¦»çº¿æ¨¡æ‹Ÿæ¡†æž¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å’Œå…¬å¼€çš„è„šæœ¬æŒ‡å—ï¼Œåˆ›å»ºä¸€ä¸ªè½¯ä»¶ç‰¹å®šçš„æŠ€èƒ½é›†ã€‚è¯¥æ¡†æž¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä»»åŠ¡åˆ›å»ºå’ŒæŠ€èƒ½ç”Ÿæˆï¼Œé€šè¿‡æ‰§è¡Œåé¦ˆæ¥ä¼˜åŒ–å’ŒéªŒè¯è„šæœ¬ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºŽå›¾ç¥žç»ç½‘ç»œï¼ˆGNNï¼‰çš„é“¾æŽ¥é¢„æµ‹æ¨¡åž‹ï¼Œä»¥æ•æ‰APIä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»Žè€Œç”Ÿæˆå¤šæ ·åŒ–çš„æŠ€èƒ½ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ¡†æž¶åœ¨è‡ªåŠ¨åŒ–æˆåŠŸçŽ‡ã€å“åº”æ—¶é—´å’Œè¿è¡Œæ—¶æˆæœ¬æ–¹é¢æ˜¾è‘—ä¼˜äºŽä¼ ç»Ÿçš„ä»£ç ç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-01.html",
    "link_next": "2025-05-05.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.05",
        "en": "05/05",
        "zh": "5æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 3,
        "#cv": 3,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰æŠ€æœ¯ã€‚IGVç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œè®©ç”¨æˆ·é€šè¿‡æŽ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸Žå…¶ä¸­ã€‚æ–‡ç« è°ƒæŸ¥äº†IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚è¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªæ¨¡å—ï¼šç”Ÿæˆã€æŽ§åˆ¶ã€è®°å¿†ã€åŠ¨æ€å’Œæ™ºèƒ½ã€‚æœ€åŽï¼Œæ–‡ç« åˆ†æžäº†æ¯ä¸ªç»„ä»¶çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚",
        "title": "A Survey of Interactive Generative Video",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰æŠ€æœ¯ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le hÃ¹dÃ²ng shÄ“ngchÃ©ng shÃ¬pÃ­n (IGV) jÃ¬shÃ¹.\n\nIGVç»“åˆç”Ÿæˆå’Œäº’åŠ¨åŠŸèƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§†é¢‘å†…å®¹ï¼Œè®©ç”¨æˆ·é€šè¿‡æŽ§åˆ¶ä¿¡å·å’Œå“åº”åé¦ˆå‚ä¸Žå…¶ä¸­ã€‚\nIGV jiÃ©hÃ© shÄ“ngchÃ©ng hÃ© hÃ¹dÃ²ng gÅngnÃ©ng, chÇŽnshÄ“ng gÄo zhÃ¬liÃ ng shÃ¬pÃ­n nÃ¨irÃ³ng, rÃ ng yÃ²nghÃ¹ tÅngguÃ² kÃ²ngzhÃ¬ xÃ¬nhÃ o hÃ© xiÇŽngyÃ¬ng fÇŽnkuÃ¬ cÄnyÃ¹ qÃ­zhÅng.\n\næ–‡ç« è°ƒæŸ¥äº†IGVåœ¨æ¸¸æˆã€äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚\nWÃ©nzhÄng diÃ ochÃ¡ le IGV zÃ i yÃ³uxÃ¬, rÃ©n gÅng zhÃ¬nÃ©ng hÃ© zÃ¬dÃ²ng jiÃ shÇ zhÅng de yÃ¬ngyÃ²ng.\n\nè¿˜æå‡ºäº†ç†æƒ³IGVç³»ç»Ÿçš„äº”ä¸ªæ¨¡å—ï¼šç”Ÿæˆã€æŽ§åˆ¶ã€è®°å¿†ã€åŠ¨æ€å’Œæ™ºèƒ½ã€‚\nHÃ¡i tÃ­chÅ« le lÇxiÇŽng IGV xÃ¬tÇ’ng de wÇ” gÃ¨ mÃ³kuÃ i: shÄ“ngchÃ©ng, kÃ²ngzhÃ¬, jÃ¬yÃ¬, dÃ²ngtÃ i hÃ© zhÃ¬nÃ©ng.\n\næœ€åŽï¼Œæ–‡ç« åˆ†æžäº†æ¯ä¸ªç»„ä»¶çš„æŠ€æœ¯æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚\nZuÃ¬hÃ²u, wÃ©nzhÄng fÄ“nxi le mÄ›i gÃ¨ zÇ”jiÃ n de jÃ¬shÃ¹ tiÇŽozhÃ n hÃ© wÃ¨ilÃ¡i fÄngxiÃ ng.",
        "vocab": "[\n    {\"word\": \"äº’åŠ¨\", \"pinyin\": \"hÃ¹ dÃ²ng\", \"trans\": \"interactive\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"æŠ€æœ¯\", \"pinyin\": \"jÃ¬ shÃ¹\", \"trans\": \"technology\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ© hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"åŠŸèƒ½\", \"pinyin\": \"gÅng nÃ©ng\", \"trans\": \"function\"},\n    {\"word\": \"äº§ç”Ÿ\", \"pinyin\": \"chÇŽn shÄ“ng\", \"trans\": \"produce\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"ç”¨æˆ·\", \"pinyin\": \"yÃ²ng hÃ¹\", \"trans\": \"user\"},\n    {\"word\": \"æŽ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"control\"},\n    {\"word\": \"ä¿¡å·\", \"pinyin\": \"xÃ¬n hÃ o\", \"trans\": \"signal\"},\n    {\"word\": \"å“åº”\", \"pinyin\": \"xiÇŽng yÃ¬ng\", \"trans\": \"response\"},\n    {\"word\": \"åé¦ˆ\", \"pinyin\": \"fÇŽn kuÃ¬\", \"trans\": \"feedback\"},\n    {\"word\": \"å‚ä¸Ž\", \"pinyin\": \"cÄn yÃ¹\", \"trans\": \"participate\"},\n    {\"word\": \"è°ƒæŸ¥\", \"pinyin\": \"diÃ o chÃ¡\", \"trans\": \"investigate\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"æ¸¸æˆ\", \"pinyin\": \"yÃ³u xÃ¬\", \"trans\": \"game\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"è‡ªåŠ¨é©¾é©¶\", \"pinyin\": \"zÃ¬ dÃ²ng jiÃ  shÇ\", \"trans\": \"autonomous driving\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç†æƒ³\", \"pinyin\": \"lÇ xiÇŽng\", \"trans\": \"ideal\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"module\"},\n    {\"word\": \"è®°å¿†\", \"pinyin\": \"jÃ¬ yÃ¬\", \"trans\": \"memory\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ng tÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"æ™ºèƒ½\", \"pinyin\": \"zhÃ¬ nÃ©ng\", \"trans\": \"intelligence\"},\n    {\"word\": \"ç»„ä»¶\", \"pinyin\": \"zÇ” jiÃ n\", \"trans\": \"component\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇŽo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article introduces Interactive Generative Video (IGV) technology. IGV combines generative and interactive functions to produce high-quality video content, allowing users to participate through control signals and response feedback. The article surveys the applications of IGV in gaming, artificial intelligence, and autonomous driving. It also proposes five modules for an ideal IGV system: generation, control, memory, dynamics, and intelligence. Finally, the article analyzes the technical challenges and future directions for each component.",
        "update_ts": "2025-05-02 09:11"
    }
}