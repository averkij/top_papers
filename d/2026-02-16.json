{
    "date": {
        "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 16",
        "zh": "2æœˆ16æ—¥"
    },
    "time_utc": "2026-02-16 08:38",
    "weekday": 0,
    "issue_id": 1068,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.10388",
            "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
            "url": "https://huggingface.co/papers/2602.10388",
            "abstract": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
            "score": 183,
            "issue_id": 1066,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "5cad469a19135945",
            "authors": [
                "Zhongzhi Li",
                "Xuansheng Wu",
                "Yijiang Li",
                "Lijie Hu",
                "Ninghao Liu"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "The Hong Kong Polytechnic University",
                "University of California, San Diego",
                "University of Georgia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10388.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#optimization",
                    "#data",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Feature Activation Coverage (FAC) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° FAC Synthesis, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Models with Feature Activation Coverage",
                    "desc": "This paper introduces Feature Activation Coverage (FAC), a new metric for measuring data diversity in a way that is interpretable and relevant to downstream tasks in large language models (LLMs). Unlike traditional text-based diversity metrics, FAC focuses on task-relevant features that directly impact model performance. The authors also present FAC Synthesis, a framework that uses a sparse autoencoder to identify and generate synthetic data samples that enhance feature diversity. Their experiments demonstrate that this approach significantly improves performance across various tasks, while also facilitating knowledge transfer between different model architectures."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¤šæ ·æ€§é©±åŠ¨åˆæˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œç§°ä¸ºç‰¹å¾æ¿€æ´»è¦†ç›–ï¼ˆFeature Activation Coverageï¼ŒFACï¼‰ï¼Œç”¨äºè¯„ä¼°å¯è§£é‡Šç‰¹å¾ç©ºé—´ä¸­çš„æ•°æ®å¤šæ ·æ€§ã€‚é€šè¿‡è¿™ä¸€æŒ‡æ ‡ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºFACåˆæˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨è¯†åˆ«ç§å­æ•°æ®é›†ä¸­ç¼ºå¤±çš„ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆåæ˜ è¿™äº›ç‰¹å¾çš„åˆæˆæ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼ˆå¦‚æŒ‡ä»¤è·Ÿéšã€æ¯’æ€§æ£€æµ‹ã€å¥–åŠ±å»ºæ¨¡å’Œè¡Œä¸ºå¼•å¯¼ï¼‰æ˜¾è‘—æé«˜äº†æ•°æ®å¤šæ ·æ€§å’Œä¸‹æ¸¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ä¸åŒæ¨¡å‹å®¶æ—ä¹‹é—´å­˜åœ¨å…±äº«çš„å¯è§£é‡Šç‰¹å¾ç©ºé—´ï¼Œä»è€Œå®ç°è·¨æ¨¡å‹çŸ¥è¯†è½¬ç§»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11858",
            "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
            "url": "https://huggingface.co/papers/2602.11858",
            "abstract": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
            "score": 49,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "317748ff53afa40a",
            "authors": [
                "Lai Wei",
                "Liangbo He",
                "Jun Lan",
                "Lingzhong Dong",
                "Yutong Cai",
                "Siyuan Li",
                "Huijia Zhu",
                "Weiqiang Wang",
                "Linghe Kong",
                "Yue Wang",
                "Zhuosheng Zhang",
                "Weiran Huang"
            ],
            "affiliations": [
                "Ant Group",
                "School of Computer Science, Shanghai Jiao Tong University",
                "Shanghai Innovation Institute",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11858.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¾Ğ·Ñ‘Ñ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Region-to-Image Distillation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ ÑÑ‚Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ĞµÑ‘ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ZoomBench Ñ 845 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑˆĞµÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹."
                },
                "en": {
                    "title": "Zooming In for Better Visual Understanding",
                    "desc": "This paper introduces Region-to-Image Distillation, a method that enhances fine-grained visual perception in Multimodal Large Language Models (MLLMs) by allowing them to perform iterative zooming during inference. By transforming the zooming process from inference to training, the model can learn to focus on important details without needing repeated tool calls or visual re-encoding. The authors create a dataset called ZoomBench to evaluate the model's performance on fine-grained visual question answering (VQA) tasks. Results show that the distilled models achieve superior performance on various benchmarks, improving both fine-grained perception and overall multimodal cognition."
                },
                "zh": {
                    "title": "åŒºåŸŸåˆ°å›¾åƒè’¸é¦ï¼šæå‡ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºåŒºåŸŸåˆ°å›¾åƒè’¸é¦çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡åœ¨è®­ç»ƒé˜¶æ®µå†…éƒ¨åŒ–è¿­ä»£ç¼©æ”¾çš„è¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†æ¨ç†æ—¶å¯¹å·¥å…·çš„é‡å¤è°ƒç”¨å’Œè§†è§‰é‡ç¼–ç çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è§†è§‰é—®ç­”æ•°æ®ï¼Œå¹¶å°†è¿™ç§åŒºåŸŸåŸºç¡€çš„ç›‘ç£ä¿¡æ¯è’¸é¦åˆ°å®Œæ•´å›¾åƒä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è¿™ç§è®­ç»ƒçš„å°å‹å­¦ç”Ÿæ¨¡å‹åœ¨ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¹Ÿæå‡äº†å¤šæ¨¡æ€è®¤çŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12705",
            "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
            "url": "https://huggingface.co/papers/2602.12705",
            "abstract": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.",
            "score": 39,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4d7d55c2c4c47d84",
            "authors": [
                "Baorong Shi",
                "Bo Cui",
                "Boyuan Jiang",
                "Deli Yu",
                "Fang Qian",
                "Haihua Yang",
                "Huichao Wang",
                "Jiale Chen",
                "Jianfei Pan",
                "Jieqiong Cao",
                "Jinghao Lin",
                "Kai Wu",
                "Lin Yang",
                "Shengsheng Yao",
                "Tao Chen",
                "Xiaojun Xiao",
                "Xiaozhong Ji",
                "Xu Wang",
                "Yijun He",
                "Zhixiong Yang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12705.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#reasoning",
                    "#benchmark",
                    "#science",
                    "#open_source",
                    "#agents",
                    "#cv",
                    "#multimodal",
                    "#rl",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ˜Ğ˜ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "MedXIAOHE â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ foundation model, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ entity-aware continual pretraining Ğ´Ğ»Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ reinforcement learning Ğ¸ agentic training Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Medical Diagnostics with MedXIAOHE",
                    "desc": "MedXIAOHE is a cutting-edge medical vision-language model that enhances clinical understanding through advanced training techniques. It utilizes entity-aware continual pretraining to improve knowledge coverage, especially for rare diseases, and employs reinforcement learning for effective diagnostic reasoning. The model is designed to provide reliable interactions in clinical settings by integrating user preferences and evidence-based reasoning. With its ability to generate detailed reports and maintain low hallucination rates, MedXIAOHE sets a new standard for multimodal systems in healthcare."
                },
                "zh": {
                    "title": "MedXIAOHEï¼šæå‡åŒ»ç–—ç†è§£ä¸æ¨ç†çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "MedXIAOHEæ˜¯ä¸€ä¸ªåŒ»ç–—è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å®ä½“æ„ŸçŸ¥çš„æŒç»­é¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œå·¥å…·å¢å¼ºçš„æ™ºèƒ½è®­ç»ƒæ¥æå‡ä¸´åºŠç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šé¢†å…ˆçš„é—­æºå¤šæ¨¡æ€ç³»ç»Ÿã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒMedXIAOHEæå‡ºäº†ä¸€ç§å®ä½“æ„ŸçŸ¥çš„æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œç»„ç»‡å¼‚æ„åŒ»ç–—è¯­æ–™åº“ï¼Œä»¥æ‰©å¤§çŸ¥è¯†è¦†ç›–é¢å¹¶å‡å°‘é•¿å°¾é—®é¢˜ã€‚é€šè¿‡æ•´åˆç”¨æˆ·åå¥½æ ‡å‡†å’ŒåŸºäºè¯æ®çš„æ¨ç†ï¼ŒMedXIAOHEæé«˜äº†åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œæ”¯æŒå¤šæ­¥éª¤çš„å¯éªŒè¯è¯Šæ–­æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.08683",
            "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
            "url": "https://huggingface.co/papers/2602.08683",
            "abstract": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.   Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.   Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
            "score": 31,
            "issue_id": 1065,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "c412ab0be214f7a5",
            "authors": [
                "Feilong Tang",
                "Xiang An",
                "Yunyao Yan",
                "Yin Xie",
                "Bin Qin",
                "Kaicheng Yang",
                "Yifei Shen",
                "Yuanhan Zhang",
                "Chunyuan Li",
                "Shikun Feng",
                "Changrui Chen",
                "Huajie Tan",
                "Ming Hu",
                "Manyuan Zhang",
                "Bo Li",
                "Ziyong Feng",
                "Ziwei Liu",
                "Zongyuan Ge",
                "Jiankang Deng"
            ],
            "affiliations": [
                "AIM for Health Lab",
                "Glint Lab",
                "MVP Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.08683.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²ÑĞµ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾, Ñ‚Ñ€Ğ°Ñ‚Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ğ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ OneVision-Encoder Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 3-25% Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Qwen3-ViT Ğ½Ğ° 16 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Aligning Visual Understanding with Compression Principles",
                    "desc": "This paper presents a new approach to visual understanding by aligning deep learning architectures with information-theoretic principles of video compression. The authors introduce the OneVision-Encoder, which focuses on encoding only the most informative parts of video data, rather than processing all pixels uniformly. By using a method called Codec Patchification, the model efficiently captures the essential features of motion and meaning while discarding redundant information. The results show that this approach not only improves accuracy but also enhances efficiency, demonstrating that effective visual understanding can be achieved through smarter data representation."
                },
                "zh": {
                    "title": "å¯¹é½ä¿¡æ¯è®ºåŸç†ï¼Œæå‡è§†è§‰ç†è§£æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ç†è§£æ–¹æ³•ï¼Œå¼ºè°ƒå°†æ¶æ„ä¸ä¿¡æ¯è®ºçš„å‹ç¼©åŸç†å¯¹é½ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–ç¼–ç ï¼ŒOneVision-Encoderèƒ½å¤Ÿé«˜æ•ˆåœ°å‹ç¼©è§†é¢‘ä¸­çš„é¢„æµ‹è§†è§‰ç»“æ„ï¼Œä¸“æ³¨äºä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ•ˆç‡å’Œå‡†ç¡®æ€§å¹¶ä¸æ˜¯ç›¸äº’å¦¥åçš„ï¼Œè€Œæ˜¯æ­£ç›¸å…³çš„ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨è§†è§‰é€šç”¨æ€§æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12395",
            "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
            "url": "https://huggingface.co/papers/2602.12395",
            "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
            "score": 9,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "b1420fd35524cc99",
            "authors": [
                "Xirui Li",
                "Ming Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12395.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¤Ñ€Ğ°Ğ½ĞºĞµĞ½ÑˆÑ‚ĞµĞ¹Ğ½-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·: RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸, Ğ° Ğ½Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°ĞºĞ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ¾Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ RL Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾, Ğ° Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² RL, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Refining Reasoning: The Power of Reinforcement Learning in Vision-Language Models",
                    "desc": "This paper investigates how reinforcement learning (RL) enhances visual reasoning in vision-language models compared to traditional supervised fine-tuning. The authors introduce a framework to analyze the specific improvements brought by RL, focusing on the mid-to-late layers of the model. They find that RL leads to significant refinements in these layers, which are crucial for aligning visual inputs with reasoning tasks. The study emphasizes that RL's benefits are not just about better visual perception but involve a targeted enhancement of computational processes that support reasoning, challenging the reliance on benchmark scores alone for evaluation."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰æ¨ç†çš„ç³»ç»Ÿæ€§ä¼˜åŒ–",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯éªŒè¯å¥–åŠ±çš„æƒ…å†µä¸‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLåœ¨æ¨ç†èƒ½åŠ›çš„æå‡ä¸Šå¹¶ä¸æ˜¯ç®€å•çš„è§†è§‰æ„ŸçŸ¥å¢å¼ºï¼Œè€Œæ˜¯å¯¹ä¸­åå±‚è®¡ç®—çš„ç³»ç»Ÿæ€§ä¼˜åŒ–ã€‚é€šè¿‡åŠŸèƒ½å®šä½ã€å‚æ•°æ¯”è¾ƒå’Œæ¨¡å‹åˆå¹¶ç­‰æ–¹æ³•ï¼Œåˆ†æäº†RLå¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼ŒRLçš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨æ¨ç†å¯¹é½å’Œæ¨ç†æ€§èƒ½çš„æå‡ï¼Œè€Œä¸æ˜¯å•ä¸€çš„è§†è§‰æ„ŸçŸ¥æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12628",
            "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
            "url": "https://huggingface.co/papers/2602.12628",
            "abstract": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and Ï€_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on Ï€_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
            "score": 8,
            "issue_id": 1066,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "733ea7b02894402c",
            "authors": [
                "Liangzhi Shi",
                "Shuaihang Chen",
                "Feng Gao",
                "Yinuo Chen",
                "Kang Chen",
                "Tonghe Zhang",
                "Hongzhi Zhang",
                "Weinan Zhang",
                "Chao Yu",
                "Yu Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Harbin Institute of Technology",
                "Peking University",
                "Shanghai AI Laboratory",
                "Tsinghua University",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12628.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#robotics",
                    "#transfer_learning",
                    "#multimodal",
                    "#rl",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğµ: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ ÑĞºĞ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ·Ñ€ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞµÑ‘ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ - Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 24% Ğ´Ğ»Ñ OpenVLA Ğ¸ Ğ½Ğ° 20% Ğ´Ğ»Ñ Ï€â‚€.â‚… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Real-World Performance with RL-Co Training",
                    "desc": "This paper presents a reinforcement learning-based framework called RL-Co for improving the performance of vision-language-action (VLA) policies in real-world tasks. Unlike traditional methods that rely on supervised fine-tuning, RL-Co utilizes interactive simulation to enhance training while maintaining real-world capabilities. The framework consists of two stages: initial warm-starting with a mix of real and simulated data, followed by reinforcement learning fine-tuning with an auxiliary supervised loss to anchor the policy. The results show significant improvements in real-world success rates and generalization to new tasks, demonstrating the effectiveness of combining simulation with real-world data."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„å®ç”¨æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ä»¿çœŸ-çœŸå®è”åˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤äº’å¼ä»¿çœŸå’ŒçœŸå®æ•°æ®çš„ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å¯¹é™æ€æ¼”ç¤ºçš„ä¾èµ–ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®¾è®¡ï¼Œé¦–å…ˆåœ¨çœŸå®å’Œä»¿çœŸæ•°æ®çš„æ··åˆä¸Šè¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶ååœ¨ä»¿çœŸä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å¼•å…¥çœŸå®æ•°æ®çš„è¾…åŠ©ç›‘ç£æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„åˆ©ç”¨ä»¿çœŸæŠ€æœ¯æå‡çœŸå®æœºå™¨äººéƒ¨ç½²çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11236",
            "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
            "url": "https://huggingface.co/papers/2602.11236",
            "abstract": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
            "score": 8,
            "issue_id": 1065,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "812e2196c80382aa",
            "authors": [
                "Yandan Yang",
                "Shuang Zeng",
                "Tong Lin",
                "Xinyuan Chang",
                "Dekang Qi",
                "Junjin Xiao",
                "Haoyun Liu",
                "Ronghan Chen",
                "Yuzhi Chen",
                "Dongjie Huo",
                "Feng Xiong",
                "Xing Wei",
                "Zhiheng Ma",
                "Mu Xu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.11236.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#multimodal",
                    "#3d",
                    "#dataset",
                    "#training",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¼Ğ¾Ğ·Ğ³ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ‚ĞµĞ»: ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ÑÑ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹",
                    "desc": "ABot-M0 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UniACT-dataset Ñ 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ° Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (Action Manifold Learning), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ»ĞµĞ¶Ğ°Ñ‚ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Unified Framework for Efficient Embodied Agent Development",
                    "desc": "ABot-M0 is a comprehensive framework designed to enhance the development of embodied agents by standardizing various robotic data and utilizing Action Manifold Learning (AML) for better prediction efficiency. It addresses challenges in robotics, such as fragmented data and inconsistent training objectives, by creating a systematic data curation pipeline and a large-scale dataset called UniACT, which includes over 6 million trajectories. The framework emphasizes the Action Manifold Hypothesis, suggesting that effective robot actions exist on a low-dimensional manifold, which allows for improved action prediction and stability. Additionally, ABot-M0 incorporates modular perception capabilities, integrating visual language models with geometric information to enhance spatial understanding in robotic tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶ï¼Œæå‡æœºå™¨äººæ™ºèƒ½",
                    "desc": "ABot-M0æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘å…·èº«æ™ºèƒ½ä½“ï¼Œæ ‡å‡†åŒ–ä¸åŒçš„æœºå™¨äººæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŠ¨ä½œæµå½¢å­¦ä¹ æ¥æé«˜é¢„æµ‹æ•ˆç‡å’Œç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿçš„æ•°æ®æ•´ç†æµç¨‹ï¼Œä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œå°†å¼‚æ„åŸå§‹æ•°æ®è½¬åŒ–ä¸ºç»Ÿä¸€çš„é«˜æ•ˆè¡¨ç¤ºã€‚æˆ‘ä»¬æ„å»ºäº†UniACTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡600ä¸‡æ¡è½¨è¿¹å’Œ9500å°æ—¶çš„æ•°æ®ï¼Œæ¶µç›–å¤šç§æœºå™¨äººå½¢æ€å’Œä»»åŠ¡åœºæ™¯ã€‚é€šè¿‡ç»Ÿä¸€é¢„è®­ç»ƒï¼ŒABot-M0æ”¯æŒè·¨å¹³å°å’Œä»»åŠ¡çš„çŸ¥è¯†è¿ç§»ä¸æ³›åŒ–ï¼Œæ¨åŠ¨äº†é€šç”¨å…·èº«æ™ºèƒ½çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11865",
            "title": "Intelligent AI Delegation",
            "url": "https://huggingface.co/papers/2602.11865",
            "abstract": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.",
            "score": 6,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "cc77000f37a598af",
            "authors": [
                "Nenad TomaÅ¡ev",
                "Matija Franklin",
                "Simon Osindero"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11865.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ĞµĞ¶Ğ´Ñƒ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ±Ğ¾ÑĞ¼Ğ¸.æ¡†æ¶Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ¾Ñ‡Ğ¸Ğ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ ĞºĞ°Ğº Ğº AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°Ğ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚Ğ°ĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering AI Agents with Adaptive Delegation Frameworks",
                    "desc": "This paper presents an adaptive framework for AI agents to improve task decomposition and delegation. The framework allows AI agents to break down complex tasks into smaller parts and delegate them effectively to other agents or humans. It emphasizes the importance of dynamic adaptation to environmental changes and the ability to manage unexpected failures through structured authority transfer and trust mechanisms. The proposed approach aims to enhance collaboration in complex networks of AI and human agents, paving the way for more efficient task management."
                },
                "zh": {
                    "title": "æ™ºèƒ½AIä»£ç†çš„è‡ªé€‚åº”ä»»åŠ¡å§”æ‰˜æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºæ™ºèƒ½AIä»£ç†çš„ä»»åŠ¡åˆ†è§£å’Œå§”æ‰˜ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€å“åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–çš„æƒå¨è½¬ç§»å’Œä¿¡ä»»æœºåˆ¶æ¥å¤„ç†æ„å¤–å¤±è´¥ã€‚AIä»£ç†å¯ä»¥å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œå¹¶å®‰å…¨åœ°å°†å…¶å§”æ‰˜ç»™å…¶ä»–AIä»£ç†æˆ–äººç±»ã€‚æ­¤æ¡†æ¶é€‚ç”¨äºå¤æ‚çš„å§”æ‰˜ç½‘ç»œï¼Œæ—¨åœ¨ä¸ºæ–°å…´çš„ä»£ç†ç½‘ç»œå¼€å‘åè®®æä¾›æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13013",
            "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
            "url": "https://huggingface.co/papers/2602.13013",
            "abstract": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
            "score": 4,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4a007355c8a4f31f",
            "authors": [
                "Yunheng Li",
                "Hengrui Zhang",
                "Meng-Hao Guo",
                "Wenzhao Gao",
                "Shaoyong Jia",
                "Shaohui Jiao",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Tsinghua University",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13013.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#hallucinations",
                    "#synthetic",
                    "#video",
                    "#open_source",
                    "#multimodal",
                    "#dataset",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ASID-1M Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ASID-Verify Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ASID-Captioner Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ (Supervised Fine-Tuning) Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini-3-Pro Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Structured Annotations",
                    "desc": "This paper presents a new dataset and model aimed at enhancing audiovisual understanding in videos. The dataset, ASID-1M, contains one million structured annotations that provide detailed descriptions of audiovisual content, allowing for better training of models. The ASID-Captioner model utilizes these annotations through supervised fine-tuning to improve the quality of generated captions and minimize inaccuracies, known as hallucinations. The results demonstrate that this approach significantly outperforms existing models in various tasks related to audiovisual captioning and understanding."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦è§†å¬ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œç”¨äºç»†ç²’åº¦çš„è§†å¬ç†è§£ã€‚é€šè¿‡ç»“æ„åŒ–æ³¨é‡Šå’Œç›‘ç£å¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†å­—å¹•è´¨é‡å¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ASID-1Mæ•°æ®é›†ï¼ŒåŒ…å«ä¸€ç™¾ä¸‡ä¸ªç»“æ„åŒ–çš„è§†å¬æŒ‡ä»¤æ³¨é‡Šï¼Œä»¥åŠASID-Verifyæ•°æ®ç­–åˆ’ç®¡é“ï¼Œç¡®ä¿æè¿°ä¸è§†å¬å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASID-Captioneræ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†ç»†ç²’åº¦å­—å¹•è´¨é‡å¹¶æ”¹å–„äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04163",
            "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
            "url": "https://huggingface.co/papers/2602.04163",
            "abstract": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.",
            "score": 4,
            "issue_id": 1066,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "482e431c02f539b6",
            "authors": [
                "Junyu Chen",
                "Jungang Li",
                "Jing Xiong",
                "Wenjie Wang",
                "Qingyao Yang",
                "He Xiao",
                "Zhen Li",
                "Taiqiang Wu",
                "Mengzhao Chen",
                "Zhen Peng",
                "Chaofan Tao",
                "Long Shi",
                "Hongxia Yang",
                "Ngai Wong"
            ],
            "affiliations": [
                "Artificial Intelligence and Digital Finance Key Laboratory of Sichuan Province",
                "Southwestern University of Finance and Economics",
                "Sun Yat-sen University",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04163.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ BPDQ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BPDQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 72B Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ GPU."
                },
                "en": {
                    "title": "Enhancing Low-Bit Quantization with BPDQ for Better LLM Performance",
                    "desc": "The paper introduces Bit-Plane Decomposition Quantization (BPDQ), a novel approach to improve low-bit quantization for large language model (LLM) inference. BPDQ utilizes variable quantization grids derived from bit-planes and scalar coefficients, allowing for better accuracy compared to traditional fixed-grid methods. This technique addresses the limitations of post-training quantization (PTQ) by minimizing output discrepancies through iterative refinement and compensation for quantization errors. The results demonstrate that BPDQ can achieve high accuracy even at 2-bit quantization, making it suitable for resource-constrained environments."
                },
                "zh": {
                    "title": "æ¯”ç‰¹å¹³é¢åˆ†è§£é‡åŒ–ï¼šä½æ¯”ç‰¹é‡åŒ–çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•ï¼Œç§°ä¸ºæ¯”ç‰¹å¹³é¢åˆ†è§£é‡åŒ–ï¼ˆBPDQï¼‰ï¼Œæ—¨åœ¨æ”¹å–„ä½æ¯”ç‰¹é‡åŒ–çš„æ•ˆæœã€‚BPDQé€šè¿‡ä½¿ç”¨æ¥è‡ªæ¯”ç‰¹å¹³é¢çš„å¯å˜é‡åŒ–ç½‘æ ¼å’Œæ ‡é‡ç³»æ•°ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒBPDQåœ¨2æ¯”ç‰¹é‡åŒ–ä¸‹ä»èƒ½ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†è¾“å‡ºè¯¯å·®ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒBPDQçš„å¯å˜ç½‘æ ¼æ‰©å±•äº†å¯è¡Œé›†ï¼Œä½¿å¾—é‡åŒ–è¿‡ç¨‹æ›´æœ‰æ•ˆåœ°ä¸ä¼˜åŒ–ç›®æ ‡å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12829",
            "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
            "url": "https://huggingface.co/papers/2602.12829",
            "abstract": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized SchrÃ¶dinger bridge formulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized SchrÃ¶dinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
            "score": 3,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "b3ed210666f4b22f",
            "authors": [
                "Lei Lv",
                "Yunfei Li",
                "Yu Luo",
                "Fuchun Sun",
                "Xiao Ma"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Shanghai Research Institute for Intelligent Autonomous Systems",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12829.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¸Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Field Least-Energy Actor-Critic (FLAC) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ñƒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¨Ñ€Ñ‘Ğ´Ğ¸Ğ½Ğ³ĞµÑ€Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¸Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "FLAC: Harnessing Kinetic Energy for Efficient Reinforcement Learning",
                    "desc": "The Field Least-Energy Actor-Critic (FLAC) framework tackles the complexities of maximum entropy reinforcement learning by using kinetic energy to manage policy randomness. It introduces a likelihood-free approach that formulates policy optimization as a Generalized SchrÃ¶dinger Bridge problem, allowing for efficient learning without needing direct access to action log-densities. By penalizing the kinetic energy of the velocity field, FLAC ensures that the policy remains close to a high-entropy reference while maximizing returns. This method not only simplifies the optimization process but also demonstrates strong performance on high-dimensional tasks compared to existing methods."
                },
                "zh": {
                    "title": "åœºæœ€å°èƒ½é‡æ¼”å‘˜-è¯„è®ºå®¶ï¼šä¼˜åŒ–æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†åœºæœ€å°èƒ½é‡æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆFLACï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ã€‚FLACé€šè¿‡å°†åŠ¨èƒ½ä½œä¸ºæ”¿ç­–éšæœºæ€§è°ƒèŠ‚çš„ä»£ç†ï¼Œåˆ©ç”¨å¹¿ä¹‰è–›å®šè°”æ¡¥çš„æ¡†æ¶æ¥ä¼˜åŒ–æ”¿ç­–ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦æ˜¾å¼çš„åŠ¨ä½œå¯†åº¦ï¼Œè€Œæ˜¯é€šè¿‡æœ€å°åŒ–è·¯å¾„ç©ºé—´èƒ½é‡æ¥é™åˆ¶ç»ˆç«¯åŠ¨ä½œåˆ†å¸ƒçš„åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLACåœ¨é«˜ç»´åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šæˆ–ç›¸å½“äºå¼ºåŸºçº¿ï¼ŒåŒæ—¶é¿å…äº†æ˜¾å¼å¯†åº¦ä¼°è®¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12617",
            "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
            "url": "https://huggingface.co/papers/2602.12617",
            "abstract": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
            "score": 3,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "336d0e2564bbde3c",
            "authors": [
                "Modi Jin",
                "Yiming Zhang",
                "Boyuan Sun",
                "Dingwen Zhang",
                "MingMing Cheng",
                "Qibin Hou"
            ],
            "affiliations": [
                "School of Automation, Northwestern Polytechnical University",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12617.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#rl",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ GeoAgent â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°Ğ´Ñ€ĞµÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GeoSeek Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (geo-similarity reward) Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ·Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ GeoAgent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼."
                },
                "en": {
                    "title": "GeoAgent: Elevating Geolocation Reasoning with Expert Insights",
                    "desc": "GeoAgent is a machine learning model designed to improve geolocation reasoning by using a specialized dataset and unique reward mechanisms. It addresses the limitations of previous reinforcement learning methods that relied on AI-generated data, which often did not align with geographic realities. The model utilizes a new dataset called GeoSeek, which includes expert-annotated chain-of-thought data to enhance training. By implementing geo-similarity and consistency rewards, GeoAgent ensures that its reasoning is both accurate and coherent, leading to superior performance compared to existing models."
                },
                "zh": {
                    "title": "GeoAgentï¼šæå‡åœ°ç†æ¨ç†çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "GeoAgentæ˜¯ä¸€ç§æ–°æ¨¡å‹ï¼Œä¸“æ³¨äºåœ°ç†ä½ç½®æ¨ç†ï¼Œèƒ½å¤Ÿä¸äººç±»çš„æ¨ç†æ–¹å¼ç´§å¯†ç»“åˆã€‚ä¸ºäº†æé«˜åœ°ç†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†GeoSeekæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±åœ°ç†ä¸“å®¶å’Œä¸“ä¸šç©å®¶æ³¨é‡Šçš„é“¾å¼æ€ç»´æ•°æ®ç»„æˆã€‚GeoAgentè¿˜æå‡ºäº†åœ°ç†ç›¸ä¼¼æ€§å¥–åŠ±å’Œä¸€è‡´æ€§å¥–åŠ±ï¼Œä»¥å¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¥½åœ°æ”¶æ•›åˆ°æ­£ç¡®ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoAgentåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ç”Ÿæˆäº†ä¸äººç±»æ¨ç†é«˜åº¦ä¸€è‡´çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12506",
            "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
            "url": "https://huggingface.co/papers/2602.12506",
            "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
            "score": 3,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "4c8c789b18323761",
            "authors": [
                "Rosie Zhao",
                "Anshul Shah",
                "Xiaoyu Zhu",
                "Xinke Deng",
                "Zhongyu Jiang",
                "Yang Yang",
                "Joerg Liebelt",
                "Arnab Mondal"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12506.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#interpretability",
                    "#security",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#rlhf",
                    "#rl",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ perturbations Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğ¸Ğ»Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ trade-off Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ accuracy Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ¾Ñ€Ğ²Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ adversarial augmentation Ğ¸ faithfulness-aware rewards Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing VLMs: Balancing Accuracy and Faithfulness in Reasoning",
                    "desc": "This paper explores the challenges faced by vision language models (VLMs) when fine-tuned using reinforcement learning (RL) for reasoning tasks. It identifies that while RL improves performance on visual reasoning benchmarks, VLMs are still prone to issues like weak visual grounding and hallucinations. The authors demonstrate that simple textual changes can significantly impact model robustness and confidence, particularly when considering the consistency of chain-of-thought reasoning. They propose that a focus on both accuracy and faithfulness in training can help mitigate these vulnerabilities, suggesting that traditional evaluation methods may overlook critical aspects of model performance."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨ç†æ¨¡å‹çš„é²æ£’æ€§ä¸å¯ä¿¡åº¦",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒå·²æˆä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å…³é”®æŠ€æœ¯ï¼Œç°å·²æ‰©å±•è‡³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚å°½ç®¡ç»è¿‡RLå¾®è°ƒçš„VLMåœ¨è§†è§‰æ¨ç†åŸºå‡†ä¸Šæœ‰æ‰€æå‡ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°è§†è§‰åŸºç¡€è–„å¼±ã€å¹»è§‰å’Œè¿‡åº¦ä¾èµ–æ–‡æœ¬çº¿ç´¢çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ç®€å•çš„æ–‡æœ¬æ‰°åŠ¨ï¼Œå¦‚è¯¯å¯¼æ€§æ ‡é¢˜æˆ–ä¸æ­£ç¡®çš„æ¨ç†é“¾ï¼ˆCoTï¼‰ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„é²æ£’æ€§å’Œä¿¡å¿ƒï¼Œå¹¶ä¸”å½“è€ƒè™‘CoTä¸€è‡´æ€§æ—¶ï¼Œè¿™äº›å½±å“æ›´åŠ æ˜æ˜¾ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›è„†å¼±æ€§ï¼Œæˆ‘ä»¬åˆ†æäº†RLå¾®è°ƒçš„åŠ¨æ€ï¼Œå‘ç°å‡†ç¡®æ€§ä¸å¯ä¿¡åº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼šå¾®è°ƒæé«˜äº†åŸºå‡†å‡†ç¡®æ€§ï¼Œä½†å¯èƒ½åŒæ—¶å‰Šå¼±äº†CoTçš„å¯é æ€§åŠå…¶å¯¹ä¸Šä¸‹æ–‡å˜åŒ–çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12984",
            "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
            "url": "https://huggingface.co/papers/2602.12984",
            "abstract": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
            "score": 2,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "7c7bc1c866fb51ad",
            "authors": [
                "Yujiong Shen",
                "Yajie Yang",
                "Zhiheng Xi",
                "Binze Hu",
                "Huayu Sha",
                "Jiazheng Zhang",
                "Qiyuan Peng",
                "Junlin Shang",
                "Jixuan Huang",
                "Yutao Fan",
                "Jingqi Tong",
                "Shihan Dou",
                "Ming Zhang",
                "Lei Bai",
                "Zhenfei Yin",
                "Tao Gui",
                "Xingjun Ma",
                "Qi Zhang",
                "Xuanjing Huang",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12984.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#reasoning",
                    "#benchmark",
                    "#science",
                    "#agents",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ SciAgentGym Ğ¸ SciAgentBench â€” ÑÑ€ĞµĞ´Ğ° Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒĞºĞ°Ñ… Ñ 1,780 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ·ĞºĞ¾ Ñ‚ĞµÑ€ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ÑÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SciForge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ fine-tuning Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SciAgent-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ Qwen3-VL-235B-Instruct Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering AI Agents for Scientific Tool Mastery",
                    "desc": "The paper introduces SciAgentGym and SciAgentBench, which are tools designed to evaluate how well AI agents can use scientific tools in various domains. It highlights the challenges faced by current models, like GPT-5, in executing complex workflows that require multiple steps, showing a significant drop in performance as tasks become more intricate. To improve agent performance, the authors propose SciForge, a method that uses a dependency graph to create training data that helps agents learn better tool interactions. The results demonstrate that their model, SciAgent-8B, outperforms larger models in scientific tool use, indicating a step forward in developing autonomous scientific agents."
                },
                "zh": {
                    "title": "æå‡ç§‘å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“è¯„ä¼°ä¸ä¼˜åŒ–",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†SciAgentGymå’ŒSciAgentBenchï¼Œè¿™ä¸¤ä¸ªå·¥å…·ç”¨äºè¯„ä¼°ç§‘å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è€ƒè™‘æ™ºèƒ½ä½“åœ¨å¤æ‚ç§‘å­¦å·¥ä½œæµç¨‹ä¸­åè°ƒå·¥å…·çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†SciForgeï¼Œé€šè¿‡ä¾èµ–å›¾å»ºæ¨¡å·¥å…·äº¤äº’ï¼Œç”Ÿæˆé€»è¾‘æ„ŸçŸ¥çš„è®­ç»ƒè½¨è¿¹ï¼Œä»è€Œæå‡æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„SciAgent-8Båœ¨ç§‘å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸Šè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†ä¸‹ä¸€ä»£è‡ªä¸»ç§‘å­¦æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13191",
            "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
            "url": "https://huggingface.co/papers/2602.13191",
            "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
            "score": 1,
            "issue_id": 1066,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "34c3a14416ab2c78",
            "authors": [
                "Sayan Deb Sarkar",
                "RÃ©mi Pautrat",
                "Ondrej Miksik",
                "Marc Pollefeys",
                "Iro Armeni",
                "Mahdi Rad",
                "Mihai Dusmanu"
            ],
            "affiliations": [
                "ETH Zurich",
                "Microsoft Spatial AI Lab",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13191.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#video",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´ĞµĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ´ĞµĞºĞ¾Ğ² (Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° 86% Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 93% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 14 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Efficient Video Understanding with Codec Primitives",
                    "desc": "This paper presents a new approach to improve Video Language Models (VideoLMs) by utilizing video codec primitives like motion vectors and residuals. These primitives help capture important video information without the need for processing full images for every frame, which reduces computational costs. The authors introduce lightweight transformer-based encoders that effectively combine codec data with image embeddings, enhancing the model's efficiency and performance. Their method significantly decreases the time and resources needed for video understanding tasks while maintaining high accuracy across various benchmarks."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘ç¼–ç åŸè¯­æå‡è§†é¢‘ç†è§£æ•ˆç‡",
                    "desc": "è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVideoLMsï¼‰ä½¿äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿç†è§£è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚å½“å‰çš„æ–¹æ³•ä½¿ç”¨å…³é”®å¸§é‡‡æ ·æ¥é€‚åº”æœ€å¤§ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œä½†è¿™å¯èƒ½ä¼šé”™è¿‡å®è§‚äº‹ä»¶å’Œå¾®è§‚ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨è§†é¢‘ç¼–ç åŸè¯­ï¼ˆå¦‚è¿åŠ¨çŸ¢é‡å’Œæ®‹å·®ï¼‰ï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€å¹¶æé«˜æ•ˆç‡ã€‚é€šè¿‡è½»é‡çº§çš„å˜æ¢å™¨ç¼–ç å™¨ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŠ é€Ÿæ”¶æ•›å¹¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šä¿æŒæˆ–è¶…è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12684",
            "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
            "url": "https://huggingface.co/papers/2602.12684",
            "abstract": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io",
            "score": 1,
            "issue_id": 1065,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "9d96b4b03fccbc30",
            "authors": [
                "Rui Cai",
                "Jun Guo",
                "Xinze He",
                "Piaopiao Jin",
                "Jie Li",
                "Bingxuan Lin",
                "Futeng Liu",
                "Wei Liu",
                "Fei Ma",
                "Kun Ma",
                "Feng Qiu",
                "Heng Qu",
                "Yifei Su",
                "Qiao Sun",
                "Dong Wang",
                "Donghao Wang",
                "Yunhong Wang",
                "Rujie Wu",
                "Diyun Xiang",
                "Yu Yang",
                "Hangjun Ye",
                "Yuan Zhang",
                "Quanyun Zhou"
            ],
            "affiliations": [
                "Xiaomi Robotics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12684.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#robotics",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ GPU Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Xiaomi-Robotics-0: Real-Time Vision-Language-Action for Robotics",
                    "desc": "This paper presents Xiaomi-Robotics-0, a vision-language-action (VLA) model designed for robotics that excels in real-time manipulation tasks. The model is pre-trained on extensive datasets of robot trajectories and vision-language pairs, which helps it generate actions effectively while retaining important visual-semantic knowledge. To enhance performance during real-robot execution, the authors introduce innovative training techniques that minimize inference latency and ensure smooth action transitions. Extensive evaluations demonstrate that Xiaomi-Robotics-0 achieves state-of-the-art results in simulations and performs efficiently on real robots using standard hardware."
                },
                "zh": {
                    "title": "é«˜æ•ˆå®æ—¶æ‰§è¡Œçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œåä¸ºXiaomi-Robotics-0ï¼Œæ—¨åœ¨å®ç°é«˜æ€§èƒ½å’Œå®æ—¶æ‰§è¡Œã€‚è¯¥æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡çš„é¢„è®­ç»ƒå’Œä¸“é—¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œå…·å¤‡å¹¿æ³›çš„åŠ¨ä½œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é¿å…äº†è§†è§‰è¯­ä¹‰çŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚åæœŸè®­ç»ƒä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç§æŠ€æœ¯ä»¥è§£å†³çœŸå®æœºå™¨äººæ‰§è¡Œæ—¶çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒXiaomi-Robotics-0åœ¨æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡å’Œååé‡å‡è¾¾åˆ°è¡Œä¸šé¢†å…ˆæ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12612",
            "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
            "url": "https://huggingface.co/papers/2602.12612",
            "abstract": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
            "score": 1,
            "issue_id": 1067,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "8496b14bab90c5cd",
            "authors": [
                "Sein Kim",
                "Sangwu Park",
                "Hongseok Kang",
                "Wonjoong Kim",
                "Jimin Seo",
                "Yeonjun In",
                "Kanghoon Yoon",
                "Chanyoung Park"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12612.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Self-EvolveRec â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² NAS Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ²Ñ€Ğ¾Ğ´Ğµ NDCG. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‡ĞµĞ¼Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Self-EvolveRec Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ NAS Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Revolutionizing Recommender Systems with Self-EvolveRec",
                    "desc": "This paper introduces Self-EvolveRec, a new framework for improving recommender systems by combining qualitative and quantitative evaluation methods. Unlike traditional Neural Architecture Search (NAS) that relies on fixed search spaces, Self-EvolveRec uses a User Simulator to provide qualitative feedback and a Model Diagnosis Tool for quantitative analysis. This approach creates a feedback loop that helps the system adapt and improve as it evolves. The results show that Self-EvolveRec outperforms existing methods in both recommendation accuracy and user satisfaction."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ¼”åŒ–æ¨èç³»ç»Ÿçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿè®¾è®¡è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¦‚ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰ï¼Œé€šå¸¸å—åˆ°äººç±»å…ˆéªŒå®šä¹‰çš„å›ºå®šæœç´¢ç©ºé—´çš„é™åˆ¶ï¼Œé™åˆ¶äº†åˆ›æ–°ã€‚æœ€è¿‘çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¼”åŒ–æ¡†æ¶è™½ç„¶å°†ç›®æ ‡ä»å›ºå®šæœç´¢ç©ºé—´è½¬å‘å¼€æ”¾å¼ç¨‹åºç©ºé—´ï¼Œä½†ä¸»è¦ä¾èµ–äºæ ‡é‡æŒ‡æ ‡ï¼ˆå¦‚NDCGã€å‘½ä¸­ç‡ï¼‰ï¼Œæ— æ³•æä¾›æ¨¡å‹å¤±è´¥çš„å®šæ€§æ´å¯Ÿæˆ–æ”¹è¿›çš„æ–¹å‘æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Self-EvolveRecï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç”¨æˆ·æ¨¡æ‹Ÿå™¨è¿›è¡Œå®šæ€§è¯„ä¼°å’Œæ¨¡å‹è¯Šæ–­å·¥å…·è¿›è¡Œå®šé‡éªŒè¯ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–¹å‘æ€§åé¦ˆå¾ªç¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯Šæ–­å·¥å…·-æ¨¡å‹å…±åŒæ¼”åŒ–ç­–ç•¥ï¼Œä»¥ç¡®ä¿è¯„ä¼°æ ‡å‡†éšç€æ¨èæ¶æ„çš„æ¼”å˜è€ŒåŠ¨æ€é€‚åº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11910",
            "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
            "url": "https://huggingface.co/papers/2602.11910",
            "abstract": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
            "score": 1,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "c1f906e7a20a29fc",
            "authors": [
                "Åukasz Staniszewski",
                "Katarzyna Zaleska",
                "Mateusz Modrzejewski",
                "Kamil Deja"
            ],
            "affiliations": [
                "IDEAS Research Institute",
                "Warsaw University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11910.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#diffusion"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ activation patching Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¶Ğ°Ğ½Ñ€, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾Ñ‘Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞŸĞ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‚ĞµĞ¼Ğ¿ Ğ¸Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞºĞ°."
                },
                "en": {
                    "title": "Mastering Music with Attention Layers in AI",
                    "desc": "This paper explores how specific attention layers in audio diffusion models can be used to control different musical elements. It shows that these layers are responsible for representing high-level concepts like instruments and genres. By using techniques like Contrastive Activation Addition and Sparse Autoencoders, the authors demonstrate that we can manipulate audio features more precisely. This allows for targeted changes in generated music, such as adjusting tempo or mood, enhancing the creative control over AI-generated audio."
                },
                "zh": {
                    "title": "ç²¾å‡†æ“æ§éŸ³ä¹å…ƒç´ çš„éŸ³é¢‘æ‰©æ•£æ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†éŸ³é¢‘æ‰©æ•£æ¨¡å‹ä¸­æŸäº›æ³¨æ„åŠ›å±‚å¦‚ä½•æ§åˆ¶ä¸åŒçš„éŸ³ä¹æ¦‚å¿µã€‚é€šè¿‡æ¿€æ´»å¼•å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥ç²¾ç¡®åœ°æ“æ§éŸ³é¢‘ç‰¹å¾ï¼Œæ¯”å¦‚ä¹å™¨ã€å£°ä¹æˆ–éŸ³ä¹é£æ ¼çš„å­˜åœ¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¯¹æ¯”æ¿€æ´»æ·»åŠ å’Œç¨€ç–è‡ªç¼–ç å™¨å¯ä»¥åœ¨è¿™äº›å±‚ä¸­å®ç°æ›´ç²¾ç¡®çš„éŸ³é¢‘æ§åˆ¶ã€‚é€šè¿‡è°ƒæ•´è¿™äº›ç‰¹å®šå±‚çš„æ¿€æ´»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé«˜ç²¾åº¦åœ°æ”¹å˜éŸ³ä¹å…ƒç´ ï¼Œä¾‹å¦‚è°ƒèŠ‚èŠ‚å¥æˆ–æ”¹å˜æ›²è°ƒçš„æƒ…ç»ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11769",
            "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
            "url": "https://huggingface.co/papers/2602.11769",
            "abstract": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
            "score": 1,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "dd23efec6e661cb3",
            "authors": [
                "Zhenghuang Wu",
                "Kang Chen",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11769.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· 4D Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Light4D â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… 4D Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Disentangled Flow Guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ ÑĞ²ĞµÑ‚Ğ¾Ğ¼ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Temporal Consistent Attention Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¾Ñ‚ -90 Ğ´Ğ¾ 90 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Consistent 4D Video Synthesis with Light4D",
                    "desc": "Light4D is a framework that allows for the creation of 4D videos that maintain consistent lighting even when viewed from different angles. It addresses the challenges of limited training data and the need for temporal consistency in video synthesis. The method uses Disentangled Flow Guidance to control lighting while keeping the shapes of objects intact. Additionally, it employs Temporal Consistent Attention to ensure that the video appears stable and flicker-free during camera movements."
                },
                "zh": {
                    "title": "Light4Dï¼šä¸€è‡´æ€§4Dè§†é¢‘åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "Light4D æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ç›®æ ‡å…‰ç…§ä¸‹åˆæˆä¸€è‡´çš„ 4D è§†é¢‘ã€‚å®ƒé€šè¿‡è§£è€¦æµå¼•å¯¼å’Œæ—¶é—´ä¸€è‡´æ€§æ³¨æ„æœºåˆ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æç«¯è§†è§’å˜åŒ–ä¸‹ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶æœ‰æ•ˆåœ°æ§åˆ¶å…‰ç…§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLight4D åœ¨æ—¶é—´ä¸€è‡´æ€§å’Œå…‰ç…§ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†ä» -90 åˆ° 90 åº¦çš„ç›¸æœºæ—‹è½¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11757",
            "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
            "url": "https://huggingface.co/papers/2602.11757",
            "abstract": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
            "score": 1,
            "issue_id": 1065,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "3e0799f7a66fb169",
            "authors": [
                "Yi Zhang",
                "Yunshuang Wang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11757.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´",
                    "desc": "Code2Worlds â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 4D ÑÑ†ĞµĞ½ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ´ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ²ĞµĞ´Ñ‘Ğ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ´ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Language into Dynamic 4D Worlds",
                    "desc": "Code2Worlds is a framework designed for generating dynamic 4D scenes by transforming language into simulation code. It addresses the challenges of balancing local object details with the overall environment and ensuring that generated dynamics are physically accurate. The framework employs a dual-stream architecture to separate object generation from environmental orchestration, enhancing the quality of the output. Additionally, it incorporates a physics-aware closed-loop refinement process to iteratively improve the simulation code, resulting in more realistic and dynamic scene generation."
                },
                "zh": {
                    "title": "Code2Worldsï¼šå®ç°4DåŠ¨æ€åœºæ™¯ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Code2Worlds æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°† 4D åŠ¨æ€åœºæ™¯ç”Ÿæˆå½¢å¼åŒ–ä¸ºè¯­è¨€åˆ°ä»¿çœŸä»£ç ç”Ÿæˆæ¥å®ç°ç©ºé—´æ™ºèƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒæµæ¶æ„ï¼Œèƒ½å¤Ÿå°†å¯¹è±¡ç”Ÿæˆä¸ç¯å¢ƒç¼–æ’åˆ†å¼€å¤„ç†ï¼Œä»è€Œè§£å†³å¤šå°ºåº¦ä¸Šä¸‹æ–‡çº ç¼ çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒCode2Worlds è¿˜å¼•å…¥äº†ç‰©ç†æ„ŸçŸ¥çš„é—­ç¯æœºåˆ¶ï¼Œä»¥ç¡®ä¿åŠ¨æ€çš„çœŸå®æ„Ÿï¼Œé¿å…äº†å¼€æ”¾å¼ä»£ç ç”Ÿæˆå¸¦æ¥çš„ç‰©ç†å¹»è§‰ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCode2Worlds åœ¨ Code4D åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„ä¸°å¯Œæ€§å’ŒåŠ¨æ€æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11715",
            "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
            "url": "https://huggingface.co/papers/2602.11715",
            "abstract": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.",
            "score": 1,
            "issue_id": 1066,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "bda4e0b546d18bec",
            "authors": [
                "Haolei Bai",
                "Lingcheng Kong",
                "Xueyi Chen",
                "Jianmian Wang",
                "Zhiqiang Tao",
                "Huan Wang"
            ],
            "affiliations": [
                "Rochester Institute of Technology",
                "The Hong Kong University of Science and Technology",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11715.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#plp",
                    "#architecture",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… CUDA ÑĞ´ĞµÑ€",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (dLLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CUDA ÑĞ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CuKe Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ CUDA ÑĞ´ĞµÑ€ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (BiC-RL) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ DICE Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ 1.7B, 4B Ğ¸ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ KernelBench Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CUDA ÑĞ´ĞµÑ€."
                },
                "en": {
                    "title": "Revolutionizing CUDA Kernel Generation with DICE",
                    "desc": "This paper introduces diffusion large language models (dLLMs) specifically designed for generating CUDA kernels, which are essential for high-performance computing. The authors create a specialized dataset called CuKe to enhance the training of these models, addressing the scarcity of quality data in this domain. They also propose a bi-phase curated reinforcement learning framework (BiC-RL) that includes stages for infilling and generating CUDA kernels. The results show that their model, DICE, outperforms existing autoregressive and diffusion models, setting a new benchmark in CUDA kernel generation."
                },
                "zh": {
                    "title": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼šCUDAå†…æ ¸ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨CUDAå†…æ ¸ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¾—ç›Šäºä¸“é—¨çš„æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¿™ç§æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œç‰¹åˆ«é€‚åˆä»£ç ç”Ÿæˆä»»åŠ¡ã€‚ä¸ºäº†å…‹æœé«˜è´¨é‡è®­ç»ƒæ•°æ®ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†CuKeæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŒé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆBiC-RLï¼‰ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DICEç³»åˆ—æ¨¡å‹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨CUDAå†…æ ¸ç”Ÿæˆä¸Šæ˜¾è‘—ä¼˜äºåŒè§„æ¨¡çš„è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04315",
            "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
            "url": "https://huggingface.co/papers/2602.04315",
            "abstract": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
            "score": 1,
            "issue_id": 1065,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "b05531a657de0f38",
            "authors": [
                "Guoqing Ma",
                "Siheng Wang",
                "Zeyu Zhang",
                "Shan Yu",
                "Hao Tang"
            ],
            "affiliations": [
                "CASIA",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04315.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ¸Ğ· ÑĞ»Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ğº Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "GeneralVLA â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ foundation models Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ±ĞµĞ· ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ affordance Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. GeneralVLA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Robots with Zero-Shot Manipulation through GeneralVLA",
                    "desc": "GeneralVLA is a novel hierarchical model that integrates vision, language, and action to enable robots to perform tasks without needing real-world data. It leverages knowledge-guided trajectory planning to generate effective manipulation strategies in a zero-shot manner, meaning it can handle new tasks without prior examples. The model consists of three levels: a high-level module for understanding scene affordances, a mid-level agent for task comprehension and trajectory planning, and a low-level control policy for executing precise movements. This approach not only enhances generalization capabilities but also allows for scalable data generation, outperforming existing methods in various robotic tasks."
                },
                "zh": {
                    "title": "é›¶æ ·æœ¬æœºå™¨äººæ“ä½œçš„æ–°æ–¹æ³•",
                    "desc": "GeneralVLAæ˜¯ä¸€ç§å±‚æ¬¡åŒ–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡çŸ¥è¯†å¼•å¯¼çš„è½¨è¿¹è§„åˆ’å®ç°é›¶æ ·æœ¬æœºå™¨äººæ“ä½œï¼Œè€Œæ— éœ€æ”¶é›†çœŸå®ä¸–ç•Œçš„æ•°æ®ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè‡ªåŠ¨ç”Ÿæˆæœºå™¨äººæ“ä½œæ‰€éœ€çš„æ•°æ®ã€‚å®ƒé€šè¿‡é«˜å±‚çš„å¯ç”¨æ€§åˆ†å‰²æ¨¡å—å’Œä¸­å±‚çš„3Dä»£ç†è¿›è¡Œä»»åŠ¡ç†è§£å’Œè½¨è¿¹è§„åˆ’ï¼Œæœ€ç»ˆç”Ÿæˆç²¾ç¡®çš„3Dè·¯å¾„ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒGeneralVLAä¸éœ€è¦çœŸå®ä¸–ç•Œçš„æ•°æ®æ”¶é›†æˆ–äººç±»ç¤ºèŒƒï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03120",
            "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
            "url": "https://huggingface.co/papers/2602.03120",
            "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
            "score": 1,
            "issue_id": 1068,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "d17702a6622c6a0d",
            "authors": [
                "Yinggan Xu",
                "Risto Miikkulainen",
                "Xin Qiu"
            ],
            "affiliations": [
                "Cognizant AI Lab",
                "The University of Texas at Austin",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03120.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Quantized Evolution Strategies (QES) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². QES Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… ÑĞµĞ¼ÑĞ½ Ğ±ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Fine-Tuning for Quantized Large Language Models",
                    "desc": "This paper presents Quantized Evolution Strategies (QES), a new method for fine-tuning Large Language Models (LLMs) that have undergone Post-Training Quantization (PTQ). Traditional fine-tuning methods struggle with quantized models due to their discrete and non-differentiable nature, which complicates gradient computation. QES addresses this challenge by incorporating accumulated error feedback to maintain high-precision gradient signals and using a stateless seed replay to minimize memory usage. The results show that QES significantly improves performance on arithmetic reasoning tasks compared to existing zeroth-order fine-tuning techniques, enabling effective fine-tuning in the quantized space."
                },
                "zh": {
                    "title": "é‡åŒ–è¿›åŒ–ç­–ç•¥ï¼šåœ¨é‡åŒ–ç©ºé—´ä¸­å®ç°å¾®è°ƒçš„çªç ´",
                    "desc": "åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å¯¹äºåœ¨å†…å­˜å—é™è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ï¼Œä½†å®ƒä½¿æ¨¡å‹å˜å¾—é™æ€ä¸”éš¾ä»¥å¾®è°ƒã€‚æ ‡å‡†çš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä¾èµ–äºåå‘ä¼ æ’­å’Œé«˜ç²¾åº¦æƒé‡æ¥è®¡ç®—æ¢¯åº¦ï¼Œå› æ­¤æ— æ³•åº”ç”¨äºé‡åŒ–æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†é‡åŒ–è¿›åŒ–ç­–ç•¥ï¼ˆQESï¼‰ï¼Œä¸€ç§ç›´æ¥åœ¨é‡åŒ–ç©ºé—´ä¸­è¿›è¡Œå…¨å‚æ•°å¾®è°ƒçš„ä¼˜åŒ–èŒƒå¼ã€‚QESé€šè¿‡é›†æˆç´¯ç§¯è¯¯å·®åé¦ˆå’Œæ— çŠ¶æ€ç§å­é‡æ”¾ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¼€å¯äº†åœ¨é‡åŒ–ç©ºé—´ä¸­æ‰©å±•LLMsçš„å¯èƒ½æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-13.html",
    "link_next": "2026-02-17.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 5,
        "#cv": 4,
        "#rl": 8,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 2,
        "#inference": 4,
        "#3d": 3,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 13,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 14,
        "#robotics": 4,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 4,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 0,
        "#synthetic": 5,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}