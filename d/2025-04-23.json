{
    "date": {
        "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 23",
        "zh": "4æœˆ23æ—¥"
    },
    "time_utc": "2025-04-23 04:14",
    "weekday": 2,
    "issue_id": 3382,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15521",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "url": "https://huggingface.co/papers/2504.15521",
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "score": 27,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "4d1809408c9b3e49",
            "authors": [
                "Minghao Wu",
                "Weixuan Wang",
                "Sinuo Liu",
                "Huifeng Yin",
                "Xintong Wang",
                "Yu Zhao",
                "Chenyang Lyu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce",
                "Monash University",
                "The University of Edinburgh",
                "Tsinghua University",
                "UniversitÃ¤t Hamburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15521.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#low_resource",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ—Ğ° ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2000 Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿ĞµÑ€ĞµĞºĞ¾Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹, Ñ‡ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ ÑˆĞµÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ°. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Towards Equitable Multilingual Benchmarking for LLMs",
                    "desc": "This paper discusses the need for better multilingual evaluation in large language models (LLMs) as they improve in language skills. It analyzes over 2,000 multilingual benchmarks from various countries and finds that English is still overly dominant in these evaluations. The study shows that benchmarks based on original language content are more effective than translated ones, especially for local contexts. The authors propose new guidelines and research directions to create benchmarks that align more closely with human judgments and real-world applications."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šè¯­è¨€è¯„ä¼°çš„å…¬å¹³è¿›æ­¥",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€èƒ½åŠ›ä¸Šçš„ä¸æ–­è¿›æ­¥ï¼Œè¿›è¡Œç¨³å¥çš„å¤šè¯­è¨€è¯„ä¼°å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡åˆ†æäº†2021å¹´è‡³2024å¹´é—´æ¥è‡ª148ä¸ªå›½å®¶çš„2000å¤šä¸ªå¤šè¯­è¨€åŸºå‡†ï¼Œè¯„ä¼°äº†å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„è¿‡å»ã€ç°åœ¨å’Œæœªæ¥çš„å®è·µã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æŠ•å…¥äº†æ•°åƒä¸‡ç¾å…ƒï¼Œè‹±è¯­åœ¨è¿™äº›åŸºå‡†ä¸­ä»ç„¶å æ®äº†è¿‡é«˜çš„æ¯”ä¾‹ï¼Œä¸”å¤§å¤šæ•°åŸºå‡†ä¾èµ–äºåŸå§‹è¯­è¨€å†…å®¹è€Œéç¿»è¯‘ã€‚æˆ‘ä»¬å¼ºè°ƒäº†åˆ›å»ºæ–‡åŒ–å’Œè¯­è¨€å®šåˆ¶åŸºå‡†çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•æŒ‡å¯¼åŸåˆ™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16084",
            "title": "TTRL: Test-Time Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.16084",
            "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
            "score": 25,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "4445f28372c61abd",
            "authors": [
                "Yuxin Zuo",
                "Kaiyan Zhang",
                "Shang Qu",
                "Li Sheng",
                "Xuekai Zhu",
                "Biqing Qi",
                "Youbang Sun",
                "Ganqu Cui",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16084.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğº",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Test-Time Reinforcement Learning (TTRL), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TTRL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ pass@1 Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen-2.5-Math-7B Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 159% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ AIME 2024. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TTRL Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Unlabeled Data: Test-Time Reinforcement Learning",
                    "desc": "This paper explores a new approach called Test-Time Reinforcement Learning (TTRL) for training Large Language Models (LLMs) using unlabeled data. The main challenge addressed is how to estimate rewards during inference without having access to true labels. The authors find that techniques like majority voting can effectively generate rewards for reinforcement learning training. Their experiments show that TTRL significantly enhances the performance of LLMs, achieving impressive results even when only supervised by a simple metric."
                },
                "zh": {
                    "title": "æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†åœ¨æ²¡æœ‰æ˜ç¡®æ ‡ç­¾çš„æ•°æ®ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ä»»åŠ¡ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•è¿›è¡Œå¥–åŠ±ä¼°è®¡ï¼Œè€Œæ²¡æœ‰çœŸå®æ ‡ç­¾çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­çš„å¸¸è§åšæ³•ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ„æƒ³ä¸åˆ°çš„æœ‰æ•ˆå¥–åŠ±ï¼Œä»è€Œæ¨åŠ¨RLè®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œèƒ½å¤Ÿåœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè‡ªæˆ‘è¿›åŒ–ï¼Œå®éªŒç»“æœè¡¨æ˜TTRLåœ¨å¤šç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15466",
            "title": "Learning Adaptive Parallel Reasoning with Language Models",
            "url": "https://huggingface.co/papers/2504.15466",
            "abstract": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.",
            "score": 19,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "4eaf94a6429e3102",
            "authors": [
                "Jiayi Pan",
                "Xiuyu Li",
                "Long Lian",
                "Charlie Snell",
                "Yifei Zhou",
                "Adam Yala",
                "Trevor Darrell",
                "Kurt Keutzer",
                "Alane Suhr"
            ],
            "affiliations": [
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15466.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ (APR). APR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ spawn() Ğ¸ join(). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Countdown Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° APR Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Adaptive Parallel Reasoning: Optimizing Language Model Inference",
                    "desc": "This paper introduces Adaptive Parallel Reasoning (APR), a new framework designed to enhance the reasoning capabilities of language models during inference. APR combines both serialized and parallel computation methods, allowing for more efficient processing without the drawbacks of existing approaches. The framework utilizes an end-to-end reinforcement learning strategy to optimize inference threads, improving task success rates without needing fixed reasoning structures. Experimental results show that APR significantly outperforms traditional methods in terms of performance, scalability, and accuracy while maintaining similar latency levels."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å¹¶è¡Œæ¨ç†ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œç§°ä¸ºè‡ªé€‚åº”å¹¶è¡Œæ¨ç†ï¼ˆAPRï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹æ¨ç†æ–¹æ³•çš„å±€é™æ€§ã€‚APRç»“åˆäº†ä¸²è¡Œå’Œå¹¶è¡Œè®¡ç®—ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»è°ƒæ•´è®¡ç®—æ–¹å¼ã€‚é€šè¿‡ä½¿ç”¨spawn()å’Œjoin()æ“ä½œï¼ŒAPRå®ç°äº†è‡ªé€‚åº”çš„å¤šçº¿ç¨‹æ¨ç†ï¼Œä¼˜åŒ–äº†æ¨ç†çº¿ç¨‹çš„æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPRåœ¨ç›¸åŒä¸Šä¸‹æ–‡çª—å£å†…è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”åœ¨è®¡ç®—é‡å¢åŠ æ—¶å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16072",
            "title": "Describe Anything: Detailed Localized Image and Video Captioning",
            "url": "https://huggingface.co/papers/2504.16072",
            "abstract": "Generating detailed and accurate descriptions for specific regions in images and videos remains a fundamental challenge for vision-language models. We introduce the Describe Anything Model (DAM), a model designed for detailed localized captioning (DLC). DAM preserves both local details and global context through two key innovations: a focal prompt, which ensures high-resolution encoding of targeted regions, and a localized vision backbone, which integrates precise localization with its broader context. To tackle the scarcity of high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark designed to evaluate DLC without relying on reference captions. DAM sets new state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and detailed multi-sentence localized image and video captioning.",
            "score": 15,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "1749ec1e79ed8810",
            "authors": [
                "Long Lian",
                "Yifan Ding",
                "Yunhao Ge",
                "Sifei Liu",
                "Hanzi Mao",
                "Boyi Li",
                "Marco Pavone",
                "Ming-Yu Liu",
                "Trevor Darrell",
                "Adam Yala",
                "Yin Cui"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16072.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "DAM: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ DAM (Describe Anything Model) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞºĞ±Ğ¾Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ñƒ-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DLC-SDP Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DAM ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ½Ğ° 7 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Capturing Details Anywhere: The Describe Anything Model",
                    "desc": "The Describe Anything Model (DAM) addresses the challenge of generating detailed captions for specific areas in images and videos. It utilizes a focal prompt for high-resolution encoding of targeted regions and a localized vision backbone to combine local details with global context. To enhance the training data for detailed localized captioning, the model employs a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP) that leverages existing segmentation datasets and expands to unlabeled web images. DAM achieves state-of-the-art performance across multiple benchmarks for keyword, phrase, and multi-sentence captioning tasks."
                },
                "zh": {
                    "title": "æè¿°ä»»ä½•äº‹ç‰©ï¼Œç²¾å‡†æœ¬åœ°åŒ–ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œç§°ä¸ºæè¿°ä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆDAMï¼‰ï¼Œæ—¨åœ¨ä¸ºå›¾åƒå’Œè§†é¢‘ä¸­çš„ç‰¹å®šåŒºåŸŸç”Ÿæˆè¯¦ç»†çš„æœ¬åœ°åŒ–æè¿°ã€‚DAMé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°æ¥ä¿æŒå±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼šç„¦ç‚¹æç¤ºç¡®ä¿å¯¹ç›®æ ‡åŒºåŸŸçš„é«˜åˆ†è¾¨ç‡ç¼–ç ï¼Œè€Œæœ¬åœ°åŒ–è§†è§‰éª¨å¹²ç½‘ç»œåˆ™å°†ç²¾ç¡®å®šä½ä¸æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ç›¸ç»“åˆã€‚ä¸ºäº†åº”å¯¹é«˜è´¨é‡æœ¬åœ°åŒ–æè¿°æ•°æ®çš„ç¨€ç¼ºï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ çš„æ•°æ®ç®¡é“ï¼ˆDLC-SDPï¼‰ï¼Œè¯¥ç®¡é“åˆ©ç”¨ç°æœ‰çš„åˆ†å‰²æ•°æ®é›†å¹¶æ‰©å±•åˆ°æœªæ ‡è®°çš„ç½‘ç»œå›¾åƒã€‚DAMåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ¶µç›–äº†å…³é”®è¯çº§ã€çŸ­è¯­çº§å’Œè¯¦ç»†çš„å¤šå¥æœ¬åœ°åŒ–å›¾åƒå’Œè§†é¢‘æè¿°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15415",
            "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.15415",
            "abstract": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.",
            "score": 11,
            "issue_id": 3381,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "40ca992771747dac",
            "authors": [
                "David Ma",
                "Yuanxing Zhang",
                "Jincheng Ren",
                "Jarvis Guo",
                "Yifan Yao",
                "Zhenlin Wei",
                "Zhenzhu Yang",
                "Zhongyuan Peng",
                "Boyu Feng",
                "Jun Ma",
                "Xiao Gu",
                "Zhoufutu Wen",
                "King Zhu",
                "Yancheng He",
                "Meng Cao",
                "Shiwen Ni",
                "Jiaheng Liu",
                "Wenhao Huang",
                "Ge Zhang",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "ByteDance Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15415.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#open_source"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "IV-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "IV-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 967 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 2,585 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾ 13 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ 5 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 28.9% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "IV-Bench: Bridging Image Context and Video Understanding",
                    "desc": "This paper introduces IV-Bench, a new evaluation framework designed to assess how well Multimodal Large Language Models (MLLMs) understand videos using image context. It includes a dataset of 967 videos and 2,585 annotated image-text queries across various tasks, highlighting the importance of image-grounded reasoning in video comprehension. The study shows that current MLLMs struggle with this task, achieving a maximum accuracy of only 28.9%. The authors also identify factors affecting model performance and suggest that challenges in video understanding go beyond just data format alignment during training."
                },
                "zh": {
                    "title": "IV-Benchï¼šå›¾åƒåŸºç¡€è§†é¢‘ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨å›¾åƒæ¨ç†æˆ–ä¸€èˆ¬è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œå¿½è§†äº†å›¾åƒä¸Šä¸‹æ–‡åœ¨è§†é¢‘ç†è§£ä¸­çš„é‡è¦ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†IV-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°å›¾åƒåŸºç¡€è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†çš„åŸºå‡†ã€‚IV-BenchåŒ…å«967ä¸ªè§†é¢‘å’Œ2585ä¸ªç²¾å¿ƒæ³¨é‡Šçš„å›¾åƒ-æ–‡æœ¬æŸ¥è¯¢ï¼Œæ¶µç›–13ä¸ªä»»åŠ¡ï¼ˆ7ä¸ªæ„ŸçŸ¥ä»»åŠ¡å’Œ6ä¸ªæ¨ç†ä»»åŠ¡ï¼‰åŠ5ä¸ªä»£è¡¨æ€§ç±»åˆ«ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„æ¨¡å‹åœ¨å›¾åƒåŸºç¡€è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæœ€é«˜å‡†ç¡®ç‡ä»…ä¸º28.9%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13820",
            "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
            "url": "https://huggingface.co/papers/2504.13820",
            "abstract": "Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld.",
            "score": 10,
            "issue_id": 3381,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "c69df5180f187adb",
            "authors": [
                "Yang Yue",
                "Yulin Wang",
                "Chenxin Tao",
                "Pan Liu",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "PLA General Hospital",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13820.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#healthcare",
                    "#agi",
                    "#science",
                    "#training"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "CheXWorld: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CheXWorld - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CheXWorld ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CheXWorld Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "CheXWorld: A Self-Supervised Model for Radiographic Understanding",
                    "desc": "This paper introduces CheXWorld, a self-supervised world model designed for radiographic images, which aims to replicate human-like common sense knowledge in medical imaging. It develops a framework that captures three critical aspects of medical knowledge: local anatomical structures, global anatomical layouts, and domain variations in radiographs. The model is evaluated through qualitative and quantitative analyses, demonstrating its ability to effectively represent these dimensions of medical knowledge. Additionally, CheXWorld shows superior performance in transfer learning tasks compared to existing self-supervised learning methods and large-scale medical models."
                },
                "zh": {
                    "title": "CheXWorldï¼šæ”¾å°„å½±åƒçš„è‡ªç›‘ç£ä¸–ç•Œæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†CheXWorldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æ”¾å°„å½±åƒçš„è‡ªç›‘ç£ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¨¡å‹åŒæ—¶å»ºæ¨¡äº†åŒ»å­¦çŸ¥è¯†çš„ä¸‰ä¸ªé‡è¦æ–¹é¢ï¼šå±€éƒ¨è§£å‰–ç»“æ„ã€å…¨å±€è§£å‰–å¸ƒå±€å’Œé¢†åŸŸå˜åŒ–ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æï¼ŒCheXWorldæˆåŠŸæ•æ‰äº†è¿™äº›åŒ»å­¦çŸ¥è¯†çš„ç»´åº¦ï¼Œå¹¶åœ¨å…«ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»å’Œåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCheXWorldåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå¤§å‹åŒ»å­¦åŸºç¡€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14992",
            "title": "Efficient Pretraining Length Scaling",
            "url": "https://huggingface.co/papers/2504.14992",
            "abstract": "Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (PHD-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. PHD-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: PHD-SWA employs sliding window attention to preserve local dependencies, while PHD-CSWA implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.",
            "score": 8,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "a0f15db68dba9035",
            "authors": [
                "Bohong Wu",
                "Shen Yan",
                "Sijun Zhang",
                "Jianqiao Lu",
                "Yutao Zeng",
                "Ya Wang",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14992.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Parallel Hidden Decoding Transformer (PHD-Transformer), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑÑˆĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: PHD-SWA Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ PHD-CSWA Ñ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Efficient Length Scaling in Pre-Training with PHD-Transformer",
                    "desc": "This paper introduces the Parallel Hidden Decoding Transformer (PHD-Transformer), which enhances pre-training of large language models by implementing efficient length scaling. The framework utilizes a unique key-value (KV) cache management strategy that differentiates between original tokens and hidden decoding tokens, optimizing memory usage. By retaining only the KV cache of original tokens, it effectively manages long-range dependencies without increasing cache size. Additionally, two optimized variants, PHD-SWA and PHD-CSWA, are proposed to improve local dependencies and reduce pre-filling time, leading to better performance across various benchmarks."
                },
                "zh": {
                    "title": "æå‡é¢„è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°å˜æ¢å™¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå¹¶è¡Œéšè—è§£ç å˜æ¢å™¨ï¼ˆPHD-Transformerï¼‰ï¼Œæ—¨åœ¨æé«˜é¢„è®­ç»ƒé˜¶æ®µçš„é•¿åº¦ç¼©æ”¾æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ›æ–°çš„KVç¼“å­˜ç®¡ç†ç­–ç•¥ï¼ŒåŒºåˆ†åŸå§‹æ ‡è®°å’Œéšè—è§£ç æ ‡è®°ï¼Œä»è€Œåœ¨ä¿æŒæ¨ç†æ•ˆç‡çš„åŒæ—¶å®ç°æœ‰æ•ˆçš„é•¿åº¦ç¼©æ”¾ã€‚PHD-Transformer ä»…ä¿ç•™åŸå§‹æ ‡è®°çš„KVç¼“å­˜ï¼Œä»¥å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¹¶åœ¨ä½¿ç”¨åç«‹å³ä¸¢å¼ƒéšè—è§£ç æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14538",
            "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
            "url": "https://huggingface.co/papers/2504.14538",
            "abstract": "Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.",
            "score": 5,
            "issue_id": 3382,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 20",
                "zh": "4æœˆ20æ—¥"
            },
            "hash": "d5ba284df85bf086",
            "authors": [
                "Yiting Ran",
                "Xintao Wang",
                "Tian Qiu",
                "Jiaqing Liang",
                "Yanghua Xiao",
                "Deqing Yang"
            ],
            "affiliations": [
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14538.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#story_generation",
                    "#open_source"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "BookWorld: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼Ğ¸Ñ€Ñ‹ ĞºĞ½Ğ¸Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° BookWorld Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ½Ğ¸Ğ³. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ²Ñ‹Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ·Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. BookWorld Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ BookWorld Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ½Ğ¸Ğ³Ğ°Ğ¼."
                },
                "en": {
                    "title": "Bringing Fiction to Life: Simulating Book-Based Worlds with BookWorld",
                    "desc": "This paper presents BookWorld, a system designed to simulate multi-agent societies based on established fictional works. Unlike previous approaches that create agents from scratch, BookWorld incorporates existing characters and settings, allowing for more authentic interactions. It captures the complexities of fictional worlds, including character dynamics and geographical elements, enabling applications like story generation and interactive gaming. The results show that BookWorld not only produces high-quality narratives but also stays true to the original source material, outperforming earlier methods in creative storytelling."
                },
                "zh": {
                    "title": "BookWorldï¼šè™šæ„ä¸–ç•Œçš„æ™ºèƒ½ä½“æ¨¡æ‹Ÿæ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBookWorldçš„ç³»ç»Ÿï¼Œç”¨äºæ„å»ºå’Œæ¨¡æ‹ŸåŸºäºä¹¦ç±çš„å¤šæ™ºèƒ½ä½“ç¤¾ä¼šã€‚ä¸ä»¥å¾€ä»é›¶å¼€å§‹åˆ›å»ºæ™ºèƒ½ä½“ç¤¾ä¼šä¸åŒï¼ŒBookWorldèƒ½å¤Ÿåœ¨å·²æœ‰çš„è™šæ„ä¸–ç•Œå’Œè§’è‰²ä¸­è¿›è¡Œæ¨¡æ‹Ÿï¼Œå…·æœ‰é‡è¦çš„å®é™…ä»·å€¼ã€‚è¯¥ç³»ç»Ÿè€ƒè™‘äº†ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„è§’è‰²ã€è™šæ„çš„ä¸–ç•Œè§‚ä»¥åŠåœ°ç†é™åˆ¶ç­‰ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒBookWorldç”Ÿæˆçš„æ•…äº‹ä¸ä»…å¯Œæœ‰åˆ›æ„ä¸”é«˜è´¨é‡ï¼Œä¸”å¿ å®äºåŸè‘—ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16080",
            "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
            "url": "https://huggingface.co/papers/2504.16080",
            "abstract": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks.",
            "score": 4,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "b7a7c66b6a20d5de",
            "authors": [
                "Le Zhuo",
                "Liangbing Zhao",
                "Sayak Paul",
                "Yue Liao",
                "Renrui Zhang",
                "Yi Xin",
                "Peng Gao",
                "Mohamed Elhoseiny",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Hugging Face",
                "KAUST",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16080.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ReflectionFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GenRef Ğ¸Ğ· 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReflectionFlow Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Image Synthesis with ReflectionFlow",
                    "desc": "This paper introduces ReflectionFlow, a new framework designed to improve the performance of text-to-image diffusion models, especially in generating complex scenes and fine details. It leverages the concept of self-reflection, allowing models to iteratively refine their outputs through three scaling methods: noise-level, prompt-level, and reflection-level scaling. The authors created a dataset called GenRef, which contains 1 million triplets of reflections, flawed images, and enhanced images to support the reflection-level scaling process. Experimental results demonstrate that ReflectionFlow outperforms traditional methods, providing a more efficient approach to high-quality image synthesis."
                },
                "zh": {
                    "title": "åæ€æµï¼šæå‡å›¾åƒåˆæˆè´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReflectionFlowçš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤æ‚åœºæ™¯å’Œç»†èŠ‚å¤„ç†ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸‰ç§æ¨ç†æ—¶é—´çš„æ‰©å±•æ–¹å¼ï¼ŒåŒ…æ‹¬å™ªå£°çº§åˆ«æ‰©å±•ã€æç¤ºçº§åˆ«æ‰©å±•å’Œåæ€çº§åˆ«æ‰©å±•ï¼Œæ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚ç‰¹åˆ«æ˜¯åæ€çº§åˆ«æ‰©å±•ï¼Œé€šè¿‡æä¾›å¯æ“ä½œçš„åæ€ï¼Œå¸®åŠ©æ¨¡å‹è¿­ä»£è¯„ä¼°å’Œä¿®æ­£ä¹‹å‰çš„ç”Ÿæˆç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReflectionFlowåœ¨å›¾åƒåˆæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å™ªå£°çº§åˆ«æ‰©å±•æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15681",
            "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
            "url": "https://huggingface.co/papers/2504.15681",
            "abstract": "Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths/formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios.",
            "score": 4,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "58868b4baf82fddf",
            "authors": [
                "Vidi Team",
                "Celong Liu",
                "Chia-Wen Kuo",
                "Dawei Du",
                "Fan Chen",
                "Guang Chen",
                "Jiamin Yuan",
                "Lingxi Zhang",
                "Lu Guo",
                "Lusha Li",
                "Longyin Wen",
                "Qingyu Chen",
                "Rachel Deng",
                "Sijie Zhu",
                "Stuart Siew",
                "Tong Jin",
                "Wei Lu",
                "Wen Zhong",
                "Xiaohui Shen",
                "Xin Gu",
                "Xing Mei",
                "Xueqiong Qu"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance Inc. San Jose/Seattle, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15681.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#games",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Vidi: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Vidi - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. Ğ”Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VUE-TR Ñ Ğ¿ÑÑ‚ÑŒÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Vidi Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Gemini, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Vidi: Revolutionizing Video Editing with Multimodal Understanding",
                    "desc": "This paper presents Vidi, a family of Large Multimodal Models (LMMs) designed to enhance video editing by understanding various input types like vision, audio, and text. Vidi excels in temporal retrieval, which involves pinpointing specific time segments in long videos that match a given text query, making it crucial for intelligent video editing. The authors introduce the VUE-TR benchmark to evaluate Vidi's performance, featuring longer video durations, audio query support, diverse query formats, high-quality annotations, and a refined evaluation metric. Vidi demonstrates superior performance compared to existing models like GPT-4o and Gemini, showcasing its effectiveness in handling complex video editing tasks."
                },
                "zh": {
                    "title": "Vidiï¼šè§†é¢‘ç¼–è¾‘çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVidiçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç¼–è¾‘ä¸­çš„ä¿¡æ¯å¤„ç†é—®é¢˜ã€‚Vidièƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥æ¨¡æ€ï¼ŒåŒ…æ‹¬è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘ä¸­æå–ç›¸å…³æ—¶é—´æ®µã€‚è¯¥æ¨¡å‹åœ¨æ—¶é—´æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆæ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGeminiã€‚ä¸ºäº†æ”¯æŒçœŸå®åœºæ™¯ä¸­çš„å…¨é¢è¯„ä¼°ï¼Œè®ºæ–‡è¿˜æå‡ºäº†VUE-TRåŸºå‡†ï¼ŒåŒ…å«äº†å¤šä¸ªå…³é”®è¿›å±•ï¼Œå¦‚æ›´é•¿çš„è§†é¢‘æ—¶é•¿å’ŒéŸ³é¢‘æ”¯æŒç­‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15785",
            "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
            "url": "https://huggingface.co/papers/2504.15785",
            "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.",
            "score": 3,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "1b074a62628dc1e3",
            "authors": [
                "Siyu Zhou",
                "Tianyi Zhou",
                "Yijun Yang",
                "Guodong Long",
                "Deheng Ye",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "affiliations": [
                "Australian AI Institute, Faculty of Engineering and IT, University of Technology Sydney, Australia",
                "Department of Computer Science, University of Maryland, College Park, USA",
                "Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15785.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#agi",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°' Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° WALL-E 2.0, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ² ÑÑ€ĞµĞ´Ğ°Ñ… Mars Ğ¸ ALFWorld WALL-E 2.0 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing LLMs with World Alignment for Superior Agent Performance",
                    "desc": "This paper explores how large language models (LLMs) can be enhanced to create accurate world models for agents. It introduces a method called 'world alignment' that allows LLMs to learn symbolic knowledge about their environment without requiring extensive training. The proposed agent, WALL-E 2.0, utilizes model-predictive control (MPC) to plan actions efficiently by leveraging the LLM's capabilities and the aligned world model's predictions. The results show that WALL-E 2.0 significantly outperforms existing methods in complex environments, demonstrating improved learning efficiency and higher success rates."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæ„å»ºé«˜æ•ˆä¸–ç•Œæ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºå‡†ç¡®çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œä¸–ç•Œå¯¹é½â€çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¼¥è¡¥LLMsä¸ç‰¹å®šç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„çŸ¥è¯†å·®è·ã€‚é€šè¿‡æå–æ¢ç´¢è½¨è¿¹ä¸­çš„ç¬¦å·çŸ¥è¯†ï¼Œå¦‚è¡ŒåŠ¨è§„åˆ™å’ŒçŸ¥è¯†å›¾è°±ï¼Œæ¥å¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œä»¥ä¼˜åŒ–LLMä»£ç†çš„ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„ä»£ç†â€œWALL-E 2.0â€ï¼Œé€šè¿‡æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ¡†æ¶å®ç°é«˜æ•ˆçš„æœªæ¥æ­¥éª¤åŠ¨ä½œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWALL-E 2.0åœ¨å¼€æ”¾ä¸–ç•ŒæŒ‘æˆ˜ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16030",
            "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
            "url": "https://huggingface.co/papers/2504.16030",
            "abstract": "Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.",
            "score": 2,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "6b5b45c3894204ba",
            "authors": [
                "Joya Chen",
                "Ziyun Zeng",
                "Yiqi Lin",
                "Wei Li",
                "Zejun Ma",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16030.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ˜Ğ˜: Ğ¾Ñ‚ ASR Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LiveCC-7B Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚ ASR Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼. Ğ”Ğ°Ğ¶Ğµ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… VideoMME Ğ¸ OVOBench Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LiveCC-7B-Instruct Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°."
                },
                "en": {
                    "title": "Streamlining Video LLM Training with ASR Transcripts",
                    "desc": "This paper presents a new method for training Video Large Language Models (Video LLMs) using automatic speech recognition (ASR) transcripts instead of expensive human annotations. The authors introduce a streaming training approach that aligns ASR words with video frames based on their timestamps, enhancing the model's ability to understand the relationship between audio and visual content. They also create a dataset from YouTube videos and closed captions, which supports the training process and leads to the development of the LiveCC-7B-Base model. This model shows strong performance in video question answering and real-time commentary, outperforming larger models in quality while maintaining efficiency."
                },
                "zh": {
                    "title": "åˆ©ç”¨ASRå®ç°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMï¼‰è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å»‰ä»·çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æ•°æ®è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æµå¼è®­ç»ƒæ–¹æ³•ï¼Œå°†ASRå•è¯å’Œè§†é¢‘å¸§æ ¹æ®æ—¶é—´æˆ³å¯†é›†äº¤é”™ï¼Œä»è€Œå®ç°æ—¶é—´å¯¹é½çš„ç»†ç²’åº¦è§†è§‰è¯­è¨€å»ºæ¨¡ã€‚é€šè¿‡å¤„ç†YouTubeè§†é¢‘åŠå…¶å­—å¹•ï¼Œæˆ‘ä»¬æ„å»ºäº†Live-CC-5Mæ•°æ®é›†ç”¨äºé¢„è®­ç»ƒï¼Œå¹¶åˆ›å»ºäº†é«˜è´¨é‡çš„Live-WhisperX-526Kæ•°æ®é›†ç”¨äºç›‘ç£å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è§†é¢‘é—®ç­”å’Œå®æ—¶è§†é¢‘è¯„è®ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šå…ˆè¿›çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11703",
            "title": "Progent: Programmable Privilege Control for LLM Agents",
            "url": "https://huggingface.co/papers/2504.11703",
            "abstract": "LLM agents are an emerging form of AI systems where large language models (LLMs) serve as the central component, utilizing a diverse set of tools to complete user-assigned tasks. Despite their great potential, LLM agents pose significant security risks. When interacting with the external world, they may encounter malicious commands from attackers, leading to the execution of dangerous actions. A promising way to address this is by enforcing the principle of least privilege: allowing only essential actions for task completion while blocking unnecessary ones. However, achieving this is challenging, as it requires covering diverse agent scenarios while preserving both security and utility.   We introduce Progent, the first privilege control mechanism for LLM agents. At its core is a domain-specific language for flexibly expressing privilege control policies applied during agent execution. These policies provide fine-grained constraints over tool calls, deciding when tool calls are permissible and specifying fallbacks if they are not. This enables agent developers and users to craft suitable policies for their specific use cases and enforce them deterministically to guarantee security. Thanks to its modular design, integrating Progent does not alter agent internals and requires only minimal changes to agent implementation, enhancing its practicality and potential for widespread adoption. To automate policy writing, we leverage LLMs to generate policies based on user queries, which are then updated dynamically for improved security and utility. Our extensive evaluation shows that it enables strong security while preserving high utility across three distinct scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we perform an in-depth analysis, showcasing the effectiveness of its core components and the resilience of its automated policy generation against adaptive attacks.",
            "score": 2,
            "issue_id": 3380,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "01631fbca25ffa05",
            "authors": [
                "Tianneng Shi",
                "Jingxuan He",
                "Zhun Wang",
                "Linyu Wu",
                "Hongwei Li",
                "Wenbo Guo",
                "Dawn Song"
            ],
            "affiliations": [
                "UC Berkeley",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11703.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#agents"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Progent: Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Progent - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Progent Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLM Agents with Secure Privilege Control",
                    "desc": "This paper introduces Progent, a novel privilege control mechanism designed for large language model (LLM) agents, which are AI systems that utilize LLMs to perform tasks. Progent aims to enhance security by implementing the principle of least privilege, allowing only necessary actions while blocking harmful commands. It features a domain-specific language that enables developers to create fine-grained privilege policies, ensuring that tool calls are made safely and effectively. The system is designed to be easily integrated into existing LLM agents with minimal changes, and it automates policy generation using LLMs to adapt to user needs, demonstrating strong security and utility across various scenarios."
                },
                "zh": {
                    "title": "LLMä»£ç†çš„å®‰å…¨å®ˆæŠ¤è€…ï¼šProgentæƒé™æ§åˆ¶æœºåˆ¶",
                    "desc": "LLMä»£ç†æ˜¯ä¸€ç§æ–°å…´çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œç»“åˆå¤šç§å·¥å…·å®Œæˆç”¨æˆ·ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†åœ¨ä¸å¤–éƒ¨ä¸–ç•Œäº’åŠ¨æ—¶å¯èƒ½é¢ä¸´å®‰å…¨é£é™©ï¼Œå¯èƒ½ä¼šæ‰§è¡Œæ¶æ„å‘½ä»¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Progentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMä»£ç†çš„æƒé™æ§åˆ¶æœºåˆ¶ï¼Œå…è®¸åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­çµæ´»åœ°è¡¨è¾¾æƒé™æ§åˆ¶ç­–ç•¥ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¼€å‘è€…å¯ä»¥ä¸ºç‰¹å®šç”¨ä¾‹åˆ¶å®šåˆé€‚çš„ç­–ç•¥ï¼Œä»è€Œåœ¨ä¿è¯å®‰å…¨çš„åŒæ—¶æé«˜å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16082",
            "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
            "url": "https://huggingface.co/papers/2504.16082",
            "abstract": "We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.   In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents.   Code is available at: https://github.com/ziqipang/MR-Video",
            "score": 1,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "52146a0dfc463f6d",
            "authors": [
                "Ziqi Pang",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16082.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#video",
                    "#agents"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MapReduce Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "MR. Video - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ MapReduce. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² (Map) Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²ÑĞµÑ… ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² (Reduce). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. MR. Video Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LVBench Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with MapReduce",
                    "desc": "MR. Video is a framework designed for understanding long videos by applying the MapReduce principle. It processes short video clips independently in the 'Map' phase, allowing for detailed perception without the constraints of context length. In the 'Reduce' phase, it aggregates information from all clips for comprehensive reasoning, outperforming traditional methods that rely on key segment selection. This approach not only enhances accuracy but also demonstrates scalability in video analysis tasks, achieving significant improvements over existing models."
                },
                "zh": {
                    "title": "MR. Videoï¼šé•¿è§†é¢‘ç†è§£çš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†MR. Videoï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç†è§£çš„æ¡†æ¶ï¼Œé‡‡ç”¨äº†ç®€å•è€Œæœ‰æ•ˆçš„MapReduceåŸåˆ™æ¥å¤„ç†é•¿è§†é¢‘ã€‚é¦–å…ˆï¼Œé€šè¿‡ç‹¬ç«‹ä¸”å¯†é›†åœ°æ„ŸçŸ¥çŸ­è§†é¢‘ç‰‡æ®µï¼ˆMapï¼‰ï¼Œç„¶åå°†æ‰€æœ‰ç‰‡æ®µçš„ä¿¡æ¯è¿›è¡Œèšåˆï¼ˆReduceï¼‰ã€‚ä¸ä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—è§†è§‰è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒMR. Videoèƒ½å¤Ÿåœ¨ä¸å—ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œè¯¦ç»†çš„çŸ­è§†é¢‘æ„ŸçŸ¥ã€‚é€šè¿‡ä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼ŒMR. Videoåœ¨LVBenchæŒ‘æˆ˜ä¸­ç›¸æ¯”äºæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ä»£ç†å®ç°äº†è¶…è¿‡10%çš„å‡†ç¡®ç‡æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14977",
            "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
            "url": "https://huggingface.co/papers/2504.14977",
            "abstract": "Controllable character animation remains a challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose a new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking a step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield a surprisingly strong baseline. We further propose the low-noise warmup and \"large batches and small iterations\" strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce a new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by a large margin.",
            "score": 1,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "96e96789c52f6da1",
            "authors": [
                "Jingkai Zhou",
                "Yifan Wu",
                "Shikai Li",
                "Min Wei",
                "Chao Fan",
                "Weihua Chen",
                "Wei Jiang",
                "Fan Wang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Shenzhen University",
                "Southern University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14977.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RealisDance-DiT, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan-2.1, Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¾Ğ³Ñ€ĞµĞ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ 'Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ±Ğ°Ñ‚Ñ‡Ğ¸ Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RealisDance-DiT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with RealisDance-DiT",
                    "desc": "This paper addresses the challenges of controllable character animation, particularly in complex scenarios involving rare poses and dynamic environments. The authors introduce RealisDance-DiT, a model that leverages a powerful foundation model with minimal architectural modifications and flexible fine-tuning strategies. They argue that traditional methods, which rely on elaborate bypass networks, often fail to generalize well, while their approach shows significant improvements in performance. Additionally, they present a new test dataset to evaluate their method against existing benchmarks, demonstrating that RealisDance-DiT significantly outperforms prior techniques."
                },
                "zh": {
                    "title": "ç®€å•ä¿®æ”¹ï¼Œå¼ºå¤§åŠ¨ç”»ï¼",
                    "desc": "å¯æ§è§’è‰²åŠ¨ç”»ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç¨€æœ‰å§¿åŠ¿ã€é£æ ¼åŒ–è§’è‰²ã€è§’è‰²ä¸ç‰©ä½“çš„äº¤äº’ã€å¤æ‚ç…§æ˜å’ŒåŠ¨æ€åœºæ™¯æ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°è§†è§’ï¼Œè®¤ä¸ºåªè¦åŸºç¡€æ¨¡å‹è¶³å¤Ÿå¼ºå¤§ï¼Œé€šè¿‡çµæ´»çš„å¾®è°ƒç­–ç•¥å¯¹æ¨¡å‹è¿›è¡Œç®€å•ä¿®æ”¹ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†RealisDance-DiTï¼ŒåŸºäºWan-2.1è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œå¹¶å‘ç°å¹¿æ³›é‡‡ç”¨çš„å‚è€ƒç½‘ç»œè®¾è®¡å¯¹äºå¤§è§„æ¨¡DiTæ¨¡å‹å¹¶ä¸ç†æƒ³ã€‚é€šè¿‡æœ€å°çš„æ¶æ„ä¿®æ”¹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¼ºå¤§çš„åŸºçº¿æ€§èƒ½ï¼Œå¹¶æå‡ºäº†ä½å™ªå£°é¢„çƒ­å’Œâ€œå¤§æ‰¹é‡å°è¿­ä»£â€ç­–ç•¥ï¼Œä»¥åŠ é€Ÿå¾®è°ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹æ”¶æ•›ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°ä¿ç•™åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-22.html",
    "link_next": "2025-04-24.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 5,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå«åš LUFFYï¼Œç”¨äºæ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹ã€‚LUFFY ç»“åˆäº†ç¦»ç­–ç•¥æ¼”ç¤ºå’Œåœ¨ç­–ç•¥å±•å¼€ï¼ŒåŠ¨æ€å¹³è¡¡æ¨¡ä»¿å’Œæ¢ç´¢ã€‚å®ƒé€šè¿‡æ­£åˆ™åŒ–é‡è¦æ€§é‡‡æ ·æ¥é¿å…è¡¨é¢å’Œåƒµç¡¬çš„æ¨¡ä»¿ã€‚LUFFY åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åˆ†ææ˜¾ç¤ºï¼ŒLUFFY ä¸ä»…æœ‰æ•ˆæ¨¡ä»¿ï¼Œè¿˜èƒ½æ¢ç´¢è¶…è¶Šæ¼”ç¤ºçš„èƒ½åŠ›ã€‚",
        "title": "Learning to Reason under Off-Policy Guidance",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå«åš LUFFYï¼Œç”¨äºæ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹ã€‚LUFFY ç»“åˆäº†ç¦»ç­–ç•¥æ¼”ç¤ºå’Œåœ¨ç­–ç•¥å±•å¼€ï¼ŒåŠ¨æ€å¹³è¡¡æ¨¡ä»¿å’Œæ¢ç´¢ã€‚å®ƒé€šè¿‡æ­£åˆ™åŒ–é‡è¦æ€§é‡‡æ ·æ¥é¿å…è¡¨é¢å’Œåƒµç¡¬çš„æ¨¡ä»¿ã€‚LUFFY åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åˆ†ææ˜¾ç¤ºï¼ŒLUFFY ä¸ä»…æœ‰æ•ˆæ¨¡ä»¿ï¼Œè¿˜èƒ½æ¢ç´¢è¶…è¶Šæ¼”ç¤ºçš„èƒ½åŠ›ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de kuÃ ng jiÃ , jiÃ o zuÃ² LUFFY, yÃ²ng yÃº gÇi jÃ¬n dÃ  xÃ­ng tuÄ« lÇ mÃ³ xÃ­ng. LUFFY jiÃ© hÃ© le lÃ­ cÃ¨ lÃ¼Ã¨ yÇn shÃ¬ hÃ© zÃ i cÃ¨ lÃ¼Ã¨ zhÇn kÄi, dÃ²ng tÃ i pÃ­ng hÃ©ng mÃ³ fÇng hÃ© tÃ n suÇ’. tÄ tÅng guÃ² zhÃ¨ng zÃ© huÃ  zhÃ²ng yÃ o xÃ¬ng cÇi yÃ ng lÃ¡i bÃ¬ miÇn biÇo miÃ n hÃ© jiÄng yÃ¬ng de mÃ³ fÇng. LUFFY zÃ i liÃ¹ gÃ¨ shÃ¹ xuÃ© jÄ« zhÇ”n cÃ¨ shÃ¬ zhÅng qÇ” dÃ© le xiÇn zhÃ¹ jÃ¬n bÃ¹, bÃ¬ng zÃ i fÄ“n bÃ¹ wÃ i rÃ¨n wÃ¹ zhÅng biÇo xiÃ n chÅ« sÃ¨. fÄ“n xÄ« xiÇn shÃ¬, LUFFY bÃ¹ jÇn yÇ’u xiÃ o mÃ³ fÇng, hÃ¡i nÃ©ng tÃ n suÇ’ chÄo yuÃ¨ yÇn shÃ¬ de nÃ©ng lÃ¬.",
        "vocab": "[\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç¦»ç­–ç•¥\", \"pinyin\": \"lÃ­ cÃ¨lÃ¼Ã¨\", \"trans\": \"off-policy\"},\n    {\"word\": \"æ¼”ç¤º\", \"pinyin\": \"yÇnshÃ¬\", \"trans\": \"demonstration\"},\n    {\"word\": \"åœ¨ç­–ç•¥\", \"pinyin\": \"zÃ i cÃ¨lÃ¼Ã¨\", \"trans\": \"on-policy\"},\n    {\"word\": \"å±•å¼€\", \"pinyin\": \"zhÇnkÄi\", \"trans\": \"unfold\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ngtÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"å¹³è¡¡\", \"pinyin\": \"pÃ­nghÃ©ng\", \"trans\": \"balance\"},\n    {\"word\": \"æ¨¡ä»¿\", \"pinyin\": \"mÃ³fÇng\", \"trans\": \"imitate\"},\n    {\"word\": \"æ¢ç´¢\", \"pinyin\": \"tÃ nsuÇ’\", \"trans\": \"explore\"},\n    {\"word\": \"æ­£åˆ™åŒ–\", \"pinyin\": \"zhÃ¨ngzÃ©huÃ \", \"trans\": \"regularization\"},\n    {\"word\": \"é‡è¦æ€§\", \"pinyin\": \"zhÃ²ngyÃ oxÃ¬ng\", \"trans\": \"importance\"},\n    {\"word\": \"é‡‡æ ·\", \"pinyin\": \"cÇiyÃ ng\", \"trans\": \"sampling\"},\n    {\"word\": \"é¿å…\", \"pinyin\": \"bÃ¬miÇn\", \"trans\": \"avoid\"},\n    {\"word\": \"è¡¨é¢\", \"pinyin\": \"biÇomiÃ n\", \"trans\": \"surface\"},\n    {\"word\": \"åƒµç¡¬\", \"pinyin\": \"jiÄngyÃ¬ng\", \"trans\": \"rigid\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"è¿›æ­¥\", \"pinyin\": \"jÃ¬nbÃ¹\", \"trans\": \"progress\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"åˆ†å¸ƒ\", \"pinyin\": \"fÄ“nbÃ¹\", \"trans\": \"distribution\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“nxÄ«\", \"trans\": \"analysis\"},\n    {\"word\": \"æœ‰æ•ˆ\", \"pinyin\": \"yÇ’uxiÃ o\", \"trans\": \"effective\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄoyuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"}\n]",
        "trans": "This article introduces a new framework called LUFFY for improving large-scale reasoning models. LUFFY combines off-policy demonstration and on-policy expansion, dynamically balancing imitation and exploration. It avoids superficial and rigid imitation through regularized importance sampling. LUFFY has achieved significant advancements in six mathematical benchmark tests and performs excellently on out-of-distribution tasks. Analysis shows that LUFFY not only effectively imitates but also explores beyond the demonstration.",
        "update_ts": "2025-04-22 09:12"
    }
}