{
    "date": {
        "ru": "23 апреля",
        "en": "April 23",
        "zh": "4月23日"
    },
    "time_utc": "2025-04-23 04:14",
    "weekday": 2,
    "issue_id": 3382,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15521",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "url": "https://huggingface.co/papers/2504.15521",
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "score": 27,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "4d1809408c9b3e49",
            "authors": [
                "Minghao Wu",
                "Weixuan Wang",
                "Sinuo Liu",
                "Huifeng Yin",
                "Xintong Wang",
                "Yu Zhao",
                "Chenyang Lyu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce",
                "Monash University",
                "The University of Edinburgh",
                "Tsinghua University",
                "Universität Hamburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15521.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#low_resource",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "За справедливую оценку многоязычных ИИ-моделей",
                    "desc": "Статья анализирует более 2000 многоязычных бенчмарков для оценки языковых моделей, выявляя перекос в сторону английского языка. Исследование показывает, что бенчмарки на основе оригинального контента лучше коррелируют с оценками людей, чем переведенные. Авторы выделяют шесть ключевых ограничений в текущих практиках многоязычной оценки и предлагают принципы для эффективного многоязычного бенчмаркинга. Статья призывает к глобальному сотрудничеству для разработки бенчмарков, ориентированных на реальные приложения и учитывающих культурные особенности."
                },
                "en": {
                    "title": "Towards Equitable Multilingual Benchmarking for LLMs",
                    "desc": "This paper discusses the need for better multilingual evaluation in large language models (LLMs) as they improve in language skills. It analyzes over 2,000 multilingual benchmarks from various countries and finds that English is still overly dominant in these evaluations. The study shows that benchmarks based on original language content are more effective than translated ones, especially for local contexts. The authors propose new guidelines and research directions to create benchmarks that align more closely with human judgments and real-world applications."
                },
                "zh": {
                    "title": "推动多语言评估的公平进步",
                    "desc": "随着大型语言模型（LLMs）在语言能力上的不断进步，进行稳健的多语言评估变得至关重要。本文分析了2021年至2024年间来自148个国家的2000多个多语言基准，评估了多语言基准测试的过去、现在和未来的实践。研究发现，尽管投入了数千万美元，英语在这些基准中仍然占据了过高的比例，且大多数基准依赖于原始语言内容而非翻译。我们强调了创建文化和语言定制基准的重要性，并提出了有效的多语言基准测试指导原则。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16084",
            "title": "TTRL: Test-Time Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.16084",
            "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
            "score": 25,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "4445f28372c61abd",
            "authors": [
                "Yuxin Zuo",
                "Kaiyan Zhang",
                "Shang Qu",
                "Li Sheng",
                "Xuekai Zhu",
                "Biqing Qi",
                "Youbang Sun",
                "Ganqu Cui",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16084.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосовершенствование языковых моделей без меток",
                    "desc": "Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей (LLM) на данных без явных меток для задач рассуждения. Метод, названный Test-Time Reinforcement Learning (TTRL), позволяет LLM самосовершенствоваться, используя априорные знания предобученных моделей. Эксперименты показывают, что TTRL значительно улучшает производительность на различных задачах и моделях, в частности, повышая показатель pass@1 модели Qwen-2.5-Math-7B примерно на 159% на тесте AIME 2024. Результаты исследования подтверждают эффективность TTRL и подчеркивают его потенциал для широкого спектра задач и областей применения."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Unlabeled Data: Test-Time Reinforcement Learning",
                    "desc": "This paper explores a new approach called Test-Time Reinforcement Learning (TTRL) for training Large Language Models (LLMs) using unlabeled data. The main challenge addressed is how to estimate rewards during inference without having access to true labels. The authors find that techniques like majority voting can effectively generate rewards for reinforcement learning training. Their experiments show that TTRL significantly enhances the performance of LLMs, achieving impressive results even when only supervised by a simple metric."
                },
                "zh": {
                    "title": "无标签数据上的强化学习新方法",
                    "desc": "本文研究了在没有明确标签的数据上进行强化学习（RL），以解决大型语言模型（LLMs）的推理任务。主要挑战在于推理过程中如何进行奖励估计，而没有真实标签的信息。我们发现，测试时缩放（TTS）中的常见做法，如多数投票，能够产生意想不到的有效奖励，从而推动RL训练。我们提出了一种新方法——测试时强化学习（TTRL），它利用预训练模型中的先验知识，能够在无标签数据上自我进化，实验结果表明TTRL在多种任务和模型上均能持续提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15466",
            "title": "Learning Adaptive Parallel Reasoning with Language Models",
            "url": "https://huggingface.co/papers/2504.15466",
            "abstract": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.",
            "score": 19,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "4eaf94a6429e3102",
            "authors": [
                "Jiayi Pan",
                "Xiuyu Li",
                "Long Lian",
                "Charlie Snell",
                "Yifei Zhou",
                "Adam Yala",
                "Trevor Darrell",
                "Kurt Keutzer",
                "Alane Suhr"
            ],
            "affiliations": [
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15466.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное параллельное рассуждение: новый шаг к автономной оптимизации вычислений в языковых моделях",
                    "desc": "Эта статья представляет новый метод рассуждений для языковых моделей - Адаптивное Параллельное Рассуждение (APR). APR позволяет моделям оркестровать как последовательные, так и параллельные вычисления, используя операции spawn() и join(). Метод оптимизируется с помощью обучения с подкреплением, улучшая показатели успешности задач без предопределенных структур рассуждений. Эксперименты на задаче Countdown показали значительные преимущества APR в производительности, масштабируемости и точности по сравнению с существующими методами."
                },
                "en": {
                    "title": "Adaptive Parallel Reasoning: Optimizing Language Model Inference",
                    "desc": "This paper introduces Adaptive Parallel Reasoning (APR), a new framework designed to enhance the reasoning capabilities of language models during inference. APR combines both serialized and parallel computation methods, allowing for more efficient processing without the drawbacks of existing approaches. The framework utilizes an end-to-end reinforcement learning strategy to optimize inference threads, improving task success rates without needing fixed reasoning structures. Experimental results show that APR significantly outperforms traditional methods in terms of performance, scalability, and accuracy while maintaining similar latency levels."
                },
                "zh": {
                    "title": "自适应并行推理：提升语言模型推理能力的创新框架",
                    "desc": "本文提出了一种新的推理框架，称为自适应并行推理（APR），旨在解决现有语言模型推理方法的局限性。APR结合了串行和并行计算，允许模型在推理过程中灵活调整计算方式。通过使用spawn()和join()操作，APR实现了自适应的多线程推理，优化了推理线程的成功率。实验结果表明，APR在相同上下文窗口内表现更好，并且在计算量增加时具有更好的可扩展性和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16072",
            "title": "Describe Anything: Detailed Localized Image and Video Captioning",
            "url": "https://huggingface.co/papers/2504.16072",
            "abstract": "Generating detailed and accurate descriptions for specific regions in images and videos remains a fundamental challenge for vision-language models. We introduce the Describe Anything Model (DAM), a model designed for detailed localized captioning (DLC). DAM preserves both local details and global context through two key innovations: a focal prompt, which ensures high-resolution encoding of targeted regions, and a localized vision backbone, which integrates precise localization with its broader context. To tackle the scarcity of high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark designed to evaluate DLC without relying on reference captions. DAM sets new state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and detailed multi-sentence localized image and video captioning.",
            "score": 15,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "1749ec1e79ed8810",
            "authors": [
                "Long Lian",
                "Yifan Ding",
                "Yunhao Ge",
                "Sifei Liu",
                "Hanzi Mao",
                "Boyi Li",
                "Marco Pavone",
                "Ming-Yu Liu",
                "Trevor Darrell",
                "Adam Yala",
                "Yin Cui"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16072.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DAM: Точное описание любой детали изображения",
                    "desc": "Модель DAM (Describe Anything Model) представляет собой инновационный подход к детальному локализованному описанию изображений и видео. Она использует фокальный промпт и локализованный визуальный бэкбон для сохранения как локальных деталей, так и глобального контекста. Авторы также предлагают полу-контролируемый конвейер данных DLC-SDP для решения проблемы нехватки качественных данных. DAM устанавливает новый state-of-the-art на 7 бенчмарках, охватывающих различные уровни локализованного описания изображений и видео."
                },
                "en": {
                    "title": "Capturing Details Anywhere: The Describe Anything Model",
                    "desc": "The Describe Anything Model (DAM) addresses the challenge of generating detailed captions for specific areas in images and videos. It utilizes a focal prompt for high-resolution encoding of targeted regions and a localized vision backbone to combine local details with global context. To enhance the training data for detailed localized captioning, the model employs a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP) that leverages existing segmentation datasets and expands to unlabeled web images. DAM achieves state-of-the-art performance across multiple benchmarks for keyword, phrase, and multi-sentence captioning tasks."
                },
                "zh": {
                    "title": "描述任何事物，精准本地化！",
                    "desc": "本文介绍了一种新的模型，称为描述任何事物模型（DAM），旨在为图像和视频中的特定区域生成详细的本地化描述。DAM通过两个关键创新来保持局部细节和全局上下文：焦点提示确保对目标区域的高分辨率编码，而本地化视觉骨干网络则将精确定位与更广泛的上下文相结合。为了应对高质量本地化描述数据的稀缺，本文提出了一种基于半监督学习的数据管道（DLC-SDP），该管道利用现有的分割数据集并扩展到未标记的网络图像。DAM在七个基准测试中设定了新的最先进水平，涵盖了关键词级、短语级和详细的多句本地化图像和视频描述。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15415",
            "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.15415",
            "abstract": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.",
            "score": 11,
            "issue_id": 3381,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "40ca992771747dac",
            "authors": [
                "David Ma",
                "Yuanxing Zhang",
                "Jincheng Ren",
                "Jarvis Guo",
                "Yifan Yao",
                "Zhenlin Wei",
                "Zhenzhu Yang",
                "Zhongyuan Peng",
                "Boyu Feng",
                "Jun Ma",
                "Xiao Gu",
                "Zhoufutu Wen",
                "King Zhu",
                "Yancheng He",
                "Meng Cao",
                "Shiwen Ni",
                "Jiaheng Liu",
                "Wenhao Huang",
                "Ge Zhang",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "ByteDance Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15415.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "IV-Bench: новый рубеж в оценке мультимодальных ИИ для видео",
                    "desc": "IV-Bench - это новый комплексный бенчмарк для оценки восприятия и рассуждения на основе изображений в видео для мультимодальных больших языковых моделей (MLLM). Он включает 967 видео с 2,585 аннотированными запросами изображение-текст по 13 задачам и 5 категориям. Тестирование современных MLLM показало, что их производительность в этих задачах не превышает 28.9% точности. Анализ выявил ключевые факторы, влияющие на производительность моделей, включая паттерны вывода, количество кадров и разрешение."
                },
                "en": {
                    "title": "IV-Bench: Bridging Image Context and Video Understanding",
                    "desc": "This paper introduces IV-Bench, a new evaluation framework designed to assess how well Multimodal Large Language Models (MLLMs) understand videos using image context. It includes a dataset of 967 videos and 2,585 annotated image-text queries across various tasks, highlighting the importance of image-grounded reasoning in video comprehension. The study shows that current MLLMs struggle with this task, achieving a maximum accuracy of only 28.9%. The authors also identify factors affecting model performance and suggest that challenges in video understanding go beyond just data format alignment during training."
                },
                "zh": {
                    "title": "IV-Bench：图像基础视频理解的新基准",
                    "desc": "现有的多模态大型语言模型（MLLMs）评估框架主要关注图像推理或一般视频理解任务，忽视了图像上下文在视频理解中的重要作用。为了解决这个问题，我们提出了IV-Bench，这是第一个全面评估图像基础视频感知和推理的基准。IV-Bench包含967个视频和2585个精心注释的图像-文本查询，涵盖13个任务（7个感知任务和6个推理任务）及5个代表性类别。评估结果显示，当前的模型在图像基础视频感知和推理方面表现不佳，最高准确率仅为28.9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13820",
            "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
            "url": "https://huggingface.co/papers/2504.13820",
            "abstract": "Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld.",
            "score": 10,
            "issue_id": 3381,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 апреля",
                "en": "April 18",
                "zh": "4月18日"
            },
            "hash": "c69df5180f187adb",
            "authors": [
                "Yang Yue",
                "Yulin Wang",
                "Chenxin Tao",
                "Pan Liu",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "PLA General Hospital",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13820.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#healthcare",
                    "#agi",
                    "#science",
                    "#training"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "CheXWorld: самообучающаяся мировая модель для рентгенографии",
                    "desc": "Статья представляет CheXWorld - первую попытку создания самообучающейся мировой модели для рентгеновских снимков. Модель одновременно учитывает три аспекта медицинских знаний: локальные анатомические структуры, глобальные анатомические компоновки и вариации в визуальном представлении снимков. Эмпирические исследования показывают, что CheXWorld успешно захватывает эти три измерения медицинских знаний. Эксперименты по трансферному обучению демонстрируют, что CheXWorld значительно превосходит существующие методы самообучения и крупномасштабные медицинские базовые модели."
                },
                "en": {
                    "title": "CheXWorld: A Self-Supervised Model for Radiographic Understanding",
                    "desc": "This paper introduces CheXWorld, a self-supervised world model designed for radiographic images, which aims to replicate human-like common sense knowledge in medical imaging. It develops a framework that captures three critical aspects of medical knowledge: local anatomical structures, global anatomical layouts, and domain variations in radiographs. The model is evaluated through qualitative and quantitative analyses, demonstrating its ability to effectively represent these dimensions of medical knowledge. Additionally, CheXWorld shows superior performance in transfer learning tasks compared to existing self-supervised learning methods and large-scale medical models."
                },
                "zh": {
                    "title": "CheXWorld：放射影像的自监督世界模型",
                    "desc": "本论文介绍了CheXWorld，这是第一个针对放射影像的自监督世界模型。该模型同时建模了医学知识的三个重要方面：局部解剖结构、全局解剖布局和领域变化。通过定性和定量分析，CheXWorld成功捕捉了这些医学知识的维度，并在八个医学图像分类和分割基准测试中表现优异。实验结果表明，CheXWorld在性能上显著超越了现有的自监督学习方法和大型医学基础模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14992",
            "title": "Efficient Pretraining Length Scaling",
            "url": "https://huggingface.co/papers/2504.14992",
            "abstract": "Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (PHD-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. PHD-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: PHD-SWA employs sliding window attention to preserve local dependencies, while PHD-CSWA implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.",
            "score": 8,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "a0f15db68dba9035",
            "authors": [
                "Bohong Wu",
                "Shen Yan",
                "Sijun Zhang",
                "Jianqiao Lu",
                "Yutao Zeng",
                "Ya Wang",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14992.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное масштабирование длины в трансформерах с параллельным скрытым декодированием",
                    "desc": "Эта статья представляет новый подход к масштабированию длины последовательности в языковых моделях во время предварительного обучения. Авторы предлагают Parallel Hidden Decoding Transformer (PHD-Transformer), который эффективно управляет кэшем ключей и значений, различая исходные токены и скрытые токены декодирования. Представлены две оптимизированные версии: PHD-SWA с использованием скользящего окна внимания и PHD-CSWA с чанковым скользящим окном внимания. Эксперименты показывают последовательное улучшение результатов на различных бенчмарках."
                },
                "en": {
                    "title": "Efficient Length Scaling in Pre-Training with PHD-Transformer",
                    "desc": "This paper introduces the Parallel Hidden Decoding Transformer (PHD-Transformer), which enhances pre-training of large language models by implementing efficient length scaling. The framework utilizes a unique key-value (KV) cache management strategy that differentiates between original tokens and hidden decoding tokens, optimizing memory usage. By retaining only the KV cache of original tokens, it effectively manages long-range dependencies without increasing cache size. Additionally, two optimized variants, PHD-SWA and PHD-CSWA, are proposed to improve local dependencies and reduce pre-filling time, leading to better performance across various benchmarks."
                },
                "zh": {
                    "title": "提升预训练效率的创新变换器",
                    "desc": "本文介绍了一种新的框架，称为并行隐藏解码变换器（PHD-Transformer），旨在提高预训练阶段的长度缩放效率。该方法通过创新的KV缓存管理策略，区分原始标记和隐藏解码标记，从而在保持推理效率的同时实现有效的长度缩放。PHD-Transformer 仅保留原始标记的KV缓存，以处理长距离依赖关系，并在使用后立即丢弃隐藏解码标记。实验结果表明，该方法在多个基准测试中均表现出一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14538",
            "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
            "url": "https://huggingface.co/papers/2504.14538",
            "abstract": "Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.",
            "score": 5,
            "issue_id": 3382,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 апреля",
                "en": "April 20",
                "zh": "4月20日"
            },
            "hash": "d5ba284df85bf086",
            "authors": [
                "Yiting Ran",
                "Xintao Wang",
                "Tian Qiu",
                "Jiaqing Liang",
                "Yanghua Xiao",
                "Deqing Yang"
            ],
            "affiliations": [
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14538.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#story_generation",
                    "#open_source"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "BookWorld: погружение в миры книг с помощью ИИ",
                    "desc": "В статье представлена система BookWorld для создания и симуляции многоагентных обществ на основе книг. Система учитывает различные аспекты, включая разнообразие персонажей, вымышленные мировоззрения и географические ограничения. BookWorld позволяет генерировать истории, создавать интерактивные игры и проводить социальные симуляции, предлагая новые способы исследования художественных произведений. Эксперименты показали, что BookWorld превосходит предыдущие методы, генерируя качественные и креативные истории с сохранением верности оригинальным книгам."
                },
                "en": {
                    "title": "Bringing Fiction to Life: Simulating Book-Based Worlds with BookWorld",
                    "desc": "This paper presents BookWorld, a system designed to simulate multi-agent societies based on established fictional works. Unlike previous approaches that create agents from scratch, BookWorld incorporates existing characters and settings, allowing for more authentic interactions. It captures the complexities of fictional worlds, including character dynamics and geographical elements, enabling applications like story generation and interactive gaming. The results show that BookWorld not only produces high-quality narratives but also stays true to the original source material, outperforming earlier methods in creative storytelling."
                },
                "zh": {
                    "title": "BookWorld：虚构世界的智能体模拟新纪元",
                    "desc": "本文介绍了一种名为BookWorld的系统，用于构建和模拟基于书籍的多智能体社会。与以往从零开始创建智能体社会不同，BookWorld能够在已有的虚构世界和角色中进行模拟，具有重要的实际价值。该系统考虑了现实世界的复杂性，包括多样化和动态的角色、虚构的世界观以及地理限制等。通过大量实验，BookWorld生成的故事不仅富有创意且高质量，且忠实于原著，超越了之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16080",
            "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
            "url": "https://huggingface.co/papers/2504.16080",
            "abstract": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks.",
            "score": 4,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "b7a7c66b6a20d5de",
            "authors": [
                "Le Zhuo",
                "Liangbing Zhao",
                "Sayak Paul",
                "Yue Liao",
                "Renrui Zhang",
                "Yi Xin",
                "Peng Gao",
                "Mohamed Elhoseiny",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Hugging Face",
                "KAUST",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16080.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самоанализ диффузионных моделей для улучшения качества генерации изображений",
                    "desc": "ReflectionFlow - это новый подход к улучшению генерации изображений с помощью диффузионных моделей. Он вводит три оси масштабирования на этапе вывода: оптимизацию начального шума, точное семантическое управление и итеративную рефлексию для оценки и исправления предыдущих генераций. Для обучения рефлексии используется специально созданный датасет GenRef из 1 миллиона триплетов. Эксперименты показывают, что ReflectionFlow значительно превосходит наивные методы масштабирования шума при генерации сложных изображений."
                },
                "en": {
                    "title": "Enhancing Image Synthesis with ReflectionFlow",
                    "desc": "This paper introduces ReflectionFlow, a new framework designed to improve the performance of text-to-image diffusion models, especially in generating complex scenes and fine details. It leverages the concept of self-reflection, allowing models to iteratively refine their outputs through three scaling methods: noise-level, prompt-level, and reflection-level scaling. The authors created a dataset called GenRef, which contains 1 million triplets of reflections, flawed images, and enhanced images to support the reflection-level scaling process. Experimental results demonstrate that ReflectionFlow outperforms traditional methods, providing a more efficient approach to high-quality image synthesis."
                },
                "zh": {
                    "title": "反思流：提升图像合成质量的新方法",
                    "desc": "本文提出了一种名为ReflectionFlow的推理框架，旨在提高文本到图像扩散模型在复杂场景和细节处理上的表现。该框架通过引入三种推理时间的扩展方式，包括噪声级别扩展、提示级别扩展和反思级别扩展，来优化生成过程。特别是反思级别扩展，通过提供可操作的反思，帮助模型迭代评估和修正之前的生成结果。实验结果表明，ReflectionFlow在图像合成质量上显著优于传统的噪声级别扩展方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15681",
            "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
            "url": "https://huggingface.co/papers/2504.15681",
            "abstract": "Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths/formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios.",
            "score": 4,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "58868b4baf82fddf",
            "authors": [
                "Vidi Team",
                "Celong Liu",
                "Chia-Wen Kuo",
                "Dawei Du",
                "Fan Chen",
                "Guang Chen",
                "Jiamin Yuan",
                "Lingxi Zhang",
                "Lu Guo",
                "Lusha Li",
                "Longyin Wen",
                "Qingyu Chen",
                "Rachel Deng",
                "Sijie Zhu",
                "Stuart Siew",
                "Tong Jin",
                "Wei Lu",
                "Wen Zhong",
                "Xiaohui Shen",
                "Xin Gu",
                "Xing Mei",
                "Xueqiong Qu"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance Inc. San Jose/Seattle, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15681.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#games",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Vidi: Революция в понимании и редактировании видео с помощью ИИ",
                    "desc": "Vidi - это семейство крупных мультимодальных моделей (LMM) для широкого спектра задач по пониманию и редактированию видео. Модель способна обрабатывать часовые видео и выполнять темпоральный поиск, определяя временные диапазоны в видео, соответствующие текстовому запросу. Для комплексной оценки в реальных сценариях авторы представили бенчмарк VUE-TR с пятью ключевыми улучшениями, включая более длительные видео и поддержку аудиозапросов. Vidi значительно превосходит ведущие проприетарные модели, такие как GPT-4 и Gemini, в задаче темпорального поиска."
                },
                "en": {
                    "title": "Vidi: Revolutionizing Video Editing with Multimodal Understanding",
                    "desc": "This paper presents Vidi, a family of Large Multimodal Models (LMMs) designed to enhance video editing by understanding various input types like vision, audio, and text. Vidi excels in temporal retrieval, which involves pinpointing specific time segments in long videos that match a given text query, making it crucial for intelligent video editing. The authors introduce the VUE-TR benchmark to evaluate Vidi's performance, featuring longer video durations, audio query support, diverse query formats, high-quality annotations, and a refined evaluation metric. Vidi demonstrates superior performance compared to existing models like GPT-4o and Gemini, showcasing its effectiveness in handling complex video editing tasks."
                },
                "zh": {
                    "title": "Vidi：视频编辑的新纪元",
                    "desc": "本论文介绍了一种名为Vidi的大型多模态模型（LMM），旨在解决视频编辑中的信息处理问题。Vidi能够处理多种输入模态，包括视觉、音频和文本，并具备强大的时间理解能力，能够从长达数小时的视频中提取相关时间段。该模型在时间检索任务上表现优异，超越了现有的领先模型，如GPT-4o和Gemini。为了支持真实场景中的全面评估，论文还提出了VUE-TR基准，包含了多个关键进展，如更长的视频时长和音频支持等。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15785",
            "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
            "url": "https://huggingface.co/papers/2504.15785",
            "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.",
            "score": 3,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "1b074a62628dc1e3",
            "authors": [
                "Siyu Zhou",
                "Tianyi Zhou",
                "Yijun Yang",
                "Guodong Long",
                "Deheng Ye",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "affiliations": [
                "Australian AI Institute, Faculty of Engineering and IT, University of Technology Sydney, Australia",
                "Department of Computer Science, University of Maryland, College Park, USA",
                "Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15785.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#agi",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Нейросимволические модели мира повышают эффективность ИИ-агентов",
                    "desc": "Исследователи предлагают метод 'выравнивания мира' для улучшения работы больших языковых моделей (LLM) в качестве моделей мира. Этот подход извлекает символические знания об окружающей среде с помощью LLM и кодирует их в исполняемый код. Авторы представляют агента WALL-E 2.0, использующего нейросимволическую модель мира и LLM для эффективного планирования действий. В экспериментах в средах Mars и ALFWorld WALL-E 2.0 значительно превосходит существующие методы по показателям успешности и эффективности обучения."
                },
                "en": {
                    "title": "Enhancing LLMs with World Alignment for Superior Agent Performance",
                    "desc": "This paper explores how large language models (LLMs) can be enhanced to create accurate world models for agents. It introduces a method called 'world alignment' that allows LLMs to learn symbolic knowledge about their environment without requiring extensive training. The proposed agent, WALL-E 2.0, utilizes model-predictive control (MPC) to plan actions efficiently by leveraging the LLM's capabilities and the aligned world model's predictions. The results show that WALL-E 2.0 significantly outperforms existing methods in complex environments, demonstrating improved learning efficiency and higher success rates."
                },
                "zh": {
                    "title": "利用LLM构建高效世界模型的创新方法",
                    "desc": "本文探讨了如何利用大型语言模型（LLMs）构建准确的世界模型，并提出了一种名为“世界对齐”的方法，旨在弥补LLMs与特定环境动态之间的知识差距。通过提取探索轨迹中的符号知识，如行动规则和知识图谱，来增强LLMs的能力，并将其编码为可执行代码，以优化LLM代理的策略。我们还提出了一种基于模型的代理“WALL-E 2.0”，通过模型预测控制（MPC）框架实现高效的未来步骤动作规划。实验结果表明，WALL-E 2.0在开放世界挑战中显著超越了现有方法，展示了其在新环境中的学习效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16030",
            "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
            "url": "https://huggingface.co/papers/2504.16030",
            "abstract": "Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.",
            "score": 2,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "6b5b45c3894204ba",
            "authors": [
                "Joya Chen",
                "Ziyun Zeng",
                "Yiqi Lin",
                "Wei Li",
                "Zejun Ma",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16030.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в обучении видео-ИИ: от ASR к реальному комментированию",
                    "desc": "Исследователи предложили новый метод обучения видео-языковых моделей с использованием автоматического распознавания речи (ASR) вместо дорогостоящей ручной разметки. Модель LiveCC-7B обучается на потоковых данных, где текст ASR и видеокадры плотно чередуются по временным меткам. Даже без дополнительной настройки модель показывает конкурентоспособные результаты в задачах видео-вопросов и ответов, а также демонстрирует новую способность комментировать видео в реальном времени. На популярных бенчмарках VideoMME и OVOBench модель LiveCC-7B-Instruct достигает лучших результатов среди моделей сопоставимого размера."
                },
                "en": {
                    "title": "Streamlining Video LLM Training with ASR Transcripts",
                    "desc": "This paper presents a new method for training Video Large Language Models (Video LLMs) using automatic speech recognition (ASR) transcripts instead of expensive human annotations. The authors introduce a streaming training approach that aligns ASR words with video frames based on their timestamps, enhancing the model's ability to understand the relationship between audio and visual content. They also create a dataset from YouTube videos and closed captions, which supports the training process and leads to the development of the LiveCC-7B-Base model. This model shows strong performance in video question answering and real-time commentary, outperforming larger models in quality while maintaining efficiency."
                },
                "zh": {
                    "title": "利用ASR实现视频大语言模型的高效训练",
                    "desc": "本文探讨了一种新的视频大语言模型（Video LLM）训练方法，利用廉价的自动语音识别（ASR）转录数据进行大规模训练。我们提出了一种新颖的流式训练方法，将ASR单词和视频帧根据时间戳密集交错，从而实现时间对齐的细粒度视觉语言建模。通过处理YouTube视频及其字幕，我们构建了Live-CC-5M数据集用于预训练，并创建了高质量的Live-WhisperX-526K数据集用于监督微调。实验结果表明，我们的模型在视频问答和实时视频评论方面表现优异，超越了许多先进的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11703",
            "title": "Progent: Programmable Privilege Control for LLM Agents",
            "url": "https://huggingface.co/papers/2504.11703",
            "abstract": "LLM agents are an emerging form of AI systems where large language models (LLMs) serve as the central component, utilizing a diverse set of tools to complete user-assigned tasks. Despite their great potential, LLM agents pose significant security risks. When interacting with the external world, they may encounter malicious commands from attackers, leading to the execution of dangerous actions. A promising way to address this is by enforcing the principle of least privilege: allowing only essential actions for task completion while blocking unnecessary ones. However, achieving this is challenging, as it requires covering diverse agent scenarios while preserving both security and utility.   We introduce Progent, the first privilege control mechanism for LLM agents. At its core is a domain-specific language for flexibly expressing privilege control policies applied during agent execution. These policies provide fine-grained constraints over tool calls, deciding when tool calls are permissible and specifying fallbacks if they are not. This enables agent developers and users to craft suitable policies for their specific use cases and enforce them deterministically to guarantee security. Thanks to its modular design, integrating Progent does not alter agent internals and requires only minimal changes to agent implementation, enhancing its practicality and potential for widespread adoption. To automate policy writing, we leverage LLMs to generate policies based on user queries, which are then updated dynamically for improved security and utility. Our extensive evaluation shows that it enables strong security while preserving high utility across three distinct scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we perform an in-depth analysis, showcasing the effectiveness of its core components and the resilience of its automated policy generation against adaptive attacks.",
            "score": 2,
            "issue_id": 3380,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "01631fbca25ffa05",
            "authors": [
                "Tianneng Shi",
                "Jingxuan He",
                "Zhun Wang",
                "Linyu Wu",
                "Hongwei Li",
                "Wenbo Guo",
                "Dawn Song"
            ],
            "affiliations": [
                "UC Berkeley",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11703.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#agents"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Progent: Безопасность агентов LLM без компромиссов",
                    "desc": "Progent - это первый механизм контроля привилегий для агентов на основе больших языковых моделей (LLM). Он использует предметно-ориентированный язык для гибкого выражения политик контроля привилегий, применяемых во время выполнения агента. Progent позволяет разработчикам и пользователям агентов создавать подходящие политики для конкретных случаев использования и детерминированно применять их для обеспечения безопасности. Система демонстрирует высокую эффективность в обеспечении безопасности при сохранении высокой полезности в различных сценариях и бенчмарках."
                },
                "en": {
                    "title": "Empowering LLM Agents with Secure Privilege Control",
                    "desc": "This paper introduces Progent, a novel privilege control mechanism designed for large language model (LLM) agents, which are AI systems that utilize LLMs to perform tasks. Progent aims to enhance security by implementing the principle of least privilege, allowing only necessary actions while blocking harmful commands. It features a domain-specific language that enables developers to create fine-grained privilege policies, ensuring that tool calls are made safely and effectively. The system is designed to be easily integrated into existing LLM agents with minimal changes, and it automates policy generation using LLMs to adapt to user needs, demonstrating strong security and utility across various scenarios."
                },
                "zh": {
                    "title": "LLM代理的安全守护者：Progent权限控制机制",
                    "desc": "LLM代理是一种新兴的人工智能系统，利用大型语言模型（LLM）作为核心组件，结合多种工具完成用户任务。然而，这些代理在与外部世界互动时可能面临安全风险，可能会执行恶意命令。为了解决这个问题，我们提出了Progent，这是第一个针对LLM代理的权限控制机制，允许在执行过程中灵活地表达权限控制策略。通过这种方式，开发者可以为特定用例制定合适的策略，从而在保证安全的同时提高实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16082",
            "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
            "url": "https://huggingface.co/papers/2504.16082",
            "abstract": "We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.   In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents.   Code is available at: https://github.com/ziqipang/MR-Video",
            "score": 1,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "52146a0dfc463f6d",
            "authors": [
                "Ziqi Pang",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16082.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#video",
                    "#agents"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MapReduce для видео: новый уровень понимания длинного контента",
                    "desc": "MR. Video - это новый подход к пониманию длинных видео, основанный на принципе MapReduce. Он включает в себя два этапа: независимое восприятие коротких видеоклипов (Map) и совместную агрегацию информации из всех клипов (Reduce). Этот метод превосходит существующие vision-language модели и видео-агенты, обеспечивая более детальное восприятие и масштабируемую обработку. MR. Video достигает более чем 10% улучшения точности на сложном бенчмарке LVBench по сравнению с современными моделями."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with MapReduce",
                    "desc": "MR. Video is a framework designed for understanding long videos by applying the MapReduce principle. It processes short video clips independently in the 'Map' phase, allowing for detailed perception without the constraints of context length. In the 'Reduce' phase, it aggregates information from all clips for comprehensive reasoning, outperforming traditional methods that rely on key segment selection. This approach not only enhances accuracy but also demonstrates scalability in video analysis tasks, achieving significant improvements over existing models."
                },
                "zh": {
                    "title": "MR. Video：长视频理解的新方法",
                    "desc": "我们提出了MR. Video，这是一个用于长视频理解的框架，采用了简单而有效的MapReduce原则来处理长视频。首先，通过独立且密集地感知短视频片段（Map），然后将所有片段的信息进行聚合（Reduce）。与传统的序列到序列视觉语言模型相比，MR. Video能够在不受上下文长度限制的情况下，进行详细的短视频感知。通过使用大规模语言模型（LLM）代理验证其有效性，MR. Video在LVBench挑战中相比于最先进的视觉语言模型和视频代理实现了超过10%的准确率提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14977",
            "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
            "url": "https://huggingface.co/papers/2504.14977",
            "abstract": "Controllable character animation remains a challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose a new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking a step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield a surprisingly strong baseline. We further propose the low-noise warmup and \"large batches and small iterations\" strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce a new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by a large margin.",
            "score": 1,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "96e96789c52f6da1",
            "authors": [
                "Jingkai Zhou",
                "Yifan Wu",
                "Shikai Li",
                "Min Wei",
                "Chao Fan",
                "Weihua Chen",
                "Wei Jiang",
                "Fan Wang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Shenzhen University",
                "Southern University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14977.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Революция в анимации персонажей: от сложных сетей к простым модификациям",
                    "desc": "Статья представляет новый подход к управляемой анимации персонажей с использованием мощных фундаментальных моделей. Авторы предлагают RealisDance-DiT, основанную на видеомодели Wan-2.1, с минимальными модификациями архитектуры. Они вводят стратегии обучения, такие как разогрев с низким шумом и 'большие батчи и малые итерации', для ускорения сходимости модели. Эксперименты показывают, что RealisDance-DiT значительно превосходит существующие методы по различным сложным сценариям анимации."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with RealisDance-DiT",
                    "desc": "This paper addresses the challenges of controllable character animation, particularly in complex scenarios involving rare poses and dynamic environments. The authors introduce RealisDance-DiT, a model that leverages a powerful foundation model with minimal architectural modifications and flexible fine-tuning strategies. They argue that traditional methods, which rely on elaborate bypass networks, often fail to generalize well, while their approach shows significant improvements in performance. Additionally, they present a new test dataset to evaluate their method against existing benchmarks, demonstrating that RealisDance-DiT significantly outperforms prior techniques."
                },
                "zh": {
                    "title": "简单修改，强大动画！",
                    "desc": "可控角色动画仍然是一个具有挑战性的问题，尤其是在处理稀有姿势、风格化角色、角色与物体的交互、复杂照明和动态场景时。本文提出了一种新视角，认为只要基础模型足够强大，通过灵活的微调策略对模型进行简单修改，可以有效解决这些挑战。我们引入了RealisDance-DiT，基于Wan-2.1视频基础模型，并发现广泛采用的参考网络设计对于大规模DiT模型并不理想。通过最小的架构修改，我们展示了强大的基线性能，并提出了低噪声预热和“大批量小迭代”策略，以加速微调过程中的模型收敛，同时最大限度地保留基础模型的先验知识。"
                }
            }
        }
    ],
    "link_prev": "2025-04-22.html",
    "link_next": "2025-04-24.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4月22日"
    },
    "short_date_next": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4月24日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 5,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的框架，叫做 LUFFY，用于改进大型推理模型。LUFFY 结合了离策略演示和在策略展开，动态平衡模仿和探索。它通过正则化重要性采样来避免表面和僵硬的模仿。LUFFY 在六个数学基准测试中取得了显著进步，并在分布外任务中表现出色。分析显示，LUFFY 不仅有效模仿，还能探索超越演示的能力。",
        "title": "Learning to Reason under Off-Policy Guidance",
        "pinyin": "这篇文章介绍了一种新的框架，叫做 LUFFY，用于改进大型推理模型。LUFFY 结合了离策略演示和在策略展开，动态平衡模仿和探索。它通过正则化重要性采样来避免表面和僵硬的模仿。LUFFY 在六个数学基准测试中取得了显著进步，并在分布外任务中表现出色。分析显示，LUFFY 不仅有效模仿，还能探索超越演示的能力。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de kuàng jià, jiào zuò LUFFY, yòng yú gǎi jìn dà xíng tuī lǐ mó xíng. LUFFY jié hé le lí cè lüè yǎn shì hé zài cè lüè zhǎn kāi, dòng tài píng héng mó fǎng hé tàn suǒ. tā tōng guò zhèng zé huà zhòng yào xìng cǎi yàng lái bì miǎn biǎo miàn hé jiāng yìng de mó fǎng. LUFFY zài liù gè shù xué jī zhǔn cè shì zhōng qǔ dé le xiǎn zhù jìn bù, bìng zài fēn bù wài rèn wù zhōng biǎo xiàn chū sè. fēn xī xiǎn shì, LUFFY bù jǐn yǒu xiào mó fǎng, hái néng tàn suǒ chāo yuè yǎn shì de néng lì.",
        "vocab": "[\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"离策略\", \"pinyin\": \"lí cèlüè\", \"trans\": \"off-policy\"},\n    {\"word\": \"演示\", \"pinyin\": \"yǎnshì\", \"trans\": \"demonstration\"},\n    {\"word\": \"在策略\", \"pinyin\": \"zài cèlüè\", \"trans\": \"on-policy\"},\n    {\"word\": \"展开\", \"pinyin\": \"zhǎnkāi\", \"trans\": \"unfold\"},\n    {\"word\": \"动态\", \"pinyin\": \"dòngtài\", \"trans\": \"dynamic\"},\n    {\"word\": \"平衡\", \"pinyin\": \"pínghéng\", \"trans\": \"balance\"},\n    {\"word\": \"模仿\", \"pinyin\": \"mófǎng\", \"trans\": \"imitate\"},\n    {\"word\": \"探索\", \"pinyin\": \"tànsuǒ\", \"trans\": \"explore\"},\n    {\"word\": \"正则化\", \"pinyin\": \"zhèngzéhuà\", \"trans\": \"regularization\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòngyàoxìng\", \"trans\": \"importance\"},\n    {\"word\": \"采样\", \"pinyin\": \"cǎiyàng\", \"trans\": \"sampling\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"表面\", \"pinyin\": \"biǎomiàn\", \"trans\": \"surface\"},\n    {\"word\": \"僵硬\", \"pinyin\": \"jiāngyìng\", \"trans\": \"rigid\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"进步\", \"pinyin\": \"jìnbù\", \"trans\": \"progress\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cèshì\", \"trans\": \"test\"},\n    {\"word\": \"分布\", \"pinyin\": \"fēnbù\", \"trans\": \"distribution\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"有效\", \"pinyin\": \"yǒuxiào\", \"trans\": \"effective\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"}\n]",
        "trans": "This article introduces a new framework called LUFFY for improving large-scale reasoning models. LUFFY combines off-policy demonstration and on-policy expansion, dynamically balancing imitation and exploration. It avoids superficial and rigid imitation through regularized importance sampling. LUFFY has achieved significant advancements in six mathematical benchmark tests and performs excellently on out-of-distribution tasks. Analysis shows that LUFFY not only effectively imitates but also explores beyond the demonstration.",
        "update_ts": "2025-04-22 09:12"
    }
}