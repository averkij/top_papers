{
    "date": {
        "ru": "23 апреля",
        "en": "April 23",
        "zh": "4月23日"
    },
    "time_utc": "2025-04-23 12:20",
    "weekday": 2,
    "issue_id": 3390,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.15120",
            "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
            "url": "https://huggingface.co/papers/2504.15120",
            "abstract": "Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.",
            "score": 71,
            "issue_id": 3383,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "947bbf16f35c503a",
            "authors": [
                "Khalil Hennara",
                "Sara Chrouf",
                "Mohamed Motaism Hamed",
                "Zeina Aldallal",
                "Omar Hadid",
                "Safwan AlModhayan"
            ],
            "affiliations": [
                "Khobar, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15120.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#open_source",
                    "#small_models",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное расширение языковых возможностей ИИ без полного переобучения",
                    "desc": "Статья представляет новый метод интеграции нового языка в большую языковую модель (LLM) без ущерба для ее предыдущих знаний. Авторы обучили небольшую модель Kuwain с 1,5 миллиардами параметров, добавив арабский язык в существующую модель, обученную в основном на английском. Метод показал значительное улучшение производительности на арабском языке, в среднем на 8% по различным бенчмаркам, сохранив при этом существующие знания модели. Это предлагает экономичную альтернативу обучению полной модели на обоих языках."
                },
                "en": {
                    "title": "Efficient Language Integration in AI Models",
                    "desc": "This paper presents a new method for adding a new language to a large language model (LLM) without losing its existing knowledge. The authors trained a smaller model called Kuwain, which has 1.5 billion parameters, by integrating the Arabic language into an English-focused model. The approach led to an 8% improvement in Arabic performance across various benchmarks while using minimal original data. This method provides a cost-effective way to expand language capabilities in models without the need for extensive retraining."
                },
                "zh": {
                    "title": "高效整合新语言，提升模型性能",
                    "desc": "本论文提出了一种新方法，用于将新语言整合到大型语言模型（LLM）中。我们的方法能够在不损害已有知识的情况下，将阿拉伯语注入到一个主要以英语训练的小型开源模型中。通过训练一个名为Kuwain的小模型（参数为15亿），我们在阿拉伯语性能上实现了平均8%的显著提升。该方法为在英语和阿拉伯语之间进行高效的语言模型扩展提供了一种经济实惠的替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16084",
            "title": "TTRL: Test-Time Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.16084",
            "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
            "score": 46,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "4445f28372c61abd",
            "authors": [
                "Yuxin Zuo",
                "Kaiyan Zhang",
                "Shang Qu",
                "Li Sheng",
                "Xuekai Zhu",
                "Biqing Qi",
                "Youbang Sun",
                "Ganqu Cui",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16084.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосовершенствование языковых моделей без меток",
                    "desc": "Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей (LLM) на данных без явных меток для задач рассуждения. Метод, названный Test-Time Reinforcement Learning (TTRL), позволяет LLM самосовершенствоваться, используя априорные знания предобученных моделей. Эксперименты показывают, что TTRL значительно улучшает производительность на различных задачах и моделях, в частности, повышая показатель pass@1 модели Qwen-2.5-Math-7B примерно на 159% на тесте AIME 2024. Результаты исследования подтверждают эффективность TTRL и подчеркивают его потенциал для широкого спектра задач и областей применения."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Unlabeled Data: Test-Time Reinforcement Learning",
                    "desc": "This paper explores a new approach called Test-Time Reinforcement Learning (TTRL) for training Large Language Models (LLMs) using unlabeled data. The main challenge addressed is how to estimate rewards during inference without having access to true labels. The authors find that techniques like majority voting can effectively generate rewards for reinforcement learning training. Their experiments show that TTRL significantly enhances the performance of LLMs, achieving impressive results even when only supervised by a simple metric."
                },
                "zh": {
                    "title": "无标签数据上的强化学习新方法",
                    "desc": "本文研究了在没有明确标签的数据上进行强化学习（RL），以解决大型语言模型（LLMs）的推理任务。主要挑战在于推理过程中如何进行奖励估计，而没有真实标签的信息。我们发现，测试时缩放（TTS）中的常见做法，如多数投票，能够产生意想不到的有效奖励，从而推动RL训练。我们提出了一种新方法——测试时强化学习（TTRL），它利用预训练模型中的先验知识，能够在无标签数据上自我进化，实验结果表明TTRL在多种任务和模型上均能持续提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15521",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "url": "https://huggingface.co/papers/2504.15521",
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "score": 42,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "4d1809408c9b3e49",
            "authors": [
                "Minghao Wu",
                "Weixuan Wang",
                "Sinuo Liu",
                "Huifeng Yin",
                "Xintong Wang",
                "Yu Zhao",
                "Chenyang Lyu",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce",
                "Monash University",
                "The University of Edinburgh",
                "Tsinghua University",
                "Universität Hamburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15521.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#low_resource",
                    "#multilingual",
                    "#machine_translation"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "За справедливую оценку многоязычных ИИ-моделей",
                    "desc": "Статья анализирует более 2000 многоязычных бенчмарков для оценки языковых моделей, выявляя перекос в сторону английского языка. Исследование показывает, что бенчмарки на основе оригинального контента лучше коррелируют с оценками людей, чем переведенные. Авторы выделяют шесть ключевых ограничений в текущих практиках многоязычной оценки и предлагают принципы для эффективного многоязычного бенчмаркинга. Статья призывает к глобальному сотрудничеству для разработки бенчмарков, ориентированных на реальные приложения и учитывающих культурные особенности."
                },
                "en": {
                    "title": "Towards Equitable Multilingual Benchmarking for LLMs",
                    "desc": "This paper discusses the need for better multilingual evaluation in large language models (LLMs) as they improve in language skills. It analyzes over 2,000 multilingual benchmarks from various countries and finds that English is still overly dominant in these evaluations. The study shows that benchmarks based on original language content are more effective than translated ones, especially for local contexts. The authors propose new guidelines and research directions to create benchmarks that align more closely with human judgments and real-world applications."
                },
                "zh": {
                    "title": "推动多语言评估的公平进步",
                    "desc": "随着大型语言模型（LLMs）在语言能力上的不断进步，进行稳健的多语言评估变得至关重要。本文分析了2021年至2024年间来自148个国家的2000多个多语言基准，评估了多语言基准测试的过去、现在和未来的实践。研究发现，尽管投入了数千万美元，英语在这些基准中仍然占据了过高的比例，且大多数基准依赖于原始语言内容而非翻译。我们强调了创建文化和语言定制基准的重要性，并提出了有效的多语言基准测试指导原则。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16072",
            "title": "Describe Anything: Detailed Localized Image and Video Captioning",
            "url": "https://huggingface.co/papers/2504.16072",
            "abstract": "Generating detailed and accurate descriptions for specific regions in images and videos remains a fundamental challenge for vision-language models. We introduce the Describe Anything Model (DAM), a model designed for detailed localized captioning (DLC). DAM preserves both local details and global context through two key innovations: a focal prompt, which ensures high-resolution encoding of targeted regions, and a localized vision backbone, which integrates precise localization with its broader context. To tackle the scarcity of high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark designed to evaluate DLC without relying on reference captions. DAM sets new state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and detailed multi-sentence localized image and video captioning.",
            "score": 32,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "1749ec1e79ed8810",
            "authors": [
                "Long Lian",
                "Yifan Ding",
                "Yunhao Ge",
                "Sifei Liu",
                "Hanzi Mao",
                "Boyi Li",
                "Marco Pavone",
                "Ming-Yu Liu",
                "Trevor Darrell",
                "Adam Yala",
                "Yin Cui"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16072.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "DAM: Точное описание любой детали изображения",
                    "desc": "Модель DAM (Describe Anything Model) представляет собой инновационный подход к детальному локализованному описанию изображений и видео. Она использует фокальный промпт и локализованный визуальный бэкбон для сохранения как локальных деталей, так и глобального контекста. Авторы также предлагают полу-контролируемый конвейер данных DLC-SDP для решения проблемы нехватки качественных данных. DAM устанавливает новый state-of-the-art на 7 бенчмарках, охватывающих различные уровни локализованного описания изображений и видео."
                },
                "en": {
                    "title": "Capturing Details Anywhere: The Describe Anything Model",
                    "desc": "The Describe Anything Model (DAM) addresses the challenge of generating detailed captions for specific areas in images and videos. It utilizes a focal prompt for high-resolution encoding of targeted regions and a localized vision backbone to combine local details with global context. To enhance the training data for detailed localized captioning, the model employs a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP) that leverages existing segmentation datasets and expands to unlabeled web images. DAM achieves state-of-the-art performance across multiple benchmarks for keyword, phrase, and multi-sentence captioning tasks."
                },
                "zh": {
                    "title": "描述任何事物，精准本地化！",
                    "desc": "本文介绍了一种新的模型，称为描述任何事物模型（DAM），旨在为图像和视频中的特定区域生成详细的本地化描述。DAM通过两个关键创新来保持局部细节和全局上下文：焦点提示确保对目标区域的高分辨率编码，而本地化视觉骨干网络则将精确定位与更广泛的上下文相结合。为了应对高质量本地化描述数据的稀缺，本文提出了一种基于半监督学习的数据管道（DLC-SDP），该管道利用现有的分割数据集并扩展到未标记的网络图像。DAM在七个基准测试中设定了新的最先进水平，涵盖了关键词级、短语级和详细的多句本地化图像和视频描述。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15466",
            "title": "Learning Adaptive Parallel Reasoning with Language Models",
            "url": "https://huggingface.co/papers/2504.15466",
            "abstract": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.",
            "score": 27,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "4eaf94a6429e3102",
            "authors": [
                "Jiayi Pan",
                "Xiuyu Li",
                "Long Lian",
                "Charlie Snell",
                "Yifei Zhou",
                "Adam Yala",
                "Trevor Darrell",
                "Kurt Keutzer",
                "Alane Suhr"
            ],
            "affiliations": [
                "UC Berkeley",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15466.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное параллельное рассуждение: новый шаг к автономной оптимизации вычислений в языковых моделях",
                    "desc": "Эта статья представляет новый метод рассуждений для языковых моделей - Адаптивное Параллельное Рассуждение (APR). APR позволяет моделям оркестровать как последовательные, так и параллельные вычисления, используя операции spawn() и join(). Метод оптимизируется с помощью обучения с подкреплением, улучшая показатели успешности задач без предопределенных структур рассуждений. Эксперименты на задаче Countdown показали значительные преимущества APR в производительности, масштабируемости и точности по сравнению с существующими методами."
                },
                "en": {
                    "title": "Adaptive Parallel Reasoning: Optimizing Language Model Inference",
                    "desc": "This paper introduces Adaptive Parallel Reasoning (APR), a new framework designed to enhance the reasoning capabilities of language models during inference. APR combines both serialized and parallel computation methods, allowing for more efficient processing without the drawbacks of existing approaches. The framework utilizes an end-to-end reinforcement learning strategy to optimize inference threads, improving task success rates without needing fixed reasoning structures. Experimental results show that APR significantly outperforms traditional methods in terms of performance, scalability, and accuracy while maintaining similar latency levels."
                },
                "zh": {
                    "title": "自适应并行推理：提升语言模型推理能力的创新框架",
                    "desc": "本文提出了一种新的推理框架，称为自适应并行推理（APR），旨在解决现有语言模型推理方法的局限性。APR结合了串行和并行计算，允许模型在推理过程中灵活调整计算方式。通过使用spawn()和join()操作，APR实现了自适应的多线程推理，优化了推理线程的成功率。实验结果表明，APR在相同上下文窗口内表现更好，并且在计算量增加时具有更好的可扩展性和准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15415",
            "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.15415",
            "abstract": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.",
            "score": 15,
            "issue_id": 3381,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "40ca992771747dac",
            "authors": [
                "David Ma",
                "Yuanxing Zhang",
                "Jincheng Ren",
                "Jarvis Guo",
                "Yifan Yao",
                "Zhenlin Wei",
                "Zhenzhu Yang",
                "Zhongyuan Peng",
                "Boyu Feng",
                "Jun Ma",
                "Xiao Gu",
                "Zhoufutu Wen",
                "King Zhu",
                "Yancheng He",
                "Meng Cao",
                "Shiwen Ni",
                "Jiaheng Liu",
                "Wenhao Huang",
                "Ge Zhang",
                "Xiaojie Jin"
            ],
            "affiliations": [
                "ByteDance Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15415.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "IV-Bench: новый рубеж в оценке мультимодальных ИИ для видео",
                    "desc": "IV-Bench - это новый комплексный бенчмарк для оценки восприятия и рассуждения на основе изображений в видео для мультимодальных больших языковых моделей (MLLM). Он включает 967 видео с 2,585 аннотированными запросами изображение-текст по 13 задачам и 5 категориям. Тестирование современных MLLM показало, что их производительность в этих задачах не превышает 28.9% точности. Анализ выявил ключевые факторы, влияющие на производительность моделей, включая паттерны вывода, количество кадров и разрешение."
                },
                "en": {
                    "title": "IV-Bench: Bridging Image Context and Video Understanding",
                    "desc": "This paper introduces IV-Bench, a new evaluation framework designed to assess how well Multimodal Large Language Models (MLLMs) understand videos using image context. It includes a dataset of 967 videos and 2,585 annotated image-text queries across various tasks, highlighting the importance of image-grounded reasoning in video comprehension. The study shows that current MLLMs struggle with this task, achieving a maximum accuracy of only 28.9%. The authors also identify factors affecting model performance and suggest that challenges in video understanding go beyond just data format alignment during training."
                },
                "zh": {
                    "title": "IV-Bench：图像基础视频理解的新基准",
                    "desc": "现有的多模态大型语言模型（MLLMs）评估框架主要关注图像推理或一般视频理解任务，忽视了图像上下文在视频理解中的重要作用。为了解决这个问题，我们提出了IV-Bench，这是第一个全面评估图像基础视频感知和推理的基准。IV-Bench包含967个视频和2585个精心注释的图像-文本查询，涵盖13个任务（7个感知任务和6个推理任务）及5个代表性类别。评估结果显示，当前的模型在图像基础视频感知和推理方面表现不佳，最高准确率仅为28.9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14538",
            "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
            "url": "https://huggingface.co/papers/2504.14538",
            "abstract": "Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.",
            "score": 14,
            "issue_id": 3382,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 апреля",
                "en": "April 20",
                "zh": "4月20日"
            },
            "hash": "d5ba284df85bf086",
            "authors": [
                "Yiting Ran",
                "Xintao Wang",
                "Tian Qiu",
                "Jiaqing Liang",
                "Yanghua Xiao",
                "Deqing Yang"
            ],
            "affiliations": [
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14538.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#story_generation",
                    "#open_source"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "BookWorld: погружение в миры книг с помощью ИИ",
                    "desc": "В статье представлена система BookWorld для создания и симуляции многоагентных обществ на основе книг. Система учитывает различные аспекты, включая разнообразие персонажей, вымышленные мировоззрения и географические ограничения. BookWorld позволяет генерировать истории, создавать интерактивные игры и проводить социальные симуляции, предлагая новые способы исследования художественных произведений. Эксперименты показали, что BookWorld превосходит предыдущие методы, генерируя качественные и креативные истории с сохранением верности оригинальным книгам."
                },
                "en": {
                    "title": "Bringing Fiction to Life: Simulating Book-Based Worlds with BookWorld",
                    "desc": "This paper presents BookWorld, a system designed to simulate multi-agent societies based on established fictional works. Unlike previous approaches that create agents from scratch, BookWorld incorporates existing characters and settings, allowing for more authentic interactions. It captures the complexities of fictional worlds, including character dynamics and geographical elements, enabling applications like story generation and interactive gaming. The results show that BookWorld not only produces high-quality narratives but also stays true to the original source material, outperforming earlier methods in creative storytelling."
                },
                "zh": {
                    "title": "BookWorld：虚构世界的智能体模拟新纪元",
                    "desc": "本文介绍了一种名为BookWorld的系统，用于构建和模拟基于书籍的多智能体社会。与以往从零开始创建智能体社会不同，BookWorld能够在已有的虚构世界和角色中进行模拟，具有重要的实际价值。该系统考虑了现实世界的复杂性，包括多样化和动态的角色、虚构的世界观以及地理限制等。通过大量实验，BookWorld生成的故事不仅富有创意且高质量，且忠实于原著，超越了之前的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14992",
            "title": "Efficient Pretraining Length Scaling",
            "url": "https://huggingface.co/papers/2504.14992",
            "abstract": "Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (PHD-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. PHD-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: PHD-SWA employs sliding window attention to preserve local dependencies, while PHD-CSWA implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.",
            "score": 12,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "a0f15db68dba9035",
            "authors": [
                "Bohong Wu",
                "Shen Yan",
                "Sijun Zhang",
                "Jianqiao Lu",
                "Yutao Zeng",
                "Ya Wang",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14992.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное масштабирование длины в трансформерах с параллельным скрытым декодированием",
                    "desc": "Эта статья представляет новый подход к масштабированию длины последовательности в языковых моделях во время предварительного обучения. Авторы предлагают Parallel Hidden Decoding Transformer (PHD-Transformer), который эффективно управляет кэшем ключей и значений, различая исходные токены и скрытые токены декодирования. Представлены две оптимизированные версии: PHD-SWA с использованием скользящего окна внимания и PHD-CSWA с чанковым скользящим окном внимания. Эксперименты показывают последовательное улучшение результатов на различных бенчмарках."
                },
                "en": {
                    "title": "Efficient Length Scaling in Pre-Training with PHD-Transformer",
                    "desc": "This paper introduces the Parallel Hidden Decoding Transformer (PHD-Transformer), which enhances pre-training of large language models by implementing efficient length scaling. The framework utilizes a unique key-value (KV) cache management strategy that differentiates between original tokens and hidden decoding tokens, optimizing memory usage. By retaining only the KV cache of original tokens, it effectively manages long-range dependencies without increasing cache size. Additionally, two optimized variants, PHD-SWA and PHD-CSWA, are proposed to improve local dependencies and reduce pre-filling time, leading to better performance across various benchmarks."
                },
                "zh": {
                    "title": "提升预训练效率的创新变换器",
                    "desc": "本文介绍了一种新的框架，称为并行隐藏解码变换器（PHD-Transformer），旨在提高预训练阶段的长度缩放效率。该方法通过创新的KV缓存管理策略，区分原始标记和隐藏解码标记，从而在保持推理效率的同时实现有效的长度缩放。PHD-Transformer 仅保留原始标记的KV缓存，以处理长距离依赖关系，并在使用后立即丢弃隐藏解码标记。实验结果表明，该方法在多个基准测试中均表现出一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13820",
            "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
            "url": "https://huggingface.co/papers/2504.13820",
            "abstract": "Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld.",
            "score": 12,
            "issue_id": 3381,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 апреля",
                "en": "April 18",
                "zh": "4月18日"
            },
            "hash": "c69df5180f187adb",
            "authors": [
                "Yang Yue",
                "Yulin Wang",
                "Chenxin Tao",
                "Pan Liu",
                "Shiji Song",
                "Gao Huang"
            ],
            "affiliations": [
                "PLA General Hospital",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13820.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#healthcare",
                    "#agi",
                    "#science",
                    "#training"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "CheXWorld: самообучающаяся мировая модель для рентгенографии",
                    "desc": "Статья представляет CheXWorld - первую попытку создания самообучающейся мировой модели для рентгеновских снимков. Модель одновременно учитывает три аспекта медицинских знаний: локальные анатомические структуры, глобальные анатомические компоновки и вариации в визуальном представлении снимков. Эмпирические исследования показывают, что CheXWorld успешно захватывает эти три измерения медицинских знаний. Эксперименты по трансферному обучению демонстрируют, что CheXWorld значительно превосходит существующие методы самообучения и крупномасштабные медицинские базовые модели."
                },
                "en": {
                    "title": "CheXWorld: A Self-Supervised Model for Radiographic Understanding",
                    "desc": "This paper introduces CheXWorld, a self-supervised world model designed for radiographic images, which aims to replicate human-like common sense knowledge in medical imaging. It develops a framework that captures three critical aspects of medical knowledge: local anatomical structures, global anatomical layouts, and domain variations in radiographs. The model is evaluated through qualitative and quantitative analyses, demonstrating its ability to effectively represent these dimensions of medical knowledge. Additionally, CheXWorld shows superior performance in transfer learning tasks compared to existing self-supervised learning methods and large-scale medical models."
                },
                "zh": {
                    "title": "CheXWorld：放射影像的自监督世界模型",
                    "desc": "本论文介绍了CheXWorld，这是第一个针对放射影像的自监督世界模型。该模型同时建模了医学知识的三个重要方面：局部解剖结构、全局解剖布局和领域变化。通过定性和定量分析，CheXWorld成功捕捉了这些医学知识的维度，并在八个医学图像分类和分割基准测试中表现优异。实验结果表明，CheXWorld在性能上显著超越了现有的自监督学习方法和大型医学基础模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13162",
            "title": "Personalized Text-to-Image Generation with Auto-Regressive Models",
            "url": "https://huggingface.co/papers/2504.13162",
            "abstract": "Personalized image synthesis has emerged as a pivotal application in text-to-image generation, enabling the creation of images featuring specific subjects in diverse contexts. While diffusion models have dominated this domain, auto-regressive models, with their unified architecture for text and image modeling, remain underexplored for personalized image generation. This paper investigates the potential of optimizing auto-regressive models for personalized image synthesis, leveraging their inherent multimodal capabilities to perform this task. We propose a two-stage training strategy that combines optimization of text embeddings and fine-tuning of transformer layers. Our experiments on the auto-regressive model demonstrate that this method achieves comparable subject fidelity and prompt following to the leading diffusion-based personalization methods. The results highlight the effectiveness of auto-regressive models in personalized image generation, offering a new direction for future research in this area.",
            "score": 11,
            "issue_id": 3390,
            "pub_date": "2025-04-17",
            "pub_date_card": {
                "ru": "17 апреля",
                "en": "April 17",
                "zh": "4月17日"
            },
            "hash": "00533fa52144dd4e",
            "authors": [
                "Kaiyue Sun",
                "Xian Liu",
                "Yao Teng",
                "Xihui Liu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13162.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Авторегрессионные модели: новый взгляд на персонализированный синтез изображений",
                    "desc": "Статья исследует потенциал авторегрессионных моделей для персонализированного синтеза изображений. Авторы предлагают двухэтапную стратегию обучения, сочетающую оптимизацию текстовых эмбеддингов и тонкую настройку слоев трансформера. Эксперименты показывают, что этот метод достигает сопоставимой точности воспроизведения субъекта и следования промпту по сравнению с ведущими методами персонализации на основе диффузионных моделей. Результаты подчеркивают эффективность авторегрессионных моделей в персонализированной генерации изображений, открывая новое направление для будущих исследований в этой области."
                },
                "en": {
                    "title": "Unlocking Personalized Image Synthesis with Auto-Regressive Models",
                    "desc": "This paper explores the use of auto-regressive models for personalized image synthesis, an area that has primarily focused on diffusion models. The authors propose a two-stage training strategy that optimizes text embeddings and fine-tunes transformer layers to enhance the model's performance. Their experiments show that the optimized auto-regressive model can achieve similar levels of subject fidelity and adherence to prompts as leading diffusion-based methods. This research suggests that auto-regressive models hold significant potential for advancing personalized image generation."
                },
                "zh": {
                    "title": "自回归模型在个性化图像合成中的新方向",
                    "desc": "个性化图像合成在文本到图像生成中变得越来越重要，可以创建包含特定主题的多样化图像。尽管扩散模型在这一领域占据主导地位，但自回归模型在个性化图像生成中的潜力尚未得到充分探索。本文研究了优化自回归模型以进行个性化图像合成的可能性，利用其固有的多模态能力来执行此任务。我们提出了一种两阶段的训练策略，结合了文本嵌入的优化和变换器层的微调，实验结果表明该方法在主题保真度和提示遵循方面与领先的扩散模型相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16030",
            "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
            "url": "https://huggingface.co/papers/2504.16030",
            "abstract": "Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.",
            "score": 9,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "6b5b45c3894204ba",
            "authors": [
                "Joya Chen",
                "Ziyun Zeng",
                "Yiqi Lin",
                "Wei Li",
                "Zejun Ma",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16030.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#data",
                    "#games",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Революция в обучении видео-ИИ: от ASR к реальному комментированию",
                    "desc": "Исследователи предложили новый метод обучения видео-языковых моделей с использованием автоматического распознавания речи (ASR) вместо дорогостоящей ручной разметки. Модель LiveCC-7B обучается на потоковых данных, где текст ASR и видеокадры плотно чередуются по временным меткам. Даже без дополнительной настройки модель показывает конкурентоспособные результаты в задачах видео-вопросов и ответов, а также демонстрирует новую способность комментировать видео в реальном времени. На популярных бенчмарках VideoMME и OVOBench модель LiveCC-7B-Instruct достигает лучших результатов среди моделей сопоставимого размера."
                },
                "en": {
                    "title": "Streamlining Video LLM Training with ASR Transcripts",
                    "desc": "This paper presents a new method for training Video Large Language Models (Video LLMs) using automatic speech recognition (ASR) transcripts instead of expensive human annotations. The authors introduce a streaming training approach that aligns ASR words with video frames based on their timestamps, enhancing the model's ability to understand the relationship between audio and visual content. They also create a dataset from YouTube videos and closed captions, which supports the training process and leads to the development of the LiveCC-7B-Base model. This model shows strong performance in video question answering and real-time commentary, outperforming larger models in quality while maintaining efficiency."
                },
                "zh": {
                    "title": "利用ASR实现视频大语言模型的高效训练",
                    "desc": "本文探讨了一种新的视频大语言模型（Video LLM）训练方法，利用廉价的自动语音识别（ASR）转录数据进行大规模训练。我们提出了一种新颖的流式训练方法，将ASR单词和视频帧根据时间戳密集交错，从而实现时间对齐的细粒度视觉语言建模。通过处理YouTube视频及其字幕，我们构建了Live-CC-5M数据集用于预训练，并创建了高质量的Live-WhisperX-526K数据集用于监督微调。实验结果表明，我们的模型在视频问答和实时视频评论方面表现优异，超越了许多先进的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15681",
            "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
            "url": "https://huggingface.co/papers/2504.15681",
            "abstract": "Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths/formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios.",
            "score": 7,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "58868b4baf82fddf",
            "authors": [
                "Vidi Team",
                "Celong Liu",
                "Chia-Wen Kuo",
                "Dawei Du",
                "Fan Chen",
                "Guang Chen",
                "Jiamin Yuan",
                "Lingxi Zhang",
                "Lu Guo",
                "Lusha Li",
                "Longyin Wen",
                "Qingyu Chen",
                "Rachel Deng",
                "Sijie Zhu",
                "Stuart Siew",
                "Tong Jin",
                "Wei Lu",
                "Wen Zhong",
                "Xiaohui Shen",
                "Xin Gu",
                "Xing Mei",
                "Xueqiong Qu"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance Inc. San Jose/Seattle, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15681.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#games",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Vidi: Революция в понимании и редактировании видео с помощью ИИ",
                    "desc": "Vidi - это семейство крупных мультимодальных моделей (LMM) для широкого спектра задач по пониманию и редактированию видео. Модель способна обрабатывать часовые видео и выполнять темпоральный поиск, определяя временные диапазоны в видео, соответствующие текстовому запросу. Для комплексной оценки в реальных сценариях авторы представили бенчмарк VUE-TR с пятью ключевыми улучшениями, включая более длительные видео и поддержку аудиозапросов. Vidi значительно превосходит ведущие проприетарные модели, такие как GPT-4 и Gemini, в задаче темпорального поиска."
                },
                "en": {
                    "title": "Vidi: Revolutionizing Video Editing with Multimodal Understanding",
                    "desc": "This paper presents Vidi, a family of Large Multimodal Models (LMMs) designed to enhance video editing by understanding various input types like vision, audio, and text. Vidi excels in temporal retrieval, which involves pinpointing specific time segments in long videos that match a given text query, making it crucial for intelligent video editing. The authors introduce the VUE-TR benchmark to evaluate Vidi's performance, featuring longer video durations, audio query support, diverse query formats, high-quality annotations, and a refined evaluation metric. Vidi demonstrates superior performance compared to existing models like GPT-4o and Gemini, showcasing its effectiveness in handling complex video editing tasks."
                },
                "zh": {
                    "title": "Vidi：视频编辑的新纪元",
                    "desc": "本论文介绍了一种名为Vidi的大型多模态模型（LMM），旨在解决视频编辑中的信息处理问题。Vidi能够处理多种输入模态，包括视觉、音频和文本，并具备强大的时间理解能力，能够从长达数小时的视频中提取相关时间段。该模型在时间检索任务上表现优异，超越了现有的领先模型，如GPT-4o和Gemini。为了支持真实场景中的全面评估，论文还提出了VUE-TR基准，包含了多个关键进展，如更长的视频时长和音频支持等。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16080",
            "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
            "url": "https://huggingface.co/papers/2504.16080",
            "abstract": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks.",
            "score": 5,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "b7a7c66b6a20d5de",
            "authors": [
                "Le Zhuo",
                "Liangbing Zhao",
                "Sayak Paul",
                "Yue Liao",
                "Renrui Zhang",
                "Yi Xin",
                "Peng Gao",
                "Mohamed Elhoseiny",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Hugging Face",
                "KAUST",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16080.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Самоанализ диффузионных моделей для улучшения качества генерации изображений",
                    "desc": "ReflectionFlow - это новый подход к улучшению генерации изображений с помощью диффузионных моделей. Он вводит три оси масштабирования на этапе вывода: оптимизацию начального шума, точное семантическое управление и итеративную рефлексию для оценки и исправления предыдущих генераций. Для обучения рефлексии используется специально созданный датасет GenRef из 1 миллиона триплетов. Эксперименты показывают, что ReflectionFlow значительно превосходит наивные методы масштабирования шума при генерации сложных изображений."
                },
                "en": {
                    "title": "Enhancing Image Synthesis with ReflectionFlow",
                    "desc": "This paper introduces ReflectionFlow, a new framework designed to improve the performance of text-to-image diffusion models, especially in generating complex scenes and fine details. It leverages the concept of self-reflection, allowing models to iteratively refine their outputs through three scaling methods: noise-level, prompt-level, and reflection-level scaling. The authors created a dataset called GenRef, which contains 1 million triplets of reflections, flawed images, and enhanced images to support the reflection-level scaling process. Experimental results demonstrate that ReflectionFlow outperforms traditional methods, providing a more efficient approach to high-quality image synthesis."
                },
                "zh": {
                    "title": "反思流：提升图像合成质量的新方法",
                    "desc": "本文提出了一种名为ReflectionFlow的推理框架，旨在提高文本到图像扩散模型在复杂场景和细节处理上的表现。该框架通过引入三种推理时间的扩展方式，包括噪声级别扩展、提示级别扩展和反思级别扩展，来优化生成过程。特别是反思级别扩展，通过提供可操作的反思，帮助模型迭代评估和修正之前的生成结果。实验结果表明，ReflectionFlow在图像合成质量上显著优于传统的噪声级别扩展方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16078",
            "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
            "url": "https://huggingface.co/papers/2504.16078",
            "abstract": "The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as epsilon-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.",
            "score": 5,
            "issue_id": 3384,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "ed2a10c0c55005d7",
            "authors": [
                "Thomas Schmied",
                "Jörg Bornschein",
                "Jordi Grau-Moya",
                "Markus Wulfmeier",
                "Razvan Pascanu"
            ],
            "affiliations": [
                "ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria",
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16078.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#agents",
                    "#training",
                    "#games",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение принятия решений языковыми моделями с помощью обучения с подкреплением",
                    "desc": "В статье исследуются проблемы применения больших языковых моделей (LLM) в задачах принятия решений. Авторы выявляют три основных режима отказа: жадность, частотное смещение и разрыв между знанием и действием. Для улучшения принятия решений LLM предлагается использовать обучение с подкреплением на самостоятельно сгенерированных рассуждениях по методу цепочки мыслей. Эксперименты показывают, что такой подход улучшает исследование пространства решений и сокращает разрыв между знанием и действием в различных сценариях."
                },
                "en": {
                    "title": "Enhancing Decision-Making in LLMs through Reinforcement Learning",
                    "desc": "This paper investigates the limitations of Large Language Models (LLMs) in decision-making tasks, particularly focusing on their sub-optimal exploration strategies and the knowing-doing gap. The authors identify three main failure modes: greediness, frequency bias, and the inability to act on knowledge. To address these issues, they propose using Reinforcement Learning (RL) to fine-tune LLMs based on self-generated Chain-of-Thought (CoT) rationales. Their experiments show that this approach improves LLMs' decision-making capabilities by enhancing exploration and bridging the knowing-doing gap."
                },
                "zh": {
                    "title": "提升大型语言模型决策能力的探索与微调",
                    "desc": "大型语言模型（LLMs）的成功引发了对各种智能应用的兴趣。本文系统研究了LLMs在决策场景中表现不佳的原因，特别关注贪婪性、频率偏差和知行差距等三种常见失败模式。我们提出通过强化学习（RL）对自生成的思维链（CoT）推理进行微调，以缓解这些缺陷。实验结果表明，RL微调能够提高LLMs的决策能力，增加探索性并缩小知行差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15785",
            "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
            "url": "https://huggingface.co/papers/2504.15785",
            "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.",
            "score": 5,
            "issue_id": 3381,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "1b074a62628dc1e3",
            "authors": [
                "Siyu Zhou",
                "Tianyi Zhou",
                "Yijun Yang",
                "Guodong Long",
                "Deheng Ye",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "affiliations": [
                "Australian AI Institute, Faculty of Engineering and IT, University of Technology Sydney, Australia",
                "Department of Computer Science, University of Maryland, College Park, USA",
                "Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15785.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#agi",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Нейросимволические модели мира повышают эффективность ИИ-агентов",
                    "desc": "Исследователи предлагают метод 'выравнивания мира' для улучшения работы больших языковых моделей (LLM) в качестве моделей мира. Этот подход извлекает символические знания об окружающей среде с помощью LLM и кодирует их в исполняемый код. Авторы представляют агента WALL-E 2.0, использующего нейросимволическую модель мира и LLM для эффективного планирования действий. В экспериментах в средах Mars и ALFWorld WALL-E 2.0 значительно превосходит существующие методы по показателям успешности и эффективности обучения."
                },
                "en": {
                    "title": "Enhancing LLMs with World Alignment for Superior Agent Performance",
                    "desc": "This paper explores how large language models (LLMs) can be enhanced to create accurate world models for agents. It introduces a method called 'world alignment' that allows LLMs to learn symbolic knowledge about their environment without requiring extensive training. The proposed agent, WALL-E 2.0, utilizes model-predictive control (MPC) to plan actions efficiently by leveraging the LLM's capabilities and the aligned world model's predictions. The results show that WALL-E 2.0 significantly outperforms existing methods in complex environments, demonstrating improved learning efficiency and higher success rates."
                },
                "zh": {
                    "title": "利用LLM构建高效世界模型的创新方法",
                    "desc": "本文探讨了如何利用大型语言模型（LLMs）构建准确的世界模型，并提出了一种名为“世界对齐”的方法，旨在弥补LLMs与特定环境动态之间的知识差距。通过提取探索轨迹中的符号知识，如行动规则和知识图谱，来增强LLMs的能力，并将其编码为可执行代码，以优化LLM代理的策略。我们还提出了一种基于模型的代理“WALL-E 2.0”，通过模型预测控制（MPC）框架实现高效的未来步骤动作规划。实验结果表明，WALL-E 2.0在开放世界挑战中显著超越了现有方法，展示了其在新环境中的学习效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16082",
            "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
            "url": "https://huggingface.co/papers/2504.16082",
            "abstract": "We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.   In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents.   Code is available at: https://github.com/ziqipang/MR-Video",
            "score": 3,
            "issue_id": 3380,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "52146a0dfc463f6d",
            "authors": [
                "Ziqi Pang",
                "Yu-Xiong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16082.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#video",
                    "#agents"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MapReduce для видео: новый уровень понимания длинного контента",
                    "desc": "MR. Video - это новый подход к пониманию длинных видео, основанный на принципе MapReduce. Он включает в себя два этапа: независимое восприятие коротких видеоклипов (Map) и совместную агрегацию информации из всех клипов (Reduce). Этот метод превосходит существующие vision-language модели и видео-агенты, обеспечивая более детальное восприятие и масштабируемую обработку. MR. Video достигает более чем 10% улучшения точности на сложном бенчмарке LVBench по сравнению с современными моделями."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with MapReduce",
                    "desc": "MR. Video is a framework designed for understanding long videos by applying the MapReduce principle. It processes short video clips independently in the 'Map' phase, allowing for detailed perception without the constraints of context length. In the 'Reduce' phase, it aggregates information from all clips for comprehensive reasoning, outperforming traditional methods that rely on key segment selection. This approach not only enhances accuracy but also demonstrates scalability in video analysis tasks, achieving significant improvements over existing models."
                },
                "zh": {
                    "title": "MR. Video：长视频理解的新方法",
                    "desc": "我们提出了MR. Video，这是一个用于长视频理解的框架，采用了简单而有效的MapReduce原则来处理长视频。首先，通过独立且密集地感知短视频片段（Map），然后将所有片段的信息进行聚合（Reduce）。与传统的序列到序列视觉语言模型相比，MR. Video能够在不受上下文长度限制的情况下，进行详细的短视频感知。通过使用大规模语言模型（LLM）代理验证其有效性，MR. Video在LVBench挑战中相比于最先进的视觉语言模型和视频代理实现了超过10%的准确率提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11703",
            "title": "Progent: Programmable Privilege Control for LLM Agents",
            "url": "https://huggingface.co/papers/2504.11703",
            "abstract": "LLM agents are an emerging form of AI systems where large language models (LLMs) serve as the central component, utilizing a diverse set of tools to complete user-assigned tasks. Despite their great potential, LLM agents pose significant security risks. When interacting with the external world, they may encounter malicious commands from attackers, leading to the execution of dangerous actions. A promising way to address this is by enforcing the principle of least privilege: allowing only essential actions for task completion while blocking unnecessary ones. However, achieving this is challenging, as it requires covering diverse agent scenarios while preserving both security and utility.   We introduce Progent, the first privilege control mechanism for LLM agents. At its core is a domain-specific language for flexibly expressing privilege control policies applied during agent execution. These policies provide fine-grained constraints over tool calls, deciding when tool calls are permissible and specifying fallbacks if they are not. This enables agent developers and users to craft suitable policies for their specific use cases and enforce them deterministically to guarantee security. Thanks to its modular design, integrating Progent does not alter agent internals and requires only minimal changes to agent implementation, enhancing its practicality and potential for widespread adoption. To automate policy writing, we leverage LLMs to generate policies based on user queries, which are then updated dynamically for improved security and utility. Our extensive evaluation shows that it enables strong security while preserving high utility across three distinct scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we perform an in-depth analysis, showcasing the effectiveness of its core components and the resilience of its automated policy generation against adaptive attacks.",
            "score": 3,
            "issue_id": 3380,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "01631fbca25ffa05",
            "authors": [
                "Tianneng Shi",
                "Jingxuan He",
                "Zhun Wang",
                "Linyu Wu",
                "Hongwei Li",
                "Wenbo Guo",
                "Dawn Song"
            ],
            "affiliations": [
                "UC Berkeley",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11703.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#agents"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Progent: Безопасность агентов LLM без компромиссов",
                    "desc": "Progent - это первый механизм контроля привилегий для агентов на основе больших языковых моделей (LLM). Он использует предметно-ориентированный язык для гибкого выражения политик контроля привилегий, применяемых во время выполнения агента. Progent позволяет разработчикам и пользователям агентов создавать подходящие политики для конкретных случаев использования и детерминированно применять их для обеспечения безопасности. Система демонстрирует высокую эффективность в обеспечении безопасности при сохранении высокой полезности в различных сценариях и бенчмарках."
                },
                "en": {
                    "title": "Empowering LLM Agents with Secure Privilege Control",
                    "desc": "This paper introduces Progent, a novel privilege control mechanism designed for large language model (LLM) agents, which are AI systems that utilize LLMs to perform tasks. Progent aims to enhance security by implementing the principle of least privilege, allowing only necessary actions while blocking harmful commands. It features a domain-specific language that enables developers to create fine-grained privilege policies, ensuring that tool calls are made safely and effectively. The system is designed to be easily integrated into existing LLM agents with minimal changes, and it automates policy generation using LLMs to adapt to user needs, demonstrating strong security and utility across various scenarios."
                },
                "zh": {
                    "title": "LLM代理的安全守护者：Progent权限控制机制",
                    "desc": "LLM代理是一种新兴的人工智能系统，利用大型语言模型（LLM）作为核心组件，结合多种工具完成用户任务。然而，这些代理在与外部世界互动时可能面临安全风险，可能会执行恶意命令。为了解决这个问题，我们提出了Progent，这是第一个针对LLM代理的权限控制机制，允许在执行过程中灵活地表达权限控制策略。通过这种方式，开发者可以为特定用例制定合适的策略，从而在保证安全的同时提高实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14977",
            "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
            "url": "https://huggingface.co/papers/2504.14977",
            "abstract": "Controllable character animation remains a challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose a new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking a step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield a surprisingly strong baseline. We further propose the low-noise warmup and \"large batches and small iterations\" strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce a new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by a large margin.",
            "score": 2,
            "issue_id": 3380,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "96e96789c52f6da1",
            "authors": [
                "Jingkai Zhou",
                "Yifan Wu",
                "Shikai Li",
                "Min Wei",
                "Chao Fan",
                "Weihua Chen",
                "Wei Jiang",
                "Fan Wang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Shenzhen University",
                "Southern University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14977.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video",
                    "#architecture"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Революция в анимации персонажей: от сложных сетей к простым модификациям",
                    "desc": "Статья представляет новый подход к управляемой анимации персонажей с использованием мощных фундаментальных моделей. Авторы предлагают RealisDance-DiT, основанную на видеомодели Wan-2.1, с минимальными модификациями архитектуры. Они вводят стратегии обучения, такие как разогрев с низким шумом и 'большие батчи и малые итерации', для ускорения сходимости модели. Эксперименты показывают, что RealisDance-DiT значительно превосходит существующие методы по различным сложным сценариям анимации."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with RealisDance-DiT",
                    "desc": "This paper addresses the challenges of controllable character animation, particularly in complex scenarios involving rare poses and dynamic environments. The authors introduce RealisDance-DiT, a model that leverages a powerful foundation model with minimal architectural modifications and flexible fine-tuning strategies. They argue that traditional methods, which rely on elaborate bypass networks, often fail to generalize well, while their approach shows significant improvements in performance. Additionally, they present a new test dataset to evaluate their method against existing benchmarks, demonstrating that RealisDance-DiT significantly outperforms prior techniques."
                },
                "zh": {
                    "title": "简单修改，强大动画！",
                    "desc": "可控角色动画仍然是一个具有挑战性的问题，尤其是在处理稀有姿势、风格化角色、角色与物体的交互、复杂照明和动态场景时。本文提出了一种新视角，认为只要基础模型足够强大，通过灵活的微调策略对模型进行简单修改，可以有效解决这些挑战。我们引入了RealisDance-DiT，基于Wan-2.1视频基础模型，并发现广泛采用的参考网络设计对于大规模DiT模型并不理想。通过最小的架构修改，我们展示了强大的基线性能，并提出了低噪声预热和“大批量小迭代”策略，以加速微调过程中的模型收敛，同时最大限度地保留基础模型的先验知识。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15524",
            "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
            "url": "https://huggingface.co/papers/2504.15524",
            "abstract": "Intellectual Property (IP) is a unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and a large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domain-specific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain.",
            "score": 1,
            "issue_id": 3385,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 апреля",
                "en": "April 22",
                "zh": "4月22日"
            },
            "hash": "512d4391a358be39",
            "authors": [
                "Qiyao Wang",
                "Guhong Chen",
                "Hongbo Wang",
                "Huaren Liu",
                "Minghui Zhu",
                "Zhifei Qin",
                "Linwei Li",
                "Yilin Yue",
                "Shiqiang Wang",
                "Jiayan Li",
                "Yihang Wu",
                "Ziqiang Liu",
                "Longze Chen",
                "Run Luo",
                "Liyang Fan",
                "Jiaming Li",
                "Lei Zhang",
                "Kan Xu",
                "Hongfei Lin",
                "Hamid Alinejad-Rokny",
                "Shiwen Ni",
                "Yuan Lin",
                "Min Yang"
            ],
            "affiliations": [
                "Dalian University of Technology, China",
                "Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China",
                "Shenzhen University of Advanced Technology, China",
                "The University of New South Wales, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15524.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#open_source"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "IPBench: Новый стандарт для оценки LLM в сфере интеллектуальной собственности",
                    "desc": "Статья представляет первую комплексную таксономию задач в области интеллектуальной собственности (ИС) и двуязычный бенчмарк IPBench. IPBench охватывает 8 механизмов ИС и 20 задач, оценивая способности больших языковых моделей (LLM) в реальных сценариях применения ИС. Тестирование 16 LLM показало, что даже лучшая модель достигает только 75.8% точности. Открытые модели, специализированные на ИС и праве, отстают от закрытых общего назначения."
                },
                "en": {
                    "title": "Bridging the Gap in Intellectual Property with LLMs",
                    "desc": "This paper discusses the challenges of applying large language models (LLMs) to the complex field of intellectual property (IP), which combines technical and legal knowledge. It highlights the limitations of existing datasets that either focus too narrowly on patents or do not represent the full spectrum of IP tasks. To address this, the authors introduce IPBench, a comprehensive benchmark that includes a diverse set of tasks across various IP mechanisms, aimed at evaluating LLM performance in real-world scenarios. The study reveals that even the best LLMs struggle with IP tasks, indicating significant opportunities for further research and development in this area."
                },
                "zh": {
                    "title": "推动知识产权领域的智能化评估",
                    "desc": "本论文探讨了知识产权（IP）领域的复杂性，并提出了一个新的评估基准，称为IPBench。该基准涵盖了8种知识产权机制和20个任务，旨在评估大型语言模型（LLMs）在实际知识产权应用中的表现。研究发现，即使是表现最好的模型，其准确率也仅为75.8%，显示出改进的空间。我们公开发布了IPBench的所有数据和代码，并计划持续更新，以更好地反映知识产权领域的实际挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15485",
            "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via\n  Occluded Object Counting",
            "url": "https://huggingface.co/papers/2504.15485",
            "abstract": "Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty counting in images.",
            "score": 1,
            "issue_id": 3390,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 апреля",
                "en": "April 21",
                "zh": "4月21日"
            },
            "hash": "fd5509657de18a23",
            "authors": [
                "Atin Pothiraj",
                "Elias Stengel-Eskin",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "affiliations": [
                "cs.unc.edu"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15485.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Тест на пространственное мышление: модели компьютерного зрения проваливают подсчет скрытых объектов",
                    "desc": "Статья представляет новую задачу CAPTURe для оценки способности моделей компьютерного зрения рассуждать о скрытых объектах. Задача требует подсчета объектов, расположенных по определенному паттерну, часть которого скрыта окклюдером. Авторы оценивают производительность нескольких современных мультимодальных моделей на этой задаче. Результаты показывают, что даже самые сильные модели, такие как GPT-4o, испытывают трудности с подсчетом объектов при наличии окклюзии, что указывает на недостатки в понимании пространственных отношений."
                },
                "en": {
                    "title": "CAPTURe: Challenging VLMs to Count Beyond Occlusions",
                    "desc": "This paper introduces a new task called CAPTURe, which tests how well vision-language models (VLMs) can recognize and reason about objects that are partially or fully hidden. The task involves counting objects arranged in a pattern while inferring how the pattern continues behind an occluder. The study evaluates several VLMs and finds that they struggle significantly with counting both occluded and unoccluded patterns, indicating a lack of spatial reasoning skills. In contrast, humans perform much better, and the paper suggests that providing additional information about occluded objects can help improve model performance."
                },
                "zh": {
                    "title": "揭示遮挡物体的计数与推理能力",
                    "desc": "本论文介绍了一项新任务，称为CAPTURe，旨在测试模型对被遮挡物体的识别和推理能力。CAPTURe要求模型通过推测遮挡物体后面的模式来计数物体，这对于理解视觉场景至关重要。研究发现，尽管一些强大的视觉语言模型（VLMs）在处理未遮挡的模式时表现良好，但在遮挡情况下的计数能力显著下降。通过提供遮挡物体位置的辅助信息，可以提高模型的表现，表明模型在处理遮挡和计数方面存在不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14735",
            "title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional\n  Effects Distributions",
            "url": "https://huggingface.co/papers/2504.14735",
            "abstract": "This study introduces a novel and interpretable model, DiffVox, for matching vocal effects in music production. DiffVox, short for ``Differentiable Vocal Fx\", integrates parametric equalisation, dynamic range control, delay, and reverb with efficient differentiable implementations to enable gradient-based optimisation for parameter estimation. Vocal presets are retrieved from two datasets, comprising 70 tracks from MedleyDB and 365 tracks from a private collection. Analysis of parameter correlations highlights strong relationships between effects and parameters, such as the high-pass and low-shelf filters often behaving together to shape the low end, and the delay time correlates with the intensity of the delayed signals. Principal component analysis reveals connections to McAdams' timbre dimensions, where the most crucial component modulates the perceived spaciousness while the secondary components influence spectral brightness. Statistical testing confirms the non-Gaussian nature of the parameter distribution, highlighting the complexity of the vocal effects space. These initial findings on the parameter distributions set the foundation for future research in vocal effects modelling and automatic mixing. Our source code and datasets are accessible at https://github.com/SonyResearch/diffvox.",
            "score": 0,
            "issue_id": 3388,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 апреля",
                "en": "April 20",
                "zh": "4月20日"
            },
            "hash": "dc6fe4499710bc87",
            "authors": [
                "Chin-Yun Yu",
                "Marco A. Martínez-Ramírez",
                "Junghyun Koo",
                "Ben Hayes",
                "Wei-Hsiang Liao",
                "György Fazekas",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Centre for Digital Music, Queen Mary University of London, London, UK",
                "Sony AI, Tokyo, Japan",
                "Sony Group Corporation, Tokyo, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14735.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#open_source",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "🎤",
                "ru": {
                    "title": "DiffVox: интерпретируемая модель для оптимизации вокальных эффектов",
                    "desc": "Исследование представляет новую интерпретируемую модель DiffVox для подбора вокальных эффектов в музыкальном производстве. DiffVox интегрирует параметрическую эквализацию, контроль динамического диапазона, задержку и реверберацию с эффективными дифференцируемыми реализациями для оптимизации на основе градиентов. Анализ корреляций параметров выявляет сильные связи между эффектами, такими как совместное поведение фильтров высоких частот и низкочастотной полки для формирования низких частот. Статистическое тестирование подтверждает негауссовскую природу распределения параметров, подчеркивая сложность пространства вокальных эффектов."
                },
                "en": {
                    "title": "Revolutionizing Vocal Effects with DiffVox",
                    "desc": "This paper presents DiffVox, a new model designed to match vocal effects in music production using differentiable programming. It combines various audio processing techniques like equalization, dynamic range control, delay, and reverb, allowing for efficient optimization of parameters through gradient descent. The study analyzes vocal presets from two datasets, revealing strong correlations between different audio effects and their parameters. Additionally, it employs principal component analysis to connect these parameters to perceived audio qualities, laying the groundwork for future advancements in vocal effects modeling and automatic mixing."
                },
                "zh": {
                    "title": "DiffVox：音乐制作中的人声效果匹配新模型",
                    "desc": "本研究提出了一种新颖且可解释的模型DiffVox，用于音乐制作中的人声效果匹配。DiffVox（可微分人声效果）结合了参数均衡、动态范围控制、延迟和混响，并采用高效的可微分实现，以便进行基于梯度的参数估计优化。通过分析参数之间的相关性，发现效果与参数之间存在强关系，例如高通和低架滤波器通常一起作用于低频部分，而延迟时间与延迟信号的强度相关。主成分分析揭示了与McAdams音色维度的联系，最重要的成分调节感知的空间感，而次要成分则影响频谱亮度。"
                }
            }
        }
    ],
    "link_prev": "2025-04-22.html",
    "link_next": "2025-04-24.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "22.04",
        "en": "04/22",
        "zh": "4月22日"
    },
    "short_date_next": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4月24日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 5,
        "#cv": 4,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 2,
        "#games": 6,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。这提供了一种成本效益高的替代方案，避免了大规模重新训练。",
        "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
        "pinyin": "这篇文章介绍了一种新方法，将新语言整合到大型语言模型中。\nZhè piān wénzhāng jièshào le yī zhǒng xīn fāngfǎ, jiāng xīn yǔyán zhěnghé dào dàxíng yǔyán móxíng zhōng.\n\n该方法成功地将之前未见的目标语言加入现有模型，不影响其原有知识。\nGǎi fāngfǎ chénggōng de jiāng zhīqián wèijiàn de mùbiāo yǔyán jiārù xiànyǒu móxíng, bù yǐngxiǎng qí yuányǒu zhīshi.\n\n研究团队用15亿参数训练了一个名为Kuwain的小模型，将阿拉伯语加入主要用英语训练的开源模型。\nYánjiū tuánduì yòng 15 yì cānshǔ xùnliàn le yīgè míngwèi Kuwain de xiǎo móxíng, jiāng Ālābóyǔ jiārù zhǔyào yòng Yīngyǔ xùnliàn de kāiyuán móxíng.\n\n结果显示，阿拉伯语性能提高了8%，同时保留了原有知识。\nJiégǔo xiǎnshì, Ālābóyǔ xìngnéng tígāo le 8%, tóngshí bǎoliú le yuányǒu zhīshi.\n\n这提供了一种成本效益高的替代方案，避免了大规模重新训练。\nZhè tígōng le yī zhǒng chéngběn xiàoyì gāo de tìdài fāng'àn, bìmiǎn le dàguīmó chóngxīn xùnliàn.",
        "vocab": "[\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integrate\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"未见\", \"pinyin\": \"wèijiàn\", \"trans\": \"unseen\"},\n    {\"word\": \"目标\", \"pinyin\": \"mùbiāo\", \"trans\": \"target\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐngxiǎng\", \"trans\": \"affect\"},\n    {\"word\": \"原有\", \"pinyin\": \"yuányǒu\", \"trans\": \"original\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhīshi\", \"trans\": \"knowledge\"},\n    {\"word\": \"研究\", \"pinyin\": \"yánjiū\", \"trans\": \"research\"},\n    {\"word\": \"团队\", \"pinyin\": \"tuánduì\", \"trans\": \"team\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"},\n    {\"word\": \"名为\", \"pinyin\": \"míngwéi\", \"trans\": \"named\"},\n    {\"word\": \"小模型\", \"pinyin\": \"xiǎo móxíng\", \"trans\": \"small model\"},\n    {\"word\": \"加入\", \"pinyin\": \"jiārù\", \"trans\": \"add\"},\n    {\"word\": \"主要\", \"pinyin\": \"zhǔyào\", \"trans\": \"main\"},\n    {\"word\": \"用英语\", \"pinyin\": \"yòng yīngyǔ\", \"trans\": \"using English\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎoliú\", \"trans\": \"retain\"},\n    {\"word\": \"提供\", \"pinyin\": \"tígōng\", \"trans\": \"provide\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéngběn\", \"trans\": \"cost\"},\n    {\"word\": \"效益\", \"pinyin\": \"xiàoyì\", \"trans\": \"benefit\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"替代\", \"pinyin\": \"tìdài\", \"trans\": \"alternative\"},\n    {\"word\": \"方案\", \"pinyin\": \"fāngàn\", \"trans\": \"solution\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dàguīmó\", \"trans\": \"large-scale\"},\n    {\"word\": \"重新\", \"pinyin\": \"chóngxīn\", \"trans\": \"re-\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"train\"}\n]",
        "trans": "This article introduces a new method for integrating new languages into large language models. The method successfully incorporates previously unseen target languages into existing models without affecting their original knowledge. The research team trained a small model named Kuwain with 1.5 billion parameters, adding Arabic to a primarily English-trained open-source model. The results showed an 8% improvement in Arabic performance while retaining the original knowledge. This provides a cost-effective alternative, avoiding the need for large-scale retraining.",
        "update_ts": "2025-04-23 09:13"
    }
}