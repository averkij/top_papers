{
    "date": {
        "ru": "31 октября",
        "en": "October 31",
        "zh": "10月31日"
    },
    "time_utc": "2024-10-31 02:48",
    "weekday": 3,
    "issue_id": 347,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.20050",
            "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
            "url": "https://huggingface.co/papers/2410.20050",
            "abstract": "Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB.",
            "score": 1,
            "issue_id": 347,
            "pub_date": "2024-10-26",
            "pub_date_card": {
                "ru": "26 октября",
                "en": "October 26",
                "zh": "10月26日"
            },
            "hash": "57721469df67a2f9",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#medicine"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Революция в медицинском поиске: SL-HyDE и CMIRB открывают новые горизонты",
                    "desc": "Статья представляет новый подход к медицинскому информационному поиску под названием SL-HyDE. Этот метод использует большие языковые модели для генерации гипотетических документов на основе запроса, что помогает плотностному ретриверу находить наиболее релевантные документы. Авторы также представляют CMIRB - комплексную систему оценки для медицинского информационного поиска. Экспериментальные результаты показывают, что SL-HyDE значительно превосходит существующие методы по точности поиска."
                },
                "en": {
                    "title": "Revolutionizing Medical Retrieval with Self-Learning Hypothetical Documents",
                    "desc": "This paper addresses the challenges of zero-shot dense retrieval in medical information retrieval (MIR) due to the scarcity of labeled data. It introduces a new method called Self-Learning Hypothetical Document Embeddings (SL-HyDE), which uses large language models to create hypothetical documents that provide essential medical context for retrieval tasks. The self-learning approach refines the generation of these documents and the retrieval process using unlabeled medical data. Additionally, the authors present the Chinese Medical Information Retrieval Benchmark (CMIRB) to evaluate the performance of various models in real-world medical scenarios, demonstrating that SL-HyDE outperforms existing methods in accuracy and adaptability."
                },
                "zh": {
                    "title": "自学习假设文档嵌入：提升医学信息检索的有效性",
                    "desc": "医学信息检索（MIR）在从多种来源获取相关医学知识中至关重要，但在医学领域实现有效的零样本密集检索面临重大挑战，因为缺乏相关性标记的数据。本文提出了一种新方法，称为自学习假设文档嵌入（SL-HyDE），旨在解决这一问题。SL-HyDE利用大型语言模型（LLMs）生成基于给定查询的假设文档，这些文档包含关键的医学背景，帮助密集检索器识别最相关的文档。我们还提出了中国医学信息检索基准（CMIRB），为医学信息检索系统提供了一个全面的评估框架。"
                }
            }
        }
    ],
    "link_prev": "2024-10-30.html",
    "link_next": "2024-11-01.html",
    "short_date_prev": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10月30日"
    },
    "short_date_next": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11月1日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0
    },
    "zh": {
        "text": "这篇文章讨论了机器取消学习（MU）在深度学习模型中增强隐私和安全的重要性，特别是在大型多模态语言模型（MLLMs）中。虽然MU在文本和视觉模态上取得了显著进展，但多模态取消学习（MMU）仍然没有得到充分研究，部分原因是缺乏合适的开源基准。为了解决这个问题，作者引入了CLEAR，一个新的基准，用于评估MMU方法。CLEAR包含200个虚构个体和3700张图片，并附有相应的问答对，可以进行跨模态的全面评估。作者评估了10种MU方法，并指出了多模态遗忘的新挑战。他们还展示了简单的ell_1正则化可以显著减轻灾难性遗忘，保持模型在保留数据上的性能。数据集可在https://huggingface.co/datasets/therem/CLEAR获取。",
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "pinyin": "这篇文章讨论了机器取消学习（MU）在深度学习模型中增强隐私和安全的重要性，特别是在大型多模态语言模型（MLLMs）中。虽然MU在文本和视觉模态上取得了显著进展，但多模态取消学习（MMU）仍然没有得到充分研究，部分原因是缺乏合适的开源基准。为了解决这个问题，作者引入了CLEAR，一个新的基准，用于评估MMU方法。CLEAR包含200个虚构个体和3700张图片，并附有相应的问答对，可以进行跨模态的全面评估。作者评估了10种MU方法，并指出了多模态遗忘的新挑战。他们还展示了简单的ell_1正则化可以显著减轻灾难性遗忘，保持模型在保留数据上的性能。数据集可在https://huggingface.co/datasets/therem/CLEAR获取。\n\nzhè piān wén zhāng tǎo lùn le jī qì qǔ xiāo xué (MU) zài shēn dù xué xí mó xìng zhōng zēng qiáng yǐn sī hé ān quán de zhòng yào xìng, tè bié shì zài dà xíng duō mó shuài yǔ yán mó xìng (MLLMs) zhōng. suī rán MU zài wén běn hé shì jué mó tài shàng qu dé le xiǎn zhù jìn zhǎn, dàn duō mó shuài qǔ xiāo xué (MMU) réng rán méi yǒu dé dào chóng fèn yán jiū, bù fèn yuán yīn shì quē fá hé shì de kāi yuán jī zhǔn. wèi le jiě jué zhè gè wèn tí, zuò zhě yǐn rù le CLEAR, yī gè xīn de jī zhǔn, yòng yú píng guā MMU fāng fǎ. CLEAR bāo hán 200 gè xū gòu gè tǐ hé 3700 zhāng tú piàn, bìng fù yǒu xiāng yìng de wèn dá duì, kě yǐ jìn xíng kuà mó shuài de quán miàn píng guā. zuò zhě píng guā le 10 zhǒng MU fāng fǎ, bìng zhǐ chū le duō mó shuài yí wàng de xīn tiǎo zhàn. tā men hái zhǎn shì le jiǎn dān de ell_1 zhèng guī huà kě yǐ xiǎn zhù jiǎn qīng zāi nàn xìng yí wàng, bǎo chí mó xìng zài bǎo liú shù jù shàng de xiào nèng. shù jù jí kě zài https://huggingface.co/datasets/therem/CLEAR huò qǔ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"机器取消学习\", \"pinyin\": \"jī qì qǔ xiāo xué xí\", \"trans\": \"machine unlearning\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"隐私\", \"pinyin\": \"yǐn sī\", \"trans\": \"privacy\"},\n    {\"word\": \"安全\", \"pinyin\": \"ān quán\", \"trans\": \"security\"},\n    {\"word\": \"深度学习模型\", \"pinyin\": \"shēn dù xué xí mó xíng\", \"trans\": \"deep learning model\"},\n    {\"word\": \"多模态语言模型\", \"pinyin\": \"duō mó shuài yǔ yán mó xíng\", \"trans\": \"multimodal language model\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìn zhǎn\", \"trans\": \"progress\"},\n    {\"word\": \"多模态取消学习\", \"pinyin\": \"duō mó shuài qǔ xiāo xué xí\", \"trans\": \"multimodal unlearning\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"合适\", \"pinyin\": \"hé shì\", \"trans\": \"suitable\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"虚构\", \"pinyin\": \"xū gòu\", \"trans\": \"fictional\"},\n    {\"word\": \"个体\", \"pinyin\": \"gè tǐ\", \"trans\": \"individual\"},\n    {\"word\": \"图片\", \"pinyin\": \"tú piàn\", \"trans\": \"image\"},\n    {\"word\": \"问答对\", \"pinyin\": \"wèn dá duì\", \"trans\": \"question-answer pair\"},\n    {\"word\": \"跨模态\", \"pinyin\": \"kuà mó shuài\", \"trans\": \"cross-modal\"},\n    {\"word\": \"全面\", \"pinyin\": \"quán miàn\", \"trans\": \"comprehensive\"},\n    {\"word\": \"遗忘\", \"pinyin\": \"yí wàng\", \"trans\": \"forgetting\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"正则化\", \"pinyin\": \"zhèng zé huà\", \"trans\": \"regularization\"},\n    {\"word\": \"减轻\", \"pinyin\": \"jiǎn qīng\", \"trans\": \"alleviate\"},\n    {\"word\": \"灾难性\", \"pinyin\": \"zāi nàn xìng\", \"trans\": \"catastrophic\"},\n    {\"word\": \"保持\", \"pinyin\": \"bǎo chí\", \"trans\": \"maintain\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article discusses the importance of machine unlearning (MU) in enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs). Although MU has made significant progress in text and visual modalities, multimodal unlearning (MMU) remains under-researched, partly due to the lack of suitable open-source benchmarks. To address this issue, the authors introduce CLEAR, a new benchmark for evaluating MMU methods. CLEAR contains 200 fictional individuals and 3,700 images, along with corresponding question-answer pairs, allowing for comprehensive cross-modal evaluation. The authors evaluated 10 MU methods and highlighted new challenges in multimodal forgetting. They also demonstrated that simple ell_1 regularization can significantly mitigate catastrophic forgetting, maintaining the model's performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR.",
        "update_ts": "2024-10-30 10:13"
    }
}