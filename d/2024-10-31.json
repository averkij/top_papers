{
    "date": {
        "ru": "31 октября",
        "en": "October 31",
        "zh": "10月31日"
    },
    "time_utc": "2024-10-31 20:12",
    "weekday": 3,
    "issue_id": 359,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.23090",
            "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
            "url": "https://huggingface.co/papers/2410.23090",
            "abstract": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.",
            "score": 51,
            "issue_id": 348,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "d3cb6da7b94ee077",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "CORAL: Новый стандарт для оценки многоходовых диалоговых систем с RAG",
                    "desc": "Статья представляет новый бенчмарк CORAL для оценки систем генерации с дополнительной информацией (RAG) в многоходовых диалогах. CORAL включает в себя разнообразные информационно-поисковые беседы, автоматически созданные на основе Википедии, и охватывает ключевые задачи, такие как открытый домен, интенсивное использование знаний и смена тем. Бенчмарк поддерживает три основные задачи: поиск релевантных отрывков текста, генерация ответов и маркировка цитат. Авторы также предлагают унифицированную структуру для стандартизации различных методов RAG в диалоговых системах."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Conversations with CORAL Benchmark",
                    "desc": "This paper introduces CORAL, a benchmark aimed at improving Retrieval-Augmented Generation (RAG) systems for multi-turn conversations, which are more complex than single-turn interactions. It highlights the need for RAG models to effectively handle diverse and dynamic information-seeking dialogues, addressing challenges like open-domain coverage and topic shifts. The benchmark includes tasks such as passage retrieval, response generation, and citation labeling, providing a structured way to evaluate RAG performance. By proposing a unified framework, the authors aim to enhance the effectiveness of conversational RAG methods and identify areas for future improvement."
                },
                "zh": {
                    "title": "提升多轮对话的检索增强生成能力",
                    "desc": "本论文介绍了一种新的基准CORAL，用于评估检索增强生成（RAG）系统在多轮对话中的表现。现有研究主要集中在单轮对话上，缺乏对复杂多轮对话的深入探讨。CORAL基于维基百科自动生成多样的信息寻求对话，解决开放域覆盖、知识密集度、自由形式响应和话题转移等关键挑战。我们提出了一个统一框架，以标准化不同的对话RAG方法，并在CORAL上进行全面评估，展示了改进现有方法的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22391",
            "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
            "url": "https://huggingface.co/papers/2410.22391",
            "abstract": "In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.",
            "score": 20,
            "issue_id": 350,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "876c89e8fc188dd3",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LRAM: Быстрее трансформеров, эффективнее в реальном времени",
                    "desc": "В статье представлена новая модель LRAM (Large Recurrent Action Model) для обучения с подкреплением, основанная на архитектуре xLSTM. LRAM предлагает линейную сложность вывода и способность к экстраполяции длины последовательности, что делает её более практичной для приложений реального времени по сравнению с моделями на основе трансформеров. Эксперименты на 432 задачах из 6 доменов показали, что LRAM не уступает трансформерам по производительности и скорости. Это исследование демонстрирует потенциал современных рекуррентных архитектур для моделей с большим пространством действий в обучении с подкреплением."
                },
                "en": {
                    "title": "Fast and Effective: LRAM for Real-Time Reinforcement Learning",
                    "desc": "This paper explores the use of modern recurrent architectures, specifically xLSTM, for creating large action models in Reinforcement Learning (RL). Traditional Transformer models are powerful but suffer from slow inference times, making them unsuitable for real-time applications like robotics. The proposed Large Recurrent Action Model (LRAM) leverages the benefits of xLSTM to achieve linear-time inference complexity while maintaining strong performance. Experimental results demonstrate that LRAM outperforms Transformer-based models in both speed and effectiveness across a variety of tasks."
                },
                "zh": {
                    "title": "快速推理的强化学习新选择",
                    "desc": "近年来，强化学习（RL）领域出现了一个趋势，即使用大型离线数据集通过序列建模训练大型动作模型。现有模型主要基于Transformer架构，虽然能够生成强大的智能体，但由于推理速度慢，难以应用于实时场景，如机器人技术。最近提出的现代递归架构，如xLSTM和Mamba，具有与Transformer相似的训练并行化优势，同时提供快速推理能力。本文研究了这些现代递归架构在大型动作模型中的适用性，并提出了一种以xLSTM为核心的大型递归动作模型（LRAM），其推理复杂度为线性时间，且具有自然的序列长度外推能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23287",
            "title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
            "url": "https://huggingface.co/papers/2410.23287",
            "abstract": "We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.",
            "score": 17,
            "issue_id": 356,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "cf2371629ffd5ab5",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "REM: универсальная сегментация видео с помощью естественного языка",
                    "desc": "REM - это фреймворк для сегментации различных концепций в видео, описываемых с помощью естественного языка. Он использует визуально-языковые представления, полученные видео-диффузионными моделями на масштабных интернет-датасетах. Ключевая особенность подхода - сохранение большей части исходного представления генеративной модели при дообучении на узкоспециализированных датасетах сегментации объектов по запросу. REM способен точно сегментировать и отслеживать редкие и невиданные ранее объекты, а также обобщаться на динамические концепты, не являющиеся объектами."
                },
                "en": {
                    "title": "Segmenting Video Concepts with Natural Language Power",
                    "desc": "The REM framework is designed to segment various concepts in videos using natural language descriptions. It utilizes visual-language representations from video diffusion models trained on large datasets from the internet. By fine-tuning these models on specific datasets for Referral Object Segmentation, REM can effectively identify and track both common and rare objects. Additionally, it demonstrates the ability to generalize to dynamic concepts, achieving high performance on both in-domain and out-of-domain tasks."
                },
                "zh": {
                    "title": "REM框架：视频概念分割的新突破",
                    "desc": "我们提出了REM框架，用于通过自然语言对视频中的各种概念进行分割。该方法利用了在互联网规模数据集上学习的视觉-语言表示，结合视频扩散模型。我们的方法的关键在于尽可能保留生成模型的原始表示，同时在狭域的引用对象分割数据集上进行微调。结果表明，REM框架能够准确分割和跟踪稀有和未见过的对象，并且能够推广到非对象动态概念，如海浪的冲击。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.20779",
            "title": "Decoding Reading Goals from Eye Movements",
            "url": "https://huggingface.co/papers/2410.20779",
            "abstract": "Readers can have different goals with respect to the text they are reading. Can these goals be decoded from the pattern of their eye movements over the text? In this work, we examine for the first time whether it is possible to decode two types of reading goals that are common in daily life: information seeking and ordinary reading. Using large scale eye-tracking data, we apply to this task a wide range of state-of-the-art models for eye movements and text that cover different architectural and data representation strategies, and further introduce a new model ensemble. We systematically evaluate these models at three levels of generalization: new textual item, new participant, and the combination of both. We find that eye movements contain highly valuable signals for this task. We further perform an error analysis which builds on prior empirical findings on differences between ordinary reading and information seeking and leverages rich textual annotations. This analysis reveals key properties of textual items and participant eye movements that contribute to the difficulty of the task.",
            "score": 15,
            "issue_id": 356,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "69c16b774d32c4c1",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Разгадка целей чтения по движениям глаз",
                    "desc": "Исследование посвящено декодированию целей чтения на основе движений глаз. Авторы применяют современные модели машинного обучения для анализа крупномасштабных данных айтрекинга, чтобы различать информационный поиск и обычное чтение. Оценка моделей проводится на трех уровнях обобщения: новый текст, новый участник и их комбинация. Анализ ошибок выявляет ключевые свойства текстов и движений глаз, влияющие на сложность задачи."
                },
                "en": {
                    "title": "Decoding Reading Goals Through Eye Movements",
                    "desc": "This paper explores whether the goals of readers, such as information seeking and ordinary reading, can be inferred from their eye movement patterns. Using extensive eye-tracking data, the authors implement various advanced machine learning models to analyze these movements and introduce a new model ensemble for improved accuracy. They evaluate the models' performance across different scenarios, including new texts and new participants, demonstrating that eye movements provide significant insights into reading intentions. Additionally, an error analysis highlights specific characteristics of texts and eye movement behaviors that affect the decoding process."
                },
                "zh": {
                    "title": "解码阅读目标：眼动与文本的深度分析",
                    "desc": "本研究首次探讨了是否可以通过眼动模式解码读者的阅读目标，包括信息寻求和普通阅读。我们使用大规模的眼动追踪数据，应用多种先进的模型来分析眼动和文本，提出了一种新的模型集成方法。通过对新文本、新参与者及其组合的系统评估，我们发现眼动包含了对解码阅读目标非常有价值的信号。进一步的错误分析揭示了文本特性和参与者眼动的关键属性，这些属性影响了任务的难度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23168",
            "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
            "url": "https://huggingface.co/papers/2410.23168",
            "abstract": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer.",
            "score": 14,
            "issue_id": 353,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "4785b6a73debe15e",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Гибкое масштабирование нейросетей без полного переобучения",
                    "desc": "TokenFormer - это новая архитектура модели, которая позволяет эффективно масштабировать нейронные сети без необходимости полного переобучения. В отличие от стандартных трансформеров, TokenFormer использует механизм внимания не только между входными токенами, но и между токенами и параметрами модели. Это достигается за счет замены линейных проекций на слои внимания токен-параметр, где входные токены выступают в роли запросов, а параметры модели - в роли ключей и значений. Такой подход позволяет постепенно наращивать размер модели от 124 млн до 1,4 млрд параметров, сохраняя производительность на уровне полностью переобученных трансформеров, но значительно снижая вычислительные затраты."
                },
                "en": {
                    "title": "TokenFormer: Scalable Transformers Without Retraining",
                    "desc": "This paper presents TokenFormer, a new architecture designed to address the high computational costs associated with scaling Transformer models. Traditional Transformers require retraining from scratch when architectural changes are made, which is inefficient as model sizes increase. TokenFormer innovatively uses the attention mechanism to allow model parameters to interact with input tokens, treating parameters as tokens themselves. This approach enables flexible scaling of the model without the need for complete retraining, significantly reducing training costs while maintaining competitive performance."
                },
                "zh": {
                    "title": "TokenFormer：高效可扩展的Transformer架构",
                    "desc": "本文介绍了一种新的模型架构TokenFormer，旨在解决现有Transformer模型在扩展时的高计算成本问题。TokenFormer通过将模型参数视为令牌，利用注意力机制实现输入令牌与模型参数之间的交互，从而提高了架构的灵活性。与传统方法不同，TokenFormer允许逐步扩展模型，而无需从头开始重新训练。该模型在参数数量从1.24亿扩展到14亿的过程中，能够在保持性能的同时显著降低训练成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23123",
            "title": "On Memorization of Large Language Models in Logical Reasoning",
            "url": "https://huggingface.co/papers/2410.23123",
            "abstract": "Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Our code and data are available at https://memkklogic.github.io.",
            "score": 13,
            "issue_id": 358,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "f3e776b0854b1ec8",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Запоминание vs Рассуждение: Сложное взаимодействие в больших языковых моделях",
                    "desc": "В статье исследуется взаимосвязь между способностью больших языковых моделей (LLM) к запоминанию и их навыками логического мышления. Авторы используют динамически генерируемый набор логических задач на основе головоломок 'Рыцари и лжецы' для измерения степени запоминания. Результаты показывают, что LLM могут достигать почти идеальной точности на тренировочных примерах, но терпят неудачу при небольших изменениях в задачах. Тем не менее, исследование также демонстрирует, что дообучение моделей, несмотря на сильное запоминание, улучшает их способность к обобщению."
                },
                "en": {
                    "title": "Memorization vs. Reasoning: Unraveling LLMs' Logic Skills",
                    "desc": "This paper explores the reasoning capabilities of large language models (LLMs) and their tendency to memorize training data. The authors propose that LLMs achieve high performance on reasoning tasks by memorizing similar problems rather than genuinely understanding them. Through experiments with Knights and Knaves puzzles, they demonstrate that while LLMs can interpolate training data effectively, they struggle with slight variations, indicating reliance on memorization. However, the study also reveals that fine-tuning improves generalization, suggesting a complex relationship between memorization and reasoning in LLMs."
                },
                "zh": {
                    "title": "记忆与推理的复杂交互",
                    "desc": "大型语言模型（LLMs）在复杂推理基准测试中表现良好，但也可能出现基本推理错误。本文系统地研究了LLMs推理能力背后的机制，提出了记忆化假设，认为模型在推理任务中可能依赖于对相似问题的记忆。通过动态生成的逻辑推理基准，我们发现LLMs在微调后能够完美解决训练谜题，但在稍微改变这些谜题时却表现不佳，表明它们在解决训练谜题时严重依赖记忆。尽管微调导致了重度记忆化，但也提高了模型的泛化性能，显示出记忆与真实推理能力之间的复杂关系。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22884",
            "title": "Stealing User Prompts from Mixture of Experts",
            "url": "https://huggingface.co/papers/2410.22884",
            "abstract": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit Expert-Choice-Routing to fully disclose a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layer Mixtral model, exploiting the tie-handling behavior of the torch.topk CUDA implementation. Our results show that we can extract the entire prompt using O({VM}^2) queries (with vocabulary size V and prompt length M) or 100 queries on average per token in the setting we consider. This is the first attack to exploit architectural flaws for the purpose of extracting user prompts, introducing a new class of LLM vulnerabilities.",
            "score": 12,
            "issue_id": 351,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "50ec28e1ed4db1bb",
            "data": {
                "categories": [
                    "#architecture",
                    "#security"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Уязвимость в MoE моделях: как архитектурные особенности могут раскрыть ваш промпт",
                    "desc": "Статья описывает уязвимость в моделях Mixture-of-Experts (MoE), использующих маршрутизацию Expert-Choice-Routing. Авторы демонстрируют, как злоумышленник может эксплуатировать эту уязвимость для раскрытия промпта жертвы, если запросы обрабатываются в одном батче. Эксперимент проводился на двухслойной модели Mixtral, используя особенности реализации torch.topk CUDA. Это первая атака, эксплуатирующая архитектурные недостатки для извлечения пользовательских промптов, что открывает новый класс уязвимостей в больших языковых моделях."
                },
                "en": {
                    "title": "Exposing Prompts: A New Vulnerability in Mixture-of-Experts Models",
                    "desc": "This paper discusses a vulnerability in Mixture-of-Experts (MoE) models, which are designed to enhance the efficiency of language models by directing tokens to specific experts. The authors demonstrate that an adversary can exploit the Expert-Choice-Routing mechanism to reveal a victim's input prompt by cleverly arranging queries in the same batch. They successfully execute this attack on a two-layer Mixtral model, taking advantage of the tie-handling behavior in the torch.topk CUDA implementation. The findings indicate that the entire prompt can be extracted with a relatively small number of queries, highlighting a new class of vulnerabilities in large language models (LLMs)."
                },
                "zh": {
                    "title": "利用架构缺陷提取用户提示的攻击",
                    "desc": "混合专家模型（MoE）通过将每个令牌路由到每层的小部分专家，提高了密集语言模型的效率和可扩展性。本文展示了一个对手如何利用专家选择路由，完全泄露受害者的提示，只需将其查询与受害者的查询放在同一批次中。我们在一个两层的Mixtral模型上成功演示了这一攻击，利用了torch.topk CUDA实现中的平局处理行为。我们的结果表明，在考虑的设置中，我们可以使用O({VM}^2)的查询（其中V是词汇大小，M是提示长度）或平均每个令牌100个查询来提取整个提示，这是首次利用架构缺陷提取用户提示的攻击，介绍了一类新的大型语言模型脆弱性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.20050",
            "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
            "url": "https://huggingface.co/papers/2410.20050",
            "abstract": "Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB.",
            "score": 7,
            "issue_id": 347,
            "pub_date": "2024-10-26",
            "pub_date_card": {
                "ru": "26 октября",
                "en": "October 26",
                "zh": "10月26日"
            },
            "hash": "57721469df67a2f9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#medicine"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Революция в медицинском поиске: SL-HyDE и CMIRB открывают новые горизонты",
                    "desc": "Статья представляет новый подход к медицинскому информационному поиску под названием SL-HyDE. Этот метод использует большие языковые модели для генерации гипотетических документов на основе запроса, что помогает плотностному ретриверу находить наиболее релевантные документы. Авторы также представляют CMIRB - комплексную систему оценки для медицинского информационного поиска. Экспериментальные результаты показывают, что SL-HyDE значительно превосходит существующие методы по точности поиска."
                },
                "en": {
                    "title": "Revolutionizing Medical Retrieval with Self-Learning Hypothetical Documents",
                    "desc": "This paper addresses the challenges of zero-shot dense retrieval in medical information retrieval (MIR) due to the scarcity of labeled data. It introduces a new method called Self-Learning Hypothetical Document Embeddings (SL-HyDE), which uses large language models to create hypothetical documents that provide essential medical context for retrieval tasks. The self-learning approach refines the generation of these documents and the retrieval process using unlabeled medical data. Additionally, the authors present the Chinese Medical Information Retrieval Benchmark (CMIRB) to evaluate the performance of various models in real-world medical scenarios, demonstrating that SL-HyDE outperforms existing methods in accuracy and adaptability."
                },
                "zh": {
                    "title": "自学习假设文档嵌入：提升医学信息检索的有效性",
                    "desc": "医学信息检索（MIR）在从多种来源获取相关医学知识中至关重要，但在医学领域实现有效的零样本密集检索面临重大挑战，因为缺乏相关性标记的数据。本文提出了一种新方法，称为自学习假设文档嵌入（SL-HyDE），旨在解决这一问题。SL-HyDE利用大型语言模型（LLMs）生成基于给定查询的假设文档，这些文档包含关键的医学背景，帮助密集检索器识别最相关的文档。我们还提出了中国医学信息检索基准（CMIRB），为医学信息检索系统提供了一个全面的评估框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23277",
            "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
            "url": "https://huggingface.co/papers/2410.23277",
            "abstract": "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io",
            "score": 6,
            "issue_id": 359,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "c1aef56c5c16c883",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Двухскоростное обучение для согласованной генерации длинных видео",
                    "desc": "Статья представляет SlowFast-VGen - новую систему двухскоростного обучения для генерации длинных видео на основе действий. Модель сочетает медленное обучение динамике мира с помощью маскированной условной видео-диффузионной модели и быстрое обучение в процессе вывода с использованием временного модуля LoRA. Предложен алгоритм цикла медленно-быстрого обучения, интегрирующий внутренний цикл быстрого обучения во внешний цикл медленного. Эксперименты показывают превосходство SlowFast-VGen над базовыми моделями по различным метрикам генерации видео на основе действий."
                },
                "en": {
                    "title": "Bridging Slow and Fast Learning for Better Video Generation",
                    "desc": "This paper presents SlowFast-VGen, a new approach for generating long videos that combines slow and fast learning methods. The model uses a masked conditional video diffusion technique for slow learning of world dynamics, while a temporal LoRA module allows for fast learning to store episodic memories. By integrating these two learning speeds, the model can maintain consistency across longer video sequences and improve action-driven video generation. The authors demonstrate that their method outperforms existing models in various metrics, particularly in generating coherent long videos."
                },
                "zh": {
                    "title": "双速学习，生成更长视频！",
                    "desc": "本文提出了一种名为SlowFast-VGen的新型双速学习系统，用于生成基于动作的长视频。该系统结合了慢学习和快学习的策略，慢学习通过掩蔽条件视频扩散模型来捕捉世界动态，而快学习则利用时间LoRA模块在推理时更新参数，以高效存储情节记忆。通过引入慢-快学习循环算法，系统能够在长视频生成中保持一致性，并有效回忆多次经历的上下文信息。实验结果表明，SlowFast-VGen在多个指标上优于基线模型，特别是在长时间规划任务中表现显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22587",
            "title": "Toxicity of the Commons: Curating Open-Source Pre-Training Data",
            "url": "https://huggingface.co/papers/2410.22587",
            "abstract": "Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, there researchers are working to make language models safer. We propose a data curation pipeline to reduce harmful outputs by models trained on public domain data. There are unique challenges to working with public domain data, as these sources differ from web text in both form and content. Many sources are historical documents and are the result of Optical Character Recognition (OCR). Consequently, current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models. In this paper, we introduce a new fully open-source pipeline for open-data toxicity filtering. Our contributions are threefold. We create a custom training dataset, ToxicCommons, which is composed of texts which have been classified across five different dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence). We use this dataset to train a custom classifier, Celadon, that can be used to detect toxic content in open data more efficiently at a larger scale. Finally, we describe the balanced approach to content filtration that optimizes safety filtering with respect to the filtered data available for training.",
            "score": 6,
            "issue_id": 355,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "36253407cf358347",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Безопасные языковые модели на основе открытых данных",
                    "desc": "Статья представляет новый подход к фильтрации токсичного контента в открытых данных для обучения языковых моделей. Авторы создали датасет ToxicCommons для классификации текстов по пяти аспектам дискриминации и насилия. На его основе обучен классификатор Celadon для эффективного выявления токсичного контента в больших объемах открытых данных. Предложен сбалансированный подход к фильтрации, оптимизирующий безопасность и сохранение данных для обучения."
                },
                "en": {
                    "title": "Enhancing Safety in Open-Source Language Models",
                    "desc": "This paper discusses the development of a data curation pipeline aimed at reducing harmful outputs from large language models trained on public domain data. The authors highlight the challenges posed by the unique characteristics of public domain sources, which often include historical documents and require Optical Character Recognition (OCR). They introduce a custom training dataset called ToxicCommons, which categorizes texts based on five dimensions of toxicity. Additionally, they present a classifier named Celadon, designed to efficiently detect toxic content in open data, while also optimizing safety filtering during the training process."
                },
                "zh": {
                    "title": "开放数据的安全过滤新方法",
                    "desc": "这篇论文介绍了一个开放源代码的数据筛选流程，旨在减少使用公共领域数据训练的语言模型的有害输出。研究者们创建了一个名为ToxicCommons的自定义训练数据集，包含五个不同维度的有毒内容分类。然后，他们使用这个数据集训练了一个名为Celadon的分类器，以更高效地检测开放数据中的有毒内容。最后，论文描述了一种平衡的内容过滤方法，优化了安全过滤与可用于训练的过滤数据之间的关系。"
                }
            }
        }
    ],
    "link_prev": "2024-10-30.html",
    "link_next": "2024-11-01.html",
    "short_date_prev": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10月30日"
    },
    "short_date_next": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11月1日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 1,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章介绍了检索增强生成（RAG），一种通过外部知识检索增强大型语言模型的强大范式。尽管RAG受到广泛关注，但现有研究主要集中在单轮对话上，忽略了现实应用中复杂的多轮对话。为填补这一空白，作者提出了CORAL，一个大规模基准数据集，用于评估多轮对话中的RAG系统。CORAL包含多样的信息寻求对话，涵盖开放域、知识密集度、自由形式回复和话题转换等挑战。它支持对话RAG的三个核心任务：段落检索、回复生成和引用标记。作者提出了一个统一框架，并在CORAL上对各种对话RAG方法进行了综合评估，展示了改进现有方法的巨大潜力。",
        "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
        "pinyin": "zhè piān wén zhāng jiè shào le jiǎn suǒ zēng qiáng shēng chéng (RAG), yī zhǒng tōng guò wài bù zhī shi jiǎn suǒ zēng qiáng dà xíng yǔ yán mó xíng de qiáng dà fàn shì. jǐn guǎn RAG shòu dào guǎng fàn zhòng zhù, dàn xiàn yǒu yán jiū zhǔ yào jí zhōng zài dān lún duì huà shàng, hū lüe le xiàn shí yìng yòng zhōng fú zà de duō lún duì huà. wèi tián bǔ zhè yī kòng bái, zuò zhě tí chū le CORAL, yī gè dà guī mó bǐ zhǔn shù jù, yòng yú píng guā duō lún duì huà zhōng de RAG xì tǒng. CORAL bāo hán duō yàng de xìn xī xún qiú duì huà, hán gài kāi fàng yù, zhī shi mì dù, zì yóu xíng shì huí fù hé huà tí zhuǎn huàn děng tiǎo zhàn. tā zhī chí duì huà RAG de sān gè hé xīn rèn wù: duàn luó jiǎn suǒ, huí fù shēng chéng hé yǐn yòng biāo jì. zuò zhě tí chū le yī gè tǒng yī kuàng jià, bìng zài CORAL shàng duì zhǒng zhòng duì huà RAG fāng fǎ jìn xíng le zōng hé píng guā, zhǎn shì le gǎi jìn xiàn yǒu fāng fǎ de jù dà qián lì.",
        "vocab": "[\n    {\"word\": \"检索增强生成\", \"pinyin\": \"jiǎnsuǒ zēngqiáng shēngchéng\", \"trans\": \"Retrieval-Augmented Generation\"},\n    {\"word\": \"范式\", \"pinyin\": \"fànshì\", \"trans\": \"paradigm\"},\n    {\"word\": \"广泛\", \"pinyin\": \"guǎngfàn\", \"trans\": \"extensive\"},\n    {\"word\": \"单轮\", \"pinyin\": \"dānlún\", \"trans\": \"single-turn\"},\n    {\"word\": \"对话\", \"pinyin\": \"duìhuà\", \"trans\": \"dialogue\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fùzá\", \"trans\": \"complex\"},\n    {\"word\": \"多轮\", \"pinyin\": \"duōlún\", \"trans\": \"multi-turn\"},\n    {\"word\": \"填补\", \"pinyin\": \"tiánbǔ\", \"trans\": \"fill\"},\n    {\"word\": \"空白\", \"pinyin\": \"kòngbái\", \"trans\": \"gap\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"信息寻求\", \"pinyin\": \"xìnxī xúnqiú\", \"trans\": \"information-seeking\"},\n    {\"word\": \"开放域\", \"pinyin\": \"kāifàng yù\", \"trans\": \"open-domain\"},\n    {\"word\": \"知识密集度\", \"pinyin\": \"zhīshi mìjīdù\", \"trans\": \"knowledge intensity\"},\n    {\"word\": \"自由形式\", \"pinyin\": \"zìyóu xíngshì\", \"trans\": \"free-form\"},\n    {\"word\": \"回复\", \"pinyin\": \"huífù\", \"trans\": \"response\"},\n    {\"word\": \"话题转换\", \"pinyin\": \"huàtí zhuǎnhuàn\", \"trans\": \"topic switching\"},\n    {\"word\": \"段落检索\", \"pinyin\": \"duànluò jiǎnsuǒ\", \"trans\": \"paragraph retrieval\"},\n    {\"word\": \"引用标记\", \"pinyin\": \"yǐnyòng biāojì\", \"trans\": \"citation marking\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"综合\", \"pinyin\": \"zōnghé\", \"trans\": \"comprehensive\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qiánlì\", \"trans\": \"potential\"}\n]",
        "trans": "This article introduces Retrieval-Augmented Generation (RAG), a powerful paradigm that enhances large language models through external knowledge retrieval. Although RAG has received widespread attention, existing research primarily focuses on single-turn conversations, overlooking the complex multi-turn conversations encountered in real-world applications. To address this gap, the authors propose CORAL, a large-scale benchmark dataset for evaluating RAG systems in multi-turn conversations. CORAL includes diverse information-seeking dialogues, encompassing challenges such as open domains, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks for conversational RAG: paragraph retrieval, response generation, and citation marking. The authors present a unified framework and conduct a comprehensive evaluation of various conversational RAG methods on CORAL, demonstrating the significant potential for improving existing methods.",
        "update_ts": "2024-10-31 09:22"
    }
}