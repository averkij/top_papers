{
    "date": {
        "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 31",
        "zh": "10æœˆ31æ—¥"
    },
    "time_utc": "2024-10-31 04:16",
    "weekday": 3,
    "issue_id": 348,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.23090",
            "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
            "url": "https://huggingface.co/papers/2410.23090",
            "abstract": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.",
            "score": 26,
            "issue_id": 348,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "d3cb6da7b94ee077",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "CORAL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ RAG",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CORAL Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (RAG) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. CORAL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ±ĞµÑĞµĞ´Ñ‹, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸, Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½, Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¼ĞµĞ½Ğ° Ñ‚ĞµĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ†Ğ¸Ñ‚Ğ°Ñ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² RAG Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Conversations with CORAL Benchmark",
                    "desc": "This paper introduces CORAL, a benchmark aimed at improving Retrieval-Augmented Generation (RAG) systems for multi-turn conversations, which are more complex than single-turn interactions. It highlights the need for RAG models to effectively handle diverse and dynamic information-seeking dialogues, addressing challenges like open-domain coverage and topic shifts. The benchmark includes tasks such as passage retrieval, response generation, and citation labeling, providing a structured way to evaluate RAG performance. By proposing a unified framework, the authors aim to enhance the effectiveness of conversational RAG methods and identify areas for future improvement."
                },
                "zh": {
                    "title": "æå‡å¤šè½®å¯¹è¯çš„æ£€ç´¢å¢å¼ºç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†CORALï¼Œç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•è½®å¯¹è¯ä¸Šï¼Œç¼ºä¹å¯¹å¤æ‚å¤šè½®å¯¹è¯çš„æ·±å…¥æ¢è®¨ã€‚CORALåŸºäºç»´åŸºç™¾ç§‘è‡ªåŠ¨ç”Ÿæˆå¤šæ ·çš„ä¿¡æ¯å¯»æ±‚å¯¹è¯ï¼Œè§£å†³å¼€æ”¾åŸŸè¦†ç›–ã€çŸ¥è¯†å¯†é›†åº¦ã€è‡ªç”±å½¢å¼å“åº”å’Œè¯é¢˜è½¬ç§»ç­‰å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä»¥æ ‡å‡†åŒ–ä¸åŒçš„å¯¹è¯RAGæ–¹æ³•ï¼Œå¹¶åœ¨CORALä¸Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†æ”¹è¿›ç°æœ‰æ–¹æ³•çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.20050",
            "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
            "url": "https://huggingface.co/papers/2410.20050",
            "abstract": "Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB.",
            "score": 1,
            "issue_id": 347,
            "pub_date": "2024-10-26",
            "pub_date_card": {
                "ru": "26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 26",
                "zh": "10æœˆ26æ—¥"
            },
            "hash": "57721469df67a2f9",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#medicine"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: SL-HyDE Ğ¸ CMIRB Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SL-HyDE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñƒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ CMIRB - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SL-HyDE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing Medical Retrieval with Self-Learning Hypothetical Documents",
                    "desc": "This paper addresses the challenges of zero-shot dense retrieval in medical information retrieval (MIR) due to the scarcity of labeled data. It introduces a new method called Self-Learning Hypothetical Document Embeddings (SL-HyDE), which uses large language models to create hypothetical documents that provide essential medical context for retrieval tasks. The self-learning approach refines the generation of these documents and the retrieval process using unlabeled medical data. Additionally, the authors present the Chinese Medical Information Retrieval Benchmark (CMIRB) to evaluate the performance of various models in real-world medical scenarios, demonstrating that SL-HyDE outperforms existing methods in accuracy and adaptability."
                },
                "zh": {
                    "title": "è‡ªå­¦ä¹ å‡è®¾æ–‡æ¡£åµŒå…¥ï¼šæå‡åŒ»å­¦ä¿¡æ¯æ£€ç´¢çš„æœ‰æ•ˆæ€§",
                    "desc": "åŒ»å­¦ä¿¡æ¯æ£€ç´¢ï¼ˆMIRï¼‰åœ¨ä»å¤šç§æ¥æºè·å–ç›¸å…³åŒ»å­¦çŸ¥è¯†ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸå®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬å¯†é›†æ£€ç´¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹ç›¸å…³æ€§æ ‡è®°çš„æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè‡ªå­¦ä¹ å‡è®¾æ–‡æ¡£åµŒå…¥ï¼ˆSL-HyDEï¼‰ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚SL-HyDEåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”ŸæˆåŸºäºç»™å®šæŸ¥è¯¢çš„å‡è®¾æ–‡æ¡£ï¼Œè¿™äº›æ–‡æ¡£åŒ…å«å…³é”®çš„åŒ»å­¦èƒŒæ™¯ï¼Œå¸®åŠ©å¯†é›†æ£€ç´¢å™¨è¯†åˆ«æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸­å›½åŒ»å­¦ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼ˆCMIRBï¼‰ï¼Œä¸ºåŒ»å­¦ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-10-30.html",
    "link_next": "2024-11-01.html",
    "short_date_prev": {
        "ru": "30.10",
        "en": "10/30",
        "zh": "10æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æœºå™¨å–æ¶ˆå­¦ä¹ ï¼ˆMUï¼‰åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¢å¼ºéšç§å’Œå®‰å…¨çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ã€‚è™½ç„¶MUåœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ¨¡æ€å–æ¶ˆå­¦ä¹ ï¼ˆMMUï¼‰ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹åˆé€‚çš„å¼€æºåŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†CLEARï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MMUæ–¹æ³•ã€‚CLEARåŒ…å«200ä¸ªè™šæ„ä¸ªä½“å’Œ3700å¼ å›¾ç‰‡ï¼Œå¹¶é™„æœ‰ç›¸åº”çš„é—®ç­”å¯¹ï¼Œå¯ä»¥è¿›è¡Œè·¨æ¨¡æ€çš„å…¨é¢è¯„ä¼°ã€‚ä½œè€…è¯„ä¼°äº†10ç§MUæ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å¤šæ¨¡æ€é—å¿˜çš„æ–°æŒ‘æˆ˜ã€‚ä»–ä»¬è¿˜å±•ç¤ºäº†ç®€å•çš„ell_1æ­£åˆ™åŒ–å¯ä»¥æ˜¾è‘—å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œä¿æŒæ¨¡å‹åœ¨ä¿ç•™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ•°æ®é›†å¯åœ¨https://huggingface.co/datasets/therem/CLEARè·å–ã€‚",
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æœºå™¨å–æ¶ˆå­¦ä¹ ï¼ˆMUï¼‰åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¢å¼ºéšç§å’Œå®‰å…¨çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ã€‚è™½ç„¶MUåœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ¨¡æ€å–æ¶ˆå­¦ä¹ ï¼ˆMMUï¼‰ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹åˆé€‚çš„å¼€æºåŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†CLEARï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MMUæ–¹æ³•ã€‚CLEARåŒ…å«200ä¸ªè™šæ„ä¸ªä½“å’Œ3700å¼ å›¾ç‰‡ï¼Œå¹¶é™„æœ‰ç›¸åº”çš„é—®ç­”å¯¹ï¼Œå¯ä»¥è¿›è¡Œè·¨æ¨¡æ€çš„å…¨é¢è¯„ä¼°ã€‚ä½œè€…è¯„ä¼°äº†10ç§MUæ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å¤šæ¨¡æ€é—å¿˜çš„æ–°æŒ‘æˆ˜ã€‚ä»–ä»¬è¿˜å±•ç¤ºäº†ç®€å•çš„ell_1æ­£åˆ™åŒ–å¯ä»¥æ˜¾è‘—å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œä¿æŒæ¨¡å‹åœ¨ä¿ç•™æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ•°æ®é›†å¯åœ¨https://huggingface.co/datasets/therem/CLEARè·å–ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le jÄ« qÃ¬ qÇ” xiÄo xuÃ© (MU) zÃ i shÄ“n dÃ¹ xuÃ© xÃ­ mÃ³ xÃ¬ng zhÅng zÄ“ng qiÃ¡ng yÇn sÄ« hÃ© Än quÃ¡n de zhÃ²ng yÃ o xÃ¬ng, tÃ¨ biÃ© shÃ¬ zÃ i dÃ  xÃ­ng duÅ mÃ³ shuÃ i yÇ” yÃ¡n mÃ³ xÃ¬ng (MLLMs) zhÅng. suÄ« rÃ¡n MU zÃ i wÃ©n bÄ›n hÃ© shÃ¬ juÃ© mÃ³ tÃ i shÃ ng qu dÃ© le xiÇn zhÃ¹ jÃ¬n zhÇn, dÃ n duÅ mÃ³ shuÃ i qÇ” xiÄo xuÃ© (MMU) rÃ©ng rÃ¡n mÃ©i yÇ’u dÃ© dÃ o chÃ³ng fÃ¨n yÃ¡n jiÅ«, bÃ¹ fÃ¨n yuÃ¡n yÄ«n shÃ¬ quÄ“ fÃ¡ hÃ© shÃ¬ de kÄi yuÃ¡n jÄ« zhÇ”n. wÃ¨i le jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­, zuÃ² zhÄ› yÇn rÃ¹ le CLEAR, yÄ« gÃ¨ xÄ«n de jÄ« zhÇ”n, yÃ²ng yÃº pÃ­ng guÄ MMU fÄng fÇ. CLEAR bÄo hÃ¡n 200 gÃ¨ xÅ« gÃ²u gÃ¨ tÇ hÃ© 3700 zhÄng tÃº piÃ n, bÃ¬ng fÃ¹ yÇ’u xiÄng yÃ¬ng de wÃ¨n dÃ¡ duÃ¬, kÄ› yÇ jÃ¬n xÃ­ng kuÃ  mÃ³ shuÃ i de quÃ¡n miÃ n pÃ­ng guÄ. zuÃ² zhÄ› pÃ­ng guÄ le 10 zhÇ’ng MU fÄng fÇ, bÃ¬ng zhÇ chÅ« le duÅ mÃ³ shuÃ i yÃ­ wÃ ng de xÄ«n tiÇo zhÃ n. tÄ men hÃ¡i zhÇn shÃ¬ le jiÇn dÄn de ell_1 zhÃ¨ng guÄ« huÃ  kÄ› yÇ xiÇn zhÃ¹ jiÇn qÄ«ng zÄi nÃ n xÃ¬ng yÃ­ wÃ ng, bÇo chÃ­ mÃ³ xÃ¬ng zÃ i bÇo liÃº shÃ¹ jÃ¹ shÃ ng de xiÃ o nÃ¨ng. shÃ¹ jÃ¹ jÃ­ kÄ› zÃ i https://huggingface.co/datasets/therem/CLEAR huÃ² qÇ”.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"æœºå™¨å–æ¶ˆå­¦ä¹ \", \"pinyin\": \"jÄ« qÃ¬ qÇ” xiÄo xuÃ© xÃ­\", \"trans\": \"machine unlearning\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"éšç§\", \"pinyin\": \"yÇn sÄ«\", \"trans\": \"privacy\"},\n    {\"word\": \"å®‰å…¨\", \"pinyin\": \"Än quÃ¡n\", \"trans\": \"security\"},\n    {\"word\": \"æ·±åº¦å­¦ä¹ æ¨¡å‹\", \"pinyin\": \"shÄ“n dÃ¹ xuÃ© xÃ­ mÃ³ xÃ­ng\", \"trans\": \"deep learning model\"},\n    {\"word\": \"å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹\", \"pinyin\": \"duÅ mÃ³ shuÃ i yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"multimodal language model\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"å¤šæ¨¡æ€å–æ¶ˆå­¦ä¹ \", \"pinyin\": \"duÅ mÃ³ shuÃ i qÇ” xiÄo xuÃ© xÃ­\", \"trans\": \"multimodal unlearning\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"åˆé€‚\", \"pinyin\": \"hÃ© shÃ¬\", \"trans\": \"suitable\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"è™šæ„\", \"pinyin\": \"xÅ« gÃ²u\", \"trans\": \"fictional\"},\n    {\"word\": \"ä¸ªä½“\", \"pinyin\": \"gÃ¨ tÇ\", \"trans\": \"individual\"},\n    {\"word\": \"å›¾ç‰‡\", \"pinyin\": \"tÃº piÃ n\", \"trans\": \"image\"},\n    {\"word\": \"é—®ç­”å¯¹\", \"pinyin\": \"wÃ¨n dÃ¡ duÃ¬\", \"trans\": \"question-answer pair\"},\n    {\"word\": \"è·¨æ¨¡æ€\", \"pinyin\": \"kuÃ  mÃ³ shuÃ i\", \"trans\": \"cross-modal\"},\n    {\"word\": \"å…¨é¢\", \"pinyin\": \"quÃ¡n miÃ n\", \"trans\": \"comprehensive\"},\n    {\"word\": \"é—å¿˜\", \"pinyin\": \"yÃ­ wÃ ng\", \"trans\": \"forgetting\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æ­£åˆ™åŒ–\", \"pinyin\": \"zhÃ¨ng zÃ© huÃ \", \"trans\": \"regularization\"},\n    {\"word\": \"å‡è½»\", \"pinyin\": \"jiÇn qÄ«ng\", \"trans\": \"alleviate\"},\n    {\"word\": \"ç¾éš¾æ€§\", \"pinyin\": \"zÄi nÃ n xÃ¬ng\", \"trans\": \"catastrophic\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇo chÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"}\n]",
        "trans": "This article discusses the importance of machine unlearning (MU) in enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs). Although MU has made significant progress in text and visual modalities, multimodal unlearning (MMU) remains under-researched, partly due to the lack of suitable open-source benchmarks. To address this issue, the authors introduce CLEAR, a new benchmark for evaluating MMU methods. CLEAR contains 200 fictional individuals and 3,700 images, along with corresponding question-answer pairs, allowing for comprehensive cross-modal evaluation. The authors evaluated 10 MU methods and highlighted new challenges in multimodal forgetting. They also demonstrated that simple ell_1 regularization can significantly mitigate catastrophic forgetting, maintaining the model's performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR.",
        "update_ts": "2024-10-30 10:13"
    }
}