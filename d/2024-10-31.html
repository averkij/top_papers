
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. October 31.</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xd3cb6da7b94ee077 { background: url("https://hfday.ru/img/20241030/d3cb6da7b94ee077.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd3cb6da7b94ee077:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd3cb6da7b94ee077 { background: url("https://hfday.ru/img/20241030/d3cb6da7b94ee077.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd3cb6da7b94ee077:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x876c89e8fc188dd3 { background: url("https://hfday.ru/img/20241029/876c89e8fc188dd3.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x876c89e8fc188dd3:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x876c89e8fc188dd3 { background: url("https://hfday.ru/img/20241029/876c89e8fc188dd3.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x876c89e8fc188dd3:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x69c16b774d32c4c1 { background: url("https://hfday.ru/img/20241028/69c16b774d32c4c1.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x69c16b774d32c4c1:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x69c16b774d32c4c1 { background: url("https://hfday.ru/img/20241028/69c16b774d32c4c1.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x69c16b774d32c4c1:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xcf2371629ffd5ab5 { background: url("https://hfday.ru/img/20241030/cf2371629ffd5ab5.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xcf2371629ffd5ab5:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xcf2371629ffd5ab5 { background: url("https://hfday.ru/img/20241030/cf2371629ffd5ab5.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xcf2371629ffd5ab5:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">31 октября</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-30.html">⬅️ <span id="prev-date">30.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-01.html">➡️ <span id="next-date">01.11</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'};
        let feedDateNext = {'ru': '01.11', 'en': '11/01', 'zh': '11月1日'};
        let feedDatePrev = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.23090', 'title': 'CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation', 'url': 'https://huggingface.co/papers/2410.23090', 'abstract': 'Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.', 'score': 44, 'issue_id': 348, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd3cb6da7b94ee077', 'data': {'categories': ['#benchmark', '#rag'], 'emoji': '🗣️', 'ru': {'title': 'CORAL: Новый стандарт для оценки многоходовых диалоговых систем с RAG', 'desc': 'Статья представляет новый бенчмарк CORAL для оценки систем генерации с дополнительной информацией (RAG) в многоходовых диалогах. CORAL включает в себя разнообразные информационно-поисковые беседы, автоматически созданные на основе Википедии, и охватывает ключевые задачи, такие как открытый домен, интенсивное использование знаний и смена тем. Бенчмарк поддерживает три основные задачи: поиск релевантных отрывков текста, генерация ответов и маркировка цитат. Авторы также предлагают унифицированную структуру для стандартизации различных методов RAG в диалоговых системах.'}, 'en': {'title': 'Enhancing Multi-Turn Conversations with CORAL Benchmark', 'desc': 'This paper introduces CORAL, a benchmark aimed at improving Retrieval-Augmented Generation (RAG) systems for multi-turn conversations, which are more complex than single-turn interactions. It highlights the need for RAG models to effectively handle diverse and dynamic information-seeking dialogues, addressing challenges like open-domain coverage and topic shifts. The benchmark includes tasks such as passage retrieval, response generation, and citation labeling, providing a structured way to evaluate RAG performance. By proposing a unified framework, the authors aim to enhance the effectiveness of conversational RAG methods and identify areas for future improvement.'}, 'zh': {'title': '提升多轮对话的检索增强生成能力', 'desc': '本论文介绍了一种新的基准CORAL，用于评估检索增强生成（RAG）系统在多轮对话中的表现。现有研究主要集中在单轮对话上，缺乏对复杂多轮对话的深入探讨。CORAL基于维基百科自动生成多样的信息寻求对话，解决开放域覆盖、知识密集度、自由形式响应和话题转移等关键挑战。我们提出了一个统一框架，以标准化不同的对话RAG方法，并在CORAL上进行全面评估，展示了改进现有方法的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.22391', 'title': 'A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks', 'url': 'https://huggingface.co/papers/2410.22391', 'abstract': 'In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.', 'score': 12, 'issue_id': 350, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '876c89e8fc188dd3', 'data': {'categories': ['#agents', '#architecture', '#rl'], 'emoji': '🤖', 'ru': {'title': 'LRAM: Быстрее трансформеров, эффективнее в реальном времени', 'desc': 'В статье представлена новая модель LRAM (Large Recurrent Action Model) для обучения с подкреплением, основанная на архитектуре xLSTM. LRAM предлагает линейную сложность вывода и способность к экстраполяции длины последовательности, что делает её более практичной для приложений реального времени по сравнению с моделями на основе трансформеров. Эксперименты на 432 задачах из 6 доменов показали, что LRAM не уступает трансформерам по производительности и скорости. Это исследование демонстрирует потенциал современных рекуррентных архитектур для моделей с большим пространством действий в обучении с подкреплением.'}, 'en': {'title': 'Fast and Effective: LRAM for Real-Time Reinforcement Learning', 'desc': 'This paper explores the use of modern recurrent architectures, specifically xLSTM, for creating large action models in Reinforcement Learning (RL). Traditional Transformer models are powerful but suffer from slow inference times, making them unsuitable for real-time applications like robotics. The proposed Large Recurrent Action Model (LRAM) leverages the benefits of xLSTM to achieve linear-time inference complexity while maintaining strong performance. Experimental results demonstrate that LRAM outperforms Transformer-based models in both speed and effectiveness across a variety of tasks.'}, 'zh': {'title': '快速推理的强化学习新选择', 'desc': '近年来，强化学习（RL）领域出现了一个趋势，即使用大型离线数据集通过序列建模训练大型动作模型。现有模型主要基于Transformer架构，虽然能够生成强大的智能体，但由于推理速度慢，难以应用于实时场景，如机器人技术。最近提出的现代递归架构，如xLSTM和Mamba，具有与Transformer相似的训练并行化优势，同时提供快速推理能力。本文研究了这些现代递归架构在大型动作模型中的适用性，并提出了一种以xLSTM为核心的大型递归动作模型（LRAM），其推理复杂度为线性时间，且具有自然的序列长度外推能力。'}}}, {'id': 'https://huggingface.co/papers/2410.20779', 'title': 'Decoding Reading Goals from Eye Movements', 'url': 'https://huggingface.co/papers/2410.20779', 'abstract': 'Readers can have different goals with respect to the text they are reading. Can these goals be decoded from the pattern of their eye movements over the text? In this work, we examine for the first time whether it is possible to decode two types of reading goals that are common in daily life: information seeking and ordinary reading. Using large scale eye-tracking data, we apply to this task a wide range of state-of-the-art models for eye movements and text that cover different architectural and data representation strategies, and further introduce a new model ensemble. We systematically evaluate these models at three levels of generalization: new textual item, new participant, and the combination of both. We find that eye movements contain highly valuable signals for this task. We further perform an error analysis which builds on prior empirical findings on differences between ordinary reading and information seeking and leverages rich textual annotations. This analysis reveals key properties of textual items and participant eye movements that contribute to the difficulty of the task.', 'score': 11, 'issue_id': 356, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '69c16b774d32c4c1', 'data': {'categories': ['#benchmark', '#cv', '#dataset', '#interpretability'], 'emoji': '👁️', 'ru': {'title': 'Разгадка целей чтения по движениям глаз', 'desc': 'Исследование посвящено декодированию целей чтения на основе движений глаз. Авторы применяют современные модели машинного обучения для анализа крупномасштабных данных айтрекинга, чтобы различать информационный поиск и обычное чтение. Оценка моделей проводится на трех уровнях обобщения: новый текст, новый участник и их комбинация. Анализ ошибок выявляет ключевые свойства текстов и движений глаз, влияющие на сложность задачи.'}, 'en': {'title': 'Decoding Reading Goals Through Eye Movements', 'desc': "This paper explores whether the goals of readers, such as information seeking and ordinary reading, can be inferred from their eye movement patterns. Using extensive eye-tracking data, the authors implement various advanced machine learning models to analyze these movements and introduce a new model ensemble for improved accuracy. They evaluate the models' performance across different scenarios, including new texts and new participants, demonstrating that eye movements provide significant insights into reading intentions. Additionally, an error analysis highlights specific characteristics of texts and eye movement behaviors that affect the decoding process."}, 'zh': {'title': '解码阅读目标：眼动与文本的深度分析', 'desc': '本研究首次探讨了是否可以通过眼动模式解码读者的阅读目标，包括信息寻求和普通阅读。我们使用大规模的眼动追踪数据，应用多种先进的模型来分析眼动和文本，提出了一种新的模型集成方法。通过对新文本、新参与者及其组合的系统评估，我们发现眼动包含了对解码阅读目标非常有价值的信号。进一步的错误分析揭示了文本特性和参与者眼动的关键属性，这些属性影响了任务的难度。'}}}, {'id': 'https://huggingface.co/papers/2410.23287', 'title': 'ReferEverything: Towards Segmenting Everything We Can Speak of in Videos', 'url': 'https://huggingface.co/papers/2410.23287', 'abstract': "We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.", 'score': 11, 'issue_id': 356, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'cf2371629ffd5ab5', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#video'], 'emoji': '🎥', 'ru': {'title': 'REM: универсальная сегментация видео с помощью естественного языка', 'desc': 'REM - это фреймворк для сегментации различных концепций в видео, описываемых с помощью естественного языка. Он использует визуально-языковые представления, полученные видео-диффузионными моделями на масштабных интернет-датасетах. Ключевая особенность подхода - сохранение большей части исходного представления генеративной модели при дообучении на узкоспециализированных датасетах сегментации объектов по запросу. REM способен точно сегментировать и отслеживать редкие и невиданные ранее объекты, а также обобщаться на динамические концепты, не являющиеся объектами.'}, 'en': {'title': 'Segmenting Video Concepts with Natural Language Power', 'desc': 'The REM framework is designed to segment various concepts in videos using natural language descriptions. It utilizes visual-language representations from video diffusion models trained on large datasets from the internet. By fine-tuning these models on specific datasets for Referral Object Segmentation, REM can effectively identify and track both common and rare objects. Additionally, it demonstrates the ability to generalize to dynamic concepts, achieving high performance on both in-domain and out-of-domain tasks.'}, 'zh': {'title': 'REM框架：视频概念分割的新突破', 'desc': '我们提出了REM框架，用于通过自然语言对视频中的各种概念进行分割。该方法利用了在互联网规模数据集上学习的视觉-语言表示，结合视频扩散模型。我们的方法的关键在于尽可能保留生成模型的原始表示，同时在狭域的引用对象分割数据集上进行微调。结果表明，REM框架能够准确分割和跟踪稀有和未见过的对象，并且能够推广到非对象动态概念，如海浪的冲击。'}}}, {'id': 'https://huggingface.co/papers/2410.22884', 'title': 'Stealing User Prompts from Mixture of Experts', 'url': 'https://huggingface.co/papers/2410.22884', 'abstract': "Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit Expert-Choice-Routing to fully disclose a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layer Mixtral model, exploiting the tie-handling behavior of the torch.topk CUDA implementation. Our results show that we can extract the entire prompt using O({VM}^2) queries (with vocabulary size V and prompt length M) or 100 queries on average per token in the setting we consider. This is the first attack to exploit architectural flaws for the purpose of extracting user prompts, introducing a new class of LLM vulnerabilities.", 'score': 6, 'issue_id': 351, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '50ec28e1ed4db1bb', 'data': {'categories': ['#architecture', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Уязвимость в MoE моделях: как архитектурные особенности могут раскрыть ваш промпт', 'desc': 'Статья описывает уязвимость в моделях Mixture-of-Experts (MoE), использующих маршрутизацию Expert-Choice-Routing. Авторы демонстрируют, как злоумышленник может эксплуатировать эту уязвимость для раскрытия промпта жертвы, если запросы обрабатываются в одном батче. Эксперимент проводился на двухслойной модели Mixtral, используя особенности реализации torch.topk CUDA. Это первая атака, эксплуатирующая архитектурные недостатки для извлечения пользовательских промптов, что открывает новый класс уязвимостей в больших языковых моделях.'}, 'en': {'title': 'Exposing Prompts: A New Vulnerability in Mixture-of-Experts Models', 'desc': "This paper discusses a vulnerability in Mixture-of-Experts (MoE) models, which are designed to enhance the efficiency of language models by directing tokens to specific experts. The authors demonstrate that an adversary can exploit the Expert-Choice-Routing mechanism to reveal a victim's input prompt by cleverly arranging queries in the same batch. They successfully execute this attack on a two-layer Mixtral model, taking advantage of the tie-handling behavior in the torch.topk CUDA implementation. The findings indicate that the entire prompt can be extracted with a relatively small number of queries, highlighting a new class of vulnerabilities in large language models (LLMs)."}, 'zh': {'title': '利用架构缺陷提取用户提示的攻击', 'desc': '混合专家模型（MoE）通过将每个令牌路由到每层的小部分专家，提高了密集语言模型的效率和可扩展性。本文展示了一个对手如何利用专家选择路由，完全泄露受害者的提示，只需将其查询与受害者的查询放在同一批次中。我们在一个两层的Mixtral模型上成功演示了这一攻击，利用了torch.topk CUDA实现中的平局处理行为。我们的结果表明，在考虑的设置中，我们可以使用O({VM}^2)的查询（其中V是词汇大小，M是提示长度）或平均每个令牌100个查询来提取整个提示，这是首次利用架构缺陷提取用户提示的攻击，介绍了一类新的大型语言模型脆弱性。'}}}, {'id': 'https://huggingface.co/papers/2410.20050', 'title': 'AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels', 'url': 'https://huggingface.co/papers/2410.20050', 'abstract': 'Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB.', 'score': 6, 'issue_id': 347, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '57721469df67a2f9', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#medicine'], 'emoji': '🩺', 'ru': {'title': 'Революция в медицинском поиске: SL-HyDE и CMIRB открывают новые горизонты', 'desc': 'Статья представляет новый подход к медицинскому информационному поиску под названием SL-HyDE. Этот метод использует большие языковые модели для генерации гипотетических документов на основе запроса, что помогает плотностному ретриверу находить наиболее релевантные документы. Авторы также представляют CMIRB - комплексную систему оценки для медицинского информационного поиска. Экспериментальные результаты показывают, что SL-HyDE значительно превосходит существующие методы по точности поиска.'}, 'en': {'title': 'Revolutionizing Medical Retrieval with Self-Learning Hypothetical Documents', 'desc': 'This paper addresses the challenges of zero-shot dense retrieval in medical information retrieval (MIR) due to the scarcity of labeled data. It introduces a new method called Self-Learning Hypothetical Document Embeddings (SL-HyDE), which uses large language models to create hypothetical documents that provide essential medical context for retrieval tasks. The self-learning approach refines the generation of these documents and the retrieval process using unlabeled medical data. Additionally, the authors present the Chinese Medical Information Retrieval Benchmark (CMIRB) to evaluate the performance of various models in real-world medical scenarios, demonstrating that SL-HyDE outperforms existing methods in accuracy and adaptability.'}, 'zh': {'title': '自学习假设文档嵌入：提升医学信息检索的有效性', 'desc': '医学信息检索（MIR）在从多种来源获取相关医学知识中至关重要，但在医学领域实现有效的零样本密集检索面临重大挑战，因为缺乏相关性标记的数据。本文提出了一种新方法，称为自学习假设文档嵌入（SL-HyDE），旨在解决这一问题。SL-HyDE利用大型语言模型（LLMs）生成基于给定查询的假设文档，这些文档包含关键的医学背景，帮助密集检索器识别最相关的文档。我们还提出了中国医学信息检索基准（CMIRB），为医学信息检索系统提供了一个全面的评估框架。'}}}, {'id': 'https://huggingface.co/papers/2410.23168', 'title': 'TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters', 'url': 'https://huggingface.co/papers/2410.23168', 'abstract': 'Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer.', 'score': 5, 'issue_id': 353, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '4785b6a73debe15e', 'data': {'categories': ['#architecture', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Гибкое масштабирование нейросетей без полного переобучения', 'desc': 'TokenFormer - это новая архитектура модели, которая позволяет эффективно масштабировать нейронные сети без необходимости полного переобучения. В отличие от стандартных трансформеров, TokenFormer использует механизм внимания не только между входными токенами, но и между токенами и параметрами модели. Это достигается за счет замены линейных проекций на слои внимания токен-параметр, где входные токены выступают в роли запросов, а параметры модели - в роли ключей и значений. Такой подход позволяет постепенно наращивать размер модели от 124 млн до 1,4 млрд параметров, сохраняя производительность на уровне полностью переобученных трансформеров, но значительно снижая вычислительные затраты.'}, 'en': {'title': 'TokenFormer: Scalable Transformers Without Retraining', 'desc': 'This paper presents TokenFormer, a new architecture designed to address the high computational costs associated with scaling Transformer models. Traditional Transformers require retraining from scratch when architectural changes are made, which is inefficient as model sizes increase. TokenFormer innovatively uses the attention mechanism to allow model parameters to interact with input tokens, treating parameters as tokens themselves. This approach enables flexible scaling of the model without the need for complete retraining, significantly reducing training costs while maintaining competitive performance.'}, 'zh': {'title': 'TokenFormer：高效可扩展的Transformer架构', 'desc': '本文介绍了一种新的模型架构TokenFormer，旨在解决现有Transformer模型在扩展时的高计算成本问题。TokenFormer通过将模型参数视为令牌，利用注意力机制实现输入令牌与模型参数之间的交互，从而提高了架构的灵活性。与传统方法不同，TokenFormer允许逐步扩展模型，而无需从头开始重新训练。该模型在参数数量从1.24亿扩展到14亿的过程中，能够在保持性能的同时显著降低训练成本。'}}}, {'id': 'https://huggingface.co/papers/2410.23123', 'title': 'On Memorization of Large Language Models in Logical Reasoning', 'url': 'https://huggingface.co/papers/2410.23123', 'abstract': "Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Our code and data are available at https://memkklogic.github.io.", 'score': 4, 'issue_id': 358, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'f3e776b0854b1ec8', 'data': {'categories': ['#interpretability', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Запоминание vs Рассуждение: Сложное взаимодействие в больших языковых моделях', 'desc': "В статье исследуется взаимосвязь между способностью больших языковых моделей (LLM) к запоминанию и их навыками логического мышления. Авторы используют динамически генерируемый набор логических задач на основе головоломок 'Рыцари и лжецы' для измерения степени запоминания. Результаты показывают, что LLM могут достигать почти идеальной точности на тренировочных примерах, но терпят неудачу при небольших изменениях в задачах. Тем не менее, исследование также демонстрирует, что дообучение моделей, несмотря на сильное запоминание, улучшает их способность к обобщению."}, 'en': {'title': "Memorization vs. Reasoning: Unraveling LLMs' Logic Skills", 'desc': 'This paper explores the reasoning capabilities of large language models (LLMs) and their tendency to memorize training data. The authors propose that LLMs achieve high performance on reasoning tasks by memorizing similar problems rather than genuinely understanding them. Through experiments with Knights and Knaves puzzles, they demonstrate that while LLMs can interpolate training data effectively, they struggle with slight variations, indicating reliance on memorization. However, the study also reveals that fine-tuning improves generalization, suggesting a complex relationship between memorization and reasoning in LLMs.'}, 'zh': {'title': '记忆与推理的复杂交互', 'desc': '大型语言模型（LLMs）在复杂推理基准测试中表现良好，但也可能出现基本推理错误。本文系统地研究了LLMs推理能力背后的机制，提出了记忆化假设，认为模型在推理任务中可能依赖于对相似问题的记忆。通过动态生成的逻辑推理基准，我们发现LLMs在微调后能够完美解决训练谜题，但在稍微改变这些谜题时却表现不佳，表明它们在解决训练谜题时严重依赖记忆。尽管微调导致了重度记忆化，但也提高了模型的泛化性能，显示出记忆与真实推理能力之间的复杂关系。'}}}, {'id': 'https://huggingface.co/papers/2410.22587', 'title': 'Toxicity of the Commons: Curating Open-Source Pre-Training Data', 'url': 'https://huggingface.co/papers/2410.22587', 'abstract': 'Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, there researchers are working to make language models safer. We propose a data curation pipeline to reduce harmful outputs by models trained on public domain data. There are unique challenges to working with public domain data, as these sources differ from web text in both form and content. Many sources are historical documents and are the result of Optical Character Recognition (OCR). Consequently, current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models. In this paper, we introduce a new fully open-source pipeline for open-data toxicity filtering. Our contributions are threefold. We create a custom training dataset, ToxicCommons, which is composed of texts which have been classified across five different dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence). We use this dataset to train a custom classifier, Celadon, that can be used to detect toxic content in open data more efficiently at a larger scale. Finally, we describe the balanced approach to content filtration that optimizes safety filtering with respect to the filtered data available for training.', 'score': 3, 'issue_id': 355, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '36253407cf358347', 'data': {'categories': ['#data', '#dataset', '#interpretability'], 'emoji': '🛡️', 'ru': {'title': 'Безопасные языковые модели на основе открытых данных', 'desc': 'Статья представляет новый подход к фильтрации токсичного контента в открытых данных для обучения языковых моделей. Авторы создали датасет ToxicCommons для классификации текстов по пяти аспектам дискриминации и насилия. На его основе обучен классификатор Celadon для эффективного выявления токсичного контента в больших объемах открытых данных. Предложен сбалансированный подход к фильтрации, оптимизирующий безопасность и сохранение данных для обучения.'}, 'en': {'title': 'Enhancing Safety in Open-Source Language Models', 'desc': 'This paper discusses the development of a data curation pipeline aimed at reducing harmful outputs from large language models trained on public domain data. The authors highlight the challenges posed by the unique characteristics of public domain sources, which often include historical documents and require Optical Character Recognition (OCR). They introduce a custom training dataset called ToxicCommons, which categorizes texts based on five dimensions of toxicity. Additionally, they present a classifier named Celadon, designed to efficiently detect toxic content in open data, while also optimizing safety filtering during the training process.'}, 'zh': {'title': '开放数据的安全过滤新方法', 'desc': '这篇论文介绍了一个开放源代码的数据筛选流程，旨在减少使用公共领域数据训练的语言模型的有害输出。研究者们创建了一个名为ToxicCommons的自定义训练数据集，包含五个不同维度的有毒内容分类。然后，他们使用这个数据集训练了一个名为Celadon的分类器，以更高效地检测开放数据中的有毒内容。最后，论文描述了一种平衡的内容过滤方法，优化了安全过滤与可用于训练的过滤数据之间的关系。'}}}, {'id': 'https://huggingface.co/papers/2410.23277', 'title': 'SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation', 'url': 'https://huggingface.co/papers/2410.23277', 'abstract': "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io", 'score': 2, 'issue_id': 359, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'c1aef56c5c16c883', 'data': {'categories': ['#benchmark', '#dataset', '#training', '#video'], 'emoji': '🎬', 'ru': {'title': 'Двухскоростное обучение для согласованной генерации длинных видео', 'desc': 'Статья представляет SlowFast-VGen - новую систему двухскоростного обучения для генерации длинных видео на основе действий. Модель сочетает медленное обучение динамике мира с помощью маскированной условной видео-диффузионной модели и быстрое обучение в процессе вывода с использованием временного модуля LoRA. Предложен алгоритм цикла медленно-быстрого обучения, интегрирующий внутренний цикл быстрого обучения во внешний цикл медленного. Эксперименты показывают превосходство SlowFast-VGen над базовыми моделями по различным метрикам генерации видео на основе действий.'}, 'en': {'title': 'Bridging Slow and Fast Learning for Better Video Generation', 'desc': 'This paper presents SlowFast-VGen, a new approach for generating long videos that combines slow and fast learning methods. The model uses a masked conditional video diffusion technique for slow learning of world dynamics, while a temporal LoRA module allows for fast learning to store episodic memories. By integrating these two learning speeds, the model can maintain consistency across longer video sequences and improve action-driven video generation. The authors demonstrate that their method outperforms existing models in various metrics, particularly in generating coherent long videos.'}, 'zh': {'title': '双速学习，生成更长视频！', 'desc': '本文提出了一种名为SlowFast-VGen的新型双速学习系统，用于生成基于动作的长视频。该系统结合了慢学习和快学习的策略，慢学习通过掩蔽条件视频扩散模型来捕捉世界动态，而快学习则利用时间LoRA模块在推理时更新参数，以高效存储情节记忆。通过引入慢-快学习循环算法，系统能够在长视频生成中保持一致性，并有效回忆多次经历的上下文信息。实验结果表明，SlowFast-VGen在多个指标上优于基线模型，特别是在长时间规划任务中表现显著提升。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture (3)', '#audio', '#benchmark (5)', '#cv (2)', '#data (2)', '#dataset (4)', '#diffusion', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference', '#interpretability (3)', '#long_context', '#math', '#medicine (1)', '#multilingual', '#multimodal (1)', '#optimization (1)', '#plp', '#rag (1)', '#reasoning (1)', '#rl (1)', '#rlhf', '#robotics', '#security (1)', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#translation', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-31 20:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-31 20:12')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    