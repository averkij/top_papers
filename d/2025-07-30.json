{
    "date": {
        "ru": "30 Ğ¸ÑĞ»Ñ",
        "en": "July 30",
        "zh": "7æœˆ30æ—¥"
    },
    "time_utc": "2025-07-30 19:13",
    "weekday": 2,
    "issue_id": 5096,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.21809",
            "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D\n  Worlds from Words or Pixels",
            "url": "https://huggingface.co/papers/2507.21809",
            "abstract": "HunyuanWorld 1.0 generates immersive 3D scenes from text and images using a semantically layered 3D mesh representation with panoramic world proxies, offering 360Â° experiences, mesh export, and disentangled object representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating immersive and playable 3D worlds from texts or images remains a fundamental challenge in computer vision and graphics. Existing world generation approaches typically fall into two categories: video-based methods that offer rich diversity but lack 3D consistency and rendering efficiency, and 3D-based methods that provide geometric consistency but struggle with limited training data and memory-inefficient representations. To address these limitations, we present HunyuanWorld 1.0, a novel framework that combines the best of both worlds for generating immersive, explorable, and interactive 3D scenes from text and image conditions. Our approach features three key advantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2) mesh export capabilities for seamless compatibility with existing computer graphics pipelines; 3) disentangled object representations for augmented interactivity. The core of our framework is a semantically layered 3D mesh representation that leverages panoramic images as 360{\\deg} world proxies for semantic-aware world decomposition and reconstruction, enabling the generation of diverse 3D worlds. Extensive experiments demonstrate that our method achieves state-of-the-art performance in generating coherent, explorable, and interactive 3D worlds while enabling versatile applications in virtual reality, physical simulation, game development, and interactive content creation.",
            "score": 60,
            "issue_id": 5082,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "591a6aaf72265dcc",
            "authors": [
                "HunyuanWorld Team",
                "Zhenwei Wang",
                "Yuhao Liu",
                "Junta Wu",
                "Zixiao Gu",
                "Haoyuan Wang",
                "Xuhui Zuo",
                "Tianyu Huang",
                "Wenhuan Li",
                "Sheng Zhang",
                "Yihang Lian",
                "Yulin Tsai",
                "Lifu Wang",
                "Sicong Liu",
                "Puhua Jiang",
                "Xianghui Yang",
                "Dongyuan Guo",
                "Yixuan Tang",
                "Xinyue Mao",
                "Jiaao Yu",
                "Junlin Yu",
                "Jihong Zhang",
                "Meng Chen",
                "Liang Dong",
                "Yiwen Jia",
                "Chao Zhang",
                "Yonghao Tan",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Minghui Chen",
                "Zhan Li",
                "Wangchen Qin",
                "Lei Wang",
                "Yifu Sun",
                "Lin Niu",
                "Xiang Yuan",
                "Xiaofeng Yang",
                "Yingping He",
                "Jie Xiao",
                "Yangyu Tao",
                "Jianchen Zhu",
                "Jinbao Xue",
                "Kai Liu",
                "Chongqing Zhao",
                "Xinming Wu",
                "Tian Liu",
                "Peng Chen",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Tengfei Wang",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21809.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº 3D-Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "HunyuanWorld 1.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¸ÑÑ‚Ğ¾Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ğµ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. HunyuanWorld 1.0 ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾- Ğ¸ 3D-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Transforming Text and Images into Immersive 3D Worlds!",
                    "desc": "HunyuanWorld 1.0 is a framework that creates immersive 3D scenes from text and images by using a unique 3D mesh representation. It combines the strengths of video-based and 3D-based methods to overcome their limitations, providing both diversity and geometric consistency. The framework allows for 360Â° experiences, enables mesh export for compatibility with graphics tools, and offers disentangled object representations for enhanced interactivity. This innovative approach leads to the generation of coherent and explorable 3D worlds, making it useful for applications in virtual reality, game development, and more."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ²‰æµ¸å¼3Dä¸–ç•Œçš„æ–°æ–¹æ³•",
                    "desc": "HunyuanWorld 1.0 æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆæ²‰æµ¸å¼çš„3Dåœºæ™¯ã€‚å®ƒç»“åˆäº†è§†é¢‘å’Œ3Dæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæä¾›360åº¦çš„æ²‰æµ¸ä½“éªŒå’Œå¯å¯¼å‡ºçš„ç½‘æ ¼ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è¯­ä¹‰åˆ†å±‚çš„3Dç½‘æ ¼è¡¨ç¤ºï¼Œåˆ©ç”¨å…¨æ™¯å›¾åƒè¿›è¡Œä¸–ç•Œçš„åˆ†è§£å’Œé‡å»ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´ã€å¯æ¢ç´¢å’Œäº’åŠ¨çš„3Dä¸–ç•Œæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22058",
            "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again",
            "url": "https://huggingface.co/papers/2507.22058",
            "abstract": "Reinforcement learning enhances discrete autoregressive modeling for image and language generation, achieving high-quality image generation and instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts.",
            "score": 23,
            "issue_id": 5080,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "33e74a909f1e3165",
            "authors": [
                "Zigang Geng",
                "Yibing Wang",
                "Yeyao Ma",
                "Chen Li",
                "Yongming Rao",
                "Shuyang Gu",
                "Zhao Zhong",
                "Qinglin Lu",
                "Han Hu",
                "Xiaosong Zhang",
                "Linus",
                "Di Wang",
                "Jie Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22058.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#cv",
                    "#diffusion",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº X-Omni, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. X-Omni Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Reinforcement Learning Boosts Image and Language Generation Quality",
                    "desc": "This paper presents a novel approach that combines reinforcement learning with discrete autoregressive modeling to improve image and language generation. The authors address common issues in generating images, such as low quality and failure to follow complex instructions, which arise from errors in autoregressive inference and discretization. By introducing a framework called X-Omni, which includes a semantic image tokenizer and a unified model for both images and language, they enhance the quality of generated outputs. The results show that X-Omni achieves state-of-the-art performance in image generation, producing aesthetically pleasing images while effectively following detailed instructions."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡å›¾åƒä¸è¯­è¨€ç”Ÿæˆçš„è´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥æå‡ç¦»æ•£è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒå’Œè¯­è¨€ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„è‡ªå›å½’å»ºæ¨¡åœ¨ç”Ÿæˆå›¾åƒæ—¶å¸¸å¸¸é¢ä¸´è§†è§‰è´¨é‡ä½ã€è¾“å‡ºå¤±çœŸå’Œå¤æ‚æŒ‡ä»¤æ‰§è¡Œä¸ä½³ç­‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºX-Omniçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¼ªå½±ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚X-Omniç»“åˆäº†è¯­ä¹‰å›¾åƒæ ‡è®°å™¨ã€ç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹å’Œç¦»çº¿æ‰©æ•£è§£ç å™¨ï¼Œè¾¾åˆ°äº†å›¾åƒç”Ÿæˆä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21990",
            "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge",
            "url": "https://huggingface.co/papers/2507.21990",
            "abstract": "A Chemical Reasoner LLM, ChemDFM-R, enhances chemical reasoning through a comprehensive dataset, mix-sourced distillation, and domain-specific reinforcement learning, achieving state-of-the-art performance with interpretable outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.",
            "score": 21,
            "issue_id": 5095,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "f47520150715227f",
            "authors": [
                "Zihan Zhao",
                "Bo Chen",
                "Ziping Wan",
                "Lu Chen",
                "Xuanze Lin",
                "Shiyang Yu",
                "Situo Zhang",
                "Da Ma",
                "Zichen Zhu",
                "Danyang Zhang",
                "Huayang Wang",
                "Zhongyang Dai",
                "Liyang Wen",
                "Xin Chen",
                "Kai Yu"
            ],
            "affiliations": [
                "Jiangsu Key Lab of Language Computing, Suzhou 215123, China",
                "School of Chemistry and Chemical Engineering Shanghai Jiao Tong University, Shanghai 200240, China",
                "Suzhou Laboratory, Suzhou, China",
                "X-LANCE Lab, School of Computer Science MoE Key Lab of Artificial Intelligence, SJTU AI Institute Shanghai Jiao Tong University, Shanghai 200240, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21990.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#interpretability",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#data",
                    "#science"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ˜Ğ˜-Ñ…Ğ¸Ğ¼Ğ¸Ğº: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "ChemDFM-R - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ½Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ChemDFM-R Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Chemical Reasoning with ChemDFM-R",
                    "desc": "The paper introduces ChemDFM-R, a Chemical Reasoner LLM designed to improve reasoning in chemistry. It utilizes a comprehensive dataset that breaks down chemical knowledge into fundamental points, enhancing the model's understanding. The authors implement a mix-sourced distillation approach, combining expert knowledge with general reasoning skills, followed by reinforcement learning tailored for chemistry. The results show that ChemDFM-R not only achieves top performance on chemical benchmarks but also offers interpretable outputs, making it useful for real-world applications."
                },
                "zh": {
                    "title": "åŒ–å­¦æ¨ç†çš„æ™ºèƒ½æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŒ–å­¦æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸ºChemDFM-Rã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºå…¨é¢çš„çŸ¥è¯†ç‚¹æ•°æ®é›†ï¼Œå¢å¼ºäº†å¯¹åŒ–å­¦åŸºæœ¬åŸç†å’Œé€»è¾‘ç»“æ„çš„ç†è§£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ··åˆæ¥æºçš„è’¸é¦ç­–ç•¥ï¼Œå°†ä¸“å®¶çŸ¥è¯†ä¸é€šç”¨æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡é¢†åŸŸç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ æ¥æå‡åŒ–å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChemDFM-Råœ¨å¤šç§åŒ–å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šçš„æ¨ç†è¾“å‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14111",
            "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2507.14111",
            "abstract": "CUDA-L1, an automated reinforcement learning framework, significantly improves CUDA optimization across various GPU architectures, achieving substantial speedups without human expertise.  \t\t\t\t\tAI-generated summary \t\t\t\t The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.   CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.   The capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.",
            "score": 13,
            "issue_id": 5085,
            "pub_date": "2025-07-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ»Ñ",
                "en": "July 18",
                "zh": "7æœˆ18æ—¥"
            },
            "hash": "a4e1e59794f4a093",
            "authors": [
                "Xiaoya Li",
                "Xiaofei Sun",
                "Albert Wang",
                "Jiwei Li",
                "Chris Shum"
            ],
            "affiliations": [
                "DeepReinforce Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14111.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ CUDA Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "CUDA-L1 - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ CUDA-ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ CUDA-ÑĞ´ĞµÑ€ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… GPU-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°. CUDA-L1 Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ CUDA, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸ Ğ½ĞµĞ¾Ñ‡ĞµĞ²Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ»Ğ°Ğ±ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ CUDA Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing CUDA Optimization with Reinforcement Learning",
                    "desc": "CUDA-L1 is an innovative reinforcement learning framework designed to automate the optimization of CUDA code for GPU architectures. It significantly enhances performance by achieving an average speedup of 17.7 times across various CUDA kernels, with peak improvements reaching up to 449 times. The framework not only learns to apply different optimization techniques but also identifies performance bottlenecks and rejects ineffective strategies. This approach allows CUDA-L1 to generalize its optimization capabilities across multiple GPU models, showcasing the potential of reinforcement learning in improving GPU efficiency without requiring human intervention."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–CUDAä¼˜åŒ–ï¼Œæå‡GPUæ€§èƒ½çš„æœªæ¥",
                    "desc": "CUDA-L1æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºCUDAä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„GPUæ¶æ„ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨NVIDIA A100ä¸Šè®­ç»ƒï¼Œå¹³å‡å®ç°äº†17.7å€çš„é€Ÿåº¦æå‡ï¼Œæœ€é«˜å¯è¾¾449å€ã€‚CUDA-L1ä¸ä»…å‘ç°äº†å¤šç§CUDAä¼˜åŒ–æŠ€æœ¯ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°ç»„åˆè¿™äº›æŠ€æœ¯ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒCUDA-L1å±•ç¤ºäº†åœ¨æ²¡æœ‰äººç±»ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•å°†ä½æ•ˆçš„æ¨¡å‹è½¬å˜ä¸ºæœ‰æ•ˆçš„CUDAä¼˜åŒ–å™¨ï¼Œæå¤§åœ°æå‡äº†GPUçš„è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21183",
            "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
            "url": "https://huggingface.co/papers/2507.21183",
            "abstract": "MaPPO, a framework for preference optimization, enhances alignment of large language models with human preferences by integrating prior reward knowledge into a Maximum a Posteriori objective, improving performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.",
            "score": 7,
            "issue_id": 5080,
            "pub_date": "2025-07-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ»Ñ",
                "en": "July 27",
                "zh": "7æœˆ27æ—¥"
            },
            "hash": "e1328e80ce51ef74",
            "authors": [
                "Guangchen Lan",
                "Sipeng Zhang",
                "Tianle Wang",
                "Yuwei Zhang",
                "Daoan Zhang",
                "Xinpeng Wei",
                "Xiaoman Pan",
                "Hongming Zhang",
                "Dong-Jun Han",
                "Christopher G. Brinton"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Purdue University",
                "Tencent AI Lab",
                "University of California, San Diego",
                "University of Rochester",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21183.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MaPPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "MaPPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. MaPPO Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DPO, Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Model Alignment with MaPPO",
                    "desc": "MaPPO is a new framework designed to improve how large language models (LLMs) align with human preferences. It does this by using a Maximum a Posteriori (MaP) approach that incorporates prior reward knowledge into the optimization process. Unlike traditional methods that rely solely on Maximum Likelihood Estimation (MLE), MaPPO enhances preference learning by addressing the limitations of binary classification in response evaluation. The framework is flexible, requiring no extra hyperparameters, and shows significant performance improvements across various benchmarks while maintaining computational efficiency."
                },
                "zh": {
                    "title": "MaPPOï¼šä¼˜åŒ–äººç±»åå¥½çš„æ–°æ¡†æ¶",
                    "desc": "MaPPOæ˜¯ä¸€ç§åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ã€‚å®ƒé€šè¿‡å°†å…ˆå‰çš„å¥–åŠ±çŸ¥è¯†æ•´åˆåˆ°æœ€å¤§åéªŒç›®æ ‡ä¸­ï¼Œæ¥æé«˜æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚ä¸ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼ŒMaPPOé€šè¿‡å¼•å…¥å…ˆå‰çš„å¥–åŠ±ä¼°è®¡ï¼Œæ‰©å±•äº†åå¥½å­¦ä¹ çš„èŒƒå¼ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„è¶…å‚æ•°ï¼Œå¹¶æ”¯æŒç¦»çº¿å’Œåœ¨çº¿çš„åå¥½ä¼˜åŒ–ï¼Œèƒ½å¤Ÿä¸å¤šç§DPOå˜ä½“å…¼å®¹ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.20240",
            "title": "AnimalClue: Recognizing Animals by their Traces",
            "url": "https://huggingface.co/papers/2507.20240",
            "abstract": "AnimalClue is a large-scale dataset for species identification from indirect evidence images, addressing challenges in classification and segmentation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Wildlife observation plays an important role in biodiversity conservation, necessitating robust methodologies for monitoring wildlife populations and interspecies interactions. Recent advances in computer vision have significantly contributed to automating fundamental wildlife observation tasks, such as animal detection and species identification. However, accurately identifying species from indirect evidence like footprints and feces remains relatively underexplored, despite its importance in contributing to wildlife monitoring. To bridge this gap, we introduce AnimalClue, the first large-scale dataset for species identification from images of indirect evidence. Our dataset consists of 159,605 bounding boxes encompassing five categories of indirect clues: footprints, feces, eggs, bones, and feathers. It covers 968 species, 200 families, and 65 orders. Each image is annotated with species-level labels, bounding boxes or segmentation masks, and fine-grained trait information, including activity patterns and habitat preferences. Unlike existing datasets primarily focused on direct visual features (e.g., animal appearances), AnimalClue presents unique challenges for classification, detection, and instance segmentation tasks due to the need for recognizing more detailed and subtle visual features. In our experiments, we extensively evaluate representative vision models and identify key challenges in animal identification from their traces. Our dataset and code are available at https://dahlian00.github.io/AnimalCluePage/",
            "score": 5,
            "issue_id": 5081,
            "pub_date": "2025-07-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ»Ñ",
                "en": "July 27",
                "zh": "7æœˆ27æ—¥"
            },
            "hash": "e2c9ce4a31f42211",
            "authors": [
                "Risa Shinoda",
                "Nakamasa Inoue",
                "Iro Laina",
                "Christian Rupprecht",
                "Hirokatsu Kataoka"
            ],
            "affiliations": [
                "Kyoto University",
                "National Institute of Advanced Industrial Science and Technology (AIST)",
                "The University of Osaka",
                "Tokyo Institute of Technology",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.20240.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ¾",
                "ru": {
                    "title": "Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞ»ĞµĞ´Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "AnimalClue - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑĞ»ĞµĞ´Ñ‹, Ğ¿Ğ¾Ğ¼ĞµÑ‚, ÑĞ¹Ñ†Ğ°, ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑŒÑ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 159,605 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 968 Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ· 200 ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ¸ 65 Ğ¾Ñ‚Ñ€ÑĞ´Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ½Ğ°Ğ±Ğ¶ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². AnimalClue Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Wildlife Insights from Indirect Evidence",
                    "desc": "AnimalClue is a comprehensive dataset designed for identifying animal species from indirect evidence such as footprints and feces. It includes over 159,000 annotated images across five categories, covering a wide range of species and providing detailed trait information. This dataset addresses the challenges of classification and segmentation in computer vision, particularly for subtle visual features that are not present in direct animal images. By evaluating various vision models on this dataset, the research highlights the complexities involved in recognizing species from indirect clues, paving the way for improved wildlife monitoring techniques."
                },
                "zh": {
                    "title": "AnimalClueï¼šé—´æ¥è¯æ®å›¾åƒçš„ç‰©ç§è¯†åˆ«æ–°çªç ´",
                    "desc": "AnimalClueæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡é—´æ¥è¯æ®å›¾åƒè¿›è¡Œç‰©ç§è¯†åˆ«ï¼Œè§£å†³åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†åŒ…å«159,605ä¸ªè¾¹ç•Œæ¡†ï¼Œæ¶µç›–è¶³è¿¹ã€ç²ªä¾¿ã€è›‹ã€éª¨å¤´å’Œç¾½æ¯›äº”ç±»é—´æ¥çº¿ç´¢ï¼Œæ¶‰åŠ968ç§ç‰©ç§ã€‚æ¯å¼ å›¾åƒéƒ½æ ‡æ³¨äº†ç‰©ç§çº§åˆ«çš„æ ‡ç­¾ã€è¾¹ç•Œæ¡†æˆ–åˆ†å‰²æ©ç ï¼Œä»¥åŠç»†è‡´çš„ç‰¹å¾ä¿¡æ¯ï¼Œå¦‚æ´»åŠ¨æ¨¡å¼å’Œæ –æ¯åœ°åå¥½ã€‚ä¸ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨ç›´æ¥è§†è§‰ç‰¹å¾ä¸åŒï¼ŒAnimalClueåœ¨è¯†åˆ«æ›´ç»†è‡´å’Œå¾®å¦™çš„è§†è§‰ç‰¹å¾æ–¹é¢æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22061",
            "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
            "url": "https://huggingface.co/papers/2507.22061",
            "abstract": "A new dataset and baseline method for motion-guided few-shot video object segmentation are introduced, addressing challenges in motion understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction.",
            "score": 4,
            "issue_id": 5080,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "d205fb0c00f383de",
            "authors": [
                "Kaining Ying",
                "Hengrui Hu",
                "Henghui Ding"
            ],
            "affiliations": [
                "Fudan University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22061.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MOVE Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 6 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² 2 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ DMA Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Video Segmentation with Motion Understanding",
                    "desc": "This paper presents a new dataset called MOVE, aimed at improving motion-guided few-shot video object segmentation (FSVOS). Unlike previous datasets that focus on static object categories, MOVE emphasizes the importance of understanding motion dynamics in videos. The authors evaluate existing state-of-the-art methods and find that they struggle with motion-guided tasks, highlighting the need for better solutions. To address this, they propose a new baseline method called Decoupled Motion Appearance Network (DMA), which shows improved performance in segmenting dynamic objects with limited annotated examples."
                },
                "zh": {
                    "title": "è¿åŠ¨å¼•å¯¼çš„å°‘æ ·æœ¬è§†é¢‘åˆ†å‰²æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®é›†å’ŒåŸºçº¿æ–¹æ³•ï¼Œç”¨äºè¿åŠ¨å¼•å¯¼çš„å°‘æ ·æœ¬è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆFSVOSï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æ ¹æ®å°‘é‡å¸¦æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œåˆ†å‰²è§†é¢‘ä¸­çš„åŠ¨æ€ç‰©ä½“ï¼Œç‰¹åˆ«å…³æ³¨è¿åŠ¨æ¨¡å¼ã€‚ç°æœ‰çš„FSVOSæ•°æ®é›†å’Œæ–¹æ³•é€šå¸¸å¿½è§†è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦è¿åŠ¨ç†è§£çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºçš„MOVEæ•°æ®é›†å’Œè§£è€¦è¿åŠ¨å¤–è§‚ç½‘ç»œï¼ˆDMAï¼‰åŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å°‘æ ·æœ¬è¿åŠ¨ç†è§£çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21503",
            "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via\n  Unanswerable Visual Questions",
            "url": "https://huggingface.co/papers/2507.21503",
            "abstract": "A systematic assessment of honesty in Multimodal Large Language Models (MLLMs) using a large-scale benchmark reveals that models often fail to appropriately refuse unanswerable visual questions, highlighting the need for multimodal honesty alignment methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/DSTTSD/MoHoBench.",
            "score": 1,
            "issue_id": 5092,
            "pub_date": "2025-07-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ»Ñ",
                "en": "July 29",
                "zh": "7æœˆ29æ—¥"
            },
            "hash": "3496bcf2cb298334",
            "authors": [
                "Yanxu Zhu",
                "Shitong Duan",
                "Xiangxu Zhang",
                "Jitao Sang",
                "Peng Zhang",
                "Tun Lu",
                "Xiao Zhou",
                "Jing Yao",
                "Xiaoyuan Yi",
                "Xing Xie"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Fudan University",
                "Microsoft Research Asia",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21503.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ§ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ¿Ğ¾Ğ´ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MoHoBench Ñ 12 000+ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Honesty in Multimodal AI Models",
                    "desc": "This paper evaluates the honesty of Multimodal Large Language Models (MLLMs) when faced with visual questions that cannot be answered. It introduces MoHoBench, a benchmark with over 12,000 visual question samples, to systematically assess how well these models refuse to answer unanswerable questions. The study finds that many MLLMs struggle to decline answering when they should, indicating a gap in their honesty capabilities. The authors propose initial alignment methods to enhance the honesty of MLLMs, emphasizing the need for tailored approaches that consider both language and visual inputs."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¯šå®æ€§",
                    "desc": "æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢å¯¹æ— æ³•å›ç­”çš„è§†è§‰é—®é¢˜æ—¶çš„è¯šå®æ€§ã€‚æˆ‘ä»¬å®šä¹‰äº†å››ç§ä»£è¡¨æ€§çš„æ— æ³•å›ç­”é—®é¢˜ç±»å‹ï¼Œå¹¶æ„å»ºäº†MoHoBenchï¼Œä¸€ä¸ªåŒ…å«è¶…è¿‡12000ä¸ªè§†è§‰é—®é¢˜æ ·æœ¬çš„å¤§è§„æ¨¡åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨å¿…è¦æ—¶æœªèƒ½é€‚å½“åœ°æ‹’ç»å›ç­”ï¼Œè¿™è¡¨æ˜å¤šæ¨¡æ€è¯šå®æ€§å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®æ–½äº†åˆæ­¥çš„å¯¹é½æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„è¯šå®è¡Œä¸ºï¼Œä¸ºæœªæ¥çš„å¯ä¿¡MMLMsç ”ç©¶å¥ å®šåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.21364",
            "title": "Evaluating Deep Learning Models for African Wildlife Image\n  Classification: From DenseNet to Vision Transformers",
            "url": "https://huggingface.co/papers/2507.21364",
            "abstract": "A comparative study of deep learning models for wildlife image classification highlights trade-offs between accuracy, resource requirements, and deployability, with DenseNet-201 and Vision Transformer ViT-H/14 performing best among evaluated models.  \t\t\t\t\tAI-generated summary \t\t\t\t Wildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as a promising tool for biodiversity monitoring and conservation. This paper presents a comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using a public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to African-grounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation.",
            "score": 1,
            "issue_id": 5080,
            "pub_date": "2025-07-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ»Ñ",
                "en": "July 28",
                "zh": "7æœˆ28æ—¥"
            },
            "hash": "906cbe0bf6d751e4",
            "authors": [
                "Lukman Jibril Aliyu",
                "Umar Sani Muhammad",
                "Bilqisu Ismail",
                "Nasiru Muhammad",
                "Almustapha A Wakili",
                "Seid Muhie Yimam",
                "Shamsuddeen Hassan Muhammad",
                "Mustapha Abdullahi"
            ],
            "affiliations": [
                "Arewa Data Science Academy Kano, Nigeria",
                "Azman University Kano, Nigeria",
                "Imperial College London London, United Kingdom",
                "Towson University Maryland, USA",
                "Universitat Hamburg Hamburg, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.21364.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#cv",
                    "#transfer_learning",
                    "#science"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¹ Ñ„Ğ°ÑƒĞ½Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¸ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ğ² ĞÑ„Ñ€Ğ¸ĞºĞµ. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ DenseNet-201, ResNet-152, EfficientNet-B4 Ğ¸ Vision Transformer ViT-H/14 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ…. ViT-H/14 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (99%), Ğ½Ğ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº DenseNet-201 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Balancing Accuracy and Deployability in Wildlife Image Classification",
                    "desc": "This paper compares different deep learning models for classifying images of African wildlife, focusing on their accuracy, resource needs, and how easily they can be deployed. The study evaluates models like DenseNet-201 and Vision Transformer ViT-H/14, finding that DenseNet-201 offers good accuracy with lower resource requirements, while ViT-H/14 achieves higher accuracy but is more resource-intensive. The research emphasizes the importance of balancing model performance with practical deployment considerations in conservation efforts. Ultimately, the findings aim to guide the selection and use of deep learning models for effective wildlife monitoring and protection."
                },
                "zh": {
                    "title": "æ·±åº¦å­¦ä¹ åŠ©åŠ›é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤çš„æœ€ä½³é€‰æ‹©",
                    "desc": "æœ¬ç ”ç©¶æ¯”è¾ƒäº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é‡ç”ŸåŠ¨ç‰©å›¾åƒåˆ†ç±»ä¸­çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨å‡†ç¡®æ€§ã€èµ„æºéœ€æ±‚å’Œå¯éƒ¨ç½²æ€§ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«æ°´ç‰›ã€å¤§è±¡ã€çŠ€ç‰›å’Œæ–‘é©¬çš„å…¬å…±æ•°æ®é›†ï¼Œè¯„ä¼°äº†DenseNet-201ã€ResNet-152ã€EfficientNet-B4å’ŒVision Transformer ViT-H/14çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDenseNet-201åœ¨å·ç§¯ç½‘ç»œä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º67%ï¼Œè€ŒViT-H/14çš„æ•´ä½“å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°99%ï¼Œä½†è®¡ç®—æˆæœ¬æ˜¾è‘—æ›´é«˜ï¼Œå½±å“äº†å…¶éƒ¨ç½²ã€‚è¯¥ç ”ç©¶ä¸ºé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤æä¾›äº†æ·±åº¦å­¦ä¹ å·¥å…·çš„æ¨¡å‹é€‰æ‹©ã€æ•°æ®é›†å‡†å¤‡å’Œè´Ÿè´£ä»»éƒ¨ç½²çš„å®ç”¨è§è§£ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-29.html",
    "link_next": "2025-07-31.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "29.07",
        "en": "07/29",
        "zh": "7æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "31.07",
        "en": "07/31",
        "zh": "7æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}