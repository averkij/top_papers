
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. August 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 августа</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-08.html">⬅️ <span id="prev-date">08.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-12.html">➡️ <span id="next-date">12.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'};
        let feedDateNext = {'ru': '12.08', 'en': '08/12', 'zh': '8月12日'};
        let feedDatePrev = {'ru': '08.08', 'en': '08/08', 'zh': '8月8日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.06471', 'title': 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models', 'url': 'https://huggingface.co/papers/2508.06471', 'abstract': 'GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.', 'score': 70, 'issue_id': 5273, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'fdf71e495fcd6efa', 'authors': ['GLM-4. 5 Team', ':', 'Aohan Zeng', 'Xin Lv', 'Qinkai Zheng', 'Zhenyu Hou', 'Bin Chen', 'Chengxing Xie', 'Cunxiang Wang', 'Da Yin', 'Hao Zeng', 'Jiajie Zhang', 'Kedong Wang', 'Lucen Zhong', 'Mingdao Liu', 'Rui Lu', 'Shulin Cao', 'Xiaohan Zhang', 'Xuancheng Huang', 'Yao Wei', 'Yean Cheng', 'Yifan An', 'Yilin Niu', 'Yuanhao Wen', 'Yushi Bai', 'Zhengxiao Du', 'Zihan Wang', 'Zilin Zhu', 'Bohan Zhang', 'Bosi Wen', 'Bowen Wu', 'Bowen Xu', 'Can Huang', 'Casey Zhao', 'Changpeng Cai', 'Chao Yu', 'Chen Li', 'Chendi Ge', 'Chenghua Huang', 'Chenhui Zhang', 'Chenxi Xu', 'Chenzheng Zhu', 'Chuang Li', 'Congfeng Yin', 'Daoyan Lin', 'Dayong Yang', 'Dazhi Jiang', 'Ding Ai', 'Erle Zhu', 'Fei Wang', 'Gengzheng Pan', 'Guo Wang', 'Hailong Sun', 'Haitao Li', 'Haiyang Li', 'Haiyi Hu', 'Hanyu Zhang', 'Hao Peng', 'Hao Tai', 'Haoke Zhang', 'Haoran Wang', 'Haoyu Yang', 'He Liu', 'He Zhao', 'Hongwei Liu', 'Hongxi Yan', 'Huan Liu', 'Huilong Chen', 'Ji Li', 'Jiajing Zhao', 'Jiamin Ren', 'Jian Jiao', 'Jiani Zhao', 'Jianyang Yan', 'Jiaqi Wang', 'Jiayi Gui', 'Jiayue Zhao', 'Jie Liu', 'Jijie Li', 'Jing Li', 'Jing Lu', 'Jingsen Wang', 'Jingwei Yuan', 'Jingxuan Li', 'Jingzhao Du', 'Jinhua Du', 'Jinxin Liu', 'Junkai Zhi', 'Junli Gao', 'Ke Wang', 'Lekang Yang', 'Liang Xu', 'Lin Fan', 'Lindong Wu', 'Lintao Ding', 'Lu Wang', 'Man Zhang', 'Minghao Li', 'Minghuan Xu', 'Mingming Zhao', 'Mingshu Zhai', 'Pengfan Du', 'Qian Dong', 'Shangde Lei', 'Shangqing Tu', 'Shangtong Yang', 'Shaoyou Lu', 'Shijie Li', 'Shuang Li', 'Shuang-Li', 'Shuxun Yang', 'Sibo Yi', 'Tianshu Yu', 'Wei Tian', 'Weihan Wang', 'Wenbo Yu', 'Weng Lam Tam', 'Wenjie Liang', 'Wentao Liu', 'Xiao Wang', 'Xiaohan Jia', 'Xiaotao Gu', 'Xiaoying Ling', 'Xin Wang', 'Xing Fan', 'Xingru Pan', 'Xinyuan Zhang', 'Xinze Zhang', 'Xiuqing Fu', 'Xunkai Zhang', 'Yabo Xu', 'Yandong Wu', 'Yida Lu', 'Yidong Wang', 'Yilin Zhou', 'Yiming Pan', 'Ying Zhang', 'Yingli Wang', 'Yingru Li', 'Yinpei Su', 'Yipeng Geng', 'Yitong Zhu', 'Yongkun Yang', 'Yuhang Li', 'Yuhao Wu', 'Yujiang Li', 'Yunan Liu', 'Yunqing Wang', 'Yuntao Li', 'Yuxuan Zhang', 'Zezhen Liu', 'Zhen Yang', 'Zhengda Zhou', 'Zhongpei Qiao', 'Zhuoer Feng', 'Zhuorui Liu', 'Zichen Zhang', 'Zihan Wang', 'Zijun Yao', 'Zikang Wang', 'Ziqiang Liu', 'Ziwei Chai', 'Zixuan Li', 'Zuodong Zhao', 'Wenguang Chen', 'Jidong Zhai', 'Bin Xu', 'Minlie Huang', 'Hongning Wang', 'Juanzi Li', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.06471.jpg', 'data': {'categories': ['#agents', '#rl', '#open_source', '#reasoning', '#architecture', '#agi', '#training'], 'emoji': '🧠', 'ru': {'title': 'GLM-4.5: Мощная MoE-модель для рассуждений и агентного ИИ', 'desc': 'GLM-4.5 - это крупномасштабная языковая модель с архитектурой Mixture-of-Experts, содержащая 355 миллиардов параметров. Модель прошла многоэтапное обучение на 23 триллионах токенов и дополнительную постобработку с использованием экспертной итерации и обучения с подкреплением. GLM-4.5 демонстрирует высокую производительность в задачах рассуждения, агентного взаимодействия и программирования, превосходя многие конкурирующие модели с большим количеством параметров. Исследователи выпустили как полную версию GLM-4.5, так и компактную GLM-4.5-Air для продвижения исследований в области рассуждающих и агентных систем искусственного интеллекта.'}, 'en': {'title': 'GLM-4.5: Powerful Reasoning with Fewer Parameters', 'desc': 'GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors.'}, 'zh': {'title': 'GLM-4.5：强大的混合专家语言模型', 'desc': 'GLM-4.5是一种具有3550亿参数的混合专家大型语言模型，表现出色，特别是在代理、推理和编码任务上。它采用多阶段训练和强化学习的方法，经过23万亿个标记的训练，提升了模型的性能。GLM-4.5在多个基准测试中取得了优异的成绩，尤其在代理基准中排名第二。该模型的开源版本和紧凑版（GLM-4.5-Air）都已发布，旨在推动推理和代理人工智能系统的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.04825', 'title': 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off', 'url': 'https://huggingface.co/papers/2508.04825', 'abstract': 'Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.', 'score': 33, 'issue_id': 5272, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '085e45094380ed54', 'authors': ['Seungyong Lee', 'Jeong-gi Kwak'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.04825.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#inference', '#architecture', '#benchmark', '#multimodal'], 'emoji': '👚', 'ru': {'title': 'Voost: революция в виртуальной примерке одежды', 'desc': 'Voost - это унифицированная система на основе диффузионного трансформера, которая совместно обучается виртуальной примерке одежды и ее снятию. Она улучшает соответствие между одеждой и телом человека, используя двунаправленную согласованность между задачами. Voost поддерживает гибкое управление направлением генерации и категорией одежды без специфичных для задачи сетей. Система достигает лучших результатов на различных тестовых наборах данных, превосходя существующие методы по точности сопоставления и визуальному качеству.'}, 'en': {'title': 'Voost: Revolutionizing Virtual Try-On with Unified Learning', 'desc': 'Voost is a new framework that uses a diffusion transformer to improve virtual try-on and try-off tasks in fashion technology. It learns to create realistic images of people wearing clothes by understanding how garments fit the body, even when poses and appearances change. By training both tasks together, Voost enhances the relationship between garments and bodies without needing extra networks or labels. The framework also includes innovative techniques to improve performance during image generation, leading to top results in accuracy and visual quality across various benchmarks.'}, 'zh': {'title': 'Voost：虚拟试穿与试脱的统一框架', 'desc': 'Voost是一个统一的扩散变换器框架，能够同时学习虚拟试穿和试脱。它通过联合建模这两个任务，增强了服装与身体之间的对应关系，解决了在姿势和外观变化下的挑战。该框架支持灵活的生成方向和服装类别，避免了特定任务的网络和额外标签。实验结果表明，Voost在试穿和试脱的基准测试中均取得了最先进的成果。'}}}, {'id': 'https://huggingface.co/papers/2508.05731', 'title': 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2508.05731', 'abstract': 'Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.', 'score': 16, 'issue_id': 5272, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '31f17f5fb142d3e8', 'authors': ['Yuhang Liu', 'Zeyu Liu', 'Shuanghe Zhu', 'Pengxiang Li', 'Congkai Xie', 'Jiasheng Wang', 'Xueyu Hu', 'Xiaotian Han', 'Jianbo Yuan', 'Xinyao Wang', 'Shengyu Zhang', 'Hongxia Yang', 'Fei Wu'], 'affiliations': ['Amazon', 'InfiX.ai', 'The Hong Kong Polytechnic University', 'The University of Chicago', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05731.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rl', '#benchmark', '#agents', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'AEPO: Прорыв в семантическом выравнивании для мультимодальных ИИ-агентов', 'desc': 'Статья представляет новый метод оптимизации политики адаптивного исследования (AEPO) для улучшения семантического выравнивания в мультимодальных больших языковых моделях (MLLM) при взаимодействии с графическим интерфейсом пользователя. AEPO использует стратегию генерации множественных ответов для обеспечения более широкого исследования, которое затем направляется теоретически обоснованной функцией вознаграждения адаптивного исследования (AER). Модели, обученные с помощью AEPO, достигают значительных улучшений до 9.0% по сравнению с базовым методом RLVR на тестах, оценивающих обобщение и семантическое понимание. Этот подход решает проблему неэффективного исследования, которая ограничивает семантическое выравнивание в существующих методах.'}, 'en': {'title': 'Enhancing GUI Interaction with Adaptive Exploration in MLLMs', 'desc': 'Adaptive Exploration Policy Optimization (AEPO) is a novel framework designed to enhance semantic alignment in Multimodal Large Language Models (MLLMs) for effective interaction with Graphical User Interfaces (GUIs). The paper identifies that while existing methods like Reinforcement Learning with Verifiable Rewards (RLVR) improve spatial alignment, they struggle with semantic alignment due to inefficient exploration strategies. AEPO addresses this by implementing a multi-answer generation approach that encourages broader exploration, guided by an Adaptive Exploration Reward (AER) function. As a result, models trained with AEPO demonstrate significant performance improvements, achieving state-of-the-art results on various GUI grounding benchmarks.'}, 'zh': {'title': '自适应探索，提升语义对齐！', 'desc': '自适应探索策略优化（AEPO）通过增强多模态大型语言模型（MLLMs）在图形用户界面（GUI）交互中的语义对齐，显著提升了模型的性能。该方法解决了自然语言指令与UI元素之间的语义对齐问题，克服了传统强化学习方法在探索效率上的瓶颈。AEPO采用多答案生成策略，结合理论基础的自适应探索奖励函数，促进了更广泛的探索。经过AEPO训练的模型在多个GUI基准测试中取得了最高9.0%的相对提升，展示了其在语义理解和泛化能力上的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.06433', 'title': 'Memp: Exploring Agent Procedural Memory', 'url': 'https://huggingface.co/papers/2508.06433', 'abstract': 'Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.', 'score': 15, 'issue_id': 5279, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '28c51ac0d694bc54', 'authors': ['Runnan Fang', 'Yuan Liang', 'Xiaobin Wang', 'Jialong Wu', 'Shuofei Qiao', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06433.jpg', 'data': {'categories': ['#optimization', '#agi', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Memp: умная память для интеллектуальных агентов', 'desc': 'В этой статье представлена система Memp - обучаемая и обновляемая процедурная память для агентов на основе больших языковых моделей (LLM). Memp извлекает из прошлого опыта агентов как детальные пошаговые инструкции, так и абстракции более высокого уровня. Система динамически обновляет, исправляет и удаляет устаревшую информацию в своем хранилище. Эксперименты показывают, что использование Memp позволяет агентам достигать более высоких показателей успешности и эффективности при выполнении аналогичных задач.'}, 'en': {'title': 'Empowering Agents with Evolving Procedural Memory', 'desc': 'This paper introduces Memp, a learnable and updatable procedural memory system for agents that enhances their performance and efficiency in various tasks. Memp allows agents to distill their past experiences into detailed instructions and higher-level abstractions, addressing the limitations of traditional static procedural memory. The authors explore different strategies for building, retrieving, and updating this memory, ensuring it evolves with new experiences. Empirical results demonstrate that agents using Memp show improved success rates and efficiency, even when transferring memory from a stronger model to a weaker one.'}, 'zh': {'title': '智能体的可学习程序记忆提升任务表现', 'desc': '本文提出了一种可学习和可更新的程序记忆系统Memp，旨在提高智能体在任务中的表现和效率。Memp通过将过去的经验提炼为详细的指令和更高层次的抽象，帮助智能体更好地执行任务。研究探讨了程序记忆的构建、检索和更新策略，并展示了动态更新机制如何使记忆库随着新经验不断演变。实验证明，随着记忆库的优化，智能体在类似任务上的成功率和效率显著提高。'}}}, {'id': 'https://huggingface.co/papers/2508.05988', 'title': 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal', 'url': 'https://huggingface.co/papers/2508.05988', 'abstract': 'ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs.', 'score': 12, 'issue_id': 5272, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '82ad3c225d1615aa', 'authors': ['Wenhao Zeng', 'Yaoning Wang', 'Chao Hu', 'Yuling Shi', 'Chengcheng Wan', 'Hongyu Zhang', 'Xiaodong Gu'], 'affiliations': ['Chongqing University', 'East China Normal University', 'Fudan University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05988.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие рассуждений для улучшения работы с кодом', 'desc': 'ASAP - это новый фреймворк для сжатия цепочек рассуждений (Chain-of-Thought) в задачах рассуждения о коде. Он использует двухэтапный подход: сначала сохраняет основную структуру рассуждений, а затем выбирает логически важные шаги на основе метрики неожиданности первого токена. ASAP позволяет моделям автономно генерировать и использовать сжатые цепочки рассуждений во время вывода. Эксперименты показывают, что ASAP достигает современного уровня точности при значительном снижении затрат на обучение и вывод.'}, 'en': {'title': 'Efficient Code Reasoning with ASAP: Smart Compression for Better Performance', 'desc': 'The paper introduces ASAP, a new framework designed to compress Chain-of-Thought (CoT) in code reasoning while maintaining essential logical structures. It addresses the challenges posed by long reasoning traces, which can increase training costs and slow down inference. ASAP uses anchor-guided pruning to focus on core reasoning elements and applies a novel first-token surprisal metric to identify critical reasoning steps. The results demonstrate that ASAP not only improves efficiency by reducing token generation and inference latency but also maintains high accuracy in code generation tasks.'}, 'zh': {'title': 'ASAP：高效的代码推理思维链压缩框架', 'desc': '本文提出了一种名为ASAP的新的粗到细框架，用于压缩代码推理中的思维链（CoT），旨在保留核心结构和关键步骤，从而降低成本并提高效率。ASAP首先通过锚点引导修剪来保留核心推理结构，减少后续处理的搜索空间。接着，它基于新颖的首个令牌惊讶度指标，选择逻辑上重要的推理步骤进行逻辑感知修剪。实验结果表明，ASAP在多个代码生成基准上实现了最先进的准确性，同时显著降低了训练和推理成本。'}}}, {'id': 'https://huggingface.co/papers/2508.02831', 'title': 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing', 'url': 'https://huggingface.co/papers/2508.02831', 'abstract': "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)", 'score': 6, 'issue_id': 5276, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '95f1bf82d6941705', 'authors': ['Mikołaj Zieliński', 'Krzysztof Byrski', 'Tomasz Szczepanik', 'Przemysław Spurek'], 'affiliations': ['IDEAS Research Institute', 'Jagiellonian University, Faculty of Mathematics and Computer Science, Łojasiewicza 6, 30-348, Krakow, Poland', 'Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2508.02831.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🎨', 'ru': {'title': 'GENIE: Объединение NeRF и Gaussian Splatting для интерактивного редактирования 3D-сцен', 'desc': 'GENIE - это гибридная модель, объединяющая фотореалистичный рендеринг NeRF с редактируемым и структурированным представлением Gaussian Splatting. Вместо сферических гармоник для моделирования внешнего вида каждому гауссиану присваивается обучаемое векторное представление. Эти представления используются для обусловливания сети NeRF на основе k ближайших гауссианов к каждой точке запроса. Модель поддерживает интуитивно понятные манипуляции со сценой, динамическое взаимодействие и совместимость с физическим моделированием.'}, 'en': {'title': 'GENIE: Real-Time 3D Scene Editing with Photorealism and Intuition', 'desc': 'GENIE is a new model that merges the strengths of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) for 3D scene rendering and editing. It allows for photorealistic images while also enabling real-time, interactive editing through an explicit representation of scenes using Gaussian primitives. By introducing a method called Ray-Traced Gaussian Proximity Search (RT-GPS), GENIE efficiently finds the nearest Gaussians for each point, making editing faster and more intuitive. This combination supports dynamic interactions and physical simulations, making it easier to manipulate 3D scenes in a realistic way.'}, 'zh': {'title': 'GENIE：实时可编辑的3D场景渲染新方法', 'desc': 'GENIE是一种结合了NeRF的高质量光线渲染和Gaussian Splatting的可编辑结构表示的混合模型。它通过为每个高斯分布分配可训练的特征嵌入，来实现实时的局部感知编辑。GENIE还引入了基于光线追踪的高斯邻近搜索（RT-GPS），提高了高斯搜索的效率。通过这些创新，GENIE支持直观的场景操作和与物理模拟的兼容性，弥合了几何编辑与神经渲染之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2508.05547', 'title': 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2508.05547', 'abstract': 'A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.', 'score': 5, 'issue_id': 5274, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '9c2f43e8f72ea22d', 'authors': ['Hao Dong', 'Lijun Sheng', 'Jian Liang', 'Ran He', 'Eleni Chatzi', 'Olga Fink'], 'affiliations': ['EPFL, Switzerland', 'ETH Zurich, Switzerland', 'Institute of Automation, Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.05547.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#survey', '#transfer_learning', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Систематизация методов адаптации мультимодальных моделей без разметки', 'desc': 'Это обзор методов адаптации мультимодальных моделей (зрение + язык) без учителя. Авторы предлагают таксономию, основанную на доступности и характере немаркированных визуальных данных, выделяя четыре ключевые парадигмы. В статье анализируются основные методологии и стратегии адаптации для каждой парадигмы. Также рассматриваются репрезентативные бенчмарки и выделяются открытые проблемы и перспективные направления для будущих исследований.'}, 'en': {'title': 'Unlocking Vision-Language Models: A Guide to Unsupervised Adaptation', 'desc': 'This paper provides a detailed survey of unsupervised adaptation methods for Vision-Language Models (VLMs), which are designed to improve model performance without the need for labeled data. It categorizes these methods into four main types based on the availability of unlabeled visual data: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation. The authors analyze various methodologies and strategies within this framework, aiming to create a clearer understanding of how to effectively adapt VLMs to specific tasks. Furthermore, the paper discusses existing benchmarks and identifies future research opportunities in the field of unsupervised VLM adaptation.'}, 'zh': {'title': '无监督适应：提升视觉-语言模型的潜力', 'desc': '本文对视觉-语言模型（VLMs）在无监督适应方法方面进行了全面的调查。研究将现有方法根据无标签视觉数据的可用性进行分类，提出了四种主要范式：无数据迁移、无监督领域迁移、情景测试时适应和在线测试时适应。文章分析了每种范式的核心方法和适应策略，并回顾了不同应用中的代表性基准。最后，指出了未来研究的开放挑战和有前景的方向。'}}}, {'id': 'https://huggingface.co/papers/2508.05502', 'title': 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs', 'url': 'https://huggingface.co/papers/2508.05502', 'abstract': 'MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.', 'score': 4, 'issue_id': 5272, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'b7b633c31ed6e35e', 'authors': ['Yufei Gao', 'Jiaying Fei', 'Nuo Chen', 'Ruirui Chen', 'Guohang Yan', 'Yunshi Lan', 'Botian Shi'], 'affiliations': ['East China Normal University', 'Institute of High Performance Computing, A*STAR', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.05502.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#multilingual', '#training', '#data', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'MELLA: Расширение горизонтов MLLM для низкоресурсных языков', 'desc': 'Датасет MELLA представляет собой многомодальный многоязычный набор данных, направленный на улучшение работы мультимодальных больших языковых моделей (MLLM) для низкоресурсных языков. MELLA использует двойной подход, сочетая альтернативный текст с веб-страниц и подписи, сгенерированные MLLM, для улучшения лингвистических возможностей и культурной осведомленности моделей. Эксперименты показали общее улучшение производительности для восьми языков на различных архитектурах MLLM после дообучения на MELLA. Результаты подтверждают, что улучшения связаны как с расширением культурных знаний, так и с повышением лингвистических способностей моделей.'}, 'en': {'title': 'Enhancing MLLMs for Low-Resource Languages with MELLA', 'desc': 'This paper introduces MELLA, a new dataset designed to improve Multimodal Large Language Models (MLLMs) for low-resource languages. It focuses on enhancing both linguistic capabilities and cultural groundedness by using native web alt-text and MLLM-generated captions. The study highlights the limitations of existing methods that rely solely on text or machine translation, which often fail to provide rich, informative content. By fine-tuning MLLMs on MELLA, the results show significant performance improvements across multiple languages, enabling models to generate more detailed and culturally aware descriptions.'}, 'zh': {'title': '提升低资源语言的多模态语言模型', 'desc': 'MELLA是一个多模态、多语言的数据集，旨在提升低资源语言中的多模态大型语言模型（MLLM）的语言能力和文化基础。该研究提出了一种双源策略，通过收集本地网页的替代文本和MLLM生成的标题，来实现语言能力和文化基础的双重目标。实验结果表明，在MELLA上进行微调后，八种语言的模型在多种MLLM基础上普遍提高了性能，能够生成更丰富的描述。我们的数据集强调了文化知识和语言能力的增强，适用于低资源语言用户。'}}}, {'id': 'https://huggingface.co/papers/2508.01242', 'title': 'MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh', 'url': 'https://huggingface.co/papers/2508.01242', 'abstract': "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50 times larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.", 'score': 3, 'issue_id': 5274, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '425226c54ca88a3a', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Yufeng Wang', 'Yi-Hsuan Tsai', 'Yi Yang', 'Shuchang Zhou', 'Wenrui Ding', 'Takeo Igarashi', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Beihang University', 'StepFun Inc.', 'The University of Tokyo', 'UC Merced'], 'pdf_title_img': 'assets/pdf/title_img/2508.01242.jpg', 'data': {'categories': ['#games', '#dataset', '#optimization', '#3d', '#multimodal'], 'emoji': '🧊', 'ru': {'title': 'MeshLLM: Революция в обработке 3D-моделей с помощью языковых моделей', 'desc': 'MeshLLM - это новая система, использующая большие языковые модели для понимания и генерации 3D-моделей в текстовом формате. Она решает проблемы ограниченного масштаба данных и потери структурной информации при сериализации моделей. MeshLLM использует стратегию декомпозиции на примитивы, что позволяет создать большой набор данных из 1500000+ образцов. Система также предлагает новые методы вывода связности граней и локальной сборки моделей, улучшая способность языковых моделей работать с топологией и пространственными структурами.'}, 'en': {'title': 'Revolutionizing 3D Mesh Generation with Language Models', 'desc': "MeshLLM is a new framework that uses large language models to generate and interpret 3D meshes represented as text. It improves upon previous methods by introducing a Primitive-Mesh decomposition strategy, which breaks down 3D meshes into meaningful parts, allowing for a much larger dataset of over 1.5 million samples. This approach helps maintain important 3D structural information that is often lost in serialization. Additionally, MeshLLM enhances the model's understanding of mesh topology and spatial relationships through innovative training techniques, leading to superior performance in generating and understanding 3D shapes compared to existing models."}, 'zh': {'title': 'MeshLLM：重塑3D网格生成与理解的未来', 'desc': 'MeshLLM是一个新颖的框架，利用大型语言模型（LLM）来理解和生成文本序列化的3D网格。该方法通过引入原始网格分解策略，将3D网格分解为结构上有意义的子单元，从而解决了现有方法在数据集规模和3D结构信息损失方面的关键限制。我们创建了一个超过150万样本的大规模数据集，几乎是之前方法的50倍，更好地符合LLM的扩展法则。此外，MeshLLM通过推断顶点之间的面连接性和局部网格组装训练策略，显著提升了LLM捕捉网格拓扑和空间结构的能力。'}}}, {'id': 'https://huggingface.co/papers/2507.22025', 'title': 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding', 'url': 'https://huggingface.co/papers/2507.22025', 'abstract': 'UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.', 'score': 2, 'issue_id': 5274, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': 'f7ff8c2c5517f5de', 'authors': ['Shuquan Lian', 'Yuhang Wu', 'Jia Ma', 'Zihan Song', 'Bingqi Chen', 'Xiawu Zheng', 'Hui Li'], 'affiliations': ['Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22025.jpg', 'data': {'categories': ['#games', '#cv', '#optimization', '#benchmark', '#training', '#reasoning', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'UI-AGILE: Революция в обучении и выводе агентов GUI', 'desc': 'В статье представлена новая система UI-AGILE, которая улучшает работу агентов GUI с помощью усовершенствованных методов обучения и вывода. Для обучения предлагаются три новшества: функция непрерывного вознаграждения, вознаграждение за простое мышление и стратегия ресемплинга на основе обрезки. Для вывода используется метод разложенного обоснования с выбором, который повышает точность на высоких разрешениях. Эксперименты показывают, что UI-AGILE достигает передовых результатов на бенчмарках ScreenSpot-Pro и ScreenSpot-v2.'}, 'en': {'title': 'Revolutionizing GUI Agents with UI-AGILE', 'desc': 'UI-AGILE is a framework designed to improve the performance of Graphical User Interface (GUI) agents by enhancing their training and inference processes. It introduces a Continuous Reward function to encourage precise grounding, a Simple Thinking reward to optimize the balance between planning speed and accuracy, and a Cropping-based Resampling technique to address sparse rewards in complex tasks. For inference, it employs Decomposed Grounding with Selection, which enhances grounding accuracy by breaking down images into smaller sections for better processing. The framework has demonstrated state-of-the-art results on GUI benchmarks, significantly improving grounding accuracy compared to existing methods.'}, 'zh': {'title': 'UI-AGILE：提升GUI代理的智能训练与推理', 'desc': 'UI-AGILE是一个增强图形用户界面（GUI）代理的框架，通过改进训练和推理过程来提升其性能。在训练阶段，UI-AGILE引入了连续奖励函数、简单思维奖励和基于裁剪的重采样策略，以提高高精度的基础定位能力和学习复杂任务的效果。在推理阶段，采用了分解基础定位与选择的方法，显著提高了高分辨率显示器上的基础定位准确性。实验结果表明，UI-AGILE在ScreenSpot-Pro和ScreenSpot-v2基准测试中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.06494', 'title': 'LightSwitch: Multi-view Relighting with Material-guided Diffusion', 'url': 'https://huggingface.co/papers/2508.06494', 'abstract': 'Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.', 'score': 1, 'issue_id': 5279, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'f69d154601aca623', 'authors': ['Yehonathan Litman', 'Fernando De la Torre', 'Shubham Tulsiani'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06494.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Революция в 3D-релайтинге: Lightswitch переопределяет границы возможного', 'desc': 'Lightswitch - это новая система для 3D-релайтинга, использующая диффузионные модели и учитывающая свойства материалов. Она эффективно перерисовывает множество входных изображений под целевое освещение, используя многоракурсную информацию и данные о материалах. Lightswitch превосходит существующие методы по качеству и скорости работы. Система особенно эффективна для релайтинга объектов с разнообразными материалами.'}, 'en': {'title': 'Revolutionizing 3D Relighting with Lightswitch', 'desc': 'Lightswitch is a new framework designed for improving 3D relighting by using both multi-view and material information. It addresses the limitations of previous methods that relied solely on 2D image relighting, which often failed to utilize the intrinsic properties of objects. By integrating these cues into a diffusion model, Lightswitch can efficiently relight multiple images to match a desired lighting condition. The results show that it not only enhances the quality of relighting but also does so faster than existing techniques.'}, 'zh': {'title': 'Lightswitch：高效的3D重光新方法', 'desc': 'Lightswitch是一种新型的材料重光框架，通过整合多视角和材料线索，显著提升了3D重光的质量和效率。该方法能够有效地将任意数量的输入图像重光到目标光照条件，同时考虑到物体的内在特性。与传统的2D图像重光方法相比，Lightswitch在处理多视角数据时表现出更好的效果。实验结果表明，Lightswitch在合成和真实物体的重光任务中，速度快且质量高，超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2508.04482', 'title': 'OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use', 'url': 'https://huggingface.co/papers/2508.04482', 'abstract': 'The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.', 'score': 1, 'issue_id': 5284, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '4f7eaba2df530414', 'authors': ['Xueyu Hu', 'Tao Xiong', 'Biao Yi', 'Zishu Wei', 'Ruixuan Xiao', 'Yurun Chen', 'Jiasheng Ye', 'Meiling Tao', 'Xiangxin Zhou', 'Ziyu Zhao', 'Yuhuai Li', 'Shengze Xu', 'Shenzhi Wang', 'Xinchen Xu', 'Shuofei Qiao', 'Zhaokai Wang', 'Kun Kuang', 'Tieyong Zeng', 'Liang Wang', 'Jiwei Li', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou', 'Guoyin Wang', 'Keting Yin', 'Zhou Zhao', 'Hongxia Yang', 'Fan Wu', 'Shengyu Zhang', 'Fei Wu'], 'affiliations': ['1.AI', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'OPPO AI Center', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04482.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'ОС-агенты: Шаг к реальности искусственных помощников уровня Д.Ж.А.Р.В.И.С.', 'desc': 'Эта статья представляет собой обзор агентов на основе больших языковых моделей, способных взаимодействовать с операционными системами (ОС-агенты). Авторы рассматривают ключевые компоненты ОС-агентов, включая среду, пространство наблюдений и действий, а также основные возможности, такие как понимание, планирование и заземление. В работе анализируются методологии создания ОС-агентов, протоколы оценки и бенчмарки, а также обсуждаются текущие проблемы и перспективные направления исследований. Статья направлена на консолидацию исследований в области ОС-агентов и предоставление информации для академических и промышленных разработок.'}, 'en': {'title': 'Advancing AI Assistants: The Rise of OS Agents', 'desc': 'This paper surveys the development of OS Agents, which are advanced AI assistants that utilize multi-modal large language models to automate tasks on computing devices. It discusses the fundamental components of these agents, including their environment, observation space, and action space, as well as their capabilities like understanding and planning. The authors also explore methodologies for building OS Agents, focusing on domain-specific models and frameworks, and review evaluation protocols to assess their performance. Finally, the paper identifies challenges and future research directions, emphasizing the importance of safety, privacy, and personalization in the evolution of these intelligent systems.'}, 'zh': {'title': '操作系统代理：迈向智能助手的未来', 'desc': '本文探讨了操作系统代理（OS Agents）的发展，这些代理利用多模态大型语言模型（M-LLMs）在计算设备上自动化任务。我们详细介绍了OS代理的基本组成部分，包括环境、观察空间和行动空间，以及理解、规划和基础能力等关键能力。文章还讨论了构建OS代理的方法，重点关注领域特定的基础模型和代理框架。最后，我们分析了评估协议和基准测试，并指出当前面临的挑战及未来研究的方向。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (5)', '#agi (2)', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (6)', '#cv (4)', '#data (2)', '#dataset (3)', '#diffusion (2)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (6)', '#open_source (2)', '#optimization (6)', '#plp', '#rag', '#reasoning (3)', '#rl (2)', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (2)', '#synthetic', '#training (5)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-11 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-11 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-11 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    