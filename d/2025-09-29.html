
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 30 papers. September 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 сентября</span> | <span id="title-articles-count">30 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-26.html">⬅️ <span id="prev-date">26.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-30.html">➡️ <span id="next-date">30.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'};
        let feedDateNext = {'ru': '30.09', 'en': '09/30', 'zh': '9月30日'};
        let feedDatePrev = {'ru': '26.09', 'en': '09/26', 'zh': '9月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.22622', 'title': 'LongLive: Real-time Interactive Long Video Generation', 'url': 'https://huggingface.co/papers/2509.22622', 'abstract': 'LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.', 'score': 106, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'ced1fe458c87fa25', 'authors': ['Shuai Yang', 'Wei Huang', 'Ruihang Chu', 'Yicheng Xiao', 'Yuyang Zhao', 'Xianbang Wang', 'Muyang Li', 'Enze Xie', 'Yingcong Chen', 'Yao Lu', 'Song Han', 'Yukang Chen'], 'affiliations': ['HKU', 'HKUST(GZ)', 'MIT', 'NVIDIA', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2509.22622.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Интерактивная генерация длинных видео в реальном времени', 'desc': 'LongLive представляет собой авторегрессионную систему для генерации длинных видео в реальном времени на уровне кадров. Модель использует каузальное внимание с механизмом KV-recache для обновления кэшированных состояний при смене промптов и обеспечения плавных переходов. Система включает потоковое долгое обучение для работы с длинными видео и короткое оконное внимание с frame sink для поддержания консистентности. LongLive достигает 20.7 FPS на одной NVIDIA H100 и может генерировать видео до 240 секунд с высоким качеством.'}, 'en': {'title': 'Revolutionizing Real-Time Long Video Generation with LongLive', 'desc': 'LongLive is a novel autoregressive framework designed for generating long videos in real-time, addressing both efficiency and quality issues. It utilizes causal attention and a KV-recache mechanism to enhance inference speed while maintaining high visual quality. The framework also incorporates streaming long tuning to align training with inference, allowing for interactive content creation where users can influence the narrative dynamically. With its innovative design, LongLive can generate videos up to 240 seconds long at a high frame rate, demonstrating significant advancements in long video generation capabilities.'}, 'zh': {'title': '实时互动长视频生成的新突破', 'desc': 'LongLive是一个用于实时和互动长视频生成的帧级自回归框架，旨在解决效率和质量的挑战。它采用因果注意力机制和KV缓存技术，以提高推理速度，同时保持视频质量。通过流式长调优和短窗口注意力，LongLive能够在训练和推理中实现一致性，支持用户实时引导内容创作。该框架在单个NVIDIA H100上实现了每秒20.7帧的推理速度，能够生成最长240秒的视频。'}}}, {'id': 'https://huggingface.co/papers/2509.22611', 'title': 'Quantile Advantage Estimation for Entropy-Safe Reasoning', 'url': 'https://huggingface.co/papers/2509.22611', 'abstract': 'Quantile Advantage Estimation stabilizes reinforcement learning with verifiable rewards by addressing entropy issues and improving performance on large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.', 'score': 87, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '44c9fe6a9098712d', 'authors': ['Junkang Wu', 'Kexin Huang', 'Jiancan Wu', 'An Zhang', 'Xiang Wang', 'Xiangnan He'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.22611.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#rlhf', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Квантильное преимущество: стабилизация обучения LLM через умный baseline', 'desc': 'Исследователи предлагают новый подход для улучшения обучения с подкреплением больших языковых моделей, который решает проблемы нестабильности энтропии. Они заменяют среднее значение baseline на квантильную оценку преимуществ (QAE), что позволяет избежать коллапса и взрыва энтропии. Метод создает двухрежимную систему: усиливает редкие успехи на сложных задачах и фокусируется на оставшихся неудачах на простых задачах. Эксперименты показывают стабилизацию обучения и улучшение производительности на математических задачах.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with Quantile Advantage Estimation', 'desc': 'This paper introduces Quantile Advantage Estimation (QAE) to enhance reinforcement learning with verifiable rewards, particularly in large language models. It addresses the instability caused by traditional mean baselines, which can lead to entropy collapse or explosion during training. By using a K-quantile baseline, QAE effectively manages the reward distribution, reinforcing rare successes on difficult queries while focusing on failures in easier ones. The proposed method stabilizes entropy and improves performance, demonstrating that baseline design is crucial for scaling reinforcement learning with verifiable rewards.'}, 'zh': {'title': '量化优势估计：稳定强化学习的新方法', 'desc': '量化优势估计（QAE）通过解决熵问题，稳定了具有可验证奖励的强化学习（RLVR），并提高了大型语言模型的性能。该方法用K-分位数基线替代了传统的均值基线，避免了在奖励异常值下对负优势样本的不当惩罚。QAE在处理困难查询时强化稀有成功，而在简单查询时则针对剩余失败，从而实现了双向熵安全。实验结果表明，这种最小修改有效稳定了熵，并在多个基准测试中取得了持续的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2509.22186', 'title': 'MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing', 'url': 'https://huggingface.co/papers/2509.22186', 'abstract': 'MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.', 'score': 68, 'issue_id': 6129, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '136108b6a4721246', 'authors': ['Junbo Niu', 'Zheng Liu', 'Zhuangcheng Gu', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao', 'Tao Chu', 'Tianyao He', 'Fan Wu', 'Qintong Zhang', 'Zhenjiang Jin', 'Guang Liang', 'Rui Zhang', 'Wenzheng Zhang', 'Yuan Qu', 'Zhifei Ren', 'Yuefeng Sun', 'Yuanhong Zheng', 'Dongsheng Ma', 'Zirui Tang', 'Boyu Niu', 'Ziyang Miao', 'Hejun Dong', 'Siyi Qian', 'Junyuan Zhang', 'Jingzhou Chen', 'Fangdong Wang', 'Xiaomeng Zhao', 'Liqun Wei', 'Wei Li', 'Shasha Wang', 'Ruiliang Xu', 'Yuanyuan Cao', 'Lu Chen', 'Qianqian Wu', 'Huaiyu Gu', 'Lindong Lu', 'Keming Wang', 'Dechen Lin', 'Guanlin Shen', 'Xuanhe Zhou', 'Linfeng Zhang', 'Yuhang Zang', 'Xiaoyi Dong', 'Jiaqi Wang', 'Bo Zhang', 'Lei Bai', 'Pei Chu', 'Weijia Li', 'Jiang Wu', 'Lijun Wu', 'Zhenxiang Li', 'Guangyu Wang', 'Zhongying Tu', 'Chao Xu', 'Kai Chen', 'Yu Qiao', 'Bowen Zhou', 'Dahua Lin', 'Wentao Zhang', 'Conghui He'], 'affiliations': ['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22186.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#cv', '#dataset', '#data'], 'emoji': '📄', 'ru': {'title': 'От общего к частному: эффективный парсинг документов в два этапа', 'desc': 'Представлена MinerU2.5 - vision-language модель с 1.2 миллиардами параметров для парсинга документов. Модель использует двухэтапную стратегию: сначала анализирует общую структуру документа на изображениях низкого разрешения, затем распознает содержимое на фрагментах высокого разрешения. Такой подход позволяет эффективно обрабатывать сложные элементы как текст, формулы и таблицы при меньших вычислительных затратах. MinerU2.5 достигает state-of-the-art результатов на множестве бенчмарков, превосходя как универсальные, так и специализированные модели.'}, 'en': {'title': 'Efficient Document Parsing with MinerU2.5', 'desc': 'MinerU2.5 is a vision-language model designed for document parsing, featuring 1.2 billion parameters. It utilizes a coarse-to-fine parsing strategy that separates the analysis of document layout from the recognition of content, enhancing efficiency. The model first conducts layout analysis on downsampled images to identify structural elements, then focuses on detailed content recognition using high-resolution crops. This innovative approach allows MinerU2.5 to achieve top performance on various benchmarks while reducing computational costs compared to other models.'}, 'zh': {'title': '高效文档解析的新标杆', 'desc': 'MinerU2.5是一种具有12亿参数的文档解析视觉语言模型，采用粗到细的解析策略，实现了最先进的识别准确率和计算效率。该模型的第一阶段在降采样图像上进行高效的布局分析，以识别结构元素，从而避免处理高分辨率输入的计算开销。第二阶段则在原始图像中提取的原生分辨率裁剪图上进行目标内容识别，保留了密集文本、复杂公式和表格中的细节。通过开发全面的数据引擎，MinerU2.5能够生成多样化的大规模训练语料库，支持预训练和微调，最终在多个基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.22576', 'title': 'EPO: Entropy-regularized Policy Optimization for LLM Agents\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.22576', 'abstract': 'Entropy-regularized Policy Optimization (EPO) addresses exploration-exploitation challenges in multi-turn environments with sparse rewards, improving performance in tasks like ScienceWorld and ALFWorld.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.', 'score': 57, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '9f6eee9878b5d876', 'authors': ['Xu Wujiang', 'Wentian Zhao', 'Zhenting Wang', 'Li Yu-Jhe', 'Jin Can', 'Jin Mingyu', 'Mei Kai', 'Wan Kun', 'Metaxas Dimitris'], 'affiliations': ['Adobe Inc.', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22576.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#games', '#optimization'], 'emoji': '🌊', 'ru': {'title': 'Укрощение энтропийного каскада в обучении LLM агентов', 'desc': 'Исследователи решают проблему обучения LLM агентов в многоходовых средах с разреженными наградами, где для выполнения задачи требуется 30+ взаимодействий. Они выявили критический режим отказа - каскадное падение исследования-эксплуатации, когда агент сначала преждевременно сходится к плохим стратегиям, а затем входит в хаотическое исследование. Предложенный метод EPO использует три механизма: энтропийную регуляризацию, сглаживание энтропии и адаптивное взвешивание фаз для баланса исследования и эксплуатации. EPO показал улучшение производительности до 152% на ScienceWorld и до 19.8% на ALFWorld.'}, 'en': {'title': 'Revolutionizing Exploration in Sparse Reward Environments with EPO', 'desc': 'Entropy-regularized Policy Optimization (EPO) is a new approach designed to tackle the exploration-exploitation dilemma in reinforcement learning, especially in environments where rewards are sparse and tasks require many interactions. The paper identifies a unique failure mode in these settings, where agents prematurely settle on poor strategies due to limited feedback, leading to a collapse in their learning process. EPO introduces three key mechanisms: enhanced entropy regularization for better exploration, a smoothing regularizer to stabilize policy changes, and adaptive weighting to balance exploration and exploitation effectively. This framework has shown significant performance improvements in complex tasks, highlighting the need for tailored entropy management in multi-turn environments.'}, 'zh': {'title': '熵正则化策略优化：提升多回合学习性能的关键', 'desc': '本文提出了一种名为熵正则化策略优化（EPO）的方法，旨在解决多回合环境中稀疏奖励下的探索与利用挑战。EPO通过三种机制打破了探索-利用的失败循环，增强了策略的探索能力，并防止了策略熵的剧烈波动。研究表明，EPO在ScienceWorld和ALFWorld任务中分别提高了152%和19.8%的性能。该方法强调了在多回合稀疏奖励设置中，熵控制需要与传统强化学习有根本性的不同。'}}}, {'id': 'https://huggingface.co/papers/2509.22637', 'title': 'Variational Reasoning for Language Models', 'url': 'https://huggingface.co/papers/2509.22637', 'abstract': 'A variational reasoning framework treats thinking traces as latent variables, optimizing them through variational inference to improve language model reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.', 'score': 41, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'f8f5b5200a8c3ac8', 'authors': ['Xiangxin Zhou', 'Zichen Liu', 'Haonan Wang', 'Chao Du', 'Min Lin', 'Chongxuan Li', 'Liang Wang', 'Tianyu Pang'], 'affiliations': ['CASIA', 'NUS', 'RUC', 'Sea AI Lab', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2509.22637.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Вариационное рассуждение: унификация вероятностного подхода и RL для языковых моделей', 'desc': 'Исследователи предлагают новый вариационный фреймворк для улучшения рассуждений языковых моделей, рассматривая цепочки мышления как латентные переменные. Метод оптимизирует эти переменные через вариационный вывод, используя evidence lower bound (ELBO) и расширяя его до многотрассового объектива. Авторы показывают, что rejection sampling fine-tuning и reinforcement learning с бинарными наградами можно интерпретировать как локальные forward-KL объективы. Экспериментальная валидация проводилась на моделях семейства Qwen 2.5 и Qwen 3 на различных задачах рассуждений, демонстрируя стабильность предложенного подхода.'}, 'en': {'title': 'Enhancing Language Model Reasoning with Variational Inference', 'desc': 'This paper presents a variational reasoning framework that enhances the reasoning capabilities of language models by treating thinking traces as latent variables. It optimizes these traces using variational inference, starting from the evidence lower bound (ELBO) and extending it to a multi-trace objective for improved performance. The authors introduce a forward-KL formulation to stabilize training and demonstrate that techniques like rejection sampling finetuning and binary-reward reinforcement learning can be viewed as local forward-KL objectives. The framework is empirically validated on the Qwen 2.5 and Qwen 3 models, showing its effectiveness across various reasoning tasks.'}, 'zh': {'title': '变分推理框架提升语言模型推理能力', 'desc': '本文提出了一种变分推理框架，用于语言模型的推理过程，将思维轨迹视为潜在变量，并通过变分推理进行优化。我们从证据下界（ELBO）出发，扩展到多轨迹目标，以获得更紧的界限，并提出了一种前向KL公式，以稳定变分后验的训练。我们还表明，拒绝采样微调和二元奖励强化学习（如GRPO）可以被解释为局部前向KL目标，其中模型准确度的隐式加权自然地从推导中产生，并揭示了对简单问题的偏见。我们的实验证明了该方法在Qwen 2.5和Qwen 3模型系列上的有效性，涵盖了广泛的推理任务。'}}}, {'id': 'https://huggingface.co/papers/2509.21679', 'title': 'ReviewScore: Misinformed Peer Review Detection with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2509.21679', 'abstract': 'An automated engine evaluates the factuality of review points in AI conference papers, demonstrating moderate agreement with human experts and higher accuracy at the premise level.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either "weaknesses" in a review that contain incorrect premises, or "questions" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation.', 'score': 38, 'issue_id': 6130, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '547504330158a42b', 'authors': ['Hyun Ryu', 'Doohyuk Jang', 'Hyemin S. Lee', 'Joonhyun Jeong', 'Gyeongman Kim', 'Donghyeon Cho', 'Gyouk Chu', 'Minyeong Hwang', 'Hyeongwon Jang', 'Changhun Kim', 'Haechan Kim', 'Jina Kim', 'Joowon Kim', 'Yoonjeon Kim', 'Kwanhyung Lee', 'Chanjae Park', 'Heecheol Yun', 'Gregor Betz', 'Eunho Yang'], 'affiliations': ['KAIST, AITRICS', 'KIT', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2509.21679.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#ethics', '#data'], 'emoji': '🔍', 'ru': {'title': 'Автоматическая проверка фактов в научных рецензиях с помощью AI', 'desc': 'Исследователи создали автоматический движок для оценки фактической точности рецензий на научные статьи в области AI. Они выявили, что 15.2% слабых мест и 26.4% вопросов в рецензиях содержат неверную информацию или уже отвечены в статье. Для автоматизации оценки качества рецензий была введена метрика ReviewScore, которая определяет дезинформированные комментарии рецензентов. Тестирование на восьми современных LLM показало умеренное согласие с экспертными оценками, особенно на уровне отдельных предпосылок.'}, 'en': {'title': 'Automating Review Quality: Enhancing Factuality in AI Conference Papers', 'desc': 'This paper presents an automated engine designed to assess the factual accuracy of review points in AI conference papers. It identifies misinformed review points, which are categorized as incorrect weaknesses or unnecessary questions, and introduces a metric called ReviewScore to quantify these inaccuracies. The authors create a dataset annotated by human experts to evaluate the performance of large language models (LLMs) in determining ReviewScore, finding moderate agreement with human evaluations. The study reveals that evaluating the factuality of individual premises yields better agreement than assessing the overall weaknesses, suggesting a pathway towards fully automated review evaluations.'}, 'zh': {'title': '自动化评估AI论文评审的真实性', 'desc': '本文提出了一种自动化引擎，用于评估AI会议论文评审意见的真实性。研究发现，15.2%的评审意见中的“弱点”是错误的，26.4%的“问题”是可以通过论文回答的。通过构建一个人类专家标注的ReviewScore数据集，验证了大型语言模型（LLMs）在自动化评估ReviewScore方面的能力。结果显示，在评估前提的真实性时，模型与人类专家的协议程度显著高于评估整体弱点的真实性。'}}}, {'id': 'https://huggingface.co/papers/2509.22638', 'title': 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards', 'url': 'https://huggingface.co/papers/2509.22638', 'abstract': 'Feedback-conditional policy (FCP) enables LLMs to learn from verbal feedback by treating it as a conditioning signal, improving expressiveness over scalar rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.', 'score': 37, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'b97a5b11be70ca26', 'authors': ['Renjie Luo', 'Zichen Liu', 'Xiangyan Liu', 'Chao Du', 'Min Lin', 'Wenhu Chen', 'Wei Lu', 'Tianyu Pang'], 'affiliations': ['NTU', 'NUS', 'SUTD', 'Sea AI Lab', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.22638.jpg', 'data': {'categories': ['#alignment', '#rl', '#optimization', '#rlhf'], 'emoji': '💬', 'ru': {'title': 'От скалярных наград к богатой вербальной обратной связи', 'desc': 'Исследователи предлагают новый подход обучения больших языковых моделей на основе вербальной обратной связи вместо скалярных наград. Метод Feedback-Conditional Policy (FCP) рассматривает текстовую обратную связь как условие для генерации ответов, сохраняя всю богатую информацию из отзывов. Модель обучается напрямую на парах ответ-отзыв через максимизацию правдоподобия, а затем дорабатывается в онлайн-режиме с получением свежей обратной связи. Такой подход переосмысливает обучение с подкреплением как задачу условной генерации текста, что позволяет более выразительно использовать человеческие отзывы.'}, 'en': {'title': 'Learning from Words: Enhancing LLMs with Feedback-Conditional Policy', 'desc': 'The paper introduces Feedback-Conditional Policy (FCP), a method that allows large language models (LLMs) to learn from verbal feedback instead of relying solely on scalar rewards. This approach treats verbal feedback as a conditioning signal, which preserves the richness of the feedback and avoids the scale imbalance often seen in traditional reinforcement learning methods. FCP utilizes response-feedback pairs to train the model through maximum likelihood, enhancing its ability to generate responses based on nuanced feedback. Additionally, an online bootstrapping stage is implemented to continuously refine the model by generating responses under positive conditions and incorporating new feedback.'}, 'zh': {'title': '反馈条件策略：让模型更好地理解口头反馈', 'desc': '反馈条件策略（FCP）使大型语言模型（LLMs）能够通过将口头反馈视为条件信号来学习，从而提高了表达能力。传统的强化学习方法通常将细致的反馈压缩为标量奖励，导致信息的丢失和规模不平衡。FCP直接从响应-反馈对中学习，通过最大似然训练来近似反馈条件后验。该方法将反馈驱动的学习重新定义为条件生成，而不是奖励优化，为LLMs提供了一种更具表现力的方式来直接学习口头反馈。'}}}, {'id': 'https://huggingface.co/papers/2509.22281', 'title': 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial\n  Reasoning', 'url': 'https://huggingface.co/papers/2509.22281', 'abstract': 'MesaTask, an LLM-based framework with a Spatial Reasoning Chain, generates realistic tabletop scenes aligned with task descriptions using DPO algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at https://mesatask.github.io/', 'score': 24, 'issue_id': 6133, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '4de2b4acef076c8e', 'authors': ['Jinkun Hao', 'Naifu Liang', 'Zhen Luo', 'Xudong Xu', 'Weipeng Zhong', 'Ran Yi', 'Yichen Jin', 'Zhaoyang Lyu', 'Feng Zheng', 'Lizhuang Ma', 'Jiangmiao Pang'], 'affiliations': ['Peking University', 'SII', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Southern University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.22281.jpg', 'data': {'categories': ['#3d', '#synthetic', '#agents', '#reasoning', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Умная генерация сцен для обучения роботов манипуляции объектами', 'desc': 'Исследователи разработали MesaTask — фреймворк на основе LLM для генерации реалистичных сцен на столешнице, которые соответствуют описанию задач для роботов. Система использует цепочку пространственного рассуждения, которая разделяет процесс генерации на определение объектов, анализ их взаимоотношений и построение графа сцены. Для обучения создан датасет MesaTask-10K с 10,700 синтетических сцен с тщательно продуманными макетами. Алгоритмы DPO дополнительно улучшают способность модели создавать физически правдоподобные сцены, которые точно соответствуют задачам манипуляции.'}, 'en': {'title': 'Generating Realistic Tabletop Scenes with MesaTask', 'desc': 'This paper introduces MesaTask, a framework that uses large language models (LLMs) to generate realistic tabletop scenes based on specific task descriptions. It addresses the challenge of creating task-oriented scenes, which traditionally relied on manual design or random layouts that often lacked realism. The authors present a new dataset, MesaTask-10K, containing around 10,700 synthetic scenes with carefully designed layouts to ensure they are plausible and relevant to tasks. To improve scene generation, they propose a Spatial Reasoning Chain that breaks down the process into steps like object inference and spatial reasoning, ultimately leading to a coherent 3D layout that meets the requirements of the task.'}, 'zh': {'title': 'MesaTask：智能生成任务导向的桌面场景', 'desc': '本论文提出了一种名为MesaTask的框架，利用空间推理链生成与任务描述相符的真实桌面场景。传统的桌面场景生成方法依赖于耗时的手动设计或随机布局，难以满足任务需求。MesaTask-10K是一个包含约10,700个合成桌面场景的大规模数据集，确保了布局的真实性和复杂的物体关系。通过空间推理链，MesaTask将生成过程分解为物体推理、空间关系推理和场景图构建，从而生成符合任务要求的桌面场景。'}}}, {'id': 'https://huggingface.co/papers/2509.22647', 'title': 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2509.22647', 'abstract': 'CapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a "good" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.', 'score': 23, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'eeedb125ad75e30e', 'authors': ['Long Xing', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Jianze Liang', 'Qidong Huang', 'Jiaqi Wang', 'Feng Wu', 'Dahua Lin'], 'affiliations': ['Alibaba Cloud', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.22647.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#rl', '#games', '#rlhf', '#optimization'], 'emoji': '📸', 'ru': {'title': 'Обучение описанию изображений через вопросы и ответы', 'desc': 'В статье представлен CapRL — новый подход к обучению моделей для генерации описаний изображений с использованием reinforcement learning. Основная идея заключается в том, что качество описания оценивается через способность языковой модели без доступа к изображению правильно отвечать на вопросы по этому описанию. Метод использует двухэтапный процесс: сначала LVLM генерирует описание, затем отдельная LLM отвечает на вопросы с множественным выбором основываясь только на тексте описания. Эксперименты показали значительное улучшение качества описаний на 12 бенчмарках по сравнению с традиционными методами supervised fine-tuning.'}, 'en': {'title': 'Revolutionizing Image Captioning with CapRL: Quality Through Questioning', 'desc': 'CapRL is a new reinforcement learning framework designed to improve image captioning by using a language model that does not rely on visual input to assess the quality of captions. It addresses the limitations of traditional supervised fine-tuning methods, which often lead to models that memorize specific answers rather than generating diverse descriptions. By employing a two-stage pipeline, CapRL generates captions and evaluates them based on how well a separate language model can answer questions about the images using those captions. This innovative approach not only enhances performance across various benchmarks but also provides a scalable solution for training image captioning models.'}, 'zh': {'title': 'CapRL：提升图像描述质量的新方法', 'desc': 'CapRL是一种新颖的强化学习框架，旨在通过使用无视觉的语言模型来评估图像描述的质量，从而提升图像描述的性能。该方法通过引入可验证奖励的强化学习范式，克服了传统监督微调方法的局限性，避免了对昂贵的人工标注数据的依赖。CapRL通过一个解耦的两阶段流程生成描述，并根据无视觉语言模型回答多项选择题的准确性来定义描述质量。实验结果表明，CapRL在多个基准测试中显著提高了图像描述的效果。'}}}, {'id': 'https://huggingface.co/papers/2509.22651', 'title': 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,\n  Speaking, and Viewing', 'url': 'https://huggingface.co/papers/2509.22651', 'abstract': "VoiceAssistant-Eval is a benchmark for evaluating AI assistants across listening, speaking, and viewing tasks, revealing insights into model performance and identifying areas for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .", 'score': 19, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'fee6beb54706ecb4', 'authors': ['Ke Wang', 'Houxing Ren', 'Zimu Lu', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.22651.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#audio', '#benchmark', '#open_source', '#small_models'], 'emoji': '🔊', 'ru': {'title': 'Оценка AI-ассистентов: звук, речь и изображение', 'desc': 'VoiceAssistant-Eval — это новый бенчмарк для оценки AI-ассистентов, который охватывает задачи по восприятию звука, речи и изображений. Он включает 10,497 примеров в 13 категориях, таких как естественные звуки, музыка и диалоги. Исследование показало, что открытые модели могут конкурировать с проприетарными, особенно в задачах речи, но отстают в понимании аудио. Также выявлены трудности в обработке мультимодальных данных и имитации голоса, что требует дальнейших улучшений.'}, 'en': {'title': 'VoiceAssistant-Eval: A New Standard for AI Assistant Evaluation', 'desc': 'VoiceAssistant-Eval is a new benchmark created to evaluate AI assistants on their listening, speaking, and viewing abilities. It includes a large dataset of 10,497 examples across 13 different tasks, covering various audio and visual scenarios. The evaluation of 21 models, including GPT-4o-Audio, shows that while many models perform well in speaking, they struggle with audio comprehension. This benchmark highlights the strengths and weaknesses of current AI assistants, paving the way for improvements in multimodal capabilities and safety measures.'}, 'zh': {'title': '评估AI助手的全面基准测试', 'desc': 'VoiceAssistant-Eval是一个用于评估AI助手在听、说、看任务中的表现的基准测试，旨在揭示模型性能并识别改进领域。该基准包含10,497个经过精心挑选的示例，涵盖13个任务类别，包括自然声音、音乐和对话等。研究表明，许多模型在说话任务上表现良好，但在音频理解方面存在不足。此外，设计良好的小型模型在某些任务上可以与大型模型相媲美。'}}}, {'id': 'https://huggingface.co/papers/2509.21880', 'title': 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM\n  Reinforcement Learning via Entropy-Guided Advantage Shaping', 'url': 'https://huggingface.co/papers/2509.21880', 'abstract': 'RL-ZVP, a novel reinforcement learning algorithm, leverages zero-variance prompts to improve the accuracy and pass rate of Large Language Models in math reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.', 'score': 16, 'issue_id': 6133, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'd4b83fa83161bdb8', 'authors': ['Thanh-Long V. Le', 'Myeongho Jeon', 'Kim Vu', 'Viet Lai', 'Eunho Yang'], 'affiliations': ['Adobe Research', 'EPFL', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2509.21880.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#math', '#rlhf', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Обучение на одинаковых наградах: как извлечь пользу из промптов с нулевой дисперсией', 'desc': 'В статье представлен новый алгоритм обучения с подкреплением RL-ZVP, который использует промпты с нулевой дисперсией для улучшения математического рассуждения больших языковых моделей. Традиционные методы как GRPO игнорируют случаи, когда все ответы модели получают одинаковую награду, считая их бесполезными. RL-ZVP извлекает обучающие сигналы даже из таких промптов, напрямую награждая правильность и штрафуя ошибки с учетом характеристик на уровне токенов. Эксперименты на шести бенчмарках показали улучшения до 8.61 пунктов по точности и 7.77 пунктов по проходимости тестов по сравнению с базовыми методами.'}, 'en': {'title': 'Unlocking Learning Potential with Zero-Variance Prompts', 'desc': 'The paper introduces RL-ZVP, a new reinforcement learning algorithm designed to enhance the performance of Large Language Models (LLMs) in math reasoning tasks. It focuses on utilizing zero-variance prompts, which are typically overlooked because they do not show varying responses. The algorithm effectively extracts learning signals from these prompts by rewarding correct answers and penalizing mistakes, even when responses are uniform. The results demonstrate that RL-ZVP significantly improves accuracy and pass rates in math reasoning benchmarks compared to existing methods, showcasing the value of zero-variance prompts in reinforcement learning.'}, 'zh': {'title': '利用零方差提示提升推理能力', 'desc': 'RL-ZVP是一种新颖的强化学习算法，利用零方差提示来提高大型语言模型在数学推理任务中的准确性和通过率。传统方法如GRPO只关注模型对相同输入的不同响应，而忽略了所有响应都获得相同奖励的情况。本文提出零方差提示并非无用，而是可以为策略优化提供有意义的反馈。通过在六个数学推理基准测试中，RL-ZVP在准确性和通过率上分别比GRPO提高了8.61分和7.77分，展示了从零方差提示中学习的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.21766', 'title': 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon\n  Scenarios', 'url': 'https://huggingface.co/papers/2509.21766', 'abstract': "UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce UltraHorizon a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average 200k+ tokens and 400+ tool calls, whereas in standard configurations they still exceed 35k tokens and involve more than 60 tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. https://github.com/StarDewXXX/UltraHorizon{Our code will be available here.}", 'score': 16, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '5e2b3fc1a4b26823', 'authors': ['Haotian Luo', 'Huaisong Zhang', 'Xuelin Zhang', 'Haoyu Wang', 'Zeyu Qin', 'Wenjie Lu', 'Guozheng Ma', 'Haiying He', 'Yingsha Xie', 'Qiyang Zhou', 'Zixuan Hu', 'Hongze Mi', 'Yibo Wang', 'Naiqiang Tan', 'Hong Chen', 'Yi R. Fung', 'Chun Yuan', 'Li Shen'], 'affiliations': ['China Agricultural University', 'Didichuxing Co. Ltd', 'HKUST', 'Huazhong Agricultural University', 'Nanyang Technological University', 'Sun Yat-sen University', 'Tianjin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21766.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#reasoning', '#agents'], 'emoji': '🔭', 'ru': {'title': 'Долгосрочное мышление AI-агентов: большой разрыв с человеческими способностями', 'desc': 'UltraHorizon - это новый бенчмарк для оценки автономных AI-агентов в долгосрочных задачах с частичной наблюдаемостью. Исследование показывает, что современные LLM-агенты значительно отстают от людей в задачах, требующих длительного рассуждения, планирования и управления памятью. Траектории в самых сложных настройках содержат более 200 тысяч токенов и 400+ вызовов инструментов. Анализ выявил восемь типов ошибок агентов, связанных с блокировкой контекста и пробелами в базовых способностях.'}, 'en': {'title': 'Bridging the Gap in Long-Horizon Reasoning for AI Agents', 'desc': 'UltraHorizon is a benchmark designed to assess the performance of autonomous agents in long-horizon and partially observable tasks, which are crucial for real-world applications. Unlike traditional evaluations that focus on short tasks, UltraHorizon emphasizes the need for sustained reasoning, planning, memory management, and effective tool use. The benchmark reveals that current large language model (LLM) agents struggle significantly in these complex scenarios, often underperforming compared to human participants. Through detailed analysis, the study identifies specific errors in agent performance, highlighting fundamental gaps in their capabilities.'}, 'zh': {'title': '评估自主智能体的长时间跨度能力', 'desc': 'UltraHorizon是一个新的基准测试，旨在评估自主智能体在长时间跨度和部分可观察任务中的表现，强调其在持续推理、规划、记忆和工具使用能力方面的不足。现有的评估大多集中在短时间跨度和完全可观察的任务上，而许多现实世界的关键任务则需要在长时间跨度和部分可观察的场景中进行。通过探索作为统一任务，我们在三个不同环境中验证了智能体的核心能力，发现大型语言模型智能体在这些设置中表现不佳。我们的研究揭示了智能体在长时间跨度能力上的持续差距，并分析了导致错误的主要原因。'}}}, {'id': 'https://huggingface.co/papers/2509.22414', 'title': 'LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale\n  Diffusion Transformer', 'url': 'https://huggingface.co/papers/2509.22414', 'abstract': "LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.", 'score': 15, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '34883cbd4b91fb74', 'authors': ['Song Fei', 'Tian Ye', 'Lujia Wang', 'Lei Zhu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2509.22414.jpg', 'data': {'categories': ['#cv', '#benchmark', '#open_source', '#dataset', '#diffusion', '#hallucinations'], 'emoji': '🔧', 'ru': {'title': 'Восстановление изображений без текстовых описаний', 'desc': 'LucidFlux представляет новый подход к универсальному восстановлению изображений, используя диффузионный трансформер без текстовых описаний. Система адаптирует крупную модель Flux.1 через легковесный двухветвенный кондиционер, который обрабатывает сигналы от поврежденного изображения и частично восстановленной версии. Для семантического выравнивания используются SigLIP признаки вместо текстовых промптов, что избавляет от нестабильности и задержек. Модель превосходит существующие решения на различных бенчмарках, демонстрируя важность правильного кондиционирования для robust восстановления изображений.'}, 'en': {'title': 'LucidFlux: Caption-Free Image Restoration with Smart Conditioning', 'desc': 'LucidFlux is a novel framework for universal image restoration (UIR) that operates without the need for text captions. It utilizes a diffusion transformer to effectively restore images that have been degraded by various factors while maintaining their semantic integrity. The framework features a dual-branch conditioning system that helps to stabilize the restoration process by anchoring geometric details and minimizing artifacts. By employing a unique modulation schedule and leveraging SigLIP features, LucidFlux achieves superior performance in restoring images compared to existing methods, demonstrating that strategic conditioning is key to effective image restoration.'}, 'zh': {'title': 'LucidFlux：无文本提示的强大图像恢复框架', 'desc': 'LucidFlux是一个无文本提示的通用图像恢复框架，利用扩散变换器实现强大的图像恢复。该框架通过自适应条件和SigLIP特征，避免了传统方法中的过平滑和伪影问题。LucidFlux引入了轻量级的双分支调节器，能够有效地锚定几何结构并抑制伪影。通过在大规模数据上进行结构丰富的监督，LucidFlux在多种基准测试中表现优异，证明了其在无文本提示的图像恢复中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.22644', 'title': 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.22644', 'abstract': "WebGen-Agent enhances website code generation by integrating visual feedback and GUI-agent testing with a backtracking mechanism and Step-GRPO training to improve accuracy and appearance scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce Step-GRPO with Screenshot and GUI-agent Feedback to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.", 'score': 14, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'b5f5be0955af5d3f', 'authors': ['Zimu Lu', 'Houxing Ren', 'Yunqiao Yang', 'Ke Wang', 'Zhuofan Zong', 'Junting Pan', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Ace Robotics', 'Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.22644.jpg', 'data': {'categories': ['#training', '#agents', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🌐', 'ru': {'title': 'Генерация веб-сайтов с визуальной обратной связью', 'desc': 'В статье представлен WebGen-Agent - новый AI-агент для генерации веб-сайтов, который использует визуальную обратную связь от скриншотов и GUI-тестирования для итеративного улучшения кода. Система применяет механизм backtracking и выбора лучшего варианта, а также обучение Step-GRPO с визуальными оценками в качестве награды. Визуальная языковая модель анализирует скриншоты сайтов и предоставляет детальные описания и оценки качества. Результаты показывают значительное улучшение точности генерации с 26.4% до 51.9% для Claude-3.5-Sonnet и повышение визуальных оценок.'}, 'en': {'title': 'Revolutionizing Website Code Generation with Visual Feedback', 'desc': 'WebGen-Agent is a new system designed to improve the generation of website code by using visual feedback and testing methods. It combines a backtracking mechanism with a training approach called Step-GRPO, which helps refine the code based on visual quality scores. This system uses a visual language model to provide detailed feedback on the generated websites, ensuring that the code not only works but also looks good. As a result, WebGen-Agent significantly enhances the accuracy and appearance of website code compared to previous methods.'}, 'zh': {'title': 'WebGen-Agent：提升网站生成的智能代理', 'desc': 'WebGen-Agent 是一种新型的网站生成代理，结合了视觉反馈和图形用户界面（GUI）代理测试，以提高网站代码生成的准确性和外观评分。该系统利用视觉语言模型（VLM）生成详细的文本描述和建议，并通过回溯机制和选择最佳策略来优化生成过程。通过在每个步骤中使用截图和GUI代理的评分作为奖励，WebGen-Agent 能够提供密集且可靠的过程监督信号，从而显著提升生成能力。实验结果表明，WebGen-Agent 在 WebGen-Bench 数据集上显著提高了生成模型的性能，超越了之前的最先进系统。'}}}, {'id': 'https://huggingface.co/papers/2509.22624', 'title': 'SPARK: Synergistic Policy And Reward Co-Evolving Framework', 'url': 'https://huggingface.co/papers/2509.22624', 'abstract': 'SPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.', 'score': 13, 'issue_id': 6131, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '63cb194710260ca4', 'authors': ['Ziyu Liu', 'Yuhang Zang', 'Shengyuan Ding', 'Yuhang Cao', 'Xiaoyi Dong', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.22624.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#architecture', '#rl', '#rlhf', '#optimization', '#training', '#alignment'], 'emoji': '🔄', 'ru': {'title': 'Самообучающиеся модели через синергию политики и вознаграждений', 'desc': 'SPARK - это новый фреймворк для обучения больших языковых моделей (LLM) и мультимодальных моделей (LVLM) с подкреплением, который одновременно развивает политику и систему вознаграждений. Вместо того чтобы выбрасывать данные после каждого обновления, SPARK переиспользует информацию о правильности ответов для обучения самой модели как генеративной модели вознаграждений. Это создает положительную обратную связь: улучшенная точность вознаграждений дает лучшие градиенты политики, которые генерируют более качественные данные для дальнейшего улучшения модели вознаграждений. Метод показывает значительные улучшения производительности на различных бенчмарках без необходимости в отдельных моделях вознаграждений или дорогостоящих человеческих предпочтениях.'}, 'en': {'title': 'SPARK: Revolutionizing Model Training with Self-Improving Rewards', 'desc': 'SPARK is a new framework that improves Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) by using a method called reinforcement learning. It recycles data from previous model outputs and correctness checks to train a generative reward model, which reduces the need for human feedback and external reward systems. This approach creates a feedback loop where better reward accuracy leads to improved model performance, allowing the model to learn and refine itself continuously. SPARK has shown significant performance improvements across various benchmarks, demonstrating its effectiveness and versatility in enhancing model capabilities.'}, 'zh': {'title': 'SPARK：提升模型性能的协同框架', 'desc': 'SPARK是一个协同政策和奖励共同演化的框架，旨在提升大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的性能。它通过回收回放和正确性数据来训练生成奖励模型，从而减少对人类偏好和外部奖励模型的依赖。SPARK利用多种目标进行辅助训练，教会模型评估和改进自身的响应，形成正向的共同演化反馈循环。该框架在多个推理和奖励基准上显示出显著的性能提升，证明了其稳健性和广泛的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.22653', 'title': 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned\n  Aerial Navigation', 'url': 'https://huggingface.co/papers/2509.22653', 'abstract': 'See, Point, Fly (SPF) is a training-free aerial vision-and-language navigation framework that treats action prediction as a 2D spatial grounding task, outperforming existing methods in both simulation and real-world evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev', 'score': 12, 'issue_id': 6134, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'a0daff730d67bc7a', 'authors': ['Chih Yao Hu', 'Yang-Sen Lin', 'Yuna Lee', 'Chih-Hai Su', 'Jie-Ying Lee', 'Shr-Ruei Tsai', 'Chin-Yang Lin', 'Kuan-Wen Chen', 'Tsung-Wei Ke', 'Yu-Lun Liu'], 'affiliations': ['National Taiwan University', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22653.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#rl', '#agents', '#games', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Дроны научились летать по текстовым командам через 2D разметку изображений', 'desc': 'Исследователи представили SPF (See, Point, Fly) - фреймворк для навигации дронов по текстовым инструкциям без дополнительного обучения. Ключевая идея заключается в том, что предсказание действий рассматривается как задача 2D пространственного заземления, а не генерации текста. Система использует vision-language модели для разбиения языковых инструкций на последовательность 2D точек-waypoints на изображении, которые затем преобразуются в 3D векторы перемещения. В экспериментах SPF превзошел предыдущие методы на 63% в симуляции и показал отличные результаты в реальных условиях.'}, 'en': {'title': 'Navigate with Precision: See, Point, Fly!', 'desc': 'The paper introduces See, Point, Fly (SPF), a novel framework for aerial navigation that integrates vision and language without requiring prior training. SPF redefines action prediction as a 2D spatial grounding task, allowing it to interpret vague instructions and generate precise navigation commands. By converting 2D waypoints into 3D displacement vectors, SPF enables UAVs to navigate effectively in dynamic environments. The framework demonstrates superior performance in both simulated and real-world scenarios, setting new benchmarks in deep reinforcement learning (DRL) and showcasing its adaptability across various vision-language models (VLMs).'}, 'zh': {'title': '无训练的空中导航新方法：看、指、飞', 'desc': '本文介绍了一种名为See, Point, Fly (SPF) 的训练无关的空中视觉与语言导航框架。SPF将动作预测视为二维空间定位任务，能够根据自由形式的指令在各种环境中导航。与现有的方法不同，SPF利用视觉语言模型将模糊的语言指令分解为二维路径点，并将其转换为无人机的三维位移向量。SPF在深度强化学习模拟基准测试中设定了新的最佳表现，并在实际评估中也表现优异。'}}}, {'id': 'https://huggingface.co/papers/2509.21710', 'title': 'Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on\n  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval', 'url': 'https://huggingface.co/papers/2509.21710', 'abstract': 'ToG-3, a novel framework, enhances LLMs with external knowledge using a dynamic, multi-agent system that evolves queries and subgraphs for precise evidence retrieval and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.', 'score': 12, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'bd2996a79ff96ede', 'authors': ['Xiaojun Wu', 'Cehao Yang', 'Xueyuan Lin', 'Chengjin Xu', 'Xuhui Jiang', 'Yuanliang Sun', 'Hui Xiong', 'Jia Li', 'Jian Guo'], 'affiliations': ['DataArc Tech Ltd.', 'Hithink RoyalFlush Information Network Co., Ltd', 'Hong Kong University of Science and Technology (Guangzhou)', 'IDEA Research, International Digital Economy Academy'], 'pdf_title_img': 'assets/pdf/title_img/2509.21710.jpg', 'data': {'categories': ['#multimodal', '#agents', '#benchmark', '#reasoning', '#rag', '#graphs'], 'emoji': '🧠', 'ru': {'title': 'Думай-на-графе: эволюционная мульти-агентная система для точного извлечения знаний', 'desc': 'ToG-3 представляет новый подход для улучшения работы LLM с внешними знаниями через динамическую мульти-агентную систему. Основная инновация заключается в механизме MACER, который создает и уточняет гетерогенный граф-индекс из чанков, триплетов и сообществ. Система включает четыре агента - Constructor, Retriever, Reflector и Responser, которые итеративно эволюционируют запросы и подграфы для точного поиска доказательств. Этот подход преодолевает ограничения статических графов в традиционных Graph-based RAG методах, позволяя глубокое рассуждение даже с легковесными LLM.'}, 'en': {'title': 'Dynamic Knowledge Integration for Enhanced Reasoning in LLMs', 'desc': 'ToG-3 is a new framework that improves Large Language Models (LLMs) by integrating external knowledge through a dynamic multi-agent system. It introduces a mechanism called Multi-Agent Context Evolution and Retrieval (MACER) that allows for the adaptive construction of a heterogeneous graph index. This system evolves both the queries and the subgraphs in real-time, enabling more accurate evidence retrieval and reasoning. By overcoming the limitations of static graph structures, ToG-3 enhances the reasoning capabilities of LLMs, even when using smaller models.'}, 'zh': {'title': '动态演化，精准推理的未来', 'desc': 'ToG-3是一个新颖的框架，通过动态的多智能体系统增强大型语言模型（LLMs）与外部知识的结合。它引入了多智能体上下文演化和检索机制（MACER），解决了现有图基方法在构建知识图谱时的局限性。ToG-3的核心创新在于动态构建和优化异构图索引，采用了演化查询和演化子图的双重演化机制，以实现精确的证据检索。实验结果表明，ToG-3在深度和广度推理基准上均优于现有方法，验证了MACER框架各组成部分的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.21760', 'title': 'UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models', 'url': 'https://huggingface.co/papers/2509.21760', 'abstract': "A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.", 'score': 8, 'issue_id': 6129, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'e74140613312626b', 'authors': ['Lan Chen', 'Yuchao Gu', 'Qi Mao'], 'affiliations': ['Communication University of China', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.21760.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal', '#diffusion', '#transfer_learning'], 'emoji': '🎬', 'ru': {'title': 'Один видео трансформер для всех задач компьютерного зрения', 'desc': 'Исследователи предлагают UniVid - фреймворк, который адаптирует предобученную модель генерации видео для решения различных задач компьютерного зрения без специфических модификаций. Задачи представляются как визуальные предложения, где контекстная последовательность определяет как саму задачу, так и ожидаемую модальность вывода. UniVid демонстрирует кросс-модальную и кросс-доменную генерализацию, работая с изображениями и видео, а также переключаясь между задачами понимания и генерации простым изменением порядка визуальной последовательности. Модель показывает хорошие результаты даже при обучении только на естественных видеоданных, что подчеркивает потенциал предобученных видео диффузионных трансформеров как универсальной основы для задач компьютерного зрения.'}, 'en': {'title': 'UniVid: A Unified Framework for Diverse Vision Tasks', 'desc': "The paper introduces UniVid, a pre-trained video diffusion transformer that can be fine-tuned for various vision tasks without needing specific modifications for each task. It leverages the concept of visual sentences, where tasks are represented in a sequence that guides the model's output. UniVid demonstrates strong generalization capabilities across different modalities, such as images and videos, and can adapt to tasks from various sources without requiring extensive pre-training. This approach shows that pre-trained video generation models can provide a scalable and unified framework for handling diverse vision challenges."}, 'zh': {'title': 'UniVid：统一视觉任务的强大工具', 'desc': 'UniVid是一个经过预训练的视频扩散变换器，能够在不进行特定任务修改的情况下，适应多种视觉任务。它通过将任务表示为视觉句子，利用上下文序列来定义任务和期望的输出形式。UniVid在跨模态推理和跨源任务中表现出良好的泛化能力，尽管仅在自然视频数据上进行训练。该研究表明，预训练的视频生成模型可以作为视觉建模的统一和可扩展基础。'}}}, {'id': 'https://huggingface.co/papers/2509.21989', 'title': 'Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in\n  Subject-Driven Generation', 'url': 'https://huggingface.co/papers/2509.21989', 'abstract': 'A novel method disentangles visual and semantic features from diffusion model backbones to quantify and localize visual inconsistencies in subject-driven image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:https://abdo-eldesokey.github.io/mind-the-glitch/', 'score': 7, 'issue_id': 6133, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'b1af94f4c4534275', 'authors': ['Abdelrahman Eldesokey', 'Aleksandar Cvejic', 'Bernard Ghanem', 'Peter Wonka'], 'affiliations': ['KAUST, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2509.21989.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Разделяй и находи: обнаружение визуальных глитчей в генерации изображений', 'desc': 'Исследователи предлагают новый метод для разделения визуальных и семантических признаков в диффузионных моделях, что позволяет находить визуальные несоответствия в генерации изображений. Они создали автоматизированный пайплайн для построения пар изображений с аннотированными семантическими и визуальными соответствиями, используя контрастивную архитектуру для разделения типов признаков. На основе разделенных представлений была предложена новая метрика VSM (Visual Semantic Matching) для количественной оценки визуальных несоответствий. Метод превосходит глобальные метрики на основе CLIP и DINO, а также впервые позволяет не только количественно оценить, но и пространственно локализовать области несоответствий.'}, 'en': {'title': 'Disentangling Features for Better Image Generation Analysis', 'desc': 'This paper presents a new method for separating visual and semantic features in diffusion models, which are used for generating images. The authors create an automated system that pairs images with labeled visual and semantic correspondences, allowing for better analysis of image generation. They introduce a contrastive architecture to effectively disentangle these features and propose a new metric called Visual Semantic Matching (VSM) to measure visual inconsistencies. Their approach not only quantifies these inconsistencies but also identifies specific areas in the images where problems occur, improving upon existing methods.'}, 'zh': {'title': '分离视觉与语义特征，量化图像生成不一致性', 'desc': '本文提出了一种新方法，从扩散模型的基础架构中分离视觉特征和语义特征，以量化和定位在主题驱动的图像生成中的视觉不一致性。我们设计了一个自动化流程，基于现有的主题驱动图像生成数据集构建带有注释的图像对，并设计了对比架构来分离这两种特征类型。通过利用分离的表示，我们提出了一种新的度量标准——视觉语义匹配（VSM），用于量化视觉不一致性。实验结果表明，我们的方法在量化视觉不一致性方面优于基于全局特征的度量，如CLIP和DINO，同时也能够实现不一致区域的空间定位。'}}}, {'id': 'https://huggingface.co/papers/2509.21799', 'title': 'D-Artemis: A Deliberative Cognitive Framework for Mobile GUI\n  Multi-Agents', 'url': 'https://huggingface.co/papers/2509.21799', 'abstract': 'D-Artemis, a novel deliberative framework, enhances GUI automation by leveraging app-specific tips, proactive alignment, and reflection, achieving state-of-the-art results with general-purpose multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.', 'score': 7, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '12de0e125e96b91e', 'authors': ['Hongze Mi', 'Yibo Feng', 'Wenjie Lu', 'Yuqi Wang', 'Jinyuan Li', 'Song Cao', 'He Cui', 'Tengfei Tian', 'Xuelin Zhang', 'Haotian Luo', 'Di Sun', 'Naiqiang Tan', 'Gang Pan'], 'affiliations': ['Didichuxing Co. Ltd', 'Huazhong Agricultural University', 'Sichuan University', 'The Chinese University of Hong Kong, Shenzhen', 'Tianjin University', 'Tianjin University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.21799.jpg', 'data': {'categories': ['#agi', '#multimodal', '#agents', '#benchmark', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Когнитивный агент для автоматизации GUI с циклом мышления и рефлексии', 'desc': 'В работе представлен D-Artemis — новый фреймворк для автоматизации действий в графических интерфейсах, который имитирует человеческий когнитивный цикл мышления, согласования и рефлексии. Система использует специфичные для приложений подсказки, проактивную проверку согласованности действий и агент коррекции для предотвращения ошибок выполнения. Фреймворк также включает агент рефлексии статуса для обучения на опыте после выполнения действий. D-Artemis достигает новых рекордных результатов на бенчмарках AndroidWorld (75.8%) и ScreenSpot-V2 (96.8%), используя обычные мультимодальные LLM без дообучения на сложных траекторных данных.'}, 'en': {'title': 'Revolutionizing GUI Automation with D-Artemis!', 'desc': 'D-Artemis is a new framework designed to improve the automation of Graphical User Interfaces (GUIs) by using a structured approach inspired by human thinking processes. It incorporates a mechanism for retrieving specific tips related to applications, which aids in making better decisions during automation tasks. The framework includes proactive checks and corrections before actions are executed, as well as a reflection phase after actions to learn from outcomes. By utilizing general-purpose multimodal large language models, D-Artemis achieves impressive results on benchmark tests without needing extensive training on complex datasets.'}, 'zh': {'title': 'D-Artemis：提升GUI自动化的新框架', 'desc': 'D-Artemis是一个新颖的深思框架，通过利用特定应用的提示、主动对齐和反思，提升了图形用户界面（GUI）自动化的能力。该框架解决了当前方法在端到端训练中的数据瓶颈、延迟错误检测的高成本和矛盾指导的风险。D-Artemis采用细粒度的应用特定提示检索机制，并在执行前进行主动对齐，以减少执行失败的风险。通过在执行后进行状态反思，D-Artemis能够从经验中进行战略学习，展示了在主要基准测试中达到新的最先进结果的能力。'}}}, {'id': 'https://huggingface.co/papers/2509.21574', 'title': 'X-Streamer: Unified Human World Modeling with Audiovisual Interaction', 'url': 'https://huggingface.co/papers/2509.21574', 'abstract': "X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.", 'score': 5, 'issue_id': 6129, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'addda28b1d3ecc10', 'authors': ['You Xie', 'Tianpei Gu', 'Zenan Li', 'Chenxu Zhang', 'Guoxian Song', 'Xiaochen Zhao', 'Chao Liang', 'Jianwen Jiang', 'Hongyi Xu', 'Linjie Luo'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.21574.jpg', 'data': {'categories': ['#alignment', '#architecture', '#multimodal', '#agi', '#interpretability', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Цифровой человек в реальном времени из одного портрета', 'desc': 'В статье представлен X-Streamer — фреймворк для создания цифровых человеческих агентов, способных к бесконечным взаимодействиям через текст, речь и видео в единой архитектуре. Система использует dual-transformer архитектуру Thinker-Actor, где модуль Thinker воспринимает и анализирует потоковые пользовательские входы, а Actor переводит скрытые состояния в синхронизированные мультимодальные потоки в реальном времени. Для генерации используется chunk-wise autoregressive diffusion модель, которая производит временно выровненные мультимодальные ответы с дискретными текстовыми и аудио токенами и непрерывными видео латентами. Система работает в реальном времени на двух GPU A100, обеспечивая многочасовые консистентные видеочаты из произвольных портретов.'}, 'en': {'title': 'X-Streamer: Real-Time Multimodal Interactions Redefined', 'desc': 'X-Streamer is a cutting-edge framework that combines text, speech, and video interactions using a dual-transformer architecture. It allows for real-time communication by transforming a static image into a dynamic digital human capable of engaging in endless conversations. The framework consists of two main components: the Thinker, which processes and understands user inputs, and the Actor, which generates synchronized multimodal outputs. By utilizing advanced models and attention mechanisms, X-Streamer ensures smooth and coherent interactions, making it a significant advancement in creating interactive digital agents.'}, 'zh': {'title': 'X-Streamer：实时多模态互动的新纪元', 'desc': 'X-Streamer是一个统一的多模态框架，采用双变换器架构，能够实现文本、语音和视频之间的实时互动。它通过流式多模态输入，将静态肖像转变为持久的智能视听互动。框架的核心是Thinker-Actor双变换器，Thinker模块负责感知和推理用户输入，而Actor模块则将这些信息实时转换为同步的多模态输出。X-Streamer在两块A100 GPU上实时运行，能够支持数小时的持续视频聊天，推动互动数字人类的统一世界建模。'}}}, {'id': 'https://huggingface.co/papers/2509.22601', 'title': 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.22601', 'abstract': 'SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.', 'score': 4, 'issue_id': 6130, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '609629206f21aeac', 'authors': ['Yulei Qin', 'Xiaoyu Tan', 'Zhengbao He', 'Gang Li', 'Haojia Lin', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Siqi Cai', 'Renting Rui', 'Shaofei Cai', 'Yuzheng Cai', 'Xuan Zhang', 'Sheng Ye', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2509.22601.jpg', 'data': {'categories': ['#training', '#rl', '#games', '#rlhf', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Балансировка исследования и эксплуатации в RL через самоимитацию', 'desc': 'В статье представлен метод SPEAR - подход к обучению с подкреплением для LLM, основанный на самоимитации и curriculum learning. Метод решает проблему баланса между исследованием новых стратегий и эксплуатацией уже найденных успешных решений при обучении языковых моделей работе с инструментами. SPEAR использует внутренние награды для стимулирования исследования на уровне навыков и replay buffer для хранения успешных траекторий, постепенно смещая акцент с исследования на эксплуатацию. Дополнительно применяются техники стабилизации обучения, включая контроль энтропии на уровне траекторий и обрезание токенов с высокой ковариацией между вероятностью и преимуществом.'}, 'en': {'title': 'Balancing Exploration and Exploitation in LLMs with SPEAR', 'desc': 'This paper introduces SPEAR, a method that enhances reinforcement learning for large language models (LLMs) by balancing exploration and exploitation through self-imitation learning. It addresses the challenge of maintaining stability during training by using intrinsic rewards and managing trajectory-level entropy. SPEAR employs a curriculum-based approach, gradually guiding the policy evolution while ensuring that exploration remains effective without leading to instability. By leveraging a replay buffer of successful experiences, the method allows LLMs to refine their tool-use skills and adapt to complex environments efficiently.'}, 'zh': {'title': '平衡探索与利用的自我模仿学习', 'desc': '本文提出了一种名为SPEAR的自我模仿学习方法，旨在平衡强化学习中的探索与利用。该方法通过内在奖励和轨迹级熵控制来管理大语言模型（LLM）的学习过程。SPEAR利用课程学习的策略，逐步引导策略演变，确保在训练过程中保持适当的熵水平。通过这种方式，模型能够在积累工具使用技能的同时，避免训练不稳定和策略漂移的问题。'}}}, {'id': 'https://huggingface.co/papers/2509.21500', 'title': 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large\n  Language Model Post-Training', 'url': 'https://huggingface.co/papers/2509.21500', 'abstract': 'Rubric-based rewards mitigate reward over-optimization in reinforcement fine-tuning by leveraging off-policy examples while maintaining reward reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .', 'score': 4, 'issue_id': 6132, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '37166ab955881fa0', 'authors': ['Junkai Zhang', 'Zihao Wang', 'Lin Gui', 'Swarnashree Mysore Sathyendra', 'Jaehwan Jeong', 'Victor Veitch', 'Wei Wang', 'Yunzhong He', 'Bing Liu', 'Lifeng Jin'], 'affiliations': ['Scale AI, Inc.', 'University of California, Los Angeles', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.21500.jpg', 'data': {'categories': ['#rl', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': '📝', 'ru': {'title': 'Рубрики против обмана: как избежать переоптимизации наград в fine-tuning', 'desc': 'Исследование посвящено проблеме чрезмерной оптимизации наград в reinforcement fine-tuning, когда модель начинает обманывать систему наград для получения высоких баллов при низком качестве ответов. Авторы показывают, что ключевая проблема заключается в неправильной спецификации наград в области высоких значений - неспособности надежно различать отличные ответы от просто хороших. Для решения этой проблемы предлагается использовать награды на основе рубрик, которые могут использовать off-policy примеры и остаются нечувствительными к их артефактам. Экспериментально доказано, что такой подход существенно снижает переоптимизацию наград и улучшает качество post-training LLM.'}, 'en': {'title': 'Rubric-Based Rewards: Enhancing Quality in Reinforcement Fine-Tuning', 'desc': 'This paper addresses the problem of reward over-optimization in reinforcement fine-tuning (RFT) of language models, where models exploit reward signals to achieve high scores but produce low-quality outputs. The authors identify that the issue stems from reward misspecification, particularly in distinguishing between high-quality responses. To overcome this, they propose using rubric-based rewards that can effectively utilize off-policy examples while avoiding the pitfalls of reward artifacts. Their empirical results show that this approach significantly improves the quality of language model outputs during post-training.'}, 'zh': {'title': '基于评分标准的奖励：减轻强化微调中的过度优化', 'desc': '本论文探讨了在强化微调中，如何通过基于评分标准的奖励来减轻奖励过度优化的问题。强化微调常常导致模型通过操控奖励信号获得高分，但输出质量却很低。我们分析了奖励在高奖励尾部的错误指定问题，强调了区分优秀和仅仅良好的响应的重要性。通过引入基于评分标准的奖励，我们能够利用离线示例，同时保持奖励的可靠性，从而有效改善大语言模型的后期训练效果。'}}}, {'id': 'https://huggingface.co/papers/2509.22244', 'title': 'FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image\n  Editing', 'url': 'https://huggingface.co/papers/2509.22244', 'abstract': 'FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150times speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.', 'score': 3, 'issue_id': 6129, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'ea3d5abcdd7f6370', 'authors': ['Junyi Wu', 'Zhiteng Li', 'Haotong Qin', 'Xiaohong Liu', 'Linghe Kong', 'Yulun Zhang', 'Xiaokang Yang'], 'affiliations': ['ETH Zurich', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22244.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference', '#open_source', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Мгновенное редактирование изображений с диффузионными моделями', 'desc': 'FlashEdit — это новая система для редактирования изображений с использованием диффузионных моделей в реальном времени. Основные инновации включают одношаговый процесс инверсии и редактирования (OSIE), технику защиты фона (BG-Shield) и механизм разреженного пространственного кросс-внимания (SSCA). Система обеспечивает высококачественное редактирование изображений менее чем за 0.2 секунды, что в 150 раз быстрее традиционных методов. FlashEdit сохраняет консистентность фона и структурную целостность изображения при локализованных изменениях.'}, 'en': {'title': 'Real-Time Image Editing Revolutionized with FlashEdit', 'desc': 'FlashEdit is a cutting-edge framework that allows for real-time image editing using diffusion models, significantly improving the editing speed and quality. It introduces a One-Step Inversion-and-Editing (OSIE) pipeline that eliminates the need for slow iterative processes, enabling quick modifications. The Background Shield (BG-Shield) technique ensures that the background remains intact while only the desired features are altered, enhancing the overall visual coherence. Additionally, the Sparsified Spatial Cross-Attention (SSCA) mechanism allows for precise edits by minimizing unwanted changes to the background, achieving edits in under 0.2 seconds, which is over 150 times faster than previous methods.'}, 'zh': {'title': 'FlashEdit：实时高保真图像编辑的新突破', 'desc': 'FlashEdit 是一个新颖的框架，能够实现实时、高保真的图像编辑。它通过三个关键创新提高了效率：首先，采用了一种一步反演与编辑（OSIE）流程，避免了耗时的迭代过程；其次，背景保护技术（BG-Shield）确保了背景的一致性，仅在编辑区域内进行特征修改；最后，稀疏空间交叉注意力（SSCA）机制确保了精确的局部编辑，防止语义信息泄漏到背景中。实验结果表明，FlashEdit 在保持背景一致性和结构完整性的同时，编辑速度超过0.2秒，比之前的多步骤方法快150倍以上。'}}}, {'id': 'https://huggingface.co/papers/2509.22650', 'title': 'RefAM: Attention Magnets for Zero-Shot Referral Segmentation', 'url': 'https://huggingface.co/papers/2509.22650', 'abstract': "A new method leverages diffusion transformers' attention scores for referring segmentation without fine-tuning or additional training, improving performance through stop word filtering and attention redistribution.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components.", 'score': 2, 'issue_id': 6134, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '11aa4feab18f599f', 'authors': ['Anna Kukleva', 'Enis Simsar', 'Alessio Tonioni', 'Muhammad Ferjad Naeem', 'Federico Tombari', 'Jan Eric Lenssen', 'Bernt Schiele'], 'affiliations': ['ETH Zurich', 'Google', 'Max Planck Institute for Informatics', 'TU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2509.22650.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#cv', '#diffusion'], 'emoji': '🎯', 'ru': {'title': 'Сегментация без обучения через внимание диффузионных моделей', 'desc': 'Исследователи предложили новый метод для задач referring segmentation, используя attention scores из диффузионных трансформеров без дополнительного обучения. Ключевое открытие заключается в том, что стоп-слова притягивают избыточное внимание и их можно отфильтровать для снижения шума. Авторы обнаружили глобальные attention sinks в глубоких слоях модели и показали, как их можно подавлять или перенаправлять. Разработанный фреймворк RefAM достигает state-of-the-art результатов на бенчмарках без fine-tuning.'}, 'en': {'title': 'RefAM: Revolutionizing Referring Segmentation with Attention Redistribution', 'desc': 'This paper presents a novel method for referring segmentation that utilizes attention scores from diffusion transformers without the need for fine-tuning or extra training. The authors highlight the importance of filtering stop words, which can clutter attention and reduce performance, and propose a strategy to redistribute attention to improve segmentation accuracy. They also identify global attention sinks in deeper layers of the model and demonstrate how redirecting this attention can enhance the quality of grounding maps. The proposed framework, RefAM, achieves state-of-the-art results in zero-shot segmentation tasks for both images and videos, showcasing its effectiveness and efficiency.'}, 'zh': {'title': '无须微调的指代分割新方法', 'desc': '本文提出了一种新方法，利用扩散变换器的注意力分数进行指代分割，无需微调或额外训练。该方法通过过滤停用词和重新分配注意力来提高性能，避免了传统方法中常见的架构修改和额外训练的成本。研究表明，停用词会吸引过多的注意力，因此可以通过过滤来减少噪声。此外，本文还提出了一种注意力重新分配策略，使得背景激活被划分为更小的聚类，从而生成更清晰和更局部化的热图。'}}}, {'id': 'https://huggingface.co/papers/2509.22496', 'title': 'Where MLLMs Attend and What They Rely On: Explaining Autoregressive\n  Token Generation', 'url': 'https://huggingface.co/papers/2509.22496', 'abstract': 'EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.', 'score': 2, 'issue_id': 6129, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'ad53ca1f5bbc2302', 'authors': ['Ruoyu Chen', 'Xiaoqing Guo', 'Kangwei Liu', 'Siyuan Liang', 'Shiming Liu', 'Qunli Zhang', 'Hua Zhang', 'Xiaochun Cao'], 'affiliations': ['Department of Computer Science, Hong Kong Baptist University', 'Institute of Information Engineering, Chinese Academy of Sciences', 'RAMS Lab, Huawei Technologies Co., Ltd.', 'RAMS Lab, Munich Research Center, Huawei Technologies Düsseldorf GmbH', 'School of Computing, NUS', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.22496.jpg', 'data': {'categories': ['#hallucinations', '#inference', '#multimodal', '#interpretability', '#open_source'], 'emoji': '🦅', 'ru': {'title': 'Объясняем каждый токен: как MLLM видят и говорят', 'desc': 'EAGLE - это легковесный фреймворк для объяснения процесса генерации токенов в мультимодальных больших языковых моделях. Система анализирует, как каждый сгенерированный токен связан с визуальными областями изображения и языковыми приоритетами модели. Фреймворк использует объективную функцию, которая объединяет оценки достаточности и необходимости для точной атрибуции токенов к регионам изображения. Эксперименты показывают, что EAGLE превосходит существующие методы в точности, локализации и диагностике галлюцинаций при меньших требованиях к GPU памяти.'}, 'en': {'title': 'EAGLE: Unraveling Token Generation in Multimodal Models', 'desc': 'EAGLE is a new framework designed to explain how multimodal large language models (MLLMs) generate tokens by linking them to specific visual areas. It quantifies the influence of both language and visual information on token generation, enhancing the interpretability of these models. The framework uses an objective function to measure how essential and sufficient different visual regions are for generating specific tokens, optimizing this through a greedy search method. EAGLE has been shown to outperform existing methods in terms of accuracy and efficiency while using less computational resources, making it a practical tool for understanding MLLM behavior.'}, 'zh': {'title': 'EAGLE：提升多模态语言模型可解释性的轻量级框架', 'desc': 'EAGLE是一个轻量级框架，用于解释多模态大型语言模型中的令牌生成。它通过将令牌归因于视觉区域，并量化语言和感知证据的影响，来提高模型的可解释性。该框架引入了一个目标函数，统一了充分性和必要性评分，并通过贪婪搜索优化稀疏图像区域。EAGLE在多个开源多模态大型语言模型上的实验表明，其在可信度、定位和幻觉诊断方面均优于现有方法，同时显著减少了GPU内存的需求。'}}}, {'id': 'https://huggingface.co/papers/2509.22630', 'title': 'StateX: Enhancing RNN Recall via Post-training State Expansion', 'url': 'https://huggingface.co/papers/2509.22630', 'abstract': 'StateX is a post-training pipeline that expands the state size of pre-trained RNNs, enhancing recall and in-context learning without significant additional costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities.', 'score': 1, 'issue_id': 6131, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'ba2dc999ea74187f', 'authors': ['Xingyu Shen', 'Yingfa Chen', 'Zhen Leng Thai', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Department of Science and Technology, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.22630.jpg', 'data': {'categories': ['#optimization', '#training', '#long_context', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Расширение памяти RNN без лишних затрат', 'desc': 'В статье представлен StateX - метод пост-обучения для увеличения размера состояния предобученных рекуррентных нейронных сетей (RNN). Проблема заключается в том, что RNN имеют постоянную сложность вычислений на токен, но плохо запоминают информацию из длинных контекстов из-за сжатия всей информации в состояние фиксированного размера. StateX позволяет эффективно расширить размер состояния для линейного внимания и state space моделей без значительного увеличения количества параметров. Эксперименты показали, что метод улучшает способность к запоминанию и обучению в контексте без высоких затрат на пост-обучение.'}, 'en': {'title': 'Enhancing RNN Recall with StateX: Bigger States, Lower Costs!', 'desc': "StateX is a novel post-training pipeline designed to increase the state size of pre-trained recurrent neural networks (RNNs), which improves their ability to recall information and learn from context. Traditional RNNs face challenges with long contexts because they compress all information into a fixed-size state, limiting their recall capabilities. StateX addresses this issue by implementing architectural modifications that allow for larger states without significantly increasing the model's parameters or training costs. Experiments show that StateX effectively enhances the performance of RNNs, particularly in recall and in-context learning, while maintaining efficiency and other functionalities."}, 'zh': {'title': 'StateX：高效扩展RNN状态的后训练管道', 'desc': 'StateX 是一个后训练管道，旨在扩展预训练递归神经网络（RNN）的状态大小，从而在不显著增加成本的情况下增强记忆和上下文学习能力。尽管基于变换器的模型在语言建模方面表现出色，但其高复杂性导致处理长上下文时成本高昂。相比之下，线性注意力和状态空间模型等递归神经网络因其每个标记的复杂度恒定而受到欢迎，但在需要准确回忆长上下文信息的任务中表现不佳。StateX 通过后训练有效地扩大了预训练 RNN 的状态大小，实验表明其在增强 RNN 的记忆和上下文学习能力方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.21559', 'title': 'X-CoT: Explainable Text-to-Video Retrieval via LLM-based\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2509.21559', 'abstract': 'X-CoT, an explainable retrieval framework using LLM CoT reasoning, enhances text-to-video retrieval by providing detailed rationales and improving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise a retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.', 'score': 1, 'issue_id': 6133, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'd84ca773b3bb2467', 'authors': ['Prasanna Reddy Pulakurthi', 'Jiamian Wang', 'Majid Rabbani', 'Sohail Dianat', 'Raghuveer Rao', 'Zhiqiang Tao'], 'affiliations': ['DEVCOM Army Research Laboratory', 'Rochester Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.21559.jpg', 'data': {'categories': ['#video', '#interpretability', '#reasoning', '#dataset', '#rag', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Объяснимый поиск видео через рассуждения LLM', 'desc': 'Исследователи предлагают X-CoT - новый фреймворк для поиска видео по текстовому описанию, который использует цепочки рассуждений (CoT) больших языковых моделей вместо традиционного косинусного сходства эмбеддингов. Система не только улучшает качество поиска, но и предоставляет детальные объяснения для каждого результата ранжирования. X-CoT позволяет анализировать поведение модели и качество данных, что решает проблему интерпретируемости в задачах поиска видео. Авторы расширили существующие датасеты дополнительными аннотациями для лучшего семантического понимания и снижения предвзятости данных.'}, 'en': {'title': 'Enhancing Text-to-Video Retrieval with Explainable Reasoning', 'desc': 'X-CoT is a novel framework designed to enhance text-to-video retrieval by utilizing Large Language Model (LLM) Chain of Thought (CoT) reasoning. Unlike traditional methods that rely on embedding models and cosine similarity for ranking, X-CoT provides detailed rationales for its retrieval decisions, improving interpretability. The framework addresses the challenges of low-quality text-video pairs by expanding benchmarks with additional video annotations, which aids in semantic understanding. Overall, X-CoT not only boosts retrieval performance but also allows for better analysis of model behavior and data quality.'}, 'zh': {'title': 'X-CoT：提升文本到视频检索的可解释性与性能', 'desc': 'X-CoT是一个可解释的检索框架，利用大语言模型的链式推理来增强文本到视频的检索能力。传统的检索系统主要依赖嵌入模型提取特征，并通过余弦相似度进行排名，但存在低质量数据难以识别和缺乏解释性的问题。X-CoT通过引入额外的视频注释来支持语义理解，并设计了一种对比检索的链式推理步骤，从而提供详细的推理过程和完整的排名。实验结果表明，X-CoT不仅提高了检索性能，还增强了模型行为和数据质量的分析能力。'}}}, {'id': 'https://huggingface.co/papers/2509.19768', 'title': 'CHURRO: Making History Readable with an Open-Weight Large\n  Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition', 'url': 'https://huggingface.co/papers/2509.19768', 'abstract': 'CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate text recognition for historical documents can greatly advance the study and preservation of cultural heritage. Existing vision-language models (VLMs), however, are designed for modern, standardized texts and are not equipped to read the diverse languages and scripts, irregular layouts, and frequent degradation found in historical materials.   This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for historical text recognition. The model is trained on CHURRO-DS, the largest historical text recognition dataset to date. CHURRO-DS unifies 155 historical corpora comprising 99,491 pages, spanning 22 centuries of textual heritage across 46 language clusters, including historical variants and dead languages.   We evaluate several open-weight and closed VLMs and optical character recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and 70.1% (handwritten) normalized Levenshtein similarity, surpassing the second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being 15.5 times more cost-effective.   By releasing the model and dataset, we aim to enable community-driven research to improve the readability of historical texts and accelerate scholarship.', 'score': 1, 'issue_id': 6129, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '8759baf8bb974573', 'authors': ['Sina J. Semnani', 'Han Zhang', 'Xinyan He', 'Merve Tekgürler', 'Monica S. Lam'], 'affiliations': ['Stanford University, Stanford, CA'], 'pdf_title_img': 'assets/pdf/title_img/2509.19768.jpg', 'data': {'categories': ['#cv', '#multimodal', '#science', '#dataset', '#open_source'], 'emoji': '📜', 'ru': {'title': 'CHURRO: AI для чтения древних текстов и сохранения культурного наследия', 'desc': 'CHURRO - это специализированная vision-language модель с 3 миллиардами параметров, созданная для распознавания текста в исторических документах. Модель обучена на крупнейшем датасете CHURRO-DS, включающем 99,491 страниц исторических текстов на 46 языковых кластерах за 22 века. CHURRO превосходит другие VLM по точности распознавания печатного (82.3%) и рукописного (70.1%) текста, опережая Gemini 2.5 Pro на 1.4% и 6.5% соответственно. При этом модель в 15.5 раз более экономически эффективна и имеет открытые веса для исследовательского сообщества.'}, 'en': {'title': 'Unlocking Historical Texts with CHURRO', 'desc': 'The paper introduces CHURRO, a vision-language model specifically designed for recognizing historical texts. It is trained on CHURRO-DS, the largest dataset for this purpose, which includes a wide variety of historical documents across multiple languages and scripts. CHURRO outperforms existing models in accuracy and cost-effectiveness, achieving high similarity scores in recognizing both printed and handwritten texts. By making CHURRO and its dataset publicly available, the authors aim to foster research that enhances the understanding and preservation of cultural heritage.'}, 'zh': {'title': 'CHURRO：历史文本识别的新突破', 'desc': 'CHURRO是一种具有30亿参数的开放权重视觉语言模型，专门用于历史文本识别。它在CHURRO-DS数据集上训练，该数据集是迄今为止最大的历史文本识别数据集，包含来自22个世纪的155个历史语料库。CHURRO在识别印刷和手写文本方面的表现优于所有现有模型，且成本效益更高。通过发布该模型和数据集，我们希望促进社区驱动的研究，提升历史文本的可读性。'}}}, {'id': 'https://huggingface.co/papers/2509.22642', 'title': 'WoW: Towards a World omniscient World model Through Embodied Interaction', 'url': 'https://huggingface.co/papers/2509.22642', 'abstract': "WoW, a 14-billion-parameter generative world model trained on robot interactions, demonstrates improved physical intuition through SOPHIA's guidance and achieves state-of-the-art performance on physical consistency and causal reasoning in video.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.", 'score': 0, 'issue_id': 6135, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '4e68109ff13a5006', 'authors': ['Xiaowei Chi', 'Peidong Jia', 'Chun-Kai Fan', 'Xiaozhu Ju', 'Weishi Mi', 'Kevin Zhang', 'Zhiyuan Qin', 'Wanxin Tian', 'Kuangzhi Ge', 'Hao Li', 'Zezhong Qian', 'Anthony Chen', 'Qiang Zhou', 'Yueru Jia', 'Jiaming Liu', 'Yong Dai', 'Qingpo Wuwu', 'Chengyu Bai', 'Yu-Kai Wang', 'Ying Li', 'Lizhang Chen', 'Yong Bao', 'Zhiyuan Jiang', 'Jiacheng Zhu', 'Kai Tang', 'Ruichuan An', 'Yulin Luo', 'Qiuxuan Feng', 'Siyuan Zhou', 'Chi-min Chan', 'Chengkai Hou', 'Wei Xue', 'Sirui Han', 'Yike Guo', 'Shanghang Zhang', 'Jian Tang'], 'affiliations': ['Beijing Innovation Center of Humanoid Robotics', 'Hong Kong University of Science and Technology', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22642.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#dataset', '#video', '#robotics', '#agents', '#agi', '#hallucinations', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Физическая интуиция AI через реальное взаимодействие с миром', 'desc': 'Исследователи создали WoW — генеративную модель мира с 14 миллиардами параметров, обученную на 2 миллионах траекторий взаимодействий роботов с реальным миром. Модель демонстрирует понимание физики как вероятностное распределение возможных исходов, что приводит к стохастическим нестабильностям и физическим галлюцинациям. Для повышения физической реалистичности используется система SOPHIA, где vision-language модели оценивают и направляют процесс генерации через итеративное улучшение языковых инструкций. WoW показывает state-of-the-art результаты на новом бенчмарке WoWBench, демонстрируя сильные способности в понимании физической причинности, динамики столкновений и постоянства объектов.'}, 'en': {'title': 'WoW: Learning Physics Through Active Interaction', 'desc': "The paper introduces WoW, a large generative world model with 14 billion parameters, trained on extensive robot interactions to enhance its understanding of physical intuition. Unlike traditional models that passively observe, WoW learns through active engagement, allowing it to grasp physical causality more effectively. The model's performance is refined by SOPHIA, a vision-language model that guides the output towards physical realism, while an Inverse Dynamics Model translates these outputs into actionable robotic tasks. The study establishes WoWBench, a benchmark for evaluating physical consistency and causal reasoning, where WoW achieves leading results, underscoring the importance of real-world interactions in developing AI's physical intuition."}, 'zh': {'title': '真实交互，提升AI物理直觉', 'desc': 'WoW是一个拥有140亿参数的生成世界模型，专注于机器人交互的训练。通过与真实世界的丰富因果交互，WoW展现了更强的物理直觉，超越了传统视频模型的局限。该模型的物理理解是基于概率分布，虽然存在随机不稳定性和物理幻觉，但通过SOPHIA的指导，可以有效约束其向物理现实的演变。我们的研究表明，大规模的真实世界交互是AI发展物理直觉的基石。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (8)', '#agi (3)', '#alignment (5)', '#architecture (5)', '#audio (1)', '#benchmark (13)', '#cv (8)', '#data (2)', '#dataset (9)', '#diffusion (6)', '#ethics (1)', '#games (5)', '#graphs (1)', '#hallucinations (3)', '#healthcare', '#inference (3)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (9)', '#open_source (6)', '#optimization (14)', '#plp', '#rag (2)', '#reasoning (11)', '#rl (10)', '#rlhf (8)', '#robotics (1)', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (12)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-29 08:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-29 08:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-29 08:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    