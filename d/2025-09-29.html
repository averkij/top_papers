
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. September 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 сентября</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-26.html">⬅️ <span id="prev-date">26.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-30.html">➡️ <span id="next-date">30.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'};
        let feedDateNext = {'ru': '30.09', 'en': '09/30', 'zh': '9月30日'};
        let feedDatePrev = {'ru': '26.09', 'en': '09/26', 'zh': '9月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.19803', 'title': 'VCRL: Variance-based Curriculum Reinforcement Learning for Large\n  Language Models', 'url': 'https://huggingface.co/papers/2509.19803', 'abstract': "A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.", 'score': 111, 'issue_id': 6099, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '28f78fd1381a5d22', 'authors': ['Guochao Jiang', 'Wenfeng Feng', 'Guofeng Quan', 'Chuzhan Hao', 'Yuewei Zhang', 'Guohua Liu', 'Hao Wang'], 'affiliations': ['Alibaba Cloud Computing'], 'pdf_title_img': 'assets/pdf/title_img/2509.19803.jpg', 'data': {'categories': ['#math', '#training', '#rl', '#optimization', '#reasoning'], 'emoji': '📈', 'ru': {'title': 'Обучение через дисперсию: адаптивная сложность для математического мышления LLM', 'desc': 'Исследователи предложили VCRL - новый подход к обучению языковых моделей решению математических задач с использованием curriculum reinforcement learning. Метод динамически регулирует сложность обучающих примеров на основе дисперсии наград, что имитирует человеческий подход к изучению математики от простого к сложному. Авторы обнаружили, что дисперсия наград группы отражает сложность задачи для модели: слишком легкие и слишком сложные задачи имеют низкую дисперсию, а задачи умеренной сложности - высокую. Эксперименты на пяти математических benchmarks показали преимущества VCRL над существующими методами reinforcement learning для LLM.'}, 'en': {'title': 'Dynamic Difficulty for Smarter Learning in LLMs', 'desc': 'This paper introduces a new framework called VCRL, which stands for Variance-Controlled Reinforcement Learning, aimed at enhancing the performance of large language models (LLMs) in mathematical reasoning tasks. The framework adjusts the difficulty of training samples based on the variance of rewards received during training, aligning with how humans typically learn from easier to harder problems. By focusing on samples with moderate difficulty, which show higher reward variance, VCRL helps LLMs learn more effectively. Experiments demonstrate that VCRL outperforms existing reinforcement learning methods in improving LLM capabilities on various mathematical benchmarks.'}, 'zh': {'title': '动态调整样本难度，提升数学推理能力', 'desc': '这篇论文提出了一种课程强化学习框架，称为VCRL，旨在根据奖励方差动态调整训练样本的难度，以提高大型语言模型（LLM）在数学推理任务上的表现。现有的基于回合的强化学习方法未能充分考虑LLM对不同难度样本的学习能力，这与人类在解决数学问题时从易到难的认知过程相悖。研究发现，回合组奖励的方差可以部分反映当前样本的难度，适中难度的样本具有较高的方差，而过于简单或困难的样本则方差较低。通过在五个数学基准和两个模型上的实验，VCRL显示出相较于现有LLM强化学习基线的优势。'}}}, {'id': 'https://huggingface.co/papers/2509.21268', 'title': 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources', 'url': 'https://huggingface.co/papers/2509.21268', 'abstract': 'Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at https://github.com/LengSicong/MMR1.', 'score': 90, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '7153bc23f1974ebe', 'authors': ['Sicong Leng', 'Jing Wang', 'Jiaxi Li', 'Hao Zhang', 'Zhiqiang Hu', 'Boqiang Zhang', 'Yuming Jiang', 'Hang Zhang', 'Xin Li', 'Lidong Bing', 'Deli Zhao', 'Wei Lu', 'Yu Rong', 'Aixin Sun', 'Shijian Lu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2509.21268.jpg', 'data': {'categories': ['#dataset', '#training', '#architecture', '#reasoning', '#benchmark', '#rl', '#optimization', '#multimodal', '#data', '#open_source'], 'emoji': '🎯', 'ru': {'title': 'Стабилизация RL-обучения через управление дисперсией вознаграждений', 'desc': 'Исследователи предложили метод Variance-Aware Sampling (VAS) для улучшения обучения больших мультимодальных моделей рассуждения с подкреплением. Основная проблема заключается в нестабильности алгоритмов RL из-за низкой дисперсии вознаграждений, что приводит к исчезновению градиентов. VAS использует показатель Variance Promotion Score для отбора данных, которые увеличивают дисперсию вознаграждений и стабилизируют оптимизацию политики. Авторы также выпустили крупномасштабный датасет с 1.6M примерами длинных цепочек рассуждений и семейство открытых мультимодальных моделей.'}, 'en': {'title': 'Boosting Multimodal Reasoning with Variance-Aware Sampling and Quality Data', 'desc': 'This paper addresses the challenges faced by large multimodal reasoning models, particularly the lack of high-quality long chain-of-thought (CoT) data and the instability of reinforcement learning (RL) during fine-tuning. It introduces Variance-Aware Sampling (VAS), a method that enhances reward variance and stabilizes policy optimization by selecting data based on outcome variance and trajectory diversity. The authors also provide a substantial dataset of approximately 1.6 million CoT examples and 15,000 RL question-answer pairs, ensuring diversity and quality for training. Additionally, they release a set of multimodal reasoning models and establish standardized benchmarks for future research in the field.'}, 'zh': {'title': '方差感知采样提升多模态推理模型性能', 'desc': '本文提出了一种新的数据选择策略，称为方差感知采样（VAS），旨在提高多模态推理模型的性能。通过结合结果方差和轨迹多样性，VAS可以促进奖励方差，从而稳定强化学习（RL）优化过程。我们还发布了大规模的高质量长链思维（CoT）数据集，包含约160万条冷启动数据和约15000个RL问答对，以支持模型训练。实验结果表明，VAS和新数据集显著提升了模型在数学推理基准上的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.21320', 'title': 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines', 'url': 'https://huggingface.co/papers/2509.21320', 'abstract': 'A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.', 'score': 86, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'e33c7b540d84ad9a', 'authors': ['Yizhou Wang', 'Chen Tang', 'Han Deng', 'Jiabei Xiao', 'Jiaqi Liu', 'Jianyu Wu', 'Jun Yao', 'Pengze Li', 'Encheng Su', 'Lintao Wang', 'Guohang Zhuang', 'Yuchen Ren', 'Ben Fei', 'Ming Hu', 'Xin Chen', 'Dongzhan Zhou', 'Junjun He', 'Xiangyu Yue', 'Zhenfei Yin', 'Jiamin Wu', 'Qihao Zheng', 'Yuhao Zhou', 'Huihui Xu', 'Chenglong Ma', 'Yan Lu', 'Wenlong Zhang', 'Chunfeng Song', 'Philip Torr', 'Shixiang Tang', 'Xinzhu Ma', 'Wanli Ouyang', 'Lei Bai'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21320.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#transfer_learning', '#multimodal', '#data', '#science', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Универсальный AI для научных рассуждений во всех дисциплинах', 'desc': 'Исследователи создали foundation модель для научных рассуждений, которая может работать с различными типами научных данных - текстом, последовательностями и их парами. Модель была обучена на корпусе из 206 миллиардов токенов, а затем дообучена с помощью supervised fine-tuning на 40 миллионах инструкций и reinforcement learning с task-специфичным reward shaping. Она поддерживает пять семейств задач, включающих перевод между текстом и научными форматами, извлечение знаний, предсказание свойств, классификацию и генерацию последовательностей. Модель показывает лучшую кросс-доменную генерализацию и точность по сравнению со специализированными системами благодаря междисциплинарному обучению.'}, 'en': {'title': 'Empowering Scientific Reasoning with a Versatile Foundation Model', 'desc': 'This paper introduces a scientific reasoning foundation model that is trained on a vast dataset of scientific texts and sequences. It employs advanced techniques like supervised fine-tuning and reinforcement learning to improve its ability to perform various scientific tasks. The model can translate between different scientific formats, extract knowledge, predict properties, and generate sequences, making it versatile across multiple domains. By enhancing cross-domain generalization and fidelity, this model outperforms specialized systems in handling a wide range of scientific inquiries.'}, 'zh': {'title': '科学推理模型：跨领域的智能助手', 'desc': '这篇论文介绍了一种科学推理基础模型，该模型在多样的科学数据上进行预训练，支持多种任务并增强跨领域的泛化能力。模型使用了2060亿个标记的语料库，涵盖科学文本、纯序列和序列-文本对，并通过特定的训练技术进行对齐。它能够执行多达103个任务，包括文本与科学格式之间的翻译、知识提取、属性预测和分类等。与专业系统相比，该模型在指令覆盖范围、跨领域泛化和准确性方面都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2509.21240', 'title': 'Tree Search for LLM Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.21240', 'abstract': 'Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.', 'score': 71, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'bfd427434553568d', 'authors': ['Yuxiang Ji', 'Ziyu Ma', 'Yong Wang', 'Guanhua Chen', 'Xiangxiang Chu', 'Liaoni Wu'], 'affiliations': ['AMAP, Alibaba Group', 'Southern University of Science and Technology', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21240.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#rl', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Древовидный поиск для умного обучения AI-агентов', 'desc': 'Исследователи предложили Tree-GRPO - новый метод обучения с подкреплением для больших языковых моделей, основанный на поиске по дереву. Метод решает проблему разреженного обучающего сигнала в долгосрочных агентских задачах, создавая древовидные траектории взаимодействия. Tree-GRPO оценивает групповые относительные преимущества как внутри дерева, так и между деревьями, что эквивалентно пошаговому обучению предпочтениям. Эксперименты на 11 датасетах показали превосходство древовидного подхода над цепочечными методами RL.'}, 'en': {'title': 'Optimizing Language Models with Tree-Based Reinforcement Learning', 'desc': "Tree-based Group Relative Policy Optimization (Tree-GRPO) is a novel approach in reinforcement learning that enhances the performance of large language models by utilizing tree search techniques. This method improves the efficiency of rollouts and allows for better estimation of grouped relative advantages, addressing the issue of sparse supervision in long-term tasks. By structuring the agent's interactions in a tree format, Tree-GRPO can generate more rollouts within a limited resource budget, leading to improved learning signals. Experimental results show that Tree-GRPO outperforms traditional chain-based methods across various datasets and question-answering tasks."}, 'zh': {'title': '树基优化，提升强化学习效率', 'desc': '树基组相对策略优化（Tree-GRPO）通过树搜索来增强强化学习，特别是针对大型语言模型的应用。该方法通过共享公共前缀，增加了在固定预算内可实现的回合数，从而提高了学习效率。Tree-GRPO能够在树内和树间层面估计分组相对优势，解决了传统方法中稀疏监督的问题。实验结果表明，Tree-GRPO在多个数据集和问答任务中优于基于链的方法。'}}}, {'id': 'https://huggingface.co/papers/2509.20427', 'title': 'Seedream 4.0: Toward Next-generation Multimodal Image Generation', 'url': 'https://huggingface.co/papers/2509.20427', 'abstract': 'Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.', 'score': 63, 'issue_id': 6098, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'fb2f872386c520ce', 'authors': ['Team Seedream', 'Yunpeng Chen', 'Yu Gao', 'Lixue Gong', 'Meng Guo', 'Qiushan Guo', 'Zhiyao Guo', 'Xiaoxia Hou', 'Weilin Huang', 'Yixuan Huang', 'Xiaowen Jian', 'Huafeng Kuang', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yanzuo Lu', 'Zhengxiong Luo', 'Tongtong Ou', 'Guang Shi', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Wenxu Wu', 'Yonghui Wu', 'Xin Xia', 'Xuefeng Xiao', 'Shuang Xu', 'Xin Yan', 'Ceyuan Yang', 'Jianchao Yang', 'Zhonghua Zhai', 'Chenlin Zhang', 'Heng Zhang', 'Qi Zhang', 'Xinyu Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Wenliang Zhao', 'Wenjia Zhu'], 'affiliations': ['ByteDance', 'Volcano Engine'], 'pdf_title_img': 'assets/pdf/title_img/2509.20427.jpg', 'data': {'categories': ['#inference', '#training', '#games', '#multimodal', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Универсальная система для генерации и редактирования изображений нового поколения', 'desc': 'Seedream 4.0 — это высокопроизводительная мультимодальная система генерации изображений, которая объединяет синтез изображений по тексту, редактирование изображений и композицию из нескольких изображений в единой архитектуре. Система использует эффективный диффузионный трансформер с мощным VAE, что позволяет значительно сократить количество токенов изображения и обеспечить быструю генерацию изображений высокого разрешения до 4K. Модель предобучена на миллиардах пар текст-изображение и проходит мультимодальное дообучение с использованием тщательно настроенной VLM модели. Для ускорения инференса применяются техники adversarial distillation, distribution matching, квантизация и speculative decoding, что позволяет генерировать изображение 2K за 1.8 секунды.'}, 'en': {'title': 'Revolutionizing Image Generation with Seedream 4.0', 'desc': 'Seedream 4.0 is a cutting-edge multimodal image generation system that combines text-to-image synthesis, image editing, and multi-image composition into one efficient framework. It utilizes a diffusion transformer and a variational autoencoder (VAE) to significantly reduce image token counts, enabling faster training and high-resolution image generation. The model is pretrained on a vast dataset of text-image pairs, ensuring strong generalization across various scenarios. With advanced techniques for inference acceleration, Seedream 4.0 achieves state-of-the-art performance in both T2I tasks and complex image editing, making it a powerful tool for creative and professional applications.'}, 'zh': {'title': 'Seedream 4.0：多模态图像生成的新纪元', 'desc': 'Seedream 4.0 是一个高性能的多模态图像生成系统，结合了文本到图像合成、图像编辑和多图像组合。它采用了高效的扩散变换器和变分自编码器（VAE），在训练和推理过程中表现出色。该系统经过数十亿对文本-图像对的预训练，确保了强大的泛化能力和稳定性。Seedream 4.0 不仅能快速生成高分辨率图像，还在复杂任务中展现出卓越的多模态能力，推动了生成式人工智能的边界。'}}}, {'id': 'https://huggingface.co/papers/2509.21245', 'title': 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D\n  Assets', 'url': 'https://huggingface.co/papers/2509.21245', 'abstract': 'Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.', 'score': 36, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '8c4661dd2c3016bb', 'authors': ['Team Hunyuan3D', ':', 'Bowen Zhang', 'Chunchao Guo', 'Haolin Liu', 'Hongyu Yan', 'Huiwen Shi', 'Jingwei Huang', 'Junlin Yu', 'Kunhong Li', 'Linus', 'Penghao Wang', 'Qingxiang Lin', 'Sicong Liu', 'Xianghui Yang', 'Yixuan Tang', 'Yunfei Zhao', 'Zeqiang Lai', 'Zhihao Liang', 'Zibo Zhao'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2509.21245.jpg', 'data': {'categories': ['#3d', '#training', '#architecture', '#games', '#synthetic', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'Многомодальный контроль 3D-генерации для игровой индустрии', 'desc': 'Исследователи представили Hunyuan3D-Omni — единую систему для генерации 3D-объектов с множественным контролем. Модель принимает не только изображения и текст, но также облака точек, вокселы, ограничивающие рамки и скелетные позы для точного управления геометрией и позой. Система использует прогрессивную стратегию обучения с учетом сложности модальностей, что улучшает робастность при отсутствии некоторых входных данных. Эксперименты показали повышение точности генерации и практичности для производственных процессов в играх и дизайне.'}, 'en': {'title': 'Unified Control for 3D Asset Generation', 'desc': 'Hunyuan3D-Omni is a comprehensive framework designed for generating 3D assets with enhanced control and reliability. It allows the use of various conditioning signals, such as point clouds and skeletal poses, in addition to traditional images, which improves the precision of the generated models. The framework employs a unified cross-modal architecture, eliminating the need for separate processing heads for each type of input. By using a progressive sampling strategy that prioritizes more complex controls, it ensures better integration of different modalities and improves the overall robustness of the asset generation process.'}, 'zh': {'title': '统一多模态的3D资产生成框架', 'desc': 'Hunyuan3D-Omni是一个统一的3D资产生成框架，能够接受多种条件信号，从而提高生产工作流程中的可控性和鲁棒性。该框架不仅支持图像，还可以处理点云、体素、边界框和骨骼姿态先验等多种输入信号，实现对几何形状、拓扑结构和姿态的精确控制。与传统方法不同，Hunyuan3D-Omni将所有信号统一在一个跨模态架构中，避免了为每种模态单独设计模型的复杂性。通过逐步的、关注难度的采样策略，我们的模型能够有效融合多模态信息，并在处理缺失输入时表现出良好的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2509.21138', 'title': 'AutoIntent: AutoML for Text Classification', 'url': 'https://huggingface.co/papers/2509.21138', 'abstract': 'AutoIntent is an automated machine learning tool for text classification that offers end-to-end automation, including embedding model selection, classifier optimization, and decision threshold tuning, and supports multi-label classification and out-of-scope detection.  \t\t\t\t\tAI-generated summary \t\t\t\t AutoIntent is an automated machine learning tool for text classification tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with embedding model selection, classifier optimization, and decision threshold tuning, all within a modular, sklearn-like interface. The framework is designed to support multi-label classification and out-of-scope detection. AutoIntent demonstrates superior performance compared to existing AutoML tools on standard intent classification datasets and enables users to balance effectiveness and resource consumption.', 'score': 26, 'issue_id': 6104, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'aeb5276117d67ddf', 'authors': ['Ilya Alekseev', 'Roman Solomatin', 'Darina Rustamova', 'Denis Kuznetsov'], 'affiliations': ['ITMO University', 'Moscow Center for Advanced Studies', 'Moscow State University', 'dresscode.ai'], 'pdf_title_img': 'assets/pdf/title_img/2509.21138.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#data'], 'emoji': '🎯', 'ru': {'title': 'Полная автоматизация классификации текстов от эмбеддингов до решений', 'desc': 'AutoIntent представляет собой инструмент автоматизированного машинного обучения для классификации текстов. Система обеспечивает полную автоматизацию процесса: от выбора модели эмбеддингов до оптимизации классификатора и настройки порогов принятия решений. Фреймворк поддерживает многоклассовую классификацию и детекцию запросов вне области применения. AutoIntent показывает превосходную производительность по сравнению с существующими AutoML решениями на стандартных датасетах для классификации интентов.'}, 'en': {'title': 'Automate Your Text Classification with AutoIntent!', 'desc': 'AutoIntent is a cutting-edge automated machine learning tool specifically designed for text classification tasks. It streamlines the process by providing end-to-end automation, which includes selecting embedding models, optimizing classifiers, and tuning decision thresholds. The tool is versatile, supporting both multi-label classification and out-of-scope detection, making it suitable for a variety of applications. In performance tests, AutoIntent outshines existing AutoML solutions, allowing users to achieve high accuracy while managing resource usage effectively.'}, 'zh': {'title': 'AutoIntent：智能文本分类的自动化解决方案', 'desc': 'AutoIntent 是一个自动化的机器学习工具，专注于文本分类任务。它提供了端到端的自动化功能，包括嵌入模型选择、分类器优化和决策阈值调整。该框架支持多标签分类和超出范围检测，具有模块化的 sklearn 风格接口。与现有的 AutoML 工具相比，AutoIntent 在标准意图分类数据集上表现更优，帮助用户在效果和资源消耗之间取得平衡。'}}}, {'id': 'https://huggingface.co/papers/2509.21117', 'title': 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them', 'url': 'https://huggingface.co/papers/2509.21117', 'abstract': "TrustJudge, a probabilistic framework, addresses inconsistencies in LLM-as-a-judge evaluation by using distribution-sensitive scoring and likelihood-aware aggregation, improving accuracy and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.", 'score': 22, 'issue_id': 6102, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'ccc68ce7d3aad944', 'authors': ['Yidong Wang', 'Yunze Song', 'Tingyuan Zhu', 'Xuanwang Zhang', 'Zhuohao Yu', 'Hao Chen', 'Chiyu Song', 'Qiufeng Wang', 'Cunxiang Wang', 'Zhen Wu', 'Xinyu Dai', 'Yue Zhang', 'Wei Ye', 'Shikun Zhang'], 'affiliations': ['Google DeepMind', 'Institute of Science Tokyo', 'Nanjing University', 'National University of Singapore', 'Peking University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21117.jpg', 'data': {'categories': ['#architecture', '#alignment', '#interpretability', '#data', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'Делаем LLM-судей честными: вероятностный подход против противоречий в оценках', 'desc': 'Исследователи выявили критические проблемы в системах оценки, где LLM выступают в роли судей - несогласованность в скоринге и нарушения транзитивности в парных сравнениях. Они предложили TrustJudge - вероятностную систему, которая использует непрерывные оценки вместо дискретных рейтингов и учитывает вероятности предпочтений для устранения логических противоречий. Фреймворк значительно снижает несогласованность: на 8.43% в сравнении оценок и на 10.82% в транзитивности парных сравнений. Это первый систематический анализ проблем оценочных систем с LLM-судьями, предлагающий как теоретическое понимание, так и практические решения для надежной автоматической оценки.'}, 'en': {'title': 'TrustJudge: Enhancing Reliability in LLM Evaluations', 'desc': 'TrustJudge is a new framework designed to improve the evaluation of Large Language Models (LLMs) acting as judges. It tackles two main problems: inconsistencies in score comparisons and transitivity, which can lead to confusing results in evaluations. By using distribution-sensitive scoring, TrustJudge captures more information from ratings, and likelihood-aware aggregation helps resolve contradictions in preferences. This approach significantly reduces inconsistencies and enhances the accuracy of automated assessments without needing extra training or human input.'}, 'zh': {'title': 'TrustJudge：提升LLM评估一致性的创新框架', 'desc': 'TrustJudge是一个概率框架，旨在解决大型语言模型（LLM）作为评估者时的评估不一致性问题。它通过分布敏感评分和考虑似然性的聚合方法，提升了评估的准确性和可靠性。研究发现，当前评估框架存在评分比较不一致和成对传递不一致等问题，这些问题源于离散评分系统的信息损失。TrustJudge通过计算连续期望和双向偏好概率，成功克服了这些限制，提供了更精确的评估结果。'}}}, {'id': 'https://huggingface.co/papers/2509.20712', 'title': 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.20712', 'abstract': 'A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.', 'score': 16, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'ad1a5016131ff587', 'authors': ['Zhenpeng Su', 'Leiyu Pan', 'Minxuan Lv', 'Yuntao Li', 'Wenping Hu', 'Fuzheng Zhang', 'Kun Gai', 'Guorui Zhou'], 'affiliations': ['Independent', 'Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.20712.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '⚖️', 'ru': {'title': 'Сохраняем градиенты для лучшего баланса в обучении LLM', 'desc': 'В работе предлагается новый алгоритм обучения с подкреплением CE-GPPO для оптимизации больших языковых моделей. Основная проблема существующих методов типа PPO заключается в том, что они отбрасывают ценные градиенты от токенов с низкой вероятностью из-за механизма клиппинга. CE-GPPO решает эту проблему, повторно вводя градиенты от обрезанных токенов контролируемым образом, что улучшает баланс между исследованием и эксплуатацией. Эксперименты на задачах математических рассуждений показывают превосходство метода над существующими подходами.'}, 'en': {'title': 'Enhancing Exploration-Exploitation Balance in Language Models with CE-GPPO', 'desc': 'The paper presents a new reinforcement learning algorithm called CE-GPPO, which enhances the training of large language models by reintroducing gradients from clipped tokens. This approach addresses the challenge of managing policy entropy, which is crucial for balancing exploration and exploitation during training. By carefully controlling the gradients from low-probability tokens, CE-GPPO improves the stability of entropy dynamics, which is often overlooked in existing methods. The authors provide both theoretical insights and empirical results demonstrating that CE-GPPO outperforms traditional algorithms like PPO on various reasoning tasks.'}, 'zh': {'title': '重新引入梯度，优化探索与利用的平衡', 'desc': 'CE-GPPO是一种新颖的强化学习算法，旨在改善大语言模型的探索与利用平衡。该算法通过重新引入被剪切的标记的梯度，解决了现有方法在训练过程中丢失有价值的梯度信号的问题。研究表明，这些被剪切的标记在调节策略熵的演变中起着重要作用。CE-GPPO在多个数学推理基准测试中表现优异，超越了强基线模型。'}}}, {'id': 'https://huggingface.co/papers/2509.20186', 'title': 'Thinking Augmented Pre-training', 'url': 'https://huggingface.co/papers/2509.20186', 'abstract': 'Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to 100B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of 3. For a 3B parameter model, it improves the post-training performance by over 10% on several challenging reasoning benchmarks.', 'score': 16, 'issue_id': 6099, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'edd8ab635a2930b6', 'authors': ['Liang Wang', 'Nan Yang', 'Shaohan Huang', 'Li Dong', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.20186.jpg', 'data': {'categories': ['#data', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Думающие данные: как траектории рассуждений ускоряют обучение LLM в три раза', 'desc': 'В статье представлена методология Thinking augmented Pre-Training (TPT), которая улучшает эффективность обучения больших языковых моделей путем дополнения текстовых данных автоматически сгенерированными траекториями рассуждений. Подход помогает модели лучше усваивать сложные токены через пошаговое рассуждение и декомпозицию. Эксперименты показали, что TPT повышает эффективность использования данных в 3 раза и улучшает производительность модели на 10% на сложных задачах рассуждения. Методология применима к различным конфигурациям обучения до 100B токенов и разным размерам моделей.'}, 'en': {'title': 'Boosting Language Models with Thinking Trajectories', 'desc': 'This paper presents a novel approach called Thinking augmented Pre-Training (TPT) to enhance the data efficiency and performance of large language models (LLMs). By augmenting existing text data with automatically generated thinking trajectories, TPT increases the volume of training data and facilitates the learning of complex tokens through structured reasoning. The methodology addresses the challenge of limited high-quality data by making it easier for models to learn intricate concepts. Experimental results demonstrate that TPT significantly boosts LLM performance, achieving a threefold increase in data efficiency and over 10% improvement in reasoning tasks for a 3B parameter model.'}, 'zh': {'title': '思维增强预训练：提升大型语言模型的数据效率与性能', 'desc': '本文提出了一种简单且可扩展的方法，通过自动生成的思维轨迹来增强现有文本数据，从而提高大型语言模型（LLM）训练的数据效率。随着LLM预训练计算需求的急剧增长，高质量数据的可用性却依然有限，因此如何最大化现有数据的利用成为了一个重要的研究挑战。我们提出的思维增强预训练（TPT）方法，通过逐步推理和分解，使得高质量的标记更易于学习，从而有效增加训练数据的量。实验结果表明，TPT在不同模型规模和类型上显著提升了LLM的性能，数据效率提高了三倍。'}}}, {'id': 'https://huggingface.co/papers/2509.19301', 'title': 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies', 'url': 'https://huggingface.co/papers/2509.19301', 'abstract': 'A residual learning framework combines behavior cloning and reinforcement learning to improve manipulation policies on high-degree-of-freedom systems using sparse binary rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io', 'score': 16, 'issue_id': 6103, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '9db1658d8a8725c2', 'authors': ['Lars Ankile', 'Zhenyu Jiang', 'Rocky Duan', 'Guanya Shi', 'Pieter Abbeel', 'Anusha Nagabandi'], 'affiliations': ['Amazon FAR (Frontier AI & Robotics)', 'Carnegie Mellon University', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.19301.jpg', 'data': {'categories': ['#games', '#optimization', '#rl', '#robotics', '#training'], 'emoji': '🤖', 'ru': {'title': 'Остаточное обучение: от имитации к автономности', 'desc': 'Исследователи предложили новый подход, который объединяет behavior cloning и reinforcement learning через остаточное обучение. Метод использует политики behavior cloning как базовые и дообучает их с помощью легковесных поправок через эффективный по выборкам off-policy RL. Подход работает только со скудными бинарными сигналами вознаграждения и показывает эффективность на системах с высокой степенью свободы. Авторы впервые успешно применили RL для обучения гуманоидного робота с ловкими руками в реальном мире.'}, 'en': {'title': 'Enhancing Robot Manipulation with Residual Learning: Merging Behavior Cloning and Reinforcement Learning', 'desc': 'This paper introduces a novel framework that merges behavior cloning (BC) and reinforcement learning (RL) to enhance manipulation policies for complex robotic systems. By using BC as a foundation, the method applies lightweight residual corrections through off-policy RL, allowing for efficient learning from sparse binary rewards. The approach addresses challenges such as sample inefficiency and safety concerns, particularly in high-degree-of-freedom environments. The authors showcase successful real-world applications, including training a humanoid robot with dexterous hands, achieving state-of-the-art performance in vision-based tasks.'}, 'zh': {'title': '结合行为克隆与强化学习的残差学习框架', 'desc': '本文提出了一种残差学习框架，结合了行为克隆（BC）和强化学习（RL），以提高高自由度系统的操作策略。该方法利用BC策略作为基础，通过样本高效的离线RL学习轻量级的逐步残差修正。我们的方法仅需稀疏的二元奖励信号，能够有效改善高自由度系统的操作策略，并在模拟和真实世界中均取得了成功。特别是，我们首次在具有灵巧手的类人机器人上成功进行了真实世界的RL训练，展示了在各种基于视觉的任务中达到的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2509.21114', 'title': 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling', 'url': 'https://huggingface.co/papers/2509.21114', 'abstract': 'CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces a compact, invertible control-point-based parameterization, where a sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as a sequential "hair language", our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation quality, offering an expressive and scalable solution for anime hairstyle modeling. Project page: https://hyzcluster.github.io/charm/', 'score': 15, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'd22edb1ca787e3d5', 'authors': ['Yuze He', 'Yanning Zhou', 'Wang Zhao', 'Jingwen Ye', 'Yushi Bai', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University and Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21114.jpg', 'data': {'categories': ['#games', '#synthetic', '#training', '#cv', '#architecture', '#dataset'], 'emoji': '💇', 'ru': {'title': 'AI создаёт аниме-причёски через "язык волос"', 'desc': 'В статье представлена CHARM - новая система для создания аниме-причёсок с помощью AI. Вместо традиционных методов моделирования волос используется компактная параметризация на основе контрольных точек, где каждая прядь волос представлена последовательностью точек с пятью геометрическими параметрами. Авторегрессивный трансформер обрабатывает причёски как "язык волос", захватывая как локальную геометрию, так и глобальную топологию. Для обучения и тестирования создан датасет AnimeHair с 37 тысячами высококачественных аниме-причёсок.'}, 'en': {'title': 'Efficient Anime Hairstyle Generation with CHARM', 'desc': 'CHARM is a new method for creating anime hairstyles using a unique control-point-based system and an autoregressive transformer. Unlike traditional methods that use complex mesh or spline techniques, CHARM simplifies the process by representing hairstyles with a few control points, each defined by just five parameters. This makes it easier for artists to design and for machines to learn from, allowing for efficient generation of high-quality hairstyles. The framework is supported by a large dataset of 37,000 anime hairstyles, ensuring that the generated styles are both accurate and visually appealing.'}, 'zh': {'title': '高效生成动漫发型的创新框架', 'desc': 'CHARM是一种新颖的参数化表示和生成框架，专门用于动漫发型建模。与传统的发型建模方法不同，CHARM采用基于控制点的紧凑表示，能够高效地生成高保真度的动漫发型。该方法通过自回归变换器捕捉局部几何和全局发型拓扑，支持从输入图像或点云生成发型。我们还构建了一个包含37K高质量动漫发型的大规模数据集，以促进发型生成的训练和评估。'}}}, {'id': 'https://huggingface.co/papers/2509.21278', 'title': 'Does FLUX Already Know How to Perform Physically Plausible Image\n  Composition?', 'url': 'https://huggingface.co/papers/2509.21278', 'abstract': 'SHINE is a training-free framework that uses manifold-steered anchor loss and pretrained customization adapters to seamlessly insert objects into new scenes with high fidelity, addressing challenges like complex lighting and diverse inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.', 'score': 13, 'issue_id': 6101, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '3cadc9df7abcbbcc', 'authors': ['Shilin Lu', 'Zhuming Lian', 'Zihan Zhou', 'Shaocong Zhang', 'Chen Zhao', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21278.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#cv', '#open_source'], 'emoji': '✨', 'ru': {'title': 'Безупречная вставка объектов в сцены без переобучения', 'desc': 'Статья представляет SHINE - фреймворк без обучения для бесшовной вставки объектов в новые сцены с высокой точностью. Система использует manifold-steered anchor loss и предобученные адаптеры кастомизации для преодоления проблем со сложным освещением и разнообразными входными данными. Авторы предлагают новые техники подавления деградации и адаптивного смешивания фона для устранения видимых швов и низкокачественных результатов. Для оценки представлен новый бенчмарк ComplexCompo с разнообразными разрешениями и сложными условиями освещения.'}, 'en': {'title': 'Seamless Object Insertion with SHINE: High Fidelity, No Training Required!', 'desc': 'SHINE is a novel framework designed for seamlessly inserting objects into new scenes without the need for extensive training. It utilizes manifold-steered anchor loss and pretrained customization adapters to ensure high fidelity in object representation while maintaining the integrity of the background. The framework addresses common challenges in image composition, such as complex lighting and diverse input resolutions, by implementing degradation-suppression guidance and adaptive background blending. Additionally, SHINE introduces a new benchmark, ComplexCompo, to evaluate performance under various challenging conditions, demonstrating state-of-the-art results in both standard and human-aligned metrics.'}, 'zh': {'title': '无缝高保真插入的创新框架', 'desc': 'SHINE是一个无训练的框架，旨在高保真地将对象无缝插入新场景中。它采用了流形引导锚损失和预训练的定制适配器，能够有效应对复杂的光照和多样化的输入。SHINE通过引入降解抑制指导和自适应背景融合，进一步消除低质量输出和可见接缝。实验结果表明，SHINE在标准指标和人类对齐评分上表现出色，展示了其在图像合成领域的先进性。'}}}, {'id': 'https://huggingface.co/papers/2509.21072', 'title': 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web\n  Reconnaissance, Tool Generation, and Task Execution', 'url': 'https://huggingface.co/papers/2509.21072', 'abstract': 'Recon-Act, a self-evolving multi-agent framework, improves adaptability and performance on long-horizon web tasks by generating and utilizing generalized tools through reconnaissance and action teams.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.', 'score': 13, 'issue_id': 6105, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '7a6532a574905646', 'authors': ['Kaiwen He', 'Zhiwei Wang', 'Chenyi Zhuang', 'Jinjie Gu'], 'affiliations': ['AWorld Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.21072.jpg', 'data': {'categories': ['#dataset', '#agents', '#multimodal', '#optimization', '#agi'], 'emoji': '🕵️', 'ru': {'title': 'Разведка и действие: самообучающиеся агенты для веб-задач', 'desc': 'Статья представляет Recon-Act - самоэволюционирующий мульти-агентный фреймворк для решения долгосрочных задач в веб-браузере. Система состоит из команды разведки, которая анализирует ошибки и создает обобщенные инструменты, и команды действий, выполняющей задачи с использованием этих инструментов. Через сравнение неудачных и успешных траекторий система автоматически создает правила и подсказки, улучшающие производительность. Подход показал state-of-the-art результаты на датасете VisualWebArena и значительно улучшил адаптивность к новым веб-сайтам.'}, 'en': {'title': 'Empowering AI Agents with Self-Evolving Tools for Web Mastery', 'desc': "Recon-Act is a multi-agent framework designed to enhance the performance of AI agents on complex web tasks. It operates through two main teams: the Reconnaissance Team, which analyzes past actions to generate useful tools, and the Action Team, which executes tasks using these tools. By learning from both successful and unsuccessful attempts, the framework creates generalized tools that help agents adapt to new situations more effectively. This self-evolving system establishes a feedback loop that continuously improves the agents' ability to navigate long-horizon tasks on various websites."}, 'zh': {'title': '自我进化的智能体框架，提升网页任务适应性', 'desc': 'Recon-Act是一个自我进化的多智能体框架，旨在提高在长时间网页任务中的适应性和性能。该系统由侦察团队和行动团队组成，侦察团队负责工具生成和比较分析，而行动团队则处理意图分解和工具执行。通过对比错误的执行轨迹和成功的轨迹，侦察团队能够推断出解决方案，并将其抽象为通用工具，实时注册到工具库中。利用这些通用工具，Recon-Act显著提高了对未见网站的适应能力，并在长时间任务中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.14662', 'title': "Understanding the Thinking Process of Reasoning Models: A Perspective\n  from Schoenfeld's Episode Theory", 'url': 'https://huggingface.co/papers/2509.14662', 'abstract': "A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.", 'score': 11, 'issue_id': 6098, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '1ed939a345480c28', 'authors': ['Ming Li', 'Nan Zhang', 'Chenrui Fan', 'Hong Jiao', 'Yanbin Fu', 'Sydney Peters', 'Qingshu Xu', 'Robert Lissitz', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2509.14662.jpg', 'data': {'categories': ['#math', '#reasoning', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Картография мышления AI через призму человеческого познания', 'desc': 'Исследователи применили теорию эпизодов Шёнфельда, классическую когнитивную модель решения математических задач человеком, для анализа рассуждений больших языковых моделей. Они создали первый публично доступный бенчмарк для детального анализа машинного мышления, разметив тысячи предложений из решений моделей семью когнитивными метками. Анализ выявил характерные паттерны в рассуждениях LLM, включая динамику переходов между когнитивными состояниями. Эта работа предоставляет теоретически обоснованную методологию для понимания познавательных процессов AI и открывает путь к созданию более контролируемых и прозрачных систем рассуждений.'}, 'en': {'title': 'Understanding Machine Reasoning with Cognitive Frameworks', 'desc': "This paper presents a new framework that uses Schoenfeld's Episode Theory to analyze how Large Reasoning Models (LRMs) approach math problems. By applying cognitive labels to thousands of sentences from model-generated solutions, the authors create a detailed benchmark for understanding machine reasoning. The study reveals specific patterns in the reasoning processes of LRMs, highlighting how they transition between different cognitive states. This framework not only aids in interpreting LRM cognition but also sets the stage for developing more transparent and controllable reasoning systems in the future."}, 'zh': {'title': '揭示大型推理模型的推理模式', 'desc': '本文提出了一种新颖的框架，利用Schoenfeld的情节理论分析大型推理模型在解决数学问题时的推理模式。这种方法通过对模型生成的解决方案进行标注，使用七种认知标签（如计划、实施、验证）来分析推理轨迹。研究结果提供了第一个公开可用的机器推理细粒度分析基准，包括一个大型标注语料库和详细的标注指南。初步分析显示了大型推理模型推理中的独特模式，如认知状态之间的转变动态。'}}}, {'id': 'https://huggingface.co/papers/2509.20136', 'title': 'V-GameGym: Visual Game Generation for Code Large Language Models', 'url': 'https://huggingface.co/papers/2509.20136', 'abstract': 'V-GameGym is a comprehensive benchmark for evaluating code generation in game development, focusing on multimodal evaluation including playability, visual aesthetics, and user engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problem-solving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, a comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting a novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce a multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation.', 'score': 9, 'issue_id': 6101, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '8ba29ccdb4749d95', 'authors': ['Wei Zhang', 'Jack Yang', 'Renshuai Tao', 'Lingzheng Chai', 'Shawn Guo', 'Jiajun Wu', 'Xiaoming Chen', 'Ganqu Cui', 'Ning Ding', 'Xander Xu', 'Hu Wei', 'Bowen Zhou'], 'affiliations': ['AIStrong', 'Alibaba Group', 'Beijing Jiaotong University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.20136.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#games', '#multimodal', '#dataset'], 'emoji': '🎮', 'ru': {'title': 'Новый стандарт оценки AI в геймдеве: от кода к играбельности', 'desc': 'V-GameGym - это комплексный бенчмарк для оценки генерации кода в разработке игр, который включает мультимодальную оценку играбельности, визуальной эстетики и вовлеченности пользователей. Исследователи создали датасет из 2219 высококачественных образцов, организованных в 100 тематических кластеров на основе реальных репозиториев. Они разработали автоматизированную систему оценки с использованием LLM для визуального синтеза кода в полноценной UI-среде. Бенчмарк устраняет разрыв между точностью генерации кода и практическими требованиями разработки игр, предоставляя количественные метрики качества для визуального программирования.'}, 'en': {'title': 'Bridging Code Generation and Game Development with V-GameGym', 'desc': 'V-GameGym is a new benchmark designed to evaluate code generation specifically for game development. Unlike traditional benchmarks that focus only on syntax and execution, V-GameGym assesses important aspects like playability, visual appeal, and user engagement. It includes 2,219 diverse samples organized into 100 thematic clusters, ensuring a comprehensive evaluation of game development tasks. The benchmark also features a multimodal evaluation framework that uses automated pipelines for generating visual code, making it a valuable tool for improving AI in game design.'}, 'zh': {'title': 'V-GameGym：游戏开发的多模态评估基准', 'desc': 'V-GameGym是一个全面的基准测试，用于评估游戏开发中的代码生成，重点关注多模态评估，包括可玩性、视觉美学和用户参与度。现有的代码相关基准主要关注语法正确性和执行准确性，而忽视了游戏开发中至关重要的指标。为了解决当前大型语言模型在算法问题解决与实际游戏开发需求之间的差距，V-GameGym提供了2219个高质量样本，涵盖100个主题集群。我们还引入了一个多模态评估框架，利用自动化的LLM驱动管道进行视觉代码合成，确保了代码生成的准确性与实际游戏开发工作流程之间的有效连接。'}}}, {'id': 'https://huggingface.co/papers/2509.19736', 'title': 'UserRL: Training Interactive User-Centric Agent via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2509.19736', 'abstract': 'UserRL framework enhances user-centric RL agents by optimizing reward assignment and user simulation, demonstrating the importance of these factors over model scale.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.', 'score': 9, 'issue_id': 6120, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': 'e6894061ac505189', 'authors': ['Cheng Qian', 'Zuxin Liu', 'Akshara Prabhakar', 'Jielin Qiu', 'Zhiwei Liu', 'Haolin Chen', 'Shirley Kokane', 'Heng Ji', 'Weiran Yao', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2509.19736.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#rl', '#rlhf', '#agents', '#optimization'], 'emoji': '🤝', 'ru': {'title': 'Обучение RL агентов для эффективного взаимодействия с пользователями', 'desc': 'Исследователи представили UserRL - фреймворк для обучения и оценки RL агентов, ориентированных на взаимодействие с пользователями. Они систематически изучили влияние различных подходов к назначению наград и симуляции пользователей на эффективность обучения. Эксперименты показали, что начальное обучение с учителем критически важно, продуманное оценивание траекторий улучшает многоходовые взаимодействия, а выбор симулятора пользователей влияет на качество обучения. Работа демонстрирует, что грамотный дизайн системы наград и симуляции пользователей столь же важен, как и размер модели.'}, 'en': {'title': 'UserRL: Optimizing Rewards for User-Centric Reinforcement Learning', 'desc': "The UserRL framework focuses on improving reinforcement learning (RL) agents by refining how rewards are assigned and how user interactions are simulated. It emphasizes that the design of reward shaping and user simulation is just as important as the size of the model itself. Through experiments with various models, the study finds that initial training methods and thoughtful scoring of user interactions significantly enhance the agents' performance in multi-turn dialogues. The findings suggest that even less powerful simulators can effectively train agents, making the approach accessible for further research."}, 'zh': {'title': '用户中心的强化学习新路径', 'desc': '本论文提出了UserRL框架，旨在通过优化奖励分配和用户模拟来增强以用户为中心的强化学习（RL）代理。研究表明，用户交互的多样性和动态性对代理的训练提出了挑战，因此需要在标准化的环境中进行训练和评估。通过系统地调整奖励分配和评分计算，研究发现初始交互能力的解锁和有效的多轮交互设计对学习效果至关重要。此外，尽管更强大的模拟用户有助于训练，但开源模拟器仍然是一个经济有效的选择。'}}}, {'id': 'https://huggingface.co/papers/2509.21318', 'title': 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows', 'url': 'https://huggingface.co/papers/2509.21318', 'abstract': 'SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.', 'score': 8, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '37d7c4e28964afd7', 'authors': ['Hmrishav Bandyopadhyay', 'Rahim Entezari', 'Jim Scott', 'Reshinth Adithyan', 'Yi-Zhe Song', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2509.21318.jpg', 'data': {'categories': ['#dataset', '#training', '#inference', '#optimization', '#data', '#cv', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Быстрая генерация изображений для всех устройств', 'desc': 'SD3.5-Flash представляет эффективный фреймворк дистилляции для генерации изображений за несколько шагов на обычных потребительских устройствах. Исследователи разработали специальную технику дистилляции rectified flow моделей с инновационными методами как timestep sharing и split-timestep fine-tuning. Система включает комплексные оптимизации пайплайна, включая реструктуризацию text encoder и специализированную квантизацию для эффективного развертывания. Результаты показывают превосходство над существующими методами генерации за малое количество шагов, делая продвинутые генеративные AI доступными для практического применения.'}, 'en': {'title': 'Democratizing Image Generation with SD3.5-Flash', 'desc': 'SD3.5-Flash is a new framework designed to improve image generation on everyday devices by using a few-step distillation method. It focuses on simplifying complex rectified flow models to make them more efficient for consumer hardware. The framework introduces innovative techniques like timestep sharing to minimize noise during training and split-timestep fine-tuning to enhance the alignment with user prompts. Overall, SD3.5-Flash allows for faster and more memory-efficient image generation, making advanced AI technology available to a wider range of devices.'}, 'zh': {'title': '让先进生成AI触手可及', 'desc': 'SD3.5-Flash是一种高效的少步蒸馏框架，旨在提升消费者设备上的图像生成能力。该方法通过重新制定的分布匹配目标，蒸馏计算上昂贵的修正流模型，专门针对少步生成进行优化。我们引入了两个关键创新：时间步共享以减少梯度噪声，以及分步时间微调以改善提示对齐。通过全面的管道优化，我们的系统实现了快速生成和内存高效的部署，使得从手机到桌面电脑的各种设备都能轻松访问先进的生成AI。'}}}, {'id': 'https://huggingface.co/papers/2509.21070', 'title': 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical\n  Reasoning', 'url': 'https://huggingface.co/papers/2509.21070', 'abstract': 'ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME\'24, AIME\'25, HMMT-Feb\'25, BRUMO\'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.', 'score': 8, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '9ea0c009dabeaeb0', 'authors': ['Qizhi Pei', 'Zhuoshi Pan', 'Honglin Lin', 'Xin Gao', 'Yu Li', 'Zinan Tang', 'Conghui He', 'Rui Yan', 'Lijun Wu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'School of Artificial Intelligence, Wuhan University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21070.jpg', 'data': {'categories': ['#math', '#training', '#optimization', '#transfer_learning', '#reasoning', '#dataset'], 'emoji': '🧮', 'ru': {'title': 'Масштабируемая генерация сложных математических задач для обучения моделей рассуждения', 'desc': 'ScaleDiff представляет новый подход для создания сложных математических задач с целью улучшения способностей больших языковых моделей к рассуждению. Система использует адаптивную модель мышления, которая может автоматически определять сложность задач и переключаться между режимами «Thinking» и «NoThinking». Затем обученный генератор сложных задач DiffGen-8B создает новые трудные задачи в большом масштабе без необходимости дорогостоящих API запросов. Дообучение модели Qwen2.5-Math-7B-Instruct на датасете ScaleDiff-Math показало увеличение производительности на 11.3% и достижение 65.9% точности на сложных математических бенчмарках.'}, 'en': {'title': 'Revolutionizing Problem Generation for Enhanced AI Reasoning', 'desc': 'ScaleDiff introduces an adaptive thinking model that identifies and generates challenging mathematical problems, enhancing the training of large reasoning models (LRMs) in a cost-effective manner. By efficiently filtering difficult problems from existing datasets with a single forward pass, it simplifies the problem generation process, avoiding complex prompting methods. The specialized generator, DiffGen-8B, produces new difficult problems at scale, leading to significant performance improvements in models fine-tuned on this data. This approach not only reduces computational costs but also demonstrates a clear scaling effect in model performance as the number of difficult problems increases.'}, 'zh': {'title': 'ScaleDiff：高效生成困难数学问题的解决方案', 'desc': 'ScaleDiff 是一种自适应思维模型，旨在识别和生成困难的数学问题，从而提高大型推理模型的性能，并降低训练成本。该方法通过单次前向传播有效识别现有数据集中困难问题，自动切换思维模式，简化了问题生成过程。我们训练的专门困难问题生成器 DiffGen-8B 能够大规模生成新问题，避免了复杂的逐实例提示和高昂的 API 成本。通过在 ScaleDiff-Math 数据集上微调 Qwen2.5-Math-7B-Instruct，模型性能显著提升，展示了在困难基准上随着问题数量增加而提升的明显效果。'}}}, {'id': 'https://huggingface.co/papers/2509.20414', 'title': 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent', 'url': 'https://huggingface.co/papers/2509.20414', 'abstract': 'SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.', 'score': 8, 'issue_id': 6098, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '4b1e989136f96d93', 'authors': ['Yandan Yang', 'Baoxiong Jia', 'Shujie Zhang', 'Siyuan Huang'], 'affiliations': ['State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20414.jpg', 'data': {'categories': ['#3d', '#reasoning', '#games', '#alignment', '#agents'], 'emoji': '🏠', 'ru': {'title': 'Умный архитектор: AI-агент создает реалистичные 3D интерьеры через саморефлексию', 'desc': 'SceneWeaver - это агентная система для синтеза 3D сцен, которая использует языковую модель-планировщик для итеративного улучшения качества генерируемых пространств. Система объединяет различные инструменты генерации сцен через механизм рефлексии, позволяющий агенту самостоятельно оценивать физическую правдоподобность, визуальную реалистичность и семантическое соответствие инструкциям. В отличие от предыдущих подходов, ограниченных фиксированными категориями сцен, SceneWeaver способна работать с разнообразными и сложными пользовательскими запросами. Эксперименты показывают превосходство системы по всем ключевым метрикам качества при генерации как стандартных, так и нестандартных типов помещений.'}, 'en': {'title': 'SceneWeaver: Crafting Realistic 3D Environments with Iterative Refinement', 'desc': 'SceneWeaver is a framework designed for creating 3D scenes that are not only visually appealing but also physically realistic and semantically accurate. It utilizes a language model-based planner to iteratively refine the scene generation process, allowing for adjustments based on user instructions. By integrating various scene generation tools and employing a closed-loop reasoning approach, SceneWeaver can identify and correct inconsistencies in the generated scenes. This innovative method significantly improves the quality of 3D environments, making it suitable for a wide range of applications in Embodied AI.'}, 'zh': {'title': 'SceneWeaver：智能3D场景合成的新突破', 'desc': 'SceneWeaver是一个反思性代理框架，利用基于语言模型的规划器来迭代优化3D场景合成。它能够在多样化的指令下，实现高水平的物理、视觉和语义质量。该框架通过工具驱动的迭代精炼，统一了不同的场景合成范式。实验表明，SceneWeaver在物理、视觉和语义指标上超越了以往的方法，并能有效地适应复杂场景。'}}}, {'id': 'https://huggingface.co/papers/2509.21302', 'title': 'Quantized Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2509.21302', 'abstract': 'QuantVGGT, a quantization framework for Visual Geometry Grounded Transformers, achieves state-of-the-art results with memory reduction and acceleration while maintaining high reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have made remarkable progress with the use of large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has become a common practice for compressing and accelerating models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. This paper proposes the first Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two technical contributions: First, we introduce Dual-Smoothed Fine-Grained Quantization, which integrates pre-global Hadamard rotation and post-local channel smoothing to mitigate heavy-tailed distributions and inter-channel variance robustly. Second, we design Noise-Filtered Diverse Sampling, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges. Comprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with a great margin. We highlight that our 4-bit QuantVGGT can deliver a 3.7times memory reduction and 2.5times acceleration in real-hardware inference, while maintaining reconstruction accuracy above 98\\% of its full-precision counterpart. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios. Our code is released in https://github.com/wlfeng0509/QuantVGGT.', 'score': 7, 'issue_id': 6104, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '8f544c805aaf2f14', 'authors': ['Weilun Feng', 'Haotong Qin', 'Mingqiang Wu', 'Chuanguang Yang', 'Yuqi Li', 'Xiangqi Li', 'Zhulin An', 'Libo Huang', 'Yulun Zhang', 'Michele Magno', 'Yongjun Xu'], 'affiliations': ['ETH Zurich', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Shanghai Jiao Tong University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.21302.jpg', 'data': {'categories': ['#benchmark', '#3d', '#optimization', '#inference'], 'emoji': '🧊', 'ru': {'title': 'Квантизация трансформеров для 3D реконструкции с минимальной потерей качества', 'desc': 'В статье представлен QuantVGGT - первый фреймворк для квантизации Visual Geometry Grounded Transformers, которые используются для 3D реконструкции. Авторы выявили уникальные проблемы при квантизации миллиардных VGGT моделей: специальные токены создают распределения активаций с тяжелыми хвостами, а многовидовая природа 3D данных делает выбор калибровочных образцов нестабильным. Для решения этих проблем предложены два метода: Dual-Smoothed Fine-Grained Quantization для смягчения распределений и Noise-Filtered Diverse Sampling для стабильной калибровки. Эксперименты показали, что 4-битная квантизация обеспечивает уменьшение памяти в 3.7 раза и ускорение в 2.5 раза при сохранении 98% точности реконструкции.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Efficient Quantization', 'desc': 'QuantVGGT is a novel quantization framework designed specifically for Visual Geometry Grounded Transformers (VGGTs), which are advanced models for 3D reconstruction. The framework addresses the challenges of high memory usage and slow processing speeds that hinder the deployment of these models in real-world applications. It introduces two key techniques: Dual-Smoothed Fine-Grained Quantization to handle heavy-tailed activation distributions, and Noise-Filtered Diverse Sampling to stabilize calibration for quantization. As a result, QuantVGGT achieves significant improvements in memory efficiency and processing speed while maintaining high reconstruction accuracy, making it suitable for resource-limited environments.'}, 'zh': {'title': '量化框架QuantVGGT：高效加速与精度兼得', 'desc': 'QuantVGGT是一个针对视觉几何基础变换器的量化框架，旨在在保持高重建精度的同时，实现内存减少和加速。该框架通过双平滑细粒度量化和噪声过滤多样化采样两项技术贡献，解决了大规模VGGT在量化过程中面临的挑战。实验结果表明，QuantVGGT在不同基准和比特宽度上均达到了最先进的结果，显著超越了之前的量化方法。特别是，4位的QuantVGGT在实际硬件推理中实现了3.7倍的内存减少和2.5倍的加速，同时保持了超过98%的重建精度。'}}}, {'id': 'https://huggingface.co/papers/2509.21106', 'title': 'BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback', 'url': 'https://huggingface.co/papers/2509.21106', 'abstract': "BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/.", 'score': 6, 'issue_id': 6107, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '73c2d8751f7343c3', 'authors': ['Hyunseo Kim', 'Sangam Lee', 'Kwangwook Seo', 'Dongha Lee'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21106.jpg', 'data': {'categories': ['#survey', '#multimodal', '#dataset', '#alignment', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Персонализация поисковых LLM под каждого пользователя', 'desc': 'Исследователи создали бенчмарк BESPOKE для оценки персонализации в поисковых LLM, которые используют извлечение информации для генерации ответов. Существующие системы как ChatGPT и Gemini пытаются персонализировать ответы на основе истории пользователей, но систематическая оценка такой персонализации была недостаточно изучена. Бенчмарк построен на реальных данных чатов и поисковых запросов людей с детальной обратной связью и оценками предпочтений. Исследование выявляет ключевые требования для эффективной персонализации в задачах поиска информации.'}, 'en': {'title': 'BESPOKE: Personalization Benchmark for Search-Augmented LLMs', 'desc': 'BESPOKE is a new benchmark designed to evaluate how well search-augmented large language models (LLMs) personalize responses based on user data and feedback. It addresses the challenge of understanding different user intents behind the same query and aims to improve the relevance of information provided. By collecting real user chat and search histories, BESPOKE allows for a detailed analysis of how well these models meet individual preferences. This systematic evaluation helps identify essential factors for effective personalization in information-seeking tasks, paving the way for advancements in personalized AI systems.'}, 'zh': {'title': 'BESPOKE：个性化搜索的评估新基准', 'desc': 'BESPOKE是一个用于评估搜索增强型大语言模型（LLMs）个性化效果的基准，使用真实用户数据和详细反馈。该基准旨在通过收集真实的聊天和搜索历史，帮助识别用户在相同查询下的不同意图，并提供用户偏好的信息形式。尽管现有系统如ChatGPT和Gemini尝试通过用户历史实现个性化，但对这种个性化的系统评估仍然不足。BESPOKE通过长期的人工注释，结合用户的历史和反馈，提供了一个有效的个性化评估基础。'}}}, {'id': 'https://huggingface.co/papers/2509.21317', 'title': 'Interactive Recommendation Agent with Active User Commands', 'url': 'https://huggingface.co/papers/2509.21317', 'abstract': "IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.   To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.", 'score': 5, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'df6bcc9456addce3', 'authors': ['Jiakai Tang', 'Yujie Luo', 'Xunke Xi', 'Fei Sun', 'Xueyang Feng', 'Sunhao Dai', 'Chao Yi', 'Dian Chen', 'Zhujin Gao', 'Yang Li', 'Xu Chen', 'Wen Chen', 'Jian Wu', 'Yuning Jiang', 'Bo Zheng'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21317.jpg', 'data': {'categories': ['#training', '#architecture', '#reasoning', '#optimization', '#multimodal', '#agents'], 'emoji': '🗣️', 'ru': {'title': 'Управляй рекомендациями голосом - говори системе, что хочешь увидеть', 'desc': 'Исследователи представили Interactive Recommendation Feed (IRF) - новую парадигму рекомендательных систем, которая позволяет пользователям управлять рекомендациями через команды на естественном языке. Система RecBot включает два агента: Parser Agent преобразует текстовые команды в структурированные предпочтения, а Planner Agent динамически адаптирует политику рекомендаций. Для эффективного развертывания используется knowledge distillation с дополнением симуляциями. Эксперименты показали значительное улучшение удовлетворенности пользователей и бизнес-метрик по сравнению с традиционными системами пассивного feedback.'}, 'en': {'title': 'Empowering Users with Natural Language in Recommendations', 'desc': 'The paper presents the Interactive Recommendation Feed (IRF), a novel recommendation system that utilizes natural language commands to enhance user engagement and satisfaction. Unlike traditional systems that rely on passive feedback, IRF allows users to actively express their preferences through real-time linguistic inputs. This is achieved using a dual-agent architecture, where a Parser Agent interprets user commands and a Planner Agent adjusts recommendation policies dynamically. The system employs simulation-augmented knowledge distillation to optimize performance while preserving robust reasoning capabilities, leading to improved user satisfaction and better business outcomes.'}, 'zh': {'title': '自然语言驱动的智能推荐系统', 'desc': 'IRF是一种新型推荐系统，允许用户通过自然语言命令进行互动，从而提高用户满意度和商业成果。与传统推荐系统依赖被动反馈不同，IRF通过实时语言命令赋予用户主动控制推荐策略的能力。该系统采用双代理架构，解析代理将语言表达转化为结构化偏好，规划代理则动态调整推荐策略。通过模拟增强知识蒸馏，IRF在保持强大推理能力的同时，实现了高效的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.20293', 'title': 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity', 'url': 'https://huggingface.co/papers/2509.20293', 'abstract': "LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md", 'score': 5, 'issue_id': 6107, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '330d430b8bdd426b', 'authors': ['Benjamin Feuer', 'Chiung-Yi Tseng', 'Astitwa Sarthak Lathe', 'Oussama Elachqar', 'John P Dickerson'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2509.20293.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'LLM-судьи дают ненадежные оценки из-за отклонения от собственных критериев', 'desc': 'Исследователи обнаружили серьезные проблемы в бенчмарках, где LLM выступают в роли судей для оценки других моделей. Они выявили два ключевых недостатка: схематическую несогласованность (когда судьи отклоняются от собственных критериев оценки) и коллапс факторов (когда разные критерии оценки становятся неразличимыми). Для диагностики этих проблем предложены два метода: измерение схематического соответствия и психометрической валидности. На примере Arena-Hard Auto показано, что популярные LLM-судьи демонстрируют крайне высокую необъяснимую дисперсию (свыше 90% для DeepSeek-R1-32B) и сильную корреляцию между критериями (выше 0.93), что делает их оценки ненадежными.'}, 'en': {'title': 'Improving Reliability in LLM-Judged Benchmarks', 'desc': 'This paper discusses the problems with benchmarks that use large language models (LLMs) for evaluation, highlighting issues like schema incoherence and factor collapse. It introduces two diagnostic tools: schematic adherence, which measures how well judges follow their own evaluation criteria, and psychometric validity, which assesses the reliability of the benchmark results. The authors demonstrate that many popular judges exhibit high levels of unexplained variance and strong correlations between criteria, indicating unreliable rankings. They propose guidelines for creating more reliable benchmarks that can better reflect model performance without the noise introduced by current methods.'}, 'zh': {'title': '提升LLM评估基准的可靠性', 'desc': '本文探讨了使用大型语言模型（LLM）评估基准时可能出现的不可靠排名问题，主要由于评估框架不一致和因素崩溃。我们提出了两种机制来诊断这些问题：一是通过评估框架的一致性来量化评审结果的解释程度，二是通过心理测量有效性来评估基准测试中的不确定性。研究发现，流行评审者在评估时存在严重的不一致性，导致高达90%的未解释方差。我们的结果强调了设计缺陷，并提供了构建更可靠的LLM评估基准的可行原则。'}}}, {'id': 'https://huggingface.co/papers/2509.21113', 'title': 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for\n  Video Temporal Reasoning', 'url': 'https://huggingface.co/papers/2509.21113', 'abstract': 'MOSS-ChatV, a reinforcement learning framework with a DTW-based reward, improves video reasoning consistency and performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models to move beyond static perception toward coherent understanding of temporal dynamics in complex scenes. Yet existing MLLMs often exhibit process inconsistency, where intermediate reasoning drifts from video dynamics even when the final answer is correct, undermining interpretability and robustness. To address this issue, we introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time Warping (DTW)-based process reward. This rule-based reward aligns reasoning traces with temporally grounded references, enabling efficient process supervision without auxiliary reward models. We further identify dynamic state prediction as a key measure of video reasoning and construct MOSS-Video, a benchmark with annotated reasoning traces, where the training split is used to fine-tune MOSS-ChatV and the held-out split is reserved for evaluation. MOSS-ChatV achieves 87.2\\% on MOSS-Video (test) and improves performance on general video benchmarks such as MVBench and MMVU. The framework consistently yields gains across different architectures, including Qwen2.5-VL and Phi-2, confirming its broad applicability. Evaluations with GPT-4o-as-judge further show that MOSS-ChatV produces more consistent and stable reasoning traces.', 'score': 4, 'issue_id': 6100, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '8f3917f7caee4e55', 'authors': ['Sicheng Tao', 'Jungang Li', 'Yibo Yan', 'Junyan Zhang', 'Yubo Gao', 'Hanqian Li', 'ShuHang Xun', 'Yuxuan Fan', 'Hong Chen', 'Jianxiang He', 'Xuming Hu'], 'affiliations': ['HIT', 'HKUST', 'HKUST (GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2509.21113.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#video', '#rl', '#interpretability', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Согласованные рассуждения о видео через обучение с подкреплением', 'desc': 'В статье представлена MOSS-ChatV — фреймворк обучения с подкреплением для улучшения рассуждений о видео в мультимодальных больших языковых моделях. Авторы используют награду на основе Dynamic Time Warping (DTW) для выравнивания следов рассуждений с временно обоснованными эталонами. Был создан новый бенчмарк MOSS-Video с аннотированными следами рассуждений для обучения и оценки модели. Результаты показывают улучшение согласованности рассуждений и производительности на различных видео-бенчмарках, включая MVBench и MMVU.'}, 'en': {'title': 'Enhancing Video Reasoning Consistency with MOSS-ChatV', 'desc': "MOSS-ChatV is a reinforcement learning framework designed to enhance video reasoning in multimodal large language models (MLLMs). It utilizes a Dynamic Time Warping (DTW)-based reward system to ensure that the reasoning process aligns closely with the actual dynamics of the video content. This approach addresses the issue of process inconsistency, where the model's intermediate reasoning may not accurately reflect the video, even if the final answer is correct. By introducing a benchmark called MOSS-Video, MOSS-ChatV demonstrates significant improvements in reasoning consistency and performance across various video benchmarks."}, 'zh': {'title': 'MOSS-ChatV：提升视频推理一致性的强化学习框架', 'desc': 'MOSS-ChatV是一个基于强化学习的框架，采用动态时间规整（DTW）作为奖励机制，旨在提高视频推理的一致性和性能。现有的大型多模态语言模型（MLLMs）在处理视频时常常出现推理过程不一致的问题，即使最终答案正确，推理过程也可能偏离视频动态。MOSS-ChatV通过将推理轨迹与时间上对齐的参考进行对比，提供了高效的过程监督，避免了使用辅助奖励模型。该框架在多个基准测试中表现出色，证明了其在不同架构中的广泛适用性。'}}}, {'id': 'https://huggingface.co/papers/2509.21042', 'title': 'Behind RoPE: How Does Causal Mask Encode Positional Information?', 'url': 'https://huggingface.co/papers/2509.21042', 'abstract': "The causal mask in Transformer decoders induces position-dependent attention patterns, which can interact with explicit positional encodings like RoPE, affecting their relative attention score patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.", 'score': 4, 'issue_id': 6101, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': '4228468a7ee42c33', 'authors': ['Junu Kim', 'Xiao Liu', 'Zhenghao Lin', 'Lei Ji', 'Yeyun Gong', 'Edward Choi'], 'affiliations': ['KAIST', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.21042.jpg', 'data': {'categories': ['#math', '#optimization', '#architecture', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Каузальная маска как скрытый источник позиционной информации', 'desc': 'Исследователи доказали, что каузальная маска в декодерах Transformer создаёт зависящие от позиции паттерны внимания даже без параметров или каузальных зависимостей во входных данных. Анализ показал, что индуцированные паттерны внимания склонны отдавать предпочтение близким парам запрос-ключ, имитируя поведение обычных позиционных кодировок. Взаимодействие каузальной маски и RoPE искажает относительные паттерны оценок внимания RoPE, превращая их в неотносительные. Этот эффект последовательно наблюдается в современных больших языковых моделях, что подчёркивает важность рассмотрения каузальной маски как источника позиционной информации.'}, 'en': {'title': 'Causal Mask: A Hidden Source of Positional Information in Transformers', 'desc': 'This paper explores how the causal mask in Transformer decoders influences attention patterns based on position. It shows that the causal mask can create position-dependent attention scores, even without additional parameters. The study reveals that this effect tends to prioritize nearby query-key pairs, similar to traditional positional encodings like RoPE. Furthermore, the interaction between the causal mask and RoPE can distort the expected relative attention patterns, highlighting the need to consider both sources of positional information in model design.'}, 'zh': {'title': '因果掩码与位置编码的相互作用', 'desc': '在Transformer解码器中，因果掩码会引入依赖于位置的注意力模式，这与显式位置编码（如RoPE）相互作用，影响相对注意力得分模式。本文证明了因果掩码能够在没有参数或输入因果依赖的情况下，诱导出位置依赖的注意力模式。理论分析表明，这种诱导的注意力模式倾向于偏向于相邻的查询-键对，类似于常见位置编码的行为。实证分析确认，训练后的模型表现出相同的行为，学习的参数进一步放大了这些模式。'}}}, {'id': 'https://huggingface.co/papers/2509.19228', 'title': 'CompLLM: Compression for Long Context Q&A', 'url': 'https://huggingface.co/papers/2509.19228', 'abstract': 'CompLLM, a soft compression technique for LLMs, divides contexts into segments for efficient, scalable, and reusable compression, enhancing performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.', 'score': 4, 'issue_id': 6119, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': 'e9dc06e52d011d7e', 'authors': ['Gabriele Berton', 'Jayakrishnan Unnikrishnan', 'Son Tran', 'Mubarak Shah'], 'affiliations': ['Amazon', 'Center For Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2509.19228.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#long_context'], 'emoji': '🗜️', 'ru': {'title': 'Умное сегментированное сжатие контекста для эффективной работы LLM', 'desc': 'CompLLM - это новая техника сжатия для больших языковых моделей, которая решает проблему квадратичной сложности self-attention при обработке длинных контекстов. Вместо сжатия всего контекста как единого целого, CompLLM разделяет его на сегменты и сжимает каждый независимо. Это обеспечивает линейную сложность сжатия, масштабируемость до 100k токенов и возможность переиспользования сжатых сегментов между запросами. При двукратном сжатии метод ускоряет время до первого токена в 4 раза и сокращает KV cache на 50%, сохраняя качество работы модели.'}, 'en': {'title': 'Efficient Context Compression for Scalable LLMs', 'desc': 'CompLLM is a novel soft compression technique designed to improve the efficiency of Large Language Models (LLMs) when handling long contexts. By segmenting the input context and compressing each segment independently, it reduces the computational complexity from quadratic to linear, making it more scalable. This method allows for the reuse of compressed segments across different queries, significantly lowering the computational costs and speeding up processing times. Experiments show that CompLLM can achieve a 2x compression rate while maintaining performance comparable to uncompressed contexts, especially on longer sequences.'}, 'zh': {'title': 'CompLLM：高效的上下文压缩技术', 'desc': 'CompLLM是一种针对大型语言模型（LLMs）的软压缩技术，它通过将上下文划分为多个段落来实现高效、可扩展和可重用的压缩。这种方法解决了传统压缩技术在处理长上下文时的计算复杂性问题。CompLLM的设计使得压缩过程与上下文长度呈线性关系，从而提高了效率，并允许在不同查询中重用压缩段。实验结果表明，CompLLM在高上下文长度下能够显著加快首次生成时间，并在性能上与未压缩上下文相当，甚至在非常长的序列中表现更佳。'}}}, {'id': 'https://huggingface.co/papers/2509.20868', 'title': 'StyleBench: Evaluating thinking styles in Large Language Models', 'url': 'https://huggingface.co/papers/2509.20868', 'abstract': 'StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.', 'score': 3, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'fa050993ad9cfcf9', 'authors': ['Junyu Guo', 'Shangding Gu', 'Ming Jin', 'Costas Spanos', 'Javad Lavaei'], 'affiliations': ['University of California, Berkeley', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2509.20868.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Размер модели решает, какой стиль рассуждений работает лучше', 'desc': 'Исследование представляет StyleBench - бенчмарк для оценки различных стилей рассуждений в больших языковых моделях. Авторы протестировали пять методов промптинга (Chain of Thought, Tree of Thought и другие) на 15 моделях от 270M до 120B параметров. Результаты показали, что эффективность стратегий зависит от размера модели и типа задачи: поисковые методы лучше работают на открытых задачах с крупными моделями, а лаконичные стили более эффективны на четко определенных задачах. Маленькие модели часто не следуют инструкциям и склонны к угадыванию, тогда как устойчивость рассуждений появляется с увеличением масштаба модели.'}, 'en': {'title': 'Unlocking Reasoning Styles for Optimal Model Performance', 'desc': 'StyleBench is a new benchmark designed to evaluate different reasoning styles used in prompts for Large Language Models (LLMs). It examines how the effectiveness of these reasoning strategies varies depending on the model size and the type of task being performed. The study analyzes five reasoning styles across various tasks using 15 different models, revealing that no single style works best for all scenarios. The results indicate that larger models perform better with complex reasoning styles, while simpler styles are more efficient for straightforward tasks.'}, 'zh': {'title': '推理风格与模型规模的最佳选择', 'desc': 'StyleBench 是一个全面的基准测试，用于系统评估不同任务和模型中的推理风格。我们研究了五种代表性的推理风格，包括思维链（CoT）、思维树（ToT）、思维算法（AoT）、思维草图（SoT）和草稿链（CoD），并在五个推理任务上进行了评估。研究发现，没有一种推理风格在所有情况下都是最佳的，其有效性高度依赖于模型规模和任务类型。我们的分析表明，基于搜索的方法在开放性问题中表现优异，但需要大规模模型，而简洁的风格在定义明确的任务中则能显著提高效率。'}}}, {'id': 'https://huggingface.co/papers/2509.20109', 'title': 'Discrete Diffusion for Reflective Vision-Language-Action Models in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2509.20109', 'abstract': 'ReflectDrive uses a reflection mechanism with discrete diffusion and pre-trained Diffusion Language Models to generate safe trajectories for autonomous driving systems.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.', 'score': 3, 'issue_id': 6101, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '7a95d9753228508a', 'authors': ['Pengxiang Li', 'Yinan Zheng', 'Yue Wang', 'Huimin Wang', 'Hang Zhao', 'Jingjing Liu', 'Xianyuan Zhan', 'Kun Zhan', 'Xianpeng Lang'], 'affiliations': ['LiAuto', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20109.jpg', 'data': {'categories': ['#optimization', '#agents', '#diffusion', '#benchmark', '#multimodal', '#rl'], 'emoji': '🚗', 'ru': {'title': 'Безопасное автономное вождение через рефлексию и дискретную диффузию', 'desc': 'ReflectDrive представляет новый подход для автономного вождения, использующий механизм рефлексии с дискретной диффузией и предобученными Diffusion Language Models. Система дискретизирует двумерное пространство движения для создания кодовой книги действий и применяет итеративную самокоррекцию без вычисления градиентов. Ключевая особенность - механизм рефлексии, который выявляет небезопасные токены и генерирует безопасные траектории через локальный поиск и регенерацию. Метод демонстрирует значительные преимущества в безопасной генерации траекторий на бенчмарке NAVSIM.'}, 'en': {'title': 'ReflectDrive: Safe Trajectories for Autonomous Driving', 'desc': 'ReflectDrive is a novel framework designed to enhance the safety of trajectory generation for autonomous driving systems. It utilizes a reflection mechanism combined with discrete diffusion and pre-trained Diffusion Language Models to create safe driving paths. Unlike traditional methods that rely heavily on imitation learning or complex rule-based systems, ReflectDrive incorporates a safety-aware approach that allows for iterative self-correction without the need for gradient calculations. This innovative method has shown significant improvements in safety-critical scenarios, making it a promising solution for the future of autonomous driving.'}, 'zh': {'title': 'ReflectDrive：安全的自动驾驶轨迹生成新方法', 'desc': 'ReflectDrive 是一种新颖的学习框架，利用反射机制和离散扩散生成安全的自动驾驶轨迹。该方法通过离散化二维驾驶空间，构建动作代码本，并利用预训练的扩散语言模型进行规划任务。其核心是一个安全感知的反射机制，能够在不计算梯度的情况下进行迭代自我修正。经过在 NAVSIM 基准上的评估，ReflectDrive 在安全关键的轨迹生成方面表现出显著优势，为自动驾驶系统提供了可扩展和可靠的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2509.19676', 'title': 'Thinking While Listening: Simple Test Time Scaling For Audio\n  Classification', 'url': 'https://huggingface.co/papers/2509.19676', 'abstract': 'A framework incorporating reasoning into audio classification improves performance through test-time scaling and lightweight retraining of embedding matrices.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a framework that enables neural models to "think while listening" to everyday sounds, thereby enhancing audio classification performance. Motivated by recent advances in the reasoning capabilities of large language models, we address two central questions: (i) how can thinking be incorporated into existing audio classification pipelines to enable reasoning in the category space and improve performance, and (ii) can a new architecture be designed from the ground up to support both thinking and test-time scaling? We demonstrate that in both settings, our models exhibit improved classification accuracy. Leveraging test-time scaling, we observe consistent gains as the number of sampled traces increases. Furthermore, we evaluate two open-source reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are capable of zero-shot reasoning, a lightweight approach--retraining only the embedding matrix of a frozen, smaller model like GPT-2--can surpass the performance of billion-parameter text-based reasoning models.', 'score': 3, 'issue_id': 6108, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 сентября', 'en': 'September 24', 'zh': '9月24日'}, 'hash': '9423c6d2db011a9e', 'authors': ['Prateek Verma', 'Mert Pilanci'], 'affiliations': ['Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.19676.jpg', 'data': {'categories': ['#audio', '#reasoning', '#multimodal', '#training', '#open_source', '#architecture'], 'emoji': '🎧', 'ru': {'title': 'Научить AI думать во время прослушивания звуков', 'desc': "Исследователи предложили фреймворк, который позволяет нейронным моделям 'думать во время прослушивания' звуков, что улучшает качество аудиоклассификации. Подход вдохновлен успехами в области рассуждений больших языковых моделей и включает два направления: интеграцию рассуждений в существующие пайплайны и создание новых архитектур с поддержкой test-time scaling. Модели показали улучшенную точность классификации, при этом производительность растет с увеличением количества семплированных трасс. Легковесный подход с дообучением только матрицы эмбеддингов замороженной модели GPT-2 превзошел по качеству большие текстовые модели рассуждений с миллиардами параметров."}, 'en': {'title': 'Enhancing Audio Classification with Reasoning and Lightweight Retraining', 'desc': 'This paper presents a new framework that enhances audio classification by integrating reasoning capabilities into neural models. It explores how to incorporate reasoning into existing audio classification systems to improve their performance and proposes a novel architecture that supports reasoning and test-time scaling. The authors demonstrate that their models achieve better classification accuracy, especially when using test-time scaling with increased sampled traces. Additionally, they show that a lightweight retraining approach can outperform larger reasoning models, highlighting the effectiveness of optimizing smaller models for audio tasks.'}, 'zh': {'title': '推理提升音频分类性能的框架', 'desc': '本文提出了一种将推理能力融入音频分类的框架，从而提高分类性能。我们探讨了如何在现有音频分类流程中加入推理，以改善分类效果，并设计了一种新架构来支持推理和测试时的扩展。实验结果表明，无论是在现有模型上进行推理，还是在新架构中，分类准确率都有显著提升。通过测试时扩展，我们发现随着采样轨迹数量的增加，性能持续提升，且轻量级的重训练方法在某些情况下超越了大型文本推理模型。'}}}, {'id': 'https://huggingface.co/papers/2509.19282', 'title': 'OverLayBench: A Benchmark for Layout-to-Image Generation with Dense\n  Overlaps', 'url': 'https://huggingface.co/papers/2509.19282', 'abstract': 'A new benchmark and metric are introduced to evaluate layout-to-image generation models on complex overlapping bounding boxes, along with a fine-tuned model to improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, a novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, a new benchmark featuring high-quality annotations and a balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.', 'score': 3, 'issue_id': 6120, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '2e4c637aaccbbb5f', 'authors': ['Bingnan Li', 'Chen-Yu Wang', 'Haiyang Xu', 'Xiang Zhang', 'Ethan Armand', 'Divyansh Srivastava', 'Xiaojun Shan', 'Zeyuan Chen', 'Jianwen Xie', 'Zhuowen Tu'], 'affiliations': ['Lambda, Inc', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.19282.jpg', 'data': {'categories': ['#cv', '#training', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Решение проблемы перекрывающихся объектов в генерации изображений', 'desc': "Исследователи выявили проблему современных моделей генерации изображений по макету - они плохо справляются с перекрывающимися bounding box'ами. Авторы создали новую метрику OverLayScore для количественной оценки сложности перекрытий и показали, что существующие бенчмарки смещены в сторону простых случаев. Представлен новый бенчмарк OverLayBench с более сбалансированным распределением сложности и качественными аннотациями. Также предложена модель CreatiLayout-AM, дообученная на специальном датасете для улучшения работы с комплексными перекрытиями."}, 'en': {'title': 'Enhancing Layout-to-Image Generation with OverLayScore and OverLayBench', 'desc': 'This paper addresses the challenges faced by layout-to-image generation models when dealing with complex overlapping bounding boxes. It introduces a new metric called OverLayScore, which measures the complexity of these overlaps, highlighting the limitations of existing benchmarks that favor simpler cases. The authors also present OverLayBench, a benchmark with high-quality annotations that provides a more balanced evaluation of model performance across varying levels of overlap complexity. Additionally, they propose CreatiLayout-AM, a fine-tuned model aimed at improving generation quality in scenarios with significant bounding box overlap.'}, 'zh': {'title': '提升布局到图像生成的评估标准', 'desc': '本文提出了一种新的基准和评估指标，用于评估在复杂重叠边界框上的布局到图像生成模型。我们识别出两个主要挑战：大面积重叠区域和语义上几乎没有区别的重叠实例。通过定性示例和定量分析，我们展示了这些因素如何降低生成质量。为了解决这个问题，我们引入了OverLayScore，这是一种新颖的指标，用于量化重叠边界框的复杂性，并提出了OverLayBench基准，以便在更具挑战性的条件下评估模型性能。'}}}, {'id': 'https://huggingface.co/papers/2509.20878', 'title': 'The Unanticipated Asymmetry Between Perceptual Optimization and\n  Assessment', 'url': 'https://huggingface.co/papers/2509.20878', 'abstract': 'The study reveals an asymmetry between perceptual optimization and image quality assessment, showing that effective IQA metrics are not always suitable for perceptual optimization, especially under adversarial training, and highlights the importance of discriminator design in optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization.', 'score': 2, 'issue_id': 6100, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'e54e3db347b3c50f', 'authors': ['Jiabei Zhang', 'Qi Wang', 'Siyu Wu', 'Du Chen', 'Tianhe Wu'], 'affiliations': ['Beihang University', 'City University of Hong Kong', 'Institute of Microelectronics of the Chinese Academy of Sciences', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20878.jpg', 'data': {'categories': ['#training', '#cv', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Асимметрия между оптимизацией и оценкой качества изображений', 'desc': 'Исследование выявляет асимметрию между перцептуальной оптимизацией и оценкой качества изображений. Метрики IQA, которые хорошо работают для оценки качества, не всегда эффективны для перцептуальной оптимизации, особенно при adversarial обучении. Дискриминаторы эффективно подавляют артефакты во время оптимизации, но их представления плохо переносятся на задачи оценки качества изображений. Архитектура дискриминатора играет решающую роль - patch-level и сверточные архитектуры обеспечивают более точную реконструкцию деталей, чем vanilla или Transformer-based варианты.'}, 'en': {'title': 'Bridging the Gap: Optimizing Perception Beyond Quality Assessment', 'desc': 'This study investigates the differences between perceptual optimization and image quality assessment (IQA) in machine learning. It finds that metrics that work well for assessing image quality do not always perform effectively when optimizing images, particularly in adversarial training scenarios. The research emphasizes the critical role of discriminator design in the optimization process, showing that certain architectures yield better results in detail reconstruction. Overall, the paper highlights the need for a deeper understanding of how loss functions relate to IQA metrics to improve perceptual optimization techniques.'}, 'zh': {'title': '感知优化与图像质量评估的非对称性', 'desc': '本研究揭示了感知优化与图像质量评估之间的非对称性，表明有效的图像质量评估指标并不总是适合用于感知优化，尤其是在对抗训练下。感知优化主要由保真度目标驱动，强调语义一致性和整体视觉真实感，而对抗目标则通过增强感知清晰度和细节提供补充优化。我们系统分析了这一现象，发现保真度指标在图像质量评估中表现优异，但在感知优化中却未必有效，尤其在对抗训练中这种不一致性更加明显。此外，研究还表明，鉴别器的设计在优化过程中起着决定性作用，补丁级和卷积架构在细节重建方面优于传统或基于变换器的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2509.20706', 'title': 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with\n  Closed-Source Large-Audio Language Model', 'url': 'https://huggingface.co/papers/2509.20706', 'abstract': 'MI-Fuse, a denoised label fusion framework, enhances speech emotion recognition in target domains using an API-only LALM and a source-domain SER classifier, achieving better performance than the LALM and other baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) show strong zero-shot ability on speech tasks, suggesting promise for speech emotion recognition (SER). However, SER in real-world deployments often fails under domain mismatch, where source data are unavailable and powerful LALMs are accessible only through an API. We ask: given only unlabeled target-domain audio and an API-only LALM, can a student model be adapted to outperform the LALM in the target domain? To this end, we propose MI-Fuse, a denoised label fusion framework that supplements the LALM with a source-domain trained SER classifier as an auxiliary teacher. The framework draws multiple stochastic predictions from both teachers, weights their mean distributions by mutual-information-based uncertainty, and stabilizes training with an exponential moving average teacher. Experiments across three public emotion datasets and six cross-domain transfers show consistent gains, with the student surpassing the LALM and outperforming the strongest baseline by 3.9%. This approach strengthens emotion-aware speech systems without sharing source data, enabling realistic adaptation.', 'score': 2, 'issue_id': 6102, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'ae753d2855aba13d', 'authors': ['Hsiao-Ying Huang', 'Yi-Cheng Lin', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2509.20706.jpg', 'data': {'categories': ['#optimization', '#audio', '#multimodal', '#transfer_learning', '#training'], 'emoji': '🎭', 'ru': {'title': 'Слияние знаний для распознавания эмоций без исходных данных', 'desc': 'В статье предлагается фреймворк MI-Fuse для адаптации моделей распознавания эмоций в речи к новому домену без доступа к исходным данным. Метод использует большую аудио-языковую модель (LALM), доступную только через API, и классификатор из исходного домена в качестве учителей для обучения студенческой модели. Фреймворк объединяет предсказания учителей с помощью весов, основанных на взаимной информации, и стабилизирует обучение экспоненциальным скользящим средним. Эксперименты показывают, что студенческая модель превосходит LALM и другие базовые методы на 3.9% в задачах кросс-доменного переноса.'}, 'en': {'title': 'Enhancing Speech Emotion Recognition with MI-Fuse', 'desc': 'The paper introduces MI-Fuse, a framework designed to improve speech emotion recognition (SER) in situations where there is a mismatch between the source and target domains. It utilizes an API-only large audio-language model (LALM) alongside a source-domain SER classifier to enhance performance. By employing a denoised label fusion technique, MI-Fuse combines predictions from both models, using mutual information to weigh their contributions effectively. The results demonstrate that this method allows a student model to outperform the LALM and other baseline models, achieving significant improvements in emotion recognition tasks.'}, 'zh': {'title': 'MI-Fuse：提升语音情感识别的去噪标签融合框架', 'desc': 'MI-Fuse是一种去噪标签融合框架，旨在提高目标领域的语音情感识别（SER）性能。该框架结合了仅通过API访问的大型音频语言模型（LALM）和源领域训练的SER分类器，作为辅助教师。通过从两个教师模型中获取多个随机预测，并根据互信息的不确定性加权其均值分布，MI-Fuse能够稳定训练过程。实验结果表明，该方法在多个情感数据集上表现优异，学生模型的性能超过了LALM，提升幅度达到3.9%。'}}}, {'id': 'https://huggingface.co/papers/2509.20394', 'title': 'Blueprints of Trust: AI System Cards for End to End Transparency and\n  Governance', 'url': 'https://huggingface.co/papers/2509.20394', 'abstract': "The Hazard-Aware System Card (HASC) enhances AI system safety and accountability by integrating security and safety identifiers into a standardized framework.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating a comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes a standardized system of identifiers, including a novel AI Safety Hazard (ASH) ID, to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws. By providing a single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, we also compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems.", 'score': 2, 'issue_id': 6102, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '0e773214c050c323', 'authors': ['Huzaifa Sidhpurwala', 'Emily Fox', 'Garth Mollett', 'Florencio Cano Gabarda', 'Roman Zhukov'], 'affiliations': ['Red Hat'], 'pdf_title_img': 'assets/pdf/title_img/2509.20394.jpg', 'data': {'categories': ['#dataset', '#architecture', '#ethics', '#data', '#security', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Стандартизация безопасности AI через карточки систем с идентификаторами угроз', 'desc': 'В статье представлена новая система Hazard-Aware System Card (HASC), которая расширяет существующие концепции model card и system card для повышения прозрачности AI систем. Фреймворк предлагает стандартизированную систему идентификаторов, включая новый AI Safety Hazard (ASH) ID, который дополняет существующие идентификаторы безопасности типа CVE. HASC создает единый источник информации о состоянии безопасности AI системы на протяжении всего жизненного цикла. Авторы также сравнивают предложенный подход со стандартом ISO/IEC 42001:2023 и показывают, как они могут дополнять друг друга.'}, 'en': {'title': 'Enhancing AI Safety with the Hazard-Aware System Card', 'desc': 'The Hazard-Aware System Card (HASC) is a new framework aimed at improving the safety and accountability of AI systems. It combines security and safety identifiers into a standardized format, enhancing transparency in AI development and deployment. The HASC introduces a unique AI Safety Hazard (ASH) ID alongside existing security identifiers, facilitating better communication about vulnerabilities. By serving as a centralized resource, the HASC helps developers and stakeholders make informed decisions regarding AI system safety throughout its lifecycle.'}, 'zh': {'title': '提升AI系统安全与透明度的关键', 'desc': '本文介绍了一种新颖的框架——危险意识系统卡（HASC），旨在提高人工智能系统的透明度和问责制。HASC在现有的模型卡和系统卡概念基础上，整合了AI系统安全和安全状态的动态记录。该框架提出了一套标准化的标识符，包括新颖的AI安全危险（ASH）ID，以补充现有的安全标识符，如CVE，从而实现缺陷修复的清晰和一致的沟通。通过提供一个单一、可访问的真实信息来源，HASC使开发者和利益相关者能够在AI系统的整个生命周期中做出更明智的安全决策。'}}}, {'id': 'https://huggingface.co/papers/2509.18293', 'title': 'Evaluating Large Language Models for Detecting Antisemitism', 'url': 'https://huggingface.co/papers/2509.18293', 'abstract': "Evaluation of open-source LLMs for antisemitic content detection using in-context definition and a new Guided-CoT prompt shows improved performance and highlights differences in model utility, explainability, and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition as a policy guideline. We explore various prompting techniques and design a new CoT-like prompt, Guided-CoT. Guided-CoT handles the in-context policy well, increasing performance across all evaluated models, regardless of decoding configuration, model sizes, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability.", 'score': 1, 'issue_id': 6118, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 сентября', 'en': 'September 22', 'zh': '9月22日'}, 'hash': 'c28b331ea06b1a84', 'authors': ['Jay Patel', 'Hrudayangam Mehta', 'Jeremy Blackburn'], 'affiliations': ['Binghamton University, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.18293.jpg', 'data': {'categories': ['#alignment', '#training', '#open_source', '#interpretability', '#multimodal', '#dataset', '#ethics', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Guided-CoT промпты улучшают детекцию антисемитского контента в open-source LLM', 'desc': 'Исследователи оценили восемь open-source LLM на способность обнаруживать антисемитский контент, используя определения в контексте как руководящие принципы. Они разработали новый тип промпта под названием Guided-CoT, который улучшил производительность всех протестированных моделей независимо от их размера и возможностей рассуждения. Llama 3.1 70B показала лучшие результаты, чем дообученная GPT-3.5, что демонстрирует потенциал больших open-source моделей. Исследование также выявило различия между LLM в плане полезности, объяснимости и надежности при детекции враждебного контента.'}, 'en': {'title': 'Enhancing Antisemitic Content Detection with Guided-CoT Prompts', 'desc': "This paper evaluates the effectiveness of eight open-source large language models (LLMs) in detecting antisemitic content on social media. It introduces a new prompting technique called Guided-CoT, which enhances the models' performance by effectively utilizing in-context definitions as guidelines. The study finds that Llama 3.1 70B outperforms fine-tuned GPT-3.5 in this task, demonstrating the importance of model selection. Additionally, the authors analyze the errors made by the LLMs and propose new metrics to measure the differences in their reasoning and explainability."}, 'zh': {'title': '提升反犹太内容检测的模型性能', 'desc': '本研究评估了八种开源大型语言模型（LLMs）在检测反犹太内容方面的能力。我们采用了上下文定义作为政策指导，并设计了一种新的提示技术，称为Guided-CoT，显著提高了模型的性能。实验结果表明，Llama 3.1 70B在性能上优于微调后的GPT-3.5。我们还分析了模型的错误，并引入了量化语义偏差的新指标，揭示了不同模型之间的显著差异和矛盾行为。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (5)', '#agi (1)', '#alignment (4)', '#architecture (8)', '#audio (2)', '#benchmark (14)', '#cv (6)', '#data (7)', '#dataset (11)', '#diffusion (4)', '#ethics (2)', '#games (6)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (6)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (4)', '#multilingual', '#multimodal (14)', '#open_source (7)', '#optimization (20)', '#plp', '#rag', '#reasoning (14)', '#rl (8)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (21)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-29 00:52',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-29 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-29 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    