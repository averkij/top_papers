{
    "date": {
        "ru": "17 декабря",
        "en": "December 17",
        "zh": "12月17日"
    },
    "time_utc": "2024-12-17 07:11",
    "weekday": 1,
    "issue_id": 1162,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.11919",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "url": "https://huggingface.co/papers/2412.11919",
            "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at https://github.com/sunnynexus/RetroLLM.",
            "score": 18,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "35265a6474f53410",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yujia Zhou",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Huawei Poisson Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11919.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#rag"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "RetroLLM: Единая модель для точного поиска и генерации",
                    "desc": "RetroLLM - это новая унифицированная модель, объединяющая поиск и генерацию в единый процесс для больших языковых моделей. Она использует ограниченное декодирование для генерации доказательств непосредственно из корпуса текстов. Модель включает иерархические ограничения FM-индекса и опережающую стратегию декодирования для повышения точности. Эксперименты показали превосходную производительность RetroLLM на задачах вопросно-ответных систем как в рамках предметной области, так и вне ее."
                },
                "en": {
                    "title": "RetroLLM: Unifying Retrieval and Generation for Accurate Evidence Generation",
                    "desc": "This paper introduces RetroLLM, a novel framework that combines retrieval and generation in large language models (LLMs) to enhance their performance and reduce hallucinations. By integrating these processes, RetroLLM allows LLMs to generate precise evidence directly from a knowledge corpus while minimizing unnecessary input tokens. The framework employs hierarchical FM-Index constraints to filter relevant documents and a forward-looking constrained decoding strategy to ensure the accuracy of generated evidence. Experimental results show that RetroLLM outperforms existing methods on various open-domain question-answering datasets, demonstrating its effectiveness in both in-domain and out-of-domain scenarios."
                },
                "zh": {
                    "title": "整合检索与生成，提升语言模型的准确性",
                    "desc": "大型语言模型（LLMs）在生成能力上表现出色，但常常出现幻觉现象。检索增强生成（RAG）通过引入外部知识提供了有效的解决方案，但现有方法仍存在一些局限性，如额外的检索器部署成本、从检索文本块中产生的冗余输入标记，以及检索与生成缺乏联合优化。为了解决这些问题，我们提出了RetroLLM，一个将检索与生成整合为一个统一过程的框架，使LLMs能够直接从语料库中生成细粒度证据，并进行受限解码。此外，我们引入了层次FM-Index约束和前瞻性受限解码策略，以提高证据生成的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.09645",
            "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
            "url": "https://huggingface.co/papers/2412.09645",
            "abstract": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.",
            "score": 11,
            "issue_id": 1161,
            "pub_date": "2024-12-10",
            "pub_date_card": {
                "ru": "10 декабря",
                "en": "December 10",
                "zh": "12月10日"
            },
            "hash": "69f7aa2abe9671ed",
            "authors": [
                "Fan Zhang",
                "Shulin Tian",
                "Ziqi Huang",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.09645.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективная оценка генеративных моделей: человекоподобный подход",
                    "desc": "Статья представляет новый подход к оценке генеративных визуальных моделей, называемый Evaluation Agent. Этот метод имитирует человеческий подход, анализируя небольшое количество образцов за несколько раундов, что значительно ускоряет процесс оценки. Evaluation Agent предлагает эффективность, настраиваемость под нужды пользователя, объяснимость результатов и масштабируемость для различных моделей. Эксперименты показывают, что этот метод сокращает время оценки до 10% от традиционных подходов, сохраняя сопоставимую точность."
                },
                "en": {
                    "title": "Efficient and Tailored Evaluation for Visual Generative Models",
                    "desc": "This paper introduces the Evaluation Agent framework, designed to improve the evaluation of visual generative models like image and video generators. Traditional evaluation methods are slow and often fail to meet specific user needs, requiring extensive sampling that is computationally expensive. The Evaluation Agent mimics human evaluation by using fewer samples and providing detailed, tailored analyses, making the process more efficient and explainable. Experiments demonstrate that this framework can reduce evaluation time significantly while maintaining comparable results to existing methods."
                },
                "zh": {
                    "title": "高效评估生成模型的新方法",
                    "desc": "最近，视觉生成模型的进步使得高质量的图像和视频生成成为可能，应用范围广泛。然而，评估这些模型通常需要采样数百或数千张图像或视频，这使得计算过程非常耗时，尤其是对于基于扩散的模型。现有的评估方法依赖于固定的流程，忽视了用户的特定需求，并且提供的数值结果缺乏清晰的解释。为此，我们提出了评估代理框架，采用类人策略进行高效、动态的多轮评估，仅需少量样本，并提供详细的用户定制分析。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11815",
            "title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization",
            "url": "https://huggingface.co/papers/2412.11815",
            "abstract": "Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/.",
            "score": 10,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "de381dc70d0db48f",
            "authors": [
                "Junhao Zhuang",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Yong Liu",
                "Shiyi Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11815.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#rag",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "ColorFlow: Революция в автоматической колоризации изображений с сохранением идентичности",
                    "desc": "ColorFlow - это новая трёхэтапная система на основе диффузионных моделей для автоматической колоризации последовательностей чёрно-белых изображений. Она использует механизм самовнимания для извлечения цветовой идентичности и её сохранения при колоризации. Система превосходит существующие модели по нескольким метрикам, устанавливая новый стандарт в последовательной колоризации изображений. ColorFlow потенциально может принести пользу индустрии искусства, особенно в области колоризации мультфильмов и комиксов."
                },
                "en": {
                    "title": "Revolutionizing Image Sequence Colorization with ColorFlow",
                    "desc": "This paper presents ColorFlow, a novel framework designed for colorizing black-and-white image sequences while maintaining the identity of characters and objects. It utilizes a three-stage diffusion model that enhances controllability and consistency, addressing limitations found in previous methods. The framework features a dual-branch architecture that separates color identity extraction from the colorization process, allowing for effective use of color references. The authors also introduce ColorFlow-Bench, a benchmark for evaluating colorization performance, demonstrating that their approach significantly outperforms existing techniques in the field."
                },
                "zh": {
                    "title": "ColorFlow：图像序列上色的新标准",
                    "desc": "本文提出了一种名为ColorFlow的三阶段扩散模型框架，旨在自动为黑白图像序列上色，同时保持角色和物体的身份一致性。该方法通过检索增强的上色管道，利用相关的颜色参考进行图像上色，避免了现有方法中需要逐个身份微调的复杂性。ColorFlow采用双分支设计，一方面提取颜色身份，另一方面进行上色，充分利用了扩散模型的优势。通过ColorFlow-Bench基准测试，结果表明该模型在多个指标上优于现有模型，为图像序列上色设定了新标准，可能对艺术行业带来积极影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12083",
            "title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations",
            "url": "https://huggingface.co/papers/2412.12083",
            "abstract": "Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation.",
            "score": 10,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "333271d63ddd2102",
            "authors": [
                "Zhibing Li",
                "Tong Wu",
                "Jing Tan",
                "Mengchen Zhang",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12083.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#cv",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Декомпозиция изображений на геометрию и материалы с помощью диффузионной модели",
                    "desc": "IDArb - это модель на основе диффузии для декомпозиции изображений на внутренние свойства объектов. Она позволяет точно оценивать нормали поверхности и свойства материалов по нескольким изображениям с разным освещением. Модель использует новый модуль межвидового и междоменного внимания, а также стратегию обучения с аугментацией освещения. Авторы также представили новый набор данных ARB-Objaverse для обучения модели."
                },
                "en": {
                    "title": "Revolutionizing 3D Content Creation with IDArb",
                    "desc": "This paper presents IDArb, a diffusion-based model that addresses the challenge of intrinsic decomposition from multiple images with varying lighting conditions. Traditional methods are slow and struggle with ambiguities, while learning-based approaches often lack multi-view consistency. IDArb utilizes a novel cross-view, cross-domain attention mechanism and an illumination-augmented training strategy to achieve accurate estimations of surface normals and material properties. The introduction of the ARB-Objaverse dataset further enhances the model's training, leading to superior performance in various applications such as relighting and 3D reconstruction."
                },
                "zh": {
                    "title": "IDArb：多视角一致的内在分解新方法",
                    "desc": "本论文提出了一种名为IDArb的扩散模型，旨在从多视角图像中进行内在分解，捕捉几何和材料信息。与传统的优化方法相比，IDArb能够在不同光照条件下实现准确且多视角一致的表面法线和材料属性估计。我们还引入了一个新的数据集ARB-Objaverse，提供了大规模的多视角内在数据，支持模型的稳健训练。实验结果表明，IDArb在定性和定量上均优于现有的最先进方法，具有广泛的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12095",
            "title": "Causal Diffusion Transformers for Generative Modeling",
            "url": "https://huggingface.co/papers/2412.12095",
            "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
            "score": 9,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "e5107a05a397194f",
            "authors": [
                "Chaorui Deng",
                "Deyao Zh",
                "Kunchang Li",
                "Shi Guan",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12095.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "CausalFusion: Объединение авторегрессии и диффузии для мультимодального генеративного ИИ",
                    "desc": "Статья представляет Causal Diffusion - авторегрессионный аналог моделей диффузии для прогнозирования следующих токенов. Предложен CausalFusion - декодер-трансформер, который факторизует данные по токенам и уровням шума диффузии. Модель достигает передовых результатов в генерации изображений на ImageNet, сохраняя преимущества авторегрессии. CausalFusion демонстрирует мультимодальные возможности в совместной генерации изображений и подписей, а также способность к zero-shot манипуляциям изображениями."
                },
                "en": {
                    "title": "CausalFusion: Bridging Autoregressive and Diffusion Models for Enhanced Multimodal Generation",
                    "desc": "Causal Diffusion is a new approach that combines autoregressive (AR) models with diffusion models for predicting the next token in sequences. This method allows for better performance by introducing sequential factorization, which helps in transitioning smoothly between AR and diffusion generation modes. The proposed model, CausalFusion, is a transformer that effectively handles both discrete and continuous data, achieving top results in image generation tasks. Additionally, it demonstrates the ability to generate images and captions together, as well as perform image manipulations without prior training on specific tasks."
                },
                "zh": {
                    "title": "因果扩散：自回归与扩散模型的完美结合",
                    "desc": "我们提出了因果扩散（Causal Diffusion），作为扩散模型的自回归（AR）对应物。它是一种友好于离散和连续模式的下一个标记预测框架，并与现有的下一个标记预测模型（如LLaMA和GPT）兼容。通过在扩散模型中引入序列因子化，我们显著提高了性能，并实现了自回归和扩散生成模式之间的平滑过渡。我们还展示了因果融合（CausalFusion）在多模态能力方面的应用，包括联合图像生成和标题生成，以及零-shot上下文图像操作的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11605",
            "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
            "url": "https://huggingface.co/papers/2412.11605",
            "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPaR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR.",
            "score": 9,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "6e4d876c9f198e44",
            "authors": [
                "Jiale Cheng",
                "Xiao Liu",
                "Cunxiang Wang",
                "Xiaotao Gu",
                "Yida Lu",
                "Dan Zhang",
                "Yuxiao Dong",
                "Jie Tang",
                "Hongning Wang",
                "Minlie Huang"
            ],
            "affiliations": [
                "The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University",
                "The Knowledge Engineering Group (KEG), Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11605.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#rlhf",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "SPaR: точное следование инструкциям через самоигру",
                    "desc": "Статья описывает новый метод SPaR для улучшения способности языковых моделей следовать инструкциям. В отличие от существующих подходов, SPaR использует самоигру и древовидный поиск для создания более релевантных пар предпочтений. Эксперименты показывают, что модель LLaMA3-8B, обученная с помощью SPaR, превосходит GPT-4-Turbo на бенчмарке IFEval. Метод также демонстрирует хорошую масштабируемость и переносимость на другие модели."
                },
                "en": {
                    "title": "Enhancing Instruction Following with SPaR: A Self-Play Approach",
                    "desc": "This paper presents SPaR, a self-play framework designed to improve instruction-following capabilities in language models. It utilizes a tree-search strategy to refine responses, ensuring that preference pairs are valid and comparable without introducing irrelevant content variations. By training a LLaMA3-8B model with SPaR, the authors demonstrate that it outperforms GPT-4-Turbo on the IFEval benchmark while maintaining general performance. The framework also shows potential for scalability and transferability to larger models like GLM-4-9B and LLaMA3-70B, with insights on how tree search impacts inference performance."
                },
                "zh": {
                    "title": "自我对弈提升指令遵循能力",
                    "desc": "本文提出了一种名为SPaR的自我对弈框架，旨在提高语言模型对指令的遵循能力。通过树搜索自我精炼，SPaR能够生成有效且可比较的偏好对，避免了无关内容的干扰。实验表明，经过SPaR训练的LLaMA3-8B模型在IFEval基准测试中超越了GPT-4-Turbo，同时保持了其通用能力。SPaR还展示了良好的可扩展性和迁移性，显著提升了GLM-4-9B和LLaMA3-70B等模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11231",
            "title": "Smaller Language Models Are Better Instruction Evolvers",
            "url": "https://huggingface.co/papers/2412.11231",
            "abstract": "Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess a broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction Complex-Aware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: https://github.com/HypherX/Evolution-Analysis{https://github.com/HypherX/Evolution-Analysis}",
            "score": 9,
            "issue_id": 1159,
            "pub_date": "2024-12-15",
            "pub_date_card": {
                "ru": "15 декабря",
                "en": "December 15",
                "zh": "12月15日"
            },
            "hash": "0fd693d18eb484a1",
            "authors": [
                "Tingfeng Hui",
                "Lulu Zhao",
                "Guanting Dong",
                "Yaqi Zhang",
                "Hua Zhou",
                "Sen Su"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence, BAAI, Beijing, China",
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11231.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#training",
                    "#open_source",
                    "#alignment",
                    "#optimization"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Малые модели превосходят гигантов в создании инструкций",
                    "desc": "Статья исследует потенциал малых языковых моделей (SLM) в эволюции инструкций для обучения больших языковых моделей. Авторы обнаружили, что SLM могут создавать более эффективные инструкции, чем крупные модели (LLM), благодаря более широкому пространству выходных данных. Они предложили новую метрику IC-IFD для более точной оценки эффективности инструкций, учитывающую их сложность. Результаты ставят под сомнение распространенное предположение о превосходстве LLM в этой задаче."
                },
                "en": {
                    "title": "Unlocking the Power of Smaller Models in Instruction Tuning",
                    "desc": "This paper investigates the effectiveness of smaller language models (SLMs) in instruction tuning, challenging the belief that larger models like GPT-4 are always superior. The authors conduct experiments showing that SLMs can generate more effective and diverse instructions compared to their larger counterparts. They highlight that SLMs have a wider output space, allowing for richer instruction variants during the evolution process. Additionally, the paper introduces a new metric, Instruction Complex-Aware IFD (IC-IFD), to better assess the impact of instruction complexity on model performance."
                },
                "zh": {
                    "title": "小型语言模型的指令调优潜力",
                    "desc": "本研究探讨了指令调优在小型语言模型（SLMs）中的潜力，挑战了大型语言模型（LLMs）在指令演变中的主导地位。实验表明，SLMs能够生成比LLMs更有效的指令，且在指令演变过程中具有更广泛的输出空间。我们还发现现有的评估指标未能充分考虑指令的影响，因此提出了指令复杂度感知的IFD（IC-IFD）方法，以更准确地评估指令数据的有效性。通过这项研究，我们希望推动对小型语言模型的理解和应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12091",
            "title": "Wonderland: Navigating 3D Scenes from a Single Image",
            "url": "https://huggingface.co/papers/2412.12091",
            "abstract": "This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.",
            "score": 7,
            "issue_id": 1161,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "56eac228e6d5c48b",
            "authors": [
                "Hanwen Liang",
                "Junli Cao",
                "Vidit Goel",
                "Guocheng Qian",
                "Sergei Korolev",
                "Demetri Terzopoulos",
                "Konstantinos N. Plataniotis",
                "Sergey Tulyakov",
                "Jian Ren"
            ],
            "affiliations": [
                "Snap Inc.",
                "University of California, Los Angeles",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12091.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "От 2D к 3D: революция в реконструкции сцен с помощью латентных пространств",
                    "desc": "Статья представляет новый подход к созданию качественных 3D-сцен из одного изображения. Авторы предлагают использовать латентное пространство видео-диффузионной модели для предсказания 3D Gaussian Splatting. Модель обучается на латентных представлениях видео, что позволяет генерировать согласованные многоракурсные данные. Результаты показывают значительное улучшение качества 3D-реконструкции по сравнению с существующими методами, особенно для изображений вне обучающей выборки."
                },
                "en": {
                    "title": "Transforming Single Images into Rich 3D Worlds Efficiently!",
                    "desc": "This paper presents a new approach to generating high-quality 3D scenes from a single image, addressing limitations of existing methods that often require multiple views and extensive optimization. The authors introduce a large-scale reconstruction model that leverages latents from a video diffusion model, enabling the prediction of 3D Gaussian Splattings in a fast, feed-forward manner. By utilizing a video diffusion model that generates videos along specific camera paths, the method captures multi-view information while ensuring 3D consistency. The proposed model shows significant improvements in generating 3D scenes, especially for images not seen during training, marking a breakthrough in single-view 3D reconstruction."
                },
                "zh": {
                    "title": "高效生成高质量3D场景的新方法",
                    "desc": "本文探讨了如何从单张任意图像高效创建高质量、广范围的3D场景。现有方法面临多视图数据需求、每个场景优化耗时、背景视觉质量低以及未见区域重建失真等限制。我们提出了一种新颖的管道，利用视频扩散模型的潜在特征预测3D高斯点云，从而克服这些限制。通过在视频潜在空间上训练3D重建模型，我们实现了高效生成高质量、广范围的通用3D场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11258",
            "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
            "url": "https://huggingface.co/papers/2412.11258",
            "abstract": "Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on https://Gaussian-Property.github.io{this https URL}.",
            "score": 6,
            "issue_id": 1158,
            "pub_date": "2024-12-15",
            "pub_date_card": {
                "ru": "15 декабря",
                "en": "December 15",
                "zh": "12月15日"
            },
            "hash": "b8115a0ffb05a0df",
            "authors": [
                "Xinli Xu",
                "Wenhang Ge",
                "Dicong Qiu",
                "ZhiFei Chen",
                "Dongyu Yan",
                "Zhuoyun Liu",
                "Haoyu Zhao",
                "Hanfeng Zhao",
                "Shunsi Zhang",
                "Junwei Liang",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11258.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#robotics"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "GaussianProperty: Физические свойства в 3D без обучения",
                    "desc": "GaussianProperty - это безтренировочный фреймворк для присвоения физических свойств материалов 3D гауссианам. Он использует сегментационные возможности SAM и распознавание GPT-4V для анализа физических свойств на 2D изображениях, а затем проецирует их на 3D гауссианы. Фреймворк применяется для физического моделирования с использованием метода материальной точки (MPM) и прогнозирования силы захвата в робототехнике. Эксперименты подтверждают эффективность метода в понимании физических свойств из визуальных данных."
                },
                "en": {
                    "title": "Revolutionizing Physical Property Estimation with GaussianProperty",
                    "desc": "This paper presents GaussianProperty, a novel framework designed to estimate physical properties of materials from visual data without the need for extensive training. It combines the segmentation capabilities of SAM with the recognition abilities of GPT-4V(ision) to create a reasoning module that works on 2D images. The framework projects these properties into 3D Gaussians using a voting strategy, facilitating applications in dynamic simulation and robotic grasping. The authors demonstrate the effectiveness of their approach through experiments, showcasing its potential in enhancing physics-based simulations and improving robotic interactions with objects."
                },
                "zh": {
                    "title": "从视觉数据中提取物理属性的创新方法",
                    "desc": "本文介绍了一种名为GaussianProperty的框架，用于从视觉数据中估计物理属性。该方法结合了SAM的分割能力和GPT-4V(ision)的识别能力，形成了一个针对2D图像的全局-局部物理属性推理模块。通过投票策略，我们将多视角2D图像的物理属性投影到3D高斯分布上。实验结果表明，该方法在物理基础动态仿真和机器人抓取等应用中具有显著效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11834",
            "title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture",
            "url": "https://huggingface.co/papers/2412.11834",
            "abstract": "In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be a competitor to popular model architectures.",
            "score": 4,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "7090d4cb4588f236",
            "authors": [
                "Jingze Shi",
                "Bingheng Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.11834.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Чудесные матрицы: новый подход к архитектуре фундаментальных моделей",
                    "desc": "Статья представляет новый подход к улучшению фундаментальных моделей машинного обучения, объединяя преобразования последовательностей и состояний. Авторы доказывают эффективность ротационного позиционного кодирования в алгоритме дуальности пространства состояний, что снижает перплексию модели. Они предлагают динамическое маскирующее внимание, значительно улучшающее точность в задачах ассоциативного поиска. Кроме того, разработан метод кросс-доменной смеси экспертов, ускоряющий вычисления при большом количестве экспертов."
                },
                "en": {
                    "title": "Enhancing Foundation Models with Efficient Transformations",
                    "desc": "This paper introduces a novel approach to enhance foundation models by integrating sequence transformation and state transformation techniques. It demonstrates the effectiveness of rotary position embedding in reducing perplexity in hybrid attention mechanisms, leading to improved performance. The authors also present dynamic mask attention, which significantly boosts accuracy in complex recall tasks while filtering relevant information efficiently. Additionally, they propose a cross-domain mixture of experts that accelerates expert retrieval, making it a competitive alternative to existing model architectures."
                },
                "zh": {
                    "title": "结合序列与状态变换，提升基础模型效率",
                    "desc": "本文提出了一种结合序列变换和状态变换的方法，以提高基础模型的效率和效果。我们证明了旋转位置嵌入在状态空间对偶算法中的有效性，显著降低了混合二次因果自注意力和状态空间对偶的困惑度。我们还提出了动态掩码注意力，在多查询关联回忆任务中保持100%的准确率，提升了150%以上。最后，我们设计了跨域专家混合，使得专家检索的计算速度比传统方法快8到10倍，形成了具有竞争力的基础模型：奇妙矩阵。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11586",
            "title": "StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors",
            "url": "https://huggingface.co/papers/2412.11586",
            "abstract": "While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io.",
            "score": 4,
            "issue_id": 1159,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "9787d6a0befef45d",
            "authors": [
                "Xiaokun Sun",
                "Zeyu Cai",
                "Zhenyu Zhang",
                "Ying Tai",
                "Jian Yang"
            ],
            "affiliations": [
                "Nanjing University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11586.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "💇",
                "ru": {
                    "title": "Реалистичные 3D-прически из текста: новый уровень генерации аватаров",
                    "desc": "Статья представляет StrandHead - новый метод генерации 3D-аватаров головы с детализированными волосами на основе текстового описания. Авторы предлагают использовать дистилляцию 2D генеративных диффузионных моделей для создания реалистичных прядей волос без использования 3D-данных для обучения. Метод включает ряд надежных приоров для инициализации формы, геометрических примитивов и статистических характеристик причесок, что обеспечивает стабильную оптимизацию и соответствие текстовому описанию. Эксперименты показывают, что StrandHead достигает лучших результатов по реалистичности и разнообразию сгенерированных 3D-голов и причесок."
                },
                "en": {
                    "title": "StrandHead: Realistic 3D Hair Generation from Text Prompts",
                    "desc": "This paper introduces StrandHead, a new method for creating 3D head avatars from text descriptions, focusing on generating realistic hair. Unlike previous methods that struggle with hair representation, StrandHead uses a unique approach to model hair as individual strands, allowing for more detailed and diverse outputs. The method leverages 2D generative diffusion models without needing 3D data, employing reliable priors to ensure stable optimization and alignment with text prompts. The results show that StrandHead outperforms existing techniques in generating lifelike 3D heads and hair, making it suitable for applications like physical simulation in gaming engines."
                },
                "zh": {
                    "title": "StrandHead：生成独特3D发型的创新方法",
                    "desc": "本论文提出了一种新的3D头像生成方法StrandHead，能够生成具有独立发丝表示的3D头发。该方法不依赖于3D数据进行监督，而是通过提炼2D生成扩散模型来生成逼真的发丝。我们提出了一系列可靠的先验知识，包括形状初始化、几何原语和统计发型特征，从而实现稳定的优化和与文本对齐的性能。实验结果表明，StrandHead在生成3D头部和头发的真实感和多样性方面达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.10316",
            "title": "BrushEdit: All-In-One Image Inpainting and Editing",
            "url": "https://huggingface.co/papers/2412.10316",
            "abstract": "Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.",
            "score": 2,
            "issue_id": 1162,
            "pub_date": "2024-12-13",
            "pub_date_card": {
                "ru": "13 декабря",
                "en": "December 13",
                "zh": "12月13日"
            },
            "hash": "d8789f3e683b7c6b",
            "authors": [
                "Yaowei Li",
                "Yuxuan Bian",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Ying Shan",
                "Yuexian Zou",
                "Qiang Xu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Peking University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.10316.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🖌️",
                "ru": {
                    "title": "BrushEdit: Интеллектуальное редактирование изображений с помощью инструкций и инпейнтинга",
                    "desc": "Статья представляет BrushEdit - новую парадигму редактирования изображений на основе инструкций и инпейнтинга. Она использует мультимодальные большие языковые модели (MLLM) и модели инпейнтинга изображений для автономного и интерактивного редактирования. Система включает классификацию категорий редактирования, идентификацию основных объектов, получение масок и инпейнтинг области редактирования. Эксперименты показывают превосходную производительность по семи метрикам, включая сохранение области маски и согласованность эффекта редактирования."
                },
                "en": {
                    "title": "Empowering Image Editing with Interactive Instruction and Inpainting",
                    "desc": "This paper introduces BrushEdit, a new method for image editing that combines instruction-based and inpainting techniques. It addresses the limitations of current methods by allowing users to interactively specify editing regions and intensity without being constrained to black-box operations. The system uses multimodal large language models (MLLMs) alongside a dual-branch image inpainting model to classify editing categories, identify main objects, and create masks for editing. Experimental results demonstrate that BrushEdit outperforms existing methods in preserving mask regions and maintaining coherent editing effects."
                },
                "zh": {
                    "title": "自由形式的智能图像编辑新方法",
                    "desc": "本论文提出了一种新的图像编辑方法，称为BrushEdit，旨在解决现有图像编辑技术的局限性。通过结合多模态大语言模型（MLLMs）和图像修复模型，该方法实现了用户友好的自由形式指令编辑。BrushEdit能够自动识别编辑类别、主要对象，并获取编辑区域的掩码，从而支持更大幅度的图像修改。实验结果表明，该框架在多个指标上表现优越，能够有效保持掩码区域和编辑效果的一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11457",
            "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
            "url": "https://huggingface.co/papers/2412.11457",
            "abstract": "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the model's comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the model's capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise a structure-guided timestep sampling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks.",
            "score": 1,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "415b98ca8c3ed003",
            "authors": [
                "Ruijie Lu",
                "Yixin Chen",
                "Junfeng Ni",
                "Baoxiong Jia",
                "Yu Liu",
                "Diwen Wan",
                "Gang Zeng",
                "Siyuan Huang"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11457.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение структурной осведомленности для синтеза новых ракурсов сцен с несколькими объектами",
                    "desc": "Статья представляет метод MOVIS для улучшения структурной осведомленности диффузионных моделей при синтезе новых ракурсов сцен с несколькими объектами. Авторы предлагают внедрять структурно-осведомленные признаки, такие как глубина и маски объектов, в U-Net для улучшения понимания моделью пространственных отношений. Они также вводят вспомогательную задачу предсказания масок объектов с новых ракурсов и разрабатывают специальный планировщик выборки временных шагов для баланса между глобальным размещением объектов и восстановлением мелких деталей. Авторы предлагают новые метрики для оценки правдоподобия синтезированных изображений."
                },
                "en": {
                    "title": "Enhancing Multi-Object Novel View Synthesis with Structural Awareness",
                    "desc": "This paper introduces MOVIS, a method designed to improve multi-object novel view synthesis (NVS) using pre-trained diffusion models. The approach enhances the model's understanding of object structures by incorporating depth and object masks into the denoising U-Net architecture. Additionally, it introduces an auxiliary task that helps the model predict object masks for novel views, improving object differentiation and placement. The authors also propose a new sampling scheduler that balances global object placement with detailed recovery, and they evaluate the model's performance using cross-view consistency metrics alongside traditional NVS measures."
                },
                "zh": {
                    "title": "提升多物体新视角合成的结构感知能力",
                    "desc": "本文提出了一种名为MOVIS的方法，旨在提高多物体新视角合成（NVS）中的结构感知能力。通过将深度信息和物体掩码等结构感知特征注入去噪U-Net，模型能够更好地理解物体实例及其空间关系。此外，模型还被要求同时预测新视角的物体掩码，从而增强其区分和放置物体的能力。最后，本文通过分析扩散采样过程，设计了一种结构引导的时间步采样调度器，以平衡全局物体放置和细节恢复的学习。"
                }
            }
        }
    ],
    "link_prev": "2024-12-16.html",
    "link_next": "2024-12-18.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12月16日"
    },
    "short_date_next": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12月18日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 0,
        "#3d": 5,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 8,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了GenEx系统，它能通过单张RGB图像生成3D一致的想象环境。该系统利用虚幻引擎的3D数据，帮助AI代理在复杂任务中进行探索和导航。GenEx展示了高质量的世界生成和强大的3D能力，如3D映射和循环一致性。文章总结说，GenEx为提升想象空间中的 embodied AI 提供了变革性平台，并有潜力扩展到现实世界探索。",
        "title": "GenEx: Generating an Explorable World",
        "pinyin": "这篇文章介绍了GenEx系统，它能通过单张RGB图像生成3D一致的想象环境。该系统利用虚幻引擎的3D数据，帮助AI代理在复杂任务中进行探索和导航。GenEx展示了高质量的世界生成和强大的3D能力，如3D映射和循环一致性。文章总结说，GenEx为提升想象空间中的 embodied AI 提供了变革性平台，并有潜力扩展到现实世界探索。\n\nzhè piān wén zhāng jiè shào le GenEx xì tǒng, tā néng tōng guò dān zhāng RGB tú xíng shēng chéng 3D yī zhì de xiǎng xiàng huán jìng. gǎi xì tǒng lì yòng xū huàn yǐn qíng de 3D shù jù, bāng zhù AI dài lǐ zài fú zà rèn wù zhōng jìn xíng tàn suǒ hé dǎo háng. GenEx zhǎn shì le gāo zhì liàng de shì jiè shēng chéng hé qiáng dà de 3D néng lì, rú 3D yǐng shè hé xún huán yī zhì xìng. wén zhāng zǒng jiè shuō, GenEx wèi tí shēng xiǎng xiàng kōng zhōng de embodied AI tí gōng le biàn gé xìng píng tái, bìng yǒu qián lì kuò zhǎn dào xiàn shí shì jiè tàn suǒ.",
        "vocab": "[{'word': 'GenEx', 'pinyin': '', 'trans': 'GenEx'}, {'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'}, {'word': '通过', 'pinyin': 'tōngguò', 'trans': 'through'}, {'word': '单张', 'pinyin': 'dān zhāng', 'trans': 'single'}, {'word': 'RGB', 'pinyin': '', 'trans': 'RGB'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'}, {'word': '想象', 'pinyin': 'xiǎngxiàng', 'trans': 'imaginary'}, {'word': '环境', 'pinyin': 'huánjìng', 'trans': 'environment'}, {'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'}, {'word': '虚幻', 'pinyin': 'xūhuàn', 'trans': 'virtual'}, {'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'}, {'word': '数据', 'pinyin': 'shùjù', 'trans': 'data'}, {'word': '帮助', 'pinyin': 'bāngzhù', 'trans': 'help'}, {'word': 'AI', 'pinyin': '', 'trans': 'AI'}, {'word': '代理', 'pinyin': 'dàilǐ', 'trans': 'agent'}, {'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '进行', 'pinyin': 'jìnxíng', 'trans': 'conduct'}, {'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'explore'}, {'word': '导航', 'pinyin': 'dǎoháng', 'trans': 'navigate'}, {'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'}, {'word': '世界', 'pinyin': 'shìjiè', 'trans': 'world'}, {'word': '强大', 'pinyin': 'qiángdà', 'trans': 'powerful'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '映射', 'pinyin': 'yìngshè', 'trans': 'mapping'}, {'word': '循环', 'pinyin': 'xúnhuán', 'trans': 'cyclic'}, {'word': '一致性', 'pinyin': 'yīzhìxìng', 'trans': 'consistency'}, {'word': '总结', 'pinyin': 'zǒngjié', 'trans': 'summarize'}, {'word': '说', 'pinyin': 'shuō', 'trans': 'say'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'}, {'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'}, {'word': 'embodied', 'pinyin': '', 'trans': 'embodied'}, {'word': '变革性', 'pinyin': 'biàngéxìng', 'trans': 'transformative'}, {'word': '平台', 'pinyin': 'píngtái', 'trans': 'platform'}, {'word': '潜力', 'pinyin': 'qiánlì', 'trans': 'potential'}, {'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'}, {'word': '现实', 'pinyin': 'xiànshí', 'trans': 'real'}, {'word': '世界', 'pinyin': 'shìjiè', 'trans': 'world'}, {'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'explore'}]",
        "trans": "This article introduces the GenEx system, which can generate a 3D-consistent imagined environment from a single RGB image. The system leverages 3D data from the Unreal Engine to assist AI agents in exploration and navigation in complex tasks. GenEx demonstrates high-quality world generation and powerful 3D capabilities, such as 3D mapping and cyclic consistency. The article concludes that GenEx provides a transformative platform for enhancing embodied AI in imagined spaces and has the potential to extend to real-world exploration.",
        "update_ts": "2024-12-16 09:12"
    }
}