{
    "date": {
        "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 17",
        "zh": "12æœˆ17æ—¥"
    },
    "time_utc": "2024-12-17 03:28",
    "weekday": 1,
    "issue_id": 1158,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.11919",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "url": "https://huggingface.co/papers/2412.11919",
            "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at https://github.com/sunnynexus/RetroLLM.",
            "score": 13,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "35265a6474f53410",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yujia Zhou",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Huawei Poisson Lab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11919.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "RetroLLM: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "RetroLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ FM-Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¸ Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°ÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RetroLLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ĞºĞ°Ğº Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ½Ğµ ĞµĞµ."
                },
                "en": {
                    "title": "RetroLLM: Unifying Retrieval and Generation for Accurate Evidence Generation",
                    "desc": "This paper introduces RetroLLM, a novel framework that combines retrieval and generation in large language models (LLMs) to enhance their performance and reduce hallucinations. By integrating these processes, RetroLLM allows LLMs to generate precise evidence directly from a knowledge corpus while minimizing unnecessary input tokens. The framework employs hierarchical FM-Index constraints to filter relevant documents and a forward-looking constrained decoding strategy to ensure the accuracy of generated evidence. Experimental results show that RetroLLM outperforms existing methods on various open-domain question-answering datasets, demonstrating its effectiveness in both in-domain and out-of-domain scenarios."
                },
                "zh": {
                    "title": "æ•´åˆæ£€ç´¢ä¸ç”Ÿæˆï¼Œæå‡è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆèƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚é¢å¤–çš„æ£€ç´¢å™¨éƒ¨ç½²æˆæœ¬ã€ä»æ£€ç´¢æ–‡æœ¬å—ä¸­äº§ç”Ÿçš„å†—ä½™è¾“å…¥æ ‡è®°ï¼Œä»¥åŠæ£€ç´¢ä¸ç”Ÿæˆç¼ºä¹è”åˆä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªå°†æ£€ç´¢ä¸ç”Ÿæˆæ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€è¿‡ç¨‹çš„æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦è¯æ®ï¼Œå¹¶è¿›è¡Œå—é™è§£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±‚æ¬¡FM-Indexçº¦æŸå’Œå‰ç»æ€§å—é™è§£ç ç­–ç•¥ï¼Œä»¥æé«˜è¯æ®ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12083",
            "title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations",
            "url": "https://huggingface.co/papers/2412.12083",
            "abstract": "Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation.",
            "score": 1,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "333271d63ddd2102",
            "authors": [
                "Zhibing Li",
                "Tong Wu",
                "Jing Tan",
                "Mengchen Zhang",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12083.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#cv",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "IDArb - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼ĞµĞ¶Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ARB-Objaverse Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Content Creation with IDArb",
                    "desc": "This paper presents IDArb, a diffusion-based model that addresses the challenge of intrinsic decomposition from multiple images with varying lighting conditions. Traditional methods are slow and struggle with ambiguities, while learning-based approaches often lack multi-view consistency. IDArb utilizes a novel cross-view, cross-domain attention mechanism and an illumination-augmented training strategy to achieve accurate estimations of surface normals and material properties. The introduction of the ARB-Objaverse dataset further enhances the model's training, leading to superior performance in various applications such as relighting and 3D reconstruction."
                },
                "zh": {
                    "title": "IDArbï¼šå¤šè§†è§’ä¸€è‡´çš„å†…åœ¨åˆ†è§£æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºIDArbçš„æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ä»å¤šè§†è§’å›¾åƒä¸­è¿›è¡Œå†…åœ¨åˆ†è§£ï¼Œæ•æ‰å‡ ä½•å’Œææ–™ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒIDArbèƒ½å¤Ÿåœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹å®ç°å‡†ç¡®ä¸”å¤šè§†è§’ä¸€è‡´çš„è¡¨é¢æ³•çº¿å’Œææ–™å±æ€§ä¼°è®¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ARB-Objaverseï¼Œæä¾›äº†å¤§è§„æ¨¡çš„å¤šè§†è§’å†…åœ¨æ•°æ®ï¼Œæ”¯æŒæ¨¡å‹çš„ç¨³å¥è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDArbåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11457",
            "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
            "url": "https://huggingface.co/papers/2412.11457",
            "abstract": "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the model's comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the model's capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise a structure-guided timestep sampling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks.",
            "score": 0,
            "issue_id": 1158,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "415b98ca8c3ed003",
            "authors": [
                "Ruijie Lu",
                "Yixin Chen",
                "Junfeng Ni",
                "Baoxiong Jia",
                "Yu Liu",
                "Diwen Wan",
                "Gang Zeng",
                "Siyuan Huang"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11457.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#training",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MOVIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ² U-Net Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Multi-Object Novel View Synthesis with Structural Awareness",
                    "desc": "This paper introduces MOVIS, a method designed to improve multi-object novel view synthesis (NVS) using pre-trained diffusion models. The approach enhances the model's understanding of object structures by incorporating depth and object masks into the denoising U-Net architecture. Additionally, it introduces an auxiliary task that helps the model predict object masks for novel views, improving object differentiation and placement. The authors also propose a new sampling scheduler that balances global object placement with detailed recovery, and they evaluate the model's performance using cross-view consistency metrics alongside traditional NVS measures."
                },
                "zh": {
                    "title": "æå‡å¤šç‰©ä½“æ–°è§†è§’åˆæˆçš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMOVISçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šç‰©ä½“æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ä¸­çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å°†æ·±åº¦ä¿¡æ¯å’Œç‰©ä½“æ©ç ç­‰ç»“æ„æ„ŸçŸ¥ç‰¹å¾æ³¨å…¥å»å™ªU-Netï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç‰©ä½“å®ä¾‹åŠå…¶ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜è¢«è¦æ±‚åŒæ—¶é¢„æµ‹æ–°è§†è§’çš„ç‰©ä½“æ©ç ï¼Œä»è€Œå¢å¼ºå…¶åŒºåˆ†å’Œæ”¾ç½®ç‰©ä½“çš„èƒ½åŠ›ã€‚æœ€åï¼Œæœ¬æ–‡é€šè¿‡åˆ†ææ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œè®¾è®¡äº†ä¸€ç§ç»“æ„å¼•å¯¼çš„æ—¶é—´æ­¥é‡‡æ ·è°ƒåº¦å™¨ï¼Œä»¥å¹³è¡¡å…¨å±€ç‰©ä½“æ”¾ç½®å’Œç»†èŠ‚æ¢å¤çš„å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11258",
            "title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs",
            "url": "https://huggingface.co/papers/2412.11258",
            "abstract": "Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on https://Gaussian-Property.github.io{this https URL}.",
            "score": 0,
            "issue_id": 1158,
            "pub_date": "2024-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "b8115a0ffb05a0df",
            "authors": [
                "Xinli Xu",
                "Wenhang Ge",
                "Dicong Qiu",
                "ZhiFei Chen",
                "Dongyu Yan",
                "Zhuoyun Liu",
                "Haoyu Zhao",
                "Hanfeng Zhao",
                "Shunsi Zhang",
                "Junwei Liang",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11258.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#robotics"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "GaussianProperty: Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ² 3D Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "GaussianProperty - ÑÑ‚Ğ¾ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ SAM Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ GPT-4V Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ½Ğ° 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ (MPM) Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ»Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Physical Property Estimation with GaussianProperty",
                    "desc": "This paper presents GaussianProperty, a novel framework designed to estimate physical properties of materials from visual data without the need for extensive training. It combines the segmentation capabilities of SAM with the recognition abilities of GPT-4V(ision) to create a reasoning module that works on 2D images. The framework projects these properties into 3D Gaussians using a voting strategy, facilitating applications in dynamic simulation and robotic grasping. The authors demonstrate the effectiveness of their approach through experiments, showcasing its potential in enhancing physics-based simulations and improving robotic interactions with objects."
                },
                "zh": {
                    "title": "ä»è§†è§‰æ•°æ®ä¸­æå–ç‰©ç†å±æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGaussianPropertyçš„æ¡†æ¶ï¼Œç”¨äºä»è§†è§‰æ•°æ®ä¸­ä¼°è®¡ç‰©ç†å±æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†SAMçš„åˆ†å‰²èƒ½åŠ›å’ŒGPT-4V(ision)çš„è¯†åˆ«èƒ½åŠ›ï¼Œå½¢æˆäº†ä¸€ä¸ªé’ˆå¯¹2Då›¾åƒçš„å…¨å±€-å±€éƒ¨ç‰©ç†å±æ€§æ¨ç†æ¨¡å—ã€‚é€šè¿‡æŠ•ç¥¨ç­–ç•¥ï¼Œæˆ‘ä»¬å°†å¤šè§†è§’2Då›¾åƒçš„ç‰©ç†å±æ€§æŠ•å½±åˆ°3Dé«˜æ–¯åˆ†å¸ƒä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ç†åŸºç¡€åŠ¨æ€ä»¿çœŸå’Œæœºå™¨äººæŠ“å–ç­‰åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-16.html",
    "link_next": "2024-12-18.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "16.12",
        "en": "12/16",
        "zh": "12æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.12",
        "en": "12/18",
        "zh": "12æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚",
        "title": "GenEx: Generating an Explorable World",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†GenExç³»ç»Ÿï¼Œå®ƒèƒ½é€šè¿‡å•å¼ RGBå›¾åƒç”Ÿæˆ3Dä¸€è‡´çš„æƒ³è±¡ç¯å¢ƒã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è™šå¹»å¼•æ“çš„3Dæ•°æ®ï¼Œå¸®åŠ©AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­è¿›è¡Œæ¢ç´¢å’Œå¯¼èˆªã€‚GenExå±•ç¤ºäº†é«˜è´¨é‡çš„ä¸–ç•Œç”Ÿæˆå’Œå¼ºå¤§çš„3Dèƒ½åŠ›ï¼Œå¦‚3Dæ˜ å°„å’Œå¾ªç¯ä¸€è‡´æ€§ã€‚æ–‡ç« æ€»ç»“è¯´ï¼ŒGenExä¸ºæå‡æƒ³è±¡ç©ºé—´ä¸­çš„ embodied AI æä¾›äº†å˜é©æ€§å¹³å°ï¼Œå¹¶æœ‰æ½œåŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œæ¢ç´¢ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le GenEx xÃ¬ tÇ’ng, tÄ nÃ©ng tÅng guÃ² dÄn zhÄng RGB tÃº xÃ­ng shÄ“ng chÃ©ng 3D yÄ« zhÃ¬ de xiÇng xiÃ ng huÃ¡n jÃ¬ng. gÇi xÃ¬ tÇ’ng lÃ¬ yÃ²ng xÅ« huÃ n yÇn qÃ­ng de 3D shÃ¹ jÃ¹, bÄng zhÃ¹ AI dÃ i lÇ zÃ i fÃº zÃ  rÃ¨n wÃ¹ zhÅng jÃ¬n xÃ­ng tÃ n suÇ’ hÃ© dÇo hÃ¡ng. GenEx zhÇn shÃ¬ le gÄo zhÃ¬ liÃ ng de shÃ¬ jiÃ¨ shÄ“ng chÃ©ng hÃ© qiÃ¡ng dÃ  de 3D nÃ©ng lÃ¬, rÃº 3D yÇng shÃ¨ hÃ© xÃºn huÃ¡n yÄ« zhÃ¬ xÃ¬ng. wÃ©n zhÄng zÇ’ng jiÃ¨ shuÅ, GenEx wÃ¨i tÃ­ shÄ“ng xiÇng xiÃ ng kÅng zhÅng de embodied AI tÃ­ gÅng le biÃ n gÃ© xÃ¬ng pÃ­ng tÃ¡i, bÃ¬ng yÇ’u qiÃ¡n lÃ¬ kuÃ² zhÇn dÃ o xiÃ n shÃ­ shÃ¬ jiÃ¨ tÃ n suÇ’.",
        "vocab": "[{'word': 'GenEx', 'pinyin': '', 'trans': 'GenEx'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'}, {'word': 'é€šè¿‡', 'pinyin': 'tÅngguÃ²', 'trans': 'through'}, {'word': 'å•å¼ ', 'pinyin': 'dÄn zhÄng', 'trans': 'single'}, {'word': 'RGB', 'pinyin': '', 'trans': 'RGB'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': 'ä¸€è‡´', 'pinyin': 'yÄ«zhÃ¬', 'trans': 'consistent'}, {'word': 'æƒ³è±¡', 'pinyin': 'xiÇngxiÃ ng', 'trans': 'imaginary'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡njÃ¬ng', 'trans': 'environment'}, {'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'}, {'word': 'è™šå¹»', 'pinyin': 'xÅ«huÃ n', 'trans': 'virtual'}, {'word': 'å¼•æ“', 'pinyin': 'yÇnqÃ­ng', 'trans': 'engine'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹jÃ¹', 'trans': 'data'}, {'word': 'å¸®åŠ©', 'pinyin': 'bÄngzhÃ¹', 'trans': 'help'}, {'word': 'AI', 'pinyin': '', 'trans': 'AI'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'è¿›è¡Œ', 'pinyin': 'jÃ¬nxÃ­ng', 'trans': 'conduct'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}, {'word': 'å¯¼èˆª', 'pinyin': 'dÇohÃ¡ng', 'trans': 'navigate'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'}, {'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high quality'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ngdÃ ', 'trans': 'powerful'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'}, {'word': 'æ˜ å°„', 'pinyin': 'yÃ¬ngshÃ¨', 'trans': 'mapping'}, {'word': 'å¾ªç¯', 'pinyin': 'xÃºnhuÃ¡n', 'trans': 'cyclic'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ«zhÃ¬xÃ¬ng', 'trans': 'consistency'}, {'word': 'æ€»ç»“', 'pinyin': 'zÇ’ngjiÃ©', 'trans': 'summarize'}, {'word': 'è¯´', 'pinyin': 'shuÅ', 'trans': 'say'}, {'word': 'æå‡', 'pinyin': 'tÃ­shÄ“ng', 'trans': 'enhance'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'}, {'word': 'embodied', 'pinyin': '', 'trans': 'embodied'}, {'word': 'å˜é©æ€§', 'pinyin': 'biÃ ngÃ©xÃ¬ng', 'trans': 'transformative'}, {'word': 'å¹³å°', 'pinyin': 'pÃ­ngtÃ¡i', 'trans': 'platform'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²zhÇn', 'trans': 'expand'}, {'word': 'ç°å®', 'pinyin': 'xiÃ nshÃ­', 'trans': 'real'}, {'word': 'ä¸–ç•Œ', 'pinyin': 'shÃ¬jiÃ¨', 'trans': 'world'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}]",
        "trans": "This article introduces the GenEx system, which can generate a 3D-consistent imagined environment from a single RGB image. The system leverages 3D data from the Unreal Engine to assist AI agents in exploration and navigation in complex tasks. GenEx demonstrates high-quality world generation and powerful 3D capabilities, such as 3D mapping and cyclic consistency. The article concludes that GenEx provides a transformative platform for enhancing embodied AI in imagined spaces and has the potential to extend to real-world exploration.",
        "update_ts": "2024-12-16 09:12"
    }
}