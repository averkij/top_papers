{
    "date": {
        "ru": "14 октября",
        "en": "October 14",
        "zh": "10月14日"
    },
    "time_utc": "2024-10-15 16:15",
    "issue_id": 119,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.09732",
            "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
            "url": "https://huggingface.co/papers/2410.09732",
            "abstract": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/",
            "score": 44,
            "issue_id": 107,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "LOKI: Мультимодальный детектив для искусственного интеллекта",
                    "desc": "LOKI - это новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) обнаруживать синтетические данные в различных модальностях. Он включает в себя 18 тысяч вопросов по видео, изображениям, 3D, тексту и аудио, разделенных на 26 подкатегорий с четкими уровнями сложности. LOKI позволяет проводить комплексный анализ LMM через задачи грубой классификации, выбора из нескольких вариантов, а также выявления и объяснения аномалий. Авторы оценили 28 моделей LMM на этом бенчмарке, выявив их потенциал и ограничения в обнаружении синтетических данных."
                },
                "en": {
                    "title": "LOKI: Unmasking Synthetic Data with Multimodal Models",
                    "desc": "The paper introduces LOKI, a benchmark designed to evaluate large multimodal models (LMMs) in detecting synthetic data across various modalities like video, image, and text. LOKI includes 18,000 questions that test the models' ability to distinguish real from synthetic data, providing insights into their perception and reasoning skills. The benchmark features different types of questions, such as multiple-choice and anomaly detection, to thoroughly assess the models' capabilities. The study evaluates 28 models, revealing both their potential and limitations in synthetic data detection."
                },
                "zh": {
                    "title": "LOKI：多模态合成数据检测的新基准",
                    "desc": "随着AI生成内容的快速发展，未来的互联网可能会充斥着合成数据，使得辨别真实和可信的多模态数据变得越来越困难。为了应对这一挑战，我们引入了LOKI，一个用于评估大型多模态模型（LMMs）检测合成数据能力的新基准。LOKI涵盖视频、图像、3D、文本和音频等多种模态，提供了18K个精心设计的问题，帮助全面分析LMMs的能力。我们对22个开源LMMs和6个闭源模型进行了评估，揭示了它们作为合成数据检测器的潜力和一些能力发展的局限性。"
                }
            },
            "hash": "ec9d4127c5348f2d",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10139",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.10139",
            "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.",
            "score": 42,
            "issue_id": 107,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "MMIE: Новый стандарт для оценки чередующихся мультимодальных моделей",
                    "desc": "MMIE - это новый масштабный бенчмарк для оценки моделей с чередующимся мультимодальным пониманием и генерацией (LVLMs). Он содержит 20 000 тщательно отобранных мультимодальных запросов из 12 областей знаний. MMIE поддерживает как чередующиеся входные данные, так и выходные, предлагая различные форматы вопросов. Авторы также предлагают автоматизированную метрику оценки, основанную на модели, дообученной на размеченных человеком данных."
                },
                "en": {
                    "title": "MMIE: Elevating Multimodal Model Evaluation",
                    "desc": "The paper introduces MMIE, a large-scale benchmark designed to evaluate the ability of Large Vision-Language Models (LVLMs) to understand and generate both images and text in mixed sequences. MMIE includes 20,000 multimodal queries across various fields like mathematics and arts, using both multiple-choice and open-ended questions to test different skills. The authors also propose a new automated evaluation metric that uses a scoring model fine-tuned with human data to reduce bias and improve accuracy. Experiments show that while current LVLMs have room for improvement, MMIE provides a comprehensive framework for assessing their capabilities."
                },
                "zh": {
                    "title": "推动多模态理解与生成的新时代",
                    "desc": "这篇论文介绍了一种新的评估基准MMIE，用于评估大型视觉语言模型（LVLMs）的多模态理解和生成能力。MMIE包含2万条精心设计的多模态查询，涵盖数学、编程、物理等多个领域，支持交错输入和输出。研究者还提出了一种可靠的自动化评估指标，利用经过人类标注数据微调的评分模型来减少偏差，提高评估准确性。实验结果表明，即使是最好的模型也有很大的改进空间，MMIE将推动交错LVLMs的发展。"
                }
            },
            "hash": "6dc4d9e284dd99ca",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09584",
            "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2410.09584",
            "abstract": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.",
            "score": 32,
            "issue_id": 107,
            "pub_date": "2024-10-12",
            "pub_date_ru": "12 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#rag"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VIF-RAG: Новый подход к обучению RAG-систем следованию инструкциям",
                    "desc": "VIF-RAG - это первый автоматизированный и масштабируемый конвейер для улучшения следования инструкциям в системах RAG. Авторы создали набор атомарных инструкций, правила их комбинирования и верификации, а также модели для перефразирования инструкций. На основе этого был сгенерирован большой датасет VIF-RAG-QA и создан бенчмарк FollowRAG для оценки RAG-систем. Эксперименты показали, что VIF-RAG значительно улучшает способность языковых моделей следовать инструкциям в различных сценариях."
                },
                "en": {
                    "title": "Enhancing RAG Systems with VIF-RAG: A New Era of Instruction-Following Alignment",
                    "desc": "The paper introduces VIF-RAG, a novel system designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems. It uses a minimal set of manually crafted instructions and combination rules to create complex instructions, which are then verified using a Python executor. The system generates a high-quality dataset, VIF-RAG-QA, and introduces the FollowRAG Benchmark to evaluate instruction-following capabilities across various categories. The results show that VIF-RAG significantly enhances the performance of Large Language Models in following instructions within RAG contexts."
                },
                "zh": {
                    "title": "提升RAG系统的指令遵循能力",
                    "desc": "这篇论文提出了一种名为VIF-RAG的新方法，用于提高检索增强生成（RAG）系统中的指令遵循能力。研究人员通过手动设计少量基础指令，并开发组合规则来生成复杂指令，从而创建了一个高质量的数据集。通过监督模型和Python执行器，他们实现了指令重写和质量验证的自动化。最终，VIF-RAG显著提升了大语言模型在多种指令约束下的表现。"
                }
            },
            "hash": "d7ccc55af0bc4ea5",
            "pub_date_card": {
                "ru": "12 октября",
                "en": "October 12",
                "zh": "10月12日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10306",
            "title": "Animate-X: Universal Character Image Animation with Enhanced Motion Representation",
            "url": "https://huggingface.co/papers/2410.10306",
            "abstract": "Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods.",
            "score": 23,
            "issue_id": 108,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Универсальная анимация персонажей с улучшенным представлением движений",
                    "desc": "Статья представляет Animate-X - универсальную систему анимации персонажей на основе LDM, включая антропоморфных. Авторы вводят Pose Indicator для улучшенного представления движений, используя как неявные, так и явные методы. Система использует визуальные признаки CLIP для извлечения сути движения из управляющего видео. Также представлен новый бенчмарк A^2Bench для оценки производительности на универсальных анимационных изображениях."
                },
                "en": {
                    "title": "Animate-X: Bringing Characters to Life with Universal Animation",
                    "desc": "The paper introduces Animate-X, a novel animation framework designed to generate high-quality videos from reference images and pose sequences, applicable to a wide range of character types, including anthropomorphic ones. Unlike existing methods that struggle with non-human figures, Animate-X uses a Pose Indicator to capture motion patterns from driving videos, enhancing motion representation. This is achieved through both implicit methods, using CLIP visual features, and explicit methods, simulating potential inputs to improve generalization. The framework's effectiveness is validated using a new benchmark, A^2Bench, showing superior performance over current state-of-the-art techniques."
                },
                "zh": {
                    "title": "Animate-X：适用于各种角色的通用动画框架",
                    "desc": "这篇论文介绍了一种名为Animate-X的动画框架，可以从参考图像和目标姿势序列生成高质量视频。与现有方法不同，Animate-X适用于各种角色类型，包括拟人化角色。为了增强动作表示，作者引入了姿势指示器，通过隐式和显式方式捕捉驱动视频的运动模式。此外，论文还提出了一个新的基准A^2Bench，用于评估Animate-X在动画图像上的表现。"
                }
            },
            "hash": "da7093c8b8de76a8",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10563",
            "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
            "url": "https://huggingface.co/papers/2410.10563",
            "abstract": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.",
            "score": 23,
            "issue_id": 108,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MEGA-Bench: Масштабная оценка мультимодальных моделей на реальных задачах",
                    "desc": "MEGA-Bench - это новый набор данных для оценки мультимодальных моделей, включающий более 500 реальных задач. Он охватывает широкий спектр форматов вывода и использует более 40 метрик для оценки. MEGA-Bench предоставляет подробный отчет о возможностях моделей по различным измерениям. Авторы провели оценку различных современных мультимодальных моделей на этом наборе данных."
                },
                "en": {
                    "title": "MEGA-Bench: Unleashing Multimodal Evaluation Power",
                    "desc": "MEGA-Bench is a comprehensive evaluation suite designed to test machine learning models on over 500 real-world multimodal tasks. It focuses on providing high-quality, diverse data samples to ensure accurate and cost-effective model evaluation. Unlike traditional benchmarks, MEGA-Bench supports a variety of output formats, such as numbers, phrases, and JSON, and uses over 40 metrics to assess model performance. This approach allows for a detailed analysis of model capabilities across different dimensions, offering users a deeper understanding of model strengths and weaknesses."
                },
                "zh": {
                    "title": "MEGA-Bench：多模态任务的全面评估",
                    "desc": "MEGA-Bench 是一个多模态评估套件，涵盖了超过 500 个真实世界任务，旨在优化高质量数据样本的多样性和丰富性。我们收集了 505 个任务和 8000 多个样本，支持多种输出格式，如数字、短语、代码等。为了评估这些任务，我们开发了 40 多种指标，提供细致的能力报告。通过 MEGA-Bench，我们可以深入了解视觉语言模型在不同维度上的能力。"
                }
            },
            "hash": "11a6963fbbeabd49",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10783",
            "title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
            "url": "https://huggingface.co/papers/2410.10783",
            "abstract": "The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.",
            "score": 22,
            "issue_id": 109,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "LiveXiv: Эволюционирующий бенчмарк для оценки мультимодальных моделей",
                    "desc": "LiveXiv - это масштабируемый развивающийся бенчмарк, основанный на научных статьях ArXiv. Он автоматически генерирует пары вопрос-ответ по визуальному контенту (VQA) из рукописей, используя графики, диаграммы и таблицы. Бенчмарк предлагает эффективный подход к оценке производительности моделей, снижая общие затраты на оценку. LiveXiv позволяет оценить истинные способности мультимодальных моделей, избегая загрязнения тестовых данных."
                },
                "en": {
                    "title": "LiveXiv: Unveiling True Model Capabilities with Dynamic Benchmarks",
                    "desc": "The paper introduces LiveXiv, a dynamic benchmark designed to evaluate multi-modal models using scientific papers from ArXiv, ensuring no test data contamination. It automatically generates visual question-answer pairs from the content of these papers, like graphs and tables, without human intervention. The authors propose an efficient evaluation method that reduces costs by assessing only a subset of models, yet accurately estimates performance across all models. The benchmark reveals the true capabilities of large multi-modal models, with minimal performance variance between automatic and manually verified results."
                },
                "zh": {
                    "title": "LiveXiv：科学论文驱动的动态基准测试",
                    "desc": "这篇论文介绍了一种名为LiveXiv的新方法，用于评估多模态模型的能力。通过从科学ArXiv论文中自动生成视觉问答对，LiveXiv避免了测试数据污染的问题。该方法不需要人工参与，利用论文中的图表和表格等多模态内容。研究表明，这种方法不仅降低了评估成本，还能准确反映模型的真实能力。"
                }
            },
            "hash": "14085b9b484efc2d",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07985",
            "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
            "url": "https://huggingface.co/papers/2410.07985",
            "abstract": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.",
            "score": 20,
            "issue_id": 107,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#math"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Новый вызов для ИИ: олимпиадная математика как тест на интеллект",
                    "desc": "Статья представляет новый эталонный набор данных для оценки математических способностей больших языковых моделей на уровне олимпиад. Набор данных содержит 4428 задач олимпиадного уровня с тщательной человеческой аннотацией, разделенных на 33 поддомена и 10 уровней сложности. Эксперименты показали, что даже самые продвинутые модели, такие как OpenAI o1-mini и OpenAI o1-preview, испытывают трудности с решением сложных олимпиадных задач, достигая точности 60.54% и 52.55% соответственно. Это исследование выявляет значительные проблемы в области математических рассуждений на олимпиадном уровне для современных языковых моделей."
                },
                "en": {
                    "title": "Pushing the Limits: Challenging LLMs with Olympiad-Level Math",
                    "desc": "The paper introduces a new benchmark designed to test large language models (LLMs) on Olympiad-level mathematical reasoning, as existing benchmarks are no longer challenging enough. This new dataset includes 4428 competition-level problems, categorized into over 33 sub-domains and 10 difficulty levels, providing a comprehensive assessment of LLMs' capabilities. The study reveals that even advanced models like OpenAI o1-mini and OpenAI o1-preview struggle with these problems, achieving only 60.54% and 52.55% accuracy, respectively. This highlights the need for further advancements in LLMs to tackle complex mathematical reasoning tasks."
                },
                "zh": {
                    "title": "挑战大型语言模型的奥林匹克数学推理能力",
                    "desc": "近年来，大型语言模型在数学推理能力上取得了显著突破。然而，现有的基准测试如GSM8K或MATH已经被高精度解决，显示出它们对这些模型的挑战性不足。为此，我们提出了一个专门设计的综合性挑战基准，用于评估大型语言模型在奥林匹克水平的数学推理能力。实验结果表明，即使是最先进的模型在面对奥林匹克级别的数学问题时仍然面临巨大挑战。"
                }
            },
            "hash": "3d68a72c482a94bf",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10774",
            "title": "Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention",
            "url": "https://huggingface.co/papers/2410.10774",
            "abstract": "In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/",
            "score": 18,
            "issue_id": 108,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Cavia: Революция в генерации видео с контролем камеры",
                    "desc": "Cavia - это новая система для генерации видео с контролируемой камерой и возможностью создания нескольких ракурсов. Она расширяет модули пространственного и временного внимания, улучшая согласованность ракурсов и временную согласованность. Cavia позволяет пользователю точно указывать движение камеры, одновременно получая движение объектов. Эксперименты показывают, что Cavia превосходит современные методы по геометрической согласованности и визуальному качеству."
                },
                "en": {
                    "title": "Cavia: Mastering Camera Control in Image-to-Video Generation",
                    "desc": "The paper introduces Cavia, a new framework for generating videos from images with improved 3D consistency and camera control. Cavia enhances video generation by using view-integrated attention modules, which ensure both viewpoint and temporal consistency across frames. This approach allows for precise camera motion control and can be trained with a variety of data sources, including static, synthetic, and real-world videos. Experiments show that Cavia outperforms existing methods in maintaining geometric consistency and high perceptual quality in the generated videos."
                },
                "zh": {
                    "title": "Cavia：实现相机可控的多视角视频生成",
                    "desc": "近年来，图像到视频生成技术取得了显著突破，但生成帧的三维一致性和相机可控性仍未解决。Cavia是一个新框架，能够将输入图像转换为多个时空一致的视频，并允许用户精确控制相机运动。该框架通过将空间和时间注意力模块扩展为视图集成注意力模块，提高了视点和时间一致性。实验表明，Cavia在几何一致性和感知质量方面超越了现有方法。"
                }
            },
            "hash": "95f9847ff43aeaf2",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10792",
            "title": "Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations",
            "url": "https://huggingface.co/papers/2410.10792",
            "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.",
            "score": 11,
            "issue_id": 109,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#cv",
                    "#generative_models"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективная инверсия и редактирование изображений с помощью выпрямленных потоков",
                    "desc": "Статья представляет новый метод инверсии и редактирования изображений с использованием стохастических эквивалентов моделей выпрямленного потока (Rectified Flow). Авторы предлагают инверсию RF с помощью динамического оптимального управления, полученного через линейный квадратичный регулятор. Метод обеспечивает высокую производительность в задачах инверсии и редактирования изображений без дополнительного обучения. Проведенные крупномасштабные оценки подтверждают преимущество метода перед существующими подходами."
                },
                "en": {
                    "title": "Revolutionizing Image Inversion and Editing with Rectified Flows",
                    "desc": "This paper explores how to reverse the process of generating images from noise, using a method called Rectified Flows (RFs), which is less explored compared to popular Diffusion Models (DMs). The authors propose a new way to invert images back to noise using a technique called dynamic optimal control, which is more efficient than current methods that require extra training or complex optimization. They also introduce a new tool for editing images, which allows for better performance in tasks like turning sketches into images or changing parts of an image. The results show that their method is preferred by users, offering a promising alternative to existing techniques."
                },
                "zh": {
                    "title": "逆转生成模型：从图像到噪声的创新之路",
                    "desc": "这篇论文研究了如何将生成模型生成的图像逆转回结构化的噪声，以便进行恢复和编辑。作者提出了一种基于动态最优控制的逆转方法，使用线性二次调节器来实现。该方法在零样本逆转和编辑任务中表现出色，优于现有的方法。大规模的人类评估显示，用户更偏好这种新方法。"
                }
            },
            "hash": "064d3a6fd8a2ab06",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10818",
            "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models",
            "url": "https://huggingface.co/papers/2410.10818",
            "abstract": "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.",
            "score": 10,
            "issue_id": 108,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "TemporalBench: Новый стандарт для оценки понимания временной динамики в видео",
                    "desc": "TemporalBench - это новый бенчмарк для оценки детального понимания временной динамики в видео. Он состоит из примерно 10 тысяч пар вопрос-ответ по видео, основанных на высококачественных аннотациях временной динамики в видеоклипах. Бенчмарк позволяет оценивать различные аспекты временного понимания, такие как частота действий, величина движения, порядок событий, а также применим к различным задачам и моделям. Результаты показывают значительный разрыв между людьми и ИИ в понимании временной динамики видео."
                },
                "en": {
                    "title": "TemporalBench: Bridging the Gap in Video Temporal Understanding",
                    "desc": "The paper introduces TemporalBench, a new benchmark designed to evaluate fine-grained temporal understanding in videos, addressing the limitations of existing video benchmarks that lack detailed temporal annotations. TemporalBench includes around 10,000 video question-answer pairs based on high-quality human annotations, providing a unique platform for assessing temporal reasoning abilities like action frequency and event order. The benchmark reveals a significant gap in temporal understanding between humans and AI, with state-of-the-art models achieving only 38.5% accuracy. The authors propose a new evaluation metric, Multiple Binary Accuracy (MBA), to address biases in multi-choice question answering, aiming to enhance research in temporal reasoning capabilities of AI models."
                },
                "zh": {
                    "title": "提升视频时间理解的新基准：TemporalBench",
                    "desc": "这篇论文介绍了一个名为TemporalBench的新基准，用于评估视频中细粒度的时间理解能力。TemporalBench包含约1万个视频问答对，基于约2000个高质量的人类注释，详细描述了视频片段中的时间动态。研究表明，现有的顶尖模型在TemporalBench上的问答准确率仅为38.5%，显示出人类与AI在时间理解上的显著差距。论文还提出了一种新的评估方法，称为多重二元准确率（MBA），以纠正多选题中的偏差。"
                }
            },
            "hash": "96570f7d74bf91e1",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10594",
            "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
            "url": "https://huggingface.co/papers/2410.10594",
            "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25--39\\% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag .",
            "score": 8,
            "issue_id": 118,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#rag"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "VisRAG: Мультимодальный RAG для работы с документами как с изображениями",
                    "desc": "VisRAG - это новый подход к извлечению и генерации информации (RAG), основанный на использовании мультимодальных моделей (VLM). В отличие от традиционных текстовых RAG-систем, VisRAG работает напрямую с изображениями документов, сохраняя всю визуальную информацию. Эксперименты показали, что VisRAG превосходит текстовые RAG-системы на 25-39% при работе с мультимодальными документами. Авторы предоставили код и данные в открытом доступе для дальнейших исследований."
                },
                "en": {
                    "title": "VisRAG: Bridging Text and Vision for Smarter Document Understanding",
                    "desc": "The paper introduces VisRAG, a novel approach that enhances retrieval-augmented generation by incorporating visual information into the process. Unlike traditional RAG systems that rely solely on text, VisRAG uses a vision-language model to embed documents as images, preserving more information from the original source. This method significantly improves the performance of both retrieval and generation tasks, showing a 25-39% increase over text-based RAG systems. The study highlights VisRAG's strong generalization capabilities and its potential as a robust solution for handling multi-modality documents."
                },
                "zh": {
                    "title": "VisRAG：多模态文档处理的新突破",
                    "desc": "这篇论文介绍了一种名为VisRAG的新技术，它通过视觉-语言模型（VLM）来增强检索增强生成（RAG）系统的能力。VisRAG直接将文档作为图像嵌入，而不是先解析成文本，从而最大限度地保留和利用原始文档中的信息。实验表明，VisRAG在检索和生成阶段的表现优于传统的基于文本的RAG系统，性能提升达25%到39%。这种方法在多模态文档处理中展示了强大的泛化能力，是一种有前景的解决方案。"
                }
            },
            "hash": "21d912d0f36db525",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09335",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
            "url": "https://huggingface.co/papers/2410.09335",
            "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.",
            "score": 8,
            "issue_id": 107,
            "pub_date": "2024-10-12",
            "pub_date_ru": "12 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективный отбор данных для обучения больших языковых моделей",
                    "desc": "Статья посвящена проблеме выбора данных для обучения больших языковых моделей (LLM) с помощью контролируемой доводки (SFT). Авторы провели репликацию нескольких методов самооценки на наборах данных из двух миллионов примеров и обнаружили, что большинство методов не превосходят случайный выбор. Исследование показало, что разнообразие данных важнее их качества при SFT. Авторы также предложили простой, но эффективный метод фильтрации данных по длине токенов, который особенно полезен для более слабых базовых моделей."
                },
                "en": {
                    "title": "Diversity Over Quality: Rethinking Data Selection for LLM Fine-Tuning",
                    "desc": "The paper explores the challenges of selecting a representative subset of data for supervised fine-tuning of Large Language Models (LLMs). It finds that most existing data selection methods, which work well on small datasets, struggle with large-scale datasets, often performing no better than random selection. The study highlights the importance of diversity in data selection over merely focusing on high-quality data. Additionally, it suggests that filtering data by token length can enhance the performance of weaker models during fine-tuning."
                },
                "zh": {
                    "title": "多样性胜于质量：大规模数据集中的监督微调策略",
                    "desc": "这篇论文探讨了监督微调（SFT）在大语言模型（LLM）中的重要性，尤其是在选择训练数据子集时。研究发现，现有的数据选择方法在大规模数据集上表现不佳，随机选择反而效果更好。论文指出，在SFT中，数据选择的多样性比单纯追求高质量数据更为重要。通过分析，作者发现通过筛选数据的词元长度可以稳定提高结果，尤其对较弱的基础模型如Llama3效果显著。"
                }
            },
            "hash": "b683d10c4851f00f",
            "pub_date_card": {
                "ru": "12 октября",
                "en": "October 12",
                "zh": "10月12日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06634",
            "title": "Tree of Problems: Improving structured problem solving with compositionality",
            "url": "https://huggingface.co/papers/2410.06634",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.",
            "score": 5,
            "issue_id": 109,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 октября",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "🌳",
                "ru": {
                    "title": "Древо проблем: новый метод для решения сложных задач с помощью ИИ",
                    "desc": "В статье предлагается новый метод под названием Tree of Problems (ToP) для решения сложных задач с помощью больших языковых моделей. ToP является упрощенной версией метода Tree of Thoughts (ToT) и предназначен для задач, которые можно разделить на идентичные подзадачи. Авторы провели эмпирические исследования, показавшие превосходство ToP над методами ToT и Graph of Thoughts (GoT). Кроме того, ToP показал лучшие результаты, чем Chain-of-Thought (CoT) промптинг на сложных задачах рассуждения."
                },
                "en": {
                    "title": "Simplifying Complexity: Tree of Problems Revolutionizes Reasoning",
                    "desc": "The paper introduces a new method called Tree of Problems (ToP) to improve the performance of Large Language Models (LLMs) on complex reasoning tasks. ToP simplifies the Tree of Thoughts (ToT) approach by breaking down complex problems into identical subtasks, which enhances the model's ability to solve them. The authors demonstrate that ToP outperforms both ToT and Graph of Thoughts (GoT), as well as Chain-of-Thought (CoT) prompting, in handling intricate reasoning challenges. This advancement suggests a promising direction for enhancing LLMs' problem-solving capabilities."
                },
                "zh": {
                    "title": "问题树：简化复杂任务的有效方法",
                    "desc": "大型语言模型在多种任务中表现出色，尤其是在上下文学习中。对于需要逐步推理的复杂任务，链式思维提示结合自我一致性取得了显著效果。然而，一些任务对大型语言模型来说仍然很难解决。本文提出了问题树（ToP），一种更简单的思维树版本，实验结果表明其在复杂推理任务中优于思维树和思维图。"
                }
            },
            "hash": "0f052b591f948ce8",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10803",
            "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
            "url": "https://huggingface.co/papers/2410.10803",
            "abstract": "Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io",
            "score": 5,
            "issue_id": 108,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#3d",
                    "#rl",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в автономной манипуляции гуманоидных роботов",
                    "desc": "Статья представляет новый подход к автономной манипуляции гуманоидных роботов - Improved 3D Diffusion Policy (iDP3). Этот метод использует эгоцентрические 3D визуальные представления, что позволяет избежать ограничений, связанных с калибровкой камеры и сегментацией облака точек. iDP3 демонстрирует способность полноразмерного гуманоидного робота автономно выполнять задачи в различных реальных сценариях, используя только данные, собранные в лабораторных условиях. Это значительный шаг вперед в области автономной манипуляции гуманоидных роботов в разнообразных средах."
                },
                "en": {
                    "title": "Breaking Barriers: Humanoids in the Real World",
                    "desc": "The paper introduces the Improved 3D Diffusion Policy (iDP3), a new approach in 3D visuomotor policies for humanoid robots. iDP3 overcomes the limitations of previous methods by using egocentric 3D visual representations, eliminating the need for camera calibration and point-cloud segmentation. This advancement allows humanoid robots to autonomously perform tasks in various real-world environments using only lab-collected data. The research demonstrates significant progress in enabling humanoid robots to operate independently in diverse settings."
                },
                "zh": {
                    "title": "突破限制：类人机器人自主操作的新策略",
                    "desc": "这篇论文介绍了一种新的3D视觉运动策略，称为改进的3D扩散策略（iDP3），它可以让类人机器人在多种真实场景中自主操作。传统的3D视觉运动策略依赖于相机校准和点云分割，这对移动机器人来说是个挑战。iDP3通过利用自我中心的3D视觉表示，消除了这些限制。实验表明，iDP3使得全尺寸类人机器人仅凭实验室收集的数据就能在多样化的环境中执行技能。"
                }
            },
            "hash": "753b160b8fdc53e0",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10813",
            "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
            "url": "https://huggingface.co/papers/2410.10813",
            "abstract": "Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.",
            "score": 4,
            "issue_id": 109,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LongMemEval: Новый стандарт оценки долговременной памяти чат-ассистентов",
                    "desc": "Статья представляет LongMemEval - комплексный бенчмарк для оценки долгосрочной памяти чат-ассистентов на основе больших языковых моделей (LLM). Авторы выделяют пять ключевых способностей долгосрочной памяти и создают набор из 500 вопросов для их тестирования. Исследование показывает значительное снижение точности существующих систем при длительном взаимодействии. Предлагается унифицированная структура для улучшения долгосрочной памяти, включающая оптимизации индексирования, извлечения и чтения информации."
                },
                "en": {
                    "title": "Enhancing Chat Assistants with Long-Term Memory Mastery",
                    "desc": "The paper introduces LongMemEval, a benchmark to test the long-term memory abilities of chat assistants, focusing on five key areas like information extraction and temporal reasoning. It highlights the challenges faced by current systems, which show a significant drop in accuracy when dealing with sustained interactions. The authors propose a framework with innovative memory designs, such as session decomposition and time-aware query expansion, to enhance memory recall and question answering. These improvements aim to advance the personalization and reliability of conversational AI systems."
                },
                "zh": {
                    "title": "提升聊天助手的长期记忆能力",
                    "desc": "这篇论文介绍了一种名为LongMemEval的基准，用于评估聊天助手的五种核心长期记忆能力。研究发现，现有的商业聊天助手在长时间交互中记忆信息的准确率下降了30%。作者提出了一种统一框架，通过优化索引、检索和读取阶段来提升记忆性能。实验结果表明，这些优化显著提高了记忆召回率和问题回答能力。"
                }
            },
            "hash": "b100ebbfd8b25c2e",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07752",
            "title": "TVBench: Redesigning Video-Language Evaluation",
            "url": "https://huggingface.co/papers/2410.07752",
            "abstract": "Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.",
            "score": 4,
            "issue_id": 107,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый взгляд на оценку видео-языковых моделей: важность временного анализа",
                    "desc": "В статье рассматриваются проблемы существующих бенчмарков для оценки моделей видео-языкового понимания. Авторы выявили, что многие задачи можно решить без глубокого анализа временной информации в видео. Предложен новый бенчмарк TVBench, требующий высокого уровня понимания временных аспектов. Результаты показывают, что большинство современных видео-языковых моделей не справляются с TVBench, демонстрируя результаты на уровне случайного угадывания."
                },
                "en": {
                    "title": "Rethinking Video Understanding: Beyond Static Frames",
                    "desc": "The paper discusses the limitations of current video-language benchmarks, which often don't require true temporal reasoning to solve. It identifies three main issues: tasks can be solved with static frame information, overly informative text, and reliance on world knowledge. The authors propose a new benchmark, TVBench, which demands genuine temporal understanding for video question-answering. They find that most state-of-the-art models perform poorly on TVBench, highlighting the need for improved temporal reasoning in video models."
                },
                "zh": {
                    "title": "提升视频理解：从静态到动态的挑战",
                    "desc": "这篇论文探讨了视频语言模型的评估挑战，指出现有的基准测试存在问题。研究发现，许多任务可以通过单帧静态信息或文本信息解决，而不需要真正的视觉推理。为此，作者提出了一个新的基准测试TVBench，强调时间理解的重要性。结果显示，许多先进的模型在TVBench上的表现与随机水平相当，只有Gemini-Pro和Tarsier表现优异。"
                }
            },
            "hash": "dd1e54c62f09078e",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09733",
            "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.09733",
            "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/",
            "score": 3,
            "issue_id": 116,
            "pub_date": "2024-10-13",
            "pub_date_ru": "13 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "MMCOMPOSITION: новый взгляд на композиционность мультимодальных моделей",
                    "desc": "MMCOMPOSITION - это новый бенчмарк для оценки композиционности крупных мультимодальных языковых моделей (VLM). Он позволяет более точно оценить способность моделей понимать и создавать новые комбинации известных визуальных и текстовых компонентов. Бенчмарк включает в себя тестирование глубокого рассуждения о взаимодействии объектов, подсчете и сложных композициях. Анализ с помощью MMCOMPOSITION выявил ограничения существующих VLM в точном композиционном восприятии и рассуждении."
                },
                "en": {
                    "title": "Unlocking the Compositional Power of Vision-Language Models",
                    "desc": "The paper introduces MMCOMPOSITION, a new benchmark designed to evaluate the compositionality of Vision-Language Models (VLMs), which is their ability to understand and generate new combinations of visual and textual elements. This benchmark addresses the shortcomings of previous evaluations by focusing on deeper reasoning tasks such as object interactions and complex compositions. The study reveals that some open-source models outperform GPT-4o in compositionality, highlighting areas where VLMs need improvement. The findings suggest that enhancing compositional perception and reasoning is crucial for advancing VLM capabilities."
                },
                "zh": {
                    "title": "探索视觉语言模型的组合能力",
                    "desc": "大型视觉语言模型（VLMs）在多模态理解上取得了显著进展，但其组合能力仍需深入研究。组合能力是指理解和生成已知视觉和文本元素的新组合的能力。为此，研究者提出了一个名为MMCOMPOSITION的新基准，用于全面评估VLMs的组合能力。实验结果显示，GPT-4o的组合能力不如某些开源模型，这揭示了VLMs在细粒度组合感知和推理上的局限性。"
                }
            },
            "hash": "fb02158c865832bc",
            "pub_date_card": {
                "ru": "13 октября",
                "en": "October 13",
                "zh": "10月13日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09223",
            "title": "The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling",
            "url": "https://huggingface.co/papers/2410.09223",
            "abstract": "We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.",
            "score": 1,
            "issue_id": 119,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 октября",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальность и специфичность языковых структур в LLM",
                    "desc": "Исследователи применили методы механистической интерпретации для анализа внутренней структуры больших языковых моделей (LLM) и их соответствия лингвистическим структурам языков обучения. Они изучали, используют ли LLM общие внутренние схемы для обработки схожих морфосинтаксических процессов в разных языках, и различные схемы для обработки различающихся процессов. Анализ английских и китайских моно- и мультиязычных моделей показал, что модели используют одинаковые схемы для одинаковых синтаксических процессов независимо от языка, даже в случае независимо обученных моноязычных моделей. Кроме того, мультиязычные модели используют специфичные для языка компоненты при необходимости обработки лингвистических процессов, существующих только в некоторых языках."
                },
                "en": {
                    "title": "Unveiling the Linguistic Brain of AI: Shared Circuits and Unique Paths",
                    "desc": "This paper explores how large language models (LLMs) internally process linguistic structures from different languages. It investigates whether LLMs use the same internal mechanisms for similar morphosyntactic processes across languages and different mechanisms for distinct processes. The study uses English and Chinese models to show that LLMs often use the same circuitry for similar syntactic tasks, even in monolingual models. Additionally, it finds that multilingual models can adapt by using language-specific components for unique linguistic features."
                },
                "zh": {
                    "title": "探索LLM内部结构与语言结构的对应关系",
                    "desc": "这篇论文研究了大型语言模型（LLM）的内部结构是否与其训练语言的语言结构相对应。研究发现，当两种语言使用相同的形态句法过程时，模型会使用相同的内部电路来处理，即使是完全独立训练的单语模型也是如此。此外，多语言模型在处理某些语言特有的语言过程时，会使用特定的语言组件。研究结果揭示了LLM在同时建模多种语言时，如何在利用共同结构和保留语言差异之间进行权衡。"
                }
            },
            "hash": "025fd1d188bbd7f3",
            "pub_date_card": {
                "ru": "11 октября",
                "en": "October 11",
                "zh": "10月11日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10630",
            "title": "Thinking LLMs: General Instruction Following with Thought Generation",
            "url": "https://huggingface.co/papers/2410.10630",
            "abstract": "LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.",
            "score": 1,
            "issue_id": 118,
            "pub_date": "2024-10-14",
            "pub_date_ru": "14 октября",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение ИИ думать перед ответом",
                    "desc": "Статья представляет новый метод обучения языковых моделей (LLM) способности к явному мышлению перед ответом на вопросы. Авторы предлагают итеративную процедуру поиска и оптимизации, которая исследует пространство возможных генераций мыслей без использования дополнительных человеческих данных. Метод использует модель-судью для оценки ответов и оптимизацию на основе предпочтений. Результаты показывают улучшение производительности на наборах данных AlpacaEval и Arena-Hard, а также преимущества мышления в различных категориях задач."
                },
                "en": {
                    "title": "Teaching LLMs to Think Before They Speak",
                    "desc": "The paper introduces a new training method for large language models (LLMs) to enhance their ability to think explicitly before answering questions. This method involves an iterative search and optimization process that allows the model to explore and learn from different thought processes without needing additional human data. The approach uses a judge model to score thought candidates and optimizes them through preference optimization, leading to improved performance across various tasks. The results show that this thinking ability enhances the model's performance not only in reasoning tasks but also in areas like marketing and health."
                },
                "zh": {
                    "title": "让AI学会思考：无需人类数据的智能训练方法",
                    "desc": "这篇论文提出了一种新的训练方法，使得大型语言模型（LLM）在回答问题前能够进行明确的思考。通过迭代搜索和优化过程，模型可以在没有额外人类数据的情况下学习如何思考。每个指令的思考候选项通过一个评判模型进行评分，并通过偏好优化进行优化。实验结果表明，这种方法在复杂推理任务和非推理任务（如市场营销、健康和常识）上都表现出色。"
                }
            },
            "hash": "ce8e4d06cae69001",
            "pub_date_card": {
                "ru": "14 октября",
                "en": "October 14",
                "zh": "10月14日"
            }
        }
    ],
    "zh": {
        "text": "这篇文章介绍了一种新的多模态学习评估方法。现有的评估标准存在数据规模、范围和评估深度的限制，且评估指标往往耗时或有偏差。为解决这些问题，作者提出了MMIE，一个大规模的知识密集型基准，用于评估大型视觉语言模型（LVLMs）的多模态理解和生成能力。MMIE包含20K个精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域。此外，作者还提出了一种可靠的自动化评估指标，以减少偏差并提高评估准确性。实验结果表明，MMIE在综合评估多模态LVLMs方面非常有效。",
        "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
        "pinyin": "zhè piān wén zhāng jiè shào le yī zhǒng xīn de duō mó tài xué xí guāng fāng fǎ. xiàn yǒu de guāng fāng biǎo zhǔn cún zài shù jù guī mó, fàn wéi hé guāng fāng shén dù de xiàn zhì, ér guāng fāng zhǐ biǎo wǎng wǎng hào shí huò yǒu piàn chā. wèi jiě jué zhè xiē wèn ti, zuò zhě tí chū le MMIE, yī gè dà guī mó de zhī shì mì duō de bǐ zhǔn, yòng yú guāng fāng dà xíng shì jué yǔ yǔ yán mó xíng (LVLMs) de duō mó tài xué hé shēng chéng néng lì. MMIE bāo hán 20K gè jīng xīn cè huà de duō mó tài xué, hán gài 3 gè lèi bié, 12 gè lǐng yù hé 102 gè zǐ lǐng yù. cǐ wài, zuò zhě hái tí chū le yī zhǒng kě kào de zì dòng huà guāng fāng zhǐ biǎo, yǐ jiǎn shǎo piàn chā ér tí gāo guāng fāng zhǔn què xìng. shí yàn jié guǒ biǎo míng, MMIE zài zōng hé guāng fāng duō mó tài LVLMs fāng miàn fēi cháng yǒu xiào.",
        "vocab": "[{'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluation'}, {'word': '数据规模', 'pinyin': 'shù jù guī mó', 'trans': 'data scale'}, {'word': '范围', 'pinyin': 'fàn wéi', 'trans': 'scope'}, {'word': '评估深度', 'pinyin': 'píng gū shēn dù', 'trans': 'evaluation depth'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limitations'}, {'word': '耗时', 'pinyin': 'hào shí', 'trans': 'time-consuming'}, {'word': '偏差', 'pinyin': 'piān chā', 'trans': 'bias'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': 'MMIE', 'pinyin': 'MMIE', 'trans': 'MMIE'}, {'word': '大规模', 'pinyin': 'dà guī mó', 'trans': 'large-scale'}, {'word': '知识密集型', 'pinyin': 'zhī shì mì jí xíng', 'trans': 'knowledge-intensive'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '视觉语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '多模态理解', 'pinyin': 'duō mó shuài lǐ jiě', 'trans': 'multimodal understanding'}, {'word': '生成能力', 'pinyin': 'shēng chéng néng lì', 'trans': 'generative capability'}, {'word': '精心策划', 'pinyin': 'jīng xīn cè huà', 'trans': 'carefully designed'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'categories'}, {'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'domains'}, {'word': '子领域', 'pinyin': 'zǐ lǐng yù', 'trans': 'sub-domains'}, {'word': '自动化', 'pinyin': 'zì dòng huà', 'trans': 'automated'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '综合', 'pinyin': 'zōng hé', 'trans': 'comprehensive'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}]",
        "update_ts": "2024-10-15 09:48"
    },
    "weekday": 0,
    "link_prev": "2024-10-11.html",
    "link_next": "2024-10-15.html",
    "date_en": "14 October",
    "date_prev": "11 октября",
    "date_next": "15 октября",
    "short_date_prev": {
        "ru": "11.10",
        "en": "10/11",
        "zh": "10月11日"
    },
    "short_date_next": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10月15日"
    },
    "categories": {
        "#dataset": 9,
        "#data": 1,
        "#benchmark": 13,
        "#agents": 0,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 2,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 10,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 0,
        "#medicine": 0,
        "#training": 0,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#generative_models": 0
    }
}