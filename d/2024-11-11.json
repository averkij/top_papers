{
    "date": {
        "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 11",
        "zh": "11æœˆ11æ—¥"
    },
    "time_utc": "2024-11-11 09:00",
    "weekday": 0,
    "issue_id": 519,
    "home_page_url": "https://huggingface.co/papers?date=2024-11-11",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.04997",
            "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
            "url": "https://huggingface.co/papers/2411.04997",
            "abstract": "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.",
            "score": 31,
            "issue_id": 513,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "5e5b851688791e8a",
            "authors": [
                "Weiquan Huang",
                "Aoqi Wu",
                "Yifan Yang",
                "Xufang Luo",
                "Yuqing Yang",
                "Liang Hu",
                "Qi Dai",
                "Xiyang Dai",
                "Dongdong Chen",
                "Chong Luo",
                "Lili Qiu"
            ],
            "affiliations": [
                "Tongji University",
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04997.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "LLM2CLIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ LLM Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ CLIP. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° CLIP. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° CLIP."
                },
                "en": {
                    "title": "Unlocking CLIP's Potential with LLMs",
                    "desc": "This paper introduces LLM2CLIP, a new method that enhances the CLIP model by integrating large language models (LLMs) like GPT-4. By fine-tuning the LLM in the caption space using contrastive learning, the model improves its ability to understand and generate complex image captions. The LLM acts as a teacher for CLIP's visual encoder, allowing it to process longer and more intricate texts than the original CLIP could handle. The results show significant advancements in cross-modal tasks, demonstrating the effectiveness of combining LLMs with multimodal representation learning."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡CLIPçš„å¤šæ¨¡æ€å­¦ä¹ èƒ½åŠ›",
                    "desc": "CLIPæ˜¯ä¸€ä¸ªé‡è¦çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LLM2CLIPï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¢å¼ºCLIPçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹LLMè¿›è¡Œå¾®è°ƒå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹ ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå–å…¶æ–‡æœ¬èƒ½åŠ›ï¼Œä»è€Œæ˜¾è‘—æé«˜CLIPåœ¨å¤„ç†å›¾åƒæ ‡é¢˜æ—¶çš„è¡¨ç°ã€‚LLMçš„å¼ºå¤§æ–‡æœ¬ç†è§£èƒ½åŠ›ä½¿å¾—CLIPèƒ½å¤Ÿå¤„ç†æ›´é•¿å’Œæ›´å¤æ‚çš„æ–‡æœ¬ï¼Œå…‹æœäº†ä¼ ç»ŸCLIPçš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04282",
            "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding",
            "url": "https://huggingface.co/papers/2411.04282",
            "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.",
            "score": 25,
            "issue_id": 518,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "e566070395107bc0",
            "authors": [
                "Haolin Chen",
                "Yihao Feng",
                "Zuxin Liu",
                "Weiran Yao",
                "Akshara Prabhakar",
                "Shelby Heinecke",
                "Ricky Ho",
                "Phil Mui",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04282.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LaTRO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LaTRO Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GSM8K Ğ¸ ARC-Challenge Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking Reasoning Potential in Large Language Models with LaTRO",
                    "desc": "This paper presents LaTent Reasoning Optimization (LaTRO), a new framework designed to enhance the reasoning abilities of large language models (LLMs) during training. LaTRO treats reasoning as a process of sampling from a latent distribution and uses variational methods to optimize this process. The framework allows LLMs to improve their reasoning skills and assess the quality of their reasoning simultaneously, without needing external feedback. Experimental results show that LaTRO significantly boosts the performance of LLMs on reasoning tasks, indicating that pre-trained models have untapped reasoning potential that can be further developed through this method."
                },
                "zh": {
                    "title": "è§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºLaTentæ¨ç†ä¼˜åŒ–ï¼ˆLaTROï¼‰ï¼Œå®ƒé€šè¿‡å˜åˆ†æ–¹æ³•å°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä»æ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ã€‚LaTROå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè¯„ä¼°æ¨ç†è´¨é‡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaTROæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨GSM8Kå’ŒARC-Challengeæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜äº†é¢„è®­ç»ƒLLMsçš„æ½œåœ¨æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ³•å¾—åˆ°å¢å¼ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05288",
            "title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
            "url": "https://huggingface.co/papers/2411.05288",
            "abstract": "Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .",
            "score": 18,
            "issue_id": 509,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "19accdb712f507d9",
            "authors": [
                "Man Tsung Yeung",
                "Penghui Qi",
                "Min Lin",
                "Xinyi Wan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05288.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾ĞµĞ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Balancing Memory and Computation in Pipeline Parallelism for Language Models",
                    "desc": "This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è¯æ±‡å±‚ä»¥å¹³è¡¡è®¡ç®—ä¸å†…å­˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¯æ±‡å±‚å¯¼è‡´çš„è®¡ç®—å’Œå†…å­˜ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†è¯æ±‡å±‚å‡åŒ€åˆ†é…åˆ°ç®¡é“è®¾å¤‡ä¸Šï¼Œå¹¶å°†è®¡ç®—åˆ†ç»„åˆ°ç®¡é“ä¼ é€’ä¸­ã€‚ä¸ºäº†å‡å°‘æ¿€æ´»å†…å­˜å¼€é”€ï¼Œæˆ‘ä»¬è®¾è®¡äº†å‡ ç§ç®—æ³•æ¥é™ä½è¯æ±‡å±‚å†…çš„é€šä¿¡éšœç¢ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—å’Œå‚æ•°å†…å­˜ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œå¹¶åœ¨å¤§è¯æ±‡åœºæ™¯ä¸‹æ˜¾è‘—é™ä½äº†å³°å€¼å†…å­˜ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 13,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "b23d3650ace21f86",
            "authors": [
                "Yuze He",
                "Yanning Zhou",
                "Wang Zhao",
                "Zhongkai Wu",
                "Kaiwen Xiao",
                "Wei Yang",
                "Yong-Jin Liu",
                "Xiao Han"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "Tsinghua University",
                "Beihang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05738.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "StdGEN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "StdGEN - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ StdGEN Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Semantic-aware Large Reconstruction Model (S-LRM), Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ†Ğ²ĞµÑ‚ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ StdGEN Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGENï¼šé«˜æ•ˆç”Ÿæˆå¯åˆ†è§£3Dè§’è‰²çš„åˆ›æ–°ç®¡é“",
                    "desc": "StdGENæ˜¯ä¸€ç§åˆ›æ–°çš„ç®¡é“ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒStdGENåœ¨å¯åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆç»†è‡´çš„3Dè§’è‰²ï¼Œä¸”å„ä¸ªè¯­ä¹‰ç»„ä»¶å¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘åˆ†ç¦»ã€‚å…¶æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚é€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼ŒStdGENå®ç°äº†é«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ç”Ÿæˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 9,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 4",
                "zh": "11æœˆ4æ—¥"
            },
            "hash": "38beaabd86eeaa88",
            "authors": [
                "AndrÃ© Storhaug",
                "Jingyue Li"
            ],
            "affiliations": [
                "Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.02462.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ PEFT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA, (IA)^3 Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ PEFT Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° LoRA Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šæå‡å•å…ƒæµ‹è¯•ç”Ÿæˆçš„ç»æµæ€§ä¸æœ‰æ•ˆæ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒè™½ç„¶æœ‰æ•ˆï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼ŒPEFTæ–¹æ³•é€šè¿‡åªå¾®è°ƒéƒ¨åˆ†å‚æ•°æ¥é™ä½è®¡ç®—å¼€é”€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPEFTæ–¹æ³•åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æç¤ºå¾®è°ƒåœ¨æˆæœ¬å’Œèµ„æºåˆ©ç”¨ä¸Šæœ€ä¸ºæœ‰æ•ˆã€‚è®ºæ–‡è¿˜æ¯”è¾ƒäº†ä¸åŒæ¨¡å‹æ¶æ„å’Œå¤§å°ä¸‹çš„å¤šç§PEFTæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04425",
            "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
            "url": "https://huggingface.co/papers/2411.04425",
            "abstract": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.",
            "score": 8,
            "issue_id": 509,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "397d7c5c26dfad0f",
            "authors": [
                "Ishika Agarwal",
                "Krishnateja Killamsetty",
                "Lucian Popa",
                "Marina Danilevksy"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "IBM Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04425.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DELIFT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). DELIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°: Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DELIFT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° 70% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimize Data, Maximize Performance with DELIFT!",
                    "desc": "This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¾®è°ƒï¼šDELIFTç®—æ³•çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDELIFTçš„æ–°ç®—æ³•ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘å†—ä½™å’Œæ— æ•ˆæ•°æ®çš„ä½¿ç”¨ã€‚DELIFTé€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œç³»ç»Ÿæ€§åœ°æ”¹è¿›äº†ä¸‰ä¸ªå…³é”®çš„å¾®è°ƒé˜¶æ®µï¼šæŒ‡ä»¤å¾®è°ƒã€ä»»åŠ¡ç‰¹å®šå¾®è°ƒå’ŒæŒç»­å¾®è°ƒã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æˆå¯¹æ•ˆç”¨åº¦é‡ï¼Œé‡åŒ–æ•°æ®æ ·æœ¬å¯¹æ¨¡å‹å“åº”å…¶ä»–æ ·æœ¬çš„æ”¹å–„ç¨‹åº¦ï¼Œä»è€Œæœ‰æ•ˆè¯„ä¼°ä¿¡æ¯ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDELIFTèƒ½å¤Ÿåœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¾®è°ƒæ•°æ®é‡å‡å°‘å¤šè¾¾70%ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04954",
            "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM",
            "url": "https://huggingface.co/papers/2411.04954",
            "abstract": "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/",
            "score": 7,
            "issue_id": 518,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "f3ddd073293c6b26",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "CAD-MLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "CAD-MLLM - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Omni-CAD, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 450 Ñ‚Ñ‹ÑÑÑ‡ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAD-MLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğº ÑˆÑƒĞ¼Ğ°Ğ¼ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing CAD Generation with Multimodal Inputs",
                    "desc": "This paper presents CAD-MLLM, a novel system designed to generate Computer-Aided Design (CAD) models from various user inputs, including text, images, and point clouds. It utilizes large language models (LLMs) to effectively align different types of input data with the vectorized representations of CAD models. The authors introduce the Omni-CAD dataset, which is the first of its kind, containing around 450,000 instances of multimodal data paired with CAD construction sequences. The evaluation of the generated models includes new metrics for topology and surface quality, showing that CAD-MLLM outperforms existing methods and is resilient to data noise."
                },
                "zh": {
                    "title": "ç»Ÿä¸€CADç”Ÿæˆç³»ç»Ÿï¼šå¤šæ¨¡æ€è¾“å…¥çš„åˆ›æ–°åº”ç”¨",
                    "desc": "æœ¬æ–‡æ—¨åœ¨è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°ã€å›¾åƒã€ç‚¹äº‘æˆ–å…¶ç»„åˆè½»æ¾ç”ŸæˆCADæ¨¡å‹ã€‚æˆ‘ä»¬ä»‹ç»äº†CAD-MLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤ŸåŸºäºå¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹çš„ç³»ç»Ÿã€‚è¯¥æ¡†æ¶åˆ©ç”¨CADæ¨¡å‹çš„å‘½ä»¤åºåˆ—ï¼Œå¹¶é‡‡ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¸åŒæ¨¡æ€æ•°æ®å’ŒCADæ¨¡å‹çš„å‘é‡è¡¨ç¤ºè¿›è¡Œç‰¹å¾ç©ºé—´å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºOmni-CADçš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬æè¿°ã€å¤šè§†å›¾å›¾åƒã€ç‚¹å’Œå‘½ä»¤åºåˆ—ï¼Œçº¦æœ‰45ä¸‡ä¸ªå®ä¾‹ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºCAD-MLLMåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04097",
            "title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.04097",
            "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.",
            "score": 5,
            "issue_id": 516,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 6",
                "zh": "11æœˆ6æ—¥"
            },
            "hash": "62101343e5b62784",
            "authors": [
                "Maya Varma",
                "Jean-Benoit Delbrouck",
                "Zhihong Chen",
                "Akshay Chaudhari",
                "Curtis Langlotz"
            ],
            "affiliations": [
                "Stanford University",
                "Hugging Face"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04097.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#training",
                    "#healthcare",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RaVL Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). RaVL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 654 VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing VLM Robustness by Targeting Spurious Correlations at the Local Level",
                    "desc": "This paper introduces RaVL, a method designed to improve the robustness of fine-tuned vision-language models (VLMs) by addressing spurious correlations between image features and text attributes. Unlike existing methods that focus on global image-level features, RaVL operates on fine-grained local image features to identify and mitigate these correlations. It employs a region-level clustering approach to pinpoint specific image features that lead to classification errors in zero-shot scenarios. The results demonstrate that RaVL significantly enhances the discovery and mitigation of spurious correlations, leading to improved classification accuracy across various VLM architectures and domains."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹é²æ£’æ€§çš„RaVLæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRaVLçš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é²æ£’æ€§ã€‚RaVLé€šè¿‡å±€éƒ¨å›¾åƒç‰¹å¾æ¥å‘ç°å’Œå‡è½»è™šå‡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯ä»…åœ¨å…¨å±€å›¾åƒå±‚é¢è¿›è¡Œå¹²é¢„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŒºåŸŸçº§èšç±»æ¥è¯†åˆ«å¯¼è‡´é›¶-shotåˆ†ç±»é”™è¯¯çš„ç²¾ç¡®å›¾åƒç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–°çš„åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥å‡è½»è¿™äº›è™šå‡ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRaVLåœ¨å‘ç°å’Œå‡è½»è™šå‡ç›¸å…³æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04986",
            "title": "The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities",
            "url": "https://huggingface.co/papers/2411.04986",
            "abstract": "Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic \"hub\" which integrates information from various modality-specific \"spokes\" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model's dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.",
            "score": 4,
            "issue_id": 515,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 7",
                "zh": "11æœˆ7æ—¥"
            },
            "hash": "9a27aef11b630a04",
            "authors": [
                "Zhaofeng Wu",
                "Xinyan Velocity Yu",
                "Dani Yogatama",
                "Jiasen Lu",
                "Yoon Kim"
            ],
            "affiliations": [
                "MIT",
                "University of Southern California",
                "Allen Institute for AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04986.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#multimodal",
                    "#transfer_learning",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ğ±Ğ° Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Unlocking Multimodal Understanding: The Semantic Hub Hypothesis",
                    "desc": "This paper explores how modern language models can understand and process different languages and types of data by learning a shared representation space. The authors propose the 'semantic hub hypothesis', which suggests that these models organize information similarly to how the human brain integrates knowledge from various sources. They demonstrate that representations of semantically similar inputs, even from different languages or modalities, are closely aligned in the model's intermediate layers. Additionally, they show that changes in one type of data representation can influence outputs in other types, indicating that this shared space is actively used by the model rather than being a mere artifact of training."
                },
                "zh": {
                    "title": "å…±äº«è¡¨ç¤ºç©ºé—´ï¼šè·¨æ¨¡æ€ç†è§£çš„å…³é”®",
                    "desc": "ç°ä»£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€å’Œæ¨¡æ€çš„è¾“å…¥ã€‚æˆ‘ä»¬å‡è®¾æ¨¡å‹é€šè¿‡å­¦ä¹ ä¸€ä¸ªå…±äº«çš„è¡¨ç¤ºç©ºé—´æ¥è·å¾—è¿™ä¸€èƒ½åŠ›ï¼Œè¿™ä¸ªç©ºé—´å°†è¯­ä¹‰ç›¸ä¼¼çš„è¾“å…¥æ”¾åœ¨ä¸€èµ·ï¼Œå³ä½¿å®ƒä»¬æ¥è‡ªä¸åŒçš„æ¨¡æ€æˆ–è¯­è¨€ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºè¯­ä¹‰ä¸­å¿ƒå‡è®¾ï¼Œç±»ä¼¼äºç¥ç»ç§‘å­¦ä¸­çš„ä¸­å¿ƒ-è¾å°„æ¨¡å‹ï¼Œè®¤ä¸ºäººè„‘ä¸­çš„è¯­ä¹‰çŸ¥è¯†æ˜¯é€šè¿‡ä¸€ä¸ªè·¨æ¨¡æ€çš„è¯­ä¹‰â€œä¸­å¿ƒâ€ç»„ç»‡çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒè¯­è¨€ä¸­è¯­ä¹‰ç­‰ä»·çš„è¾“å…¥åœ¨æ¨¡å‹çš„ä¸­é—´å±‚å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼Œè¿™ç§å…±äº«è¡¨ç¤ºç©ºé—´åœ¨å¤„ç†è¾“å…¥æ—¶è¢«æ¨¡å‹ç§¯æåˆ©ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05457",
            "title": "Improving the detection of technical debt in Java source code with an enriched dataset",
            "url": "https://huggingface.co/papers/2411.05457",
            "abstract": "Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.",
            "score": 2,
            "issue_id": 510,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 8",
                "zh": "11æœˆ8æ—¥"
            },
            "hash": "bc9c84b19f317115",
            "authors": [
                "Nam Le Hai",
                "Anh M. T. Bui",
                "Phuong T. Nguyen",
                "Davide Di Ruscio",
                "Rick Kazman"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05457.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ³: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ° (Ğ¢Ğ”) Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¢Ğ”, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ… Ğº ĞºĞ¾Ğ´Ñƒ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¢Ğ”, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Bridging Comments and Code: Enhancing Technical Debt Detection",
                    "desc": "This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area."
                },
                "zh": {
                    "title": "æŠ€æœ¯å€ºåŠ¡è¯†åˆ«çš„æ–°è§†è§’",
                    "desc": "æŠ€æœ¯å€ºåŠ¡ï¼ˆTDï¼‰æ˜¯æŒ‡å¼€å‘è€…ä¸ºäº†å¿«é€Ÿè§£å†³é—®é¢˜è€Œé€‰æ‹©çš„ç®€å•æ–¹æ¡ˆæ‰€å¸¦æ¥çš„é¢å¤–å·¥ä½œå’Œæˆæœ¬ã€‚è‡ªæˆ‘æ‰¿è®¤çš„æŠ€æœ¯å€ºåŠ¡ï¼ˆSATDï¼‰æ˜¯å¼€å‘è€…é€šè¿‡æ–‡æœ¬æ³¨é‡Šä¸»åŠ¨è®°å½•å’Œæ‰¿è®¤çš„ä¸€ç§ç‰¹å®šç±»å‹çš„æŠ€æœ¯å€ºåŠ¡ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææ¥è‡ª974ä¸ªJavaé¡¹ç›®çš„æ³¨é‡Šå’Œç›¸å…³æºä»£ç ï¼Œåˆ›å»ºäº†é¦–ä¸ªç”±ä»£ç æ³¨é‡Šè¯†åˆ«çš„æŠ€æœ¯å€ºåŠ¡æ•°æ®é›†ï¼Œå¹¶å‘ç°è¿™äº›æ³¨é‡Šèƒ½æ˜¾è‘—æé«˜ç°æœ‰SATDæ£€æµ‹æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…ä¸ºæŠ€æœ¯å€ºåŠ¡çš„è¯†åˆ«æä¾›äº†æ–°çš„æ•°æ®é›†ï¼Œè¿˜æå‡ºäº†å¯ä½œä¸ºåŸºçº¿çš„åˆ†ç±»å™¨ï¼Œæ¨åŠ¨æœªæ¥ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11æœˆ12æ—¥"
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†StdGENï¼Œä¸€ç§åˆ›æ–°çš„ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dè§’è‰²çš„æ–¹æ³•ã€‚å®ƒèƒ½åœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»çš„è¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘ï¼‰çš„è¯¦ç»†3Dè§’è‰²ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­é‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®ƒä¸ºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰æä¾›äº†çµæ´»çš„å®šåˆ¶åŒ–3Dè§’è‰²ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le StdGEN, yÄ« zhÇ’ng chuÃ ng xÄ«n de cÃ³ng dÄn zhÄng tÃº xiÃ ng shÄ“ng chÃ©ng gÄo zhÃ¬ liÃ ng 3D juÃ© sÃ¨ de fÄng fÇ. tÄ nÃ©ng zÃ i sÄn fÄ“n zhÅng nÃ¨i shÄ“ng chÃ©ng jÃ¹ yÇ’u fÄ“n lÃ¬ de yÇ” yÃ¬ zÇ” jÃ¬n (rÃº shÄ“n tÇ, yÄ« fÃº hÃ© tÃ³u fÃ ) de xiÃ¡ng xÃ¬ 3D juÃ© sÃ¨. StdGEN de hÃ© xÄ«n shÃ¬ tÃ­ chÅ« de yÇ” yÃ¬ gÇn juÃ© dÃ  xÃ­ng chÃ³ng jiÃ n mÃ³ xÃ­ng (S-LRM), nÃ©ng gÃ²u cÃ³ng duÅ shÃ¬ jÃ¬ tÃº xiÃ ng zhÅng chÃ³ng jiÃ n jÇ hÃ©, yÃ¡n sÃ¨ hÃ© yÇ” yÃ¬. shÃ­ yÃ n zhÃ¨ng mÃ­ng, StdGEN zÃ i 3D dÃ²ng mÃ n juÃ© sÃ¨ shÄ“ng chÃ©ng fÄng miÃ n biÇo xiÇn chÅ« sÃ¨, yÅu yÃº xiÃ n yÇ’u fÄng fÇ. tÄ wÃ¨i xÅ« nÇ xiÃ n shÃ­, yÃ³u xÃ¬ hÃ© diÃ n yÇng zhÃ¬ zuÃ² dÄ›ng ti gÅng gÄ›i le lÃ­nghuÃ³ de dÃ¬ng zhÃ¬ huÃ  3D juÃ© sÃ¨.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sÄ«tÄ«dÄ« jÄ«n\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ngxÄ«n\", \"trans\": \"innovative\"},\n    {\"word\": \"è§’è‰²\", \"pinyin\": \"juÃ©sÃ¨\", \"trans\": \"character\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"ç»„ä»¶\", \"pinyin\": \"zÇ”jiÃ n\", \"trans\": \"component\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇhÃ©\", \"trans\": \"geometry\"},\n    {\"word\": \"é‡å»º\", \"pinyin\": \"chÃ³ngjiÃ n\", \"trans\": \"reconstruct\"},\n    {\"word\": \"å¤šè§†å›¾\", \"pinyin\": \"duÅshÃ¬tÃº\", \"trans\": \"multi-view\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"çµæ´»\", \"pinyin\": \"lÃ­nghuÃ³\", \"trans\": \"flexible\"},\n    {\"word\": \"å®šåˆ¶åŒ–\", \"pinyin\": \"dÃ¬ngzhÃ¬huÃ \", \"trans\": \"customized\"},\n    {\"word\": \"è™šæ‹Ÿç°å®\", \"pinyin\": \"xÅ«nÇ xiÃ nshÃ­\", \"trans\": \"virtual reality\"},\n    {\"word\": \"æ¸¸æˆ\", \"pinyin\": \"yÃ³uxÃ¬\", \"trans\": \"game\"},\n    {\"word\": \"ç”µå½±åˆ¶ä½œ\", \"pinyin\": \"diÃ nyÇng zhÃ¬zuÃ²\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}