{
    "date": {
        "ru": "11 –Ω–æ—è–±—Ä—è",
        "en": "November 11",
        "zh": "11Êúà11Êó•"
    },
    "time_utc": "2024-11-11 08:16",
    "weekday": 0,
    "issue_id": 511,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.05288",
            "title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
            "url": "https://huggingface.co/papers/2411.05288",
            "abstract": "Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .",
            "score": 7,
            "issue_id": 509,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 –Ω–æ—è–±—Ä—è",
                "en": "November 8",
                "zh": "11Êúà8Êó•"
            },
            "hash": "19accdb712f507d9",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–Ω—ã—Ö —Å–ª–æ–µ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–ª–æ–µ–≤ —Å–ª–æ–≤–∞—Ä—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏. –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Å—Ö–µ–º–∞–º–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –º–µ—Ç–æ–¥–∞–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–∏–∫–æ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –±–æ–ª—å—à–∏–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏."
                },
                "en": {
                    "title": "Balancing Memory and Computation in Pipeline Parallelism for Language Models",
                    "desc": "This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies."
                },
                "zh": {
                    "title": "‰ºòÂåñËØçÊ±áÂ±Ç‰ª•Âπ≥Ë°°ËÆ°ÁÆó‰∏éÂÜÖÂ≠ò",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåËØçÊ±áÂ±ÇÂØºËá¥ÁöÑËÆ°ÁÆóÂíåÂÜÖÂ≠ò‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåÂ∞ÜËØçÊ±áÂ±ÇÂùáÂåÄÂàÜÈÖçÂà∞ÁÆ°ÈÅìËÆæÂ§á‰∏äÔºåÂπ∂Â∞ÜËÆ°ÁÆóÂàÜÁªÑÂà∞ÁÆ°ÈÅì‰º†ÈÄí‰∏≠„ÄÇ‰∏∫‰∫ÜÂáèÂ∞ëÊøÄÊ¥ªÂÜÖÂ≠òÂºÄÈîÄÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÂá†ÁßçÁÆóÊ≥ïÊù•Èôç‰ΩéËØçÊ±áÂ±ÇÂÜÖÁöÑÈÄö‰ø°ÈöúÁ¢ç„ÄÇÈÄöËøáÁªìÂêàËøô‰∫õÊäÄÊúØÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ËÆ°ÁÆóÂíåÂèÇÊï∞ÂÜÖÂ≠ò‰πãÈó¥ÂÆûÁé∞‰∫ÜÊúâÊïàÂπ≥Ë°°ÔºåÂπ∂Âú®Â§ßËØçÊ±áÂú∫ÊôØ‰∏ãÊòæËëóÈôç‰Ωé‰∫ÜÂ≥∞ÂÄºÂÜÖÂ≠ò‰ΩøÁî®„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 7,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 –Ω–æ—è–±—Ä—è",
                "en": "November 8",
                "zh": "11Êúà8Êó•"
            },
            "hash": "b23d3650ace21f86",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "StdGEN: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ 3D-–ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º",
                    "desc": "StdGEN - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í –æ—Å–Ω–æ–≤–µ StdGEN –ª–µ–∂–∏—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞–≤—Ç–æ—Ä–∞–º–∏ Semantic-aware Large Reconstruction Model (S-LRM), —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é, —Ü–≤–µ—Ç –∏ —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏–∑ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ö–æ–Ω–≤–µ–π–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Å—Ö–µ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –º–æ–¥—É–ª—å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ StdGEN –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∞–Ω–∏–º–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–æ–º–µ—Ç—Ä–∏–∏, —Ç–µ–∫—Å—Ç—É—Ä –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGENÔºöÈ´òÊïàÁîüÊàêÂèØÂàÜËß£3DËßíËâ≤ÁöÑÂàõÊñ∞ÁÆ°ÈÅì",
                    "desc": "StdGENÊòØ‰∏ÄÁßçÂàõÊñ∞ÁöÑÁÆ°ÈÅìÔºåËÉΩÂ§ü‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêËØ≠‰πâÂàÜËß£ÁöÑÈ´òË¥®Èáè3DËßíËâ≤ÔºåÂπøÊ≥õÂ∫îÁî®‰∫éËôöÊãüÁé∞ÂÆû„ÄÅÊ∏∏ÊàèÂíåÁîµÂΩ±Âà∂‰ΩúÁ≠âÈ¢ÜÂüü„ÄÇ‰∏é‰ª•ÂæÄÊñπÊ≥ïÁõ∏ÊØîÔºåStdGENÂú®ÂèØÂàÜËß£ÊÄß„ÄÅÊúâÊïàÊÄßÂíåÊïàÁéá‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂú®‰∏âÂàÜÈíüÂÜÖÁîüÊàêÁªÜËá¥ÁöÑ3DËßíËâ≤Ôºå‰∏îÂêÑ‰∏™ËØ≠‰πâÁªÑ‰ª∂Â¶ÇË∫´‰Ωì„ÄÅË°£ÊúçÂíåÂ§¥ÂèëÂàÜÁ¶ª„ÄÇÂÖ∂Ê†∏ÂøÉÊòØËØ≠‰πâÊÑüÁü•ÁöÑÂ§ßÂûãÈáçÂª∫Ê®°ÂûãÔºàS-LRMÔºâÔºåËØ•Ê®°ÂûãÂü∫‰∫éÂèòÊç¢Âô®ÔºåËÉΩÂ§ü‰ªéÂ§öËßÜÂõæÂõæÂÉè‰∏≠ËÅîÂêàÈáçÂª∫Âá†‰Ωï„ÄÅÈ¢úËâ≤ÂíåËØ≠‰πâ„ÄÇÈÄöËøáÂºïÂÖ•ÂèØÂæÆÂàÜÁöÑÂ§öÂ±ÇËØ≠‰πâË°®Èù¢ÊèêÂèñÊñπÊ°àÔºåStdGENÂÆûÁé∞‰∫ÜÈ´òË¥®Èáè„ÄÅÂèØÂàÜËß£ÁöÑ3DËßíËâ≤ÁîüÊàêÔºåÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÂÖ∂Âú®3DÂä®Êº´ËßíËâ≤ÁîüÊàêÊñπÈù¢ÁöÑÊÄßËÉΩË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫ÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 3,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "38beaabd86eeaa88",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥—É–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (PEFT) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥—É–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –ø–æ–ª–Ω—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ PEFT, –≤–∫–ª—é—á–∞—è LoRA, (IA)^3 –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤, –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö –∏ —Ä–∞–∑–º–µ—Ä–∞—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã PEFT –º–æ–≥—É—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π, –ø—Ä–∏ —ç—Ç–æ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤, –∞ LoRA –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "ÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºöÊèêÂçáÂçïÂÖÉÊµãËØïÁîüÊàêÁöÑÁªèÊµéÊÄß‰∏éÊúâÊïàÊÄß",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂçïÂÖÉÊµãËØïÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàPEFTÔºâÊñπÊ≥ï„ÄÇ‰º†ÁªüÁöÑÂÖ®ÈáèÂæÆË∞ÉËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜÊàêÊú¨È´òÊòÇÔºåPEFTÊñπÊ≥ïÈÄöËøáÂè™ÂæÆË∞ÉÈÉ®ÂàÜÂèÇÊï∞Êù•Èôç‰ΩéËÆ°ÁÆóÂºÄÈîÄ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåPEFTÊñπÊ≥ïÂú®ÂçïÂÖÉÊµãËØïÁîüÊàê‰∏≠ËÉΩÂ§üËææÂà∞‰∏éÂÖ®ÈáèÂæÆË∞ÉÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÊèêÁ§∫ÂæÆË∞ÉÂú®ÊàêÊú¨ÂíåËµÑÊ∫êÂà©Áî®‰∏äÊúÄ‰∏∫ÊúâÊïà„ÄÇËÆ∫ÊñáËøòÊØîËæÉ‰∫Ü‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑÂíåÂ§ßÂ∞è‰∏ãÁöÑÂ§öÁßçPEFTÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÁâπÂÆö‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05457",
            "title": "Improving the detection of technical debt in Java source code with an enriched dataset",
            "url": "https://huggingface.co/papers/2411.05457",
            "abstract": "Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.",
            "score": 2,
            "issue_id": 510,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 –Ω–æ—è–±—Ä—è",
                "en": "November 8",
                "zh": "11Êúà8Êó•"
            },
            "hash": "bc9c84b19f317115",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#dataset"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥: –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞ (–¢–î) –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –ø–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—Ä–∏–º–µ—Ä—ã –¢–î, –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∫ –∫–æ–¥—É, –≤–º–µ—Å—Ç–µ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –¢–î, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Bridging Comments and Code: Enhancing Technical Debt Detection",
                    "desc": "This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area."
                },
                "zh": {
                    "title": "ÊäÄÊúØÂÄ∫Âä°ËØÜÂà´ÁöÑÊñ∞ËßÜËßí",
                    "desc": "ÊäÄÊúØÂÄ∫Âä°ÔºàTDÔºâÊòØÊåáÂºÄÂèëËÄÖ‰∏∫‰∫ÜÂø´ÈÄüËß£ÂÜ≥ÈóÆÈ¢òËÄåÈÄâÊã©ÁöÑÁÆÄÂçïÊñπÊ°àÊâÄÂ∏¶Êù•ÁöÑÈ¢ùÂ§ñÂ∑•‰ΩúÂíåÊàêÊú¨„ÄÇËá™ÊàëÊâøËÆ§ÁöÑÊäÄÊúØÂÄ∫Âä°ÔºàSATDÔºâÊòØÂºÄÂèëËÄÖÈÄöËøáÊñáÊú¨Ê≥®Èáä‰∏ªÂä®ËÆ∞ÂΩïÂíåÊâøËÆ§ÁöÑ‰∏ÄÁßçÁâπÂÆöÁ±ªÂûãÁöÑÊäÄÊúØÂÄ∫Âä°„ÄÇÊú¨ÊñáÈÄöËøáÂàÜÊûêÊù•Ëá™974‰∏™JavaÈ°πÁõÆÁöÑÊ≥®ÈáäÂíåÁõ∏ÂÖ≥Ê∫ê‰ª£Á†ÅÔºåÂàõÂª∫‰∫ÜÈ¶ñ‰∏™Áî±‰ª£Á†ÅÊ≥®ÈáäËØÜÂà´ÁöÑÊäÄÊúØÂÄ∫Âä°Êï∞ÊçÆÈõÜÔºåÂπ∂ÂèëÁé∞Ëøô‰∫õÊ≥®ÈáäËÉΩÊòæËëóÊèêÈ´òÁé∞ÊúâSATDÊ£ÄÊµãÊ®°ÂûãÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏ç‰ªÖ‰∏∫ÊäÄÊúØÂÄ∫Âä°ÁöÑËØÜÂà´Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊï∞ÊçÆÈõÜÔºåËøòÊèêÂá∫‰∫ÜÂèØ‰Ωú‰∏∫Âü∫Á∫øÁöÑÂàÜÁ±ªÂô®ÔºåÊé®Âä®Êú™Êù•Áõ∏ÂÖ≥Á†îÁ©∂ÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04425",
            "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
            "url": "https://huggingface.co/papers/2411.04425",
            "abstract": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.",
            "score": 1,
            "issue_id": 509,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 –Ω–æ—è–±—Ä—è",
                "en": "November 7",
                "zh": "11Êúà7Êó•"
            },
            "hash": "397d7c5c26dfad0f",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–µ–Ω—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º DELIFT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). DELIFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ø–∞—Ä–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏. –ê–ª–≥–æ—Ä–∏—Ç–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞: –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ DELIFT –º–æ–∂–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞ –Ω–∞ 70% –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Optimize Data, Maximize Performance with DELIFT!",
                    "desc": "This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance."
                },
                "zh": {
                    "title": "È´òÊïàÂæÆË∞ÉÔºöDELIFTÁÆóÊ≥ïÁöÑÂàõÊñ∞‰πãË∑Ø",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DELIFTÁöÑÊñ∞ÁÆóÊ≥ïÔºåÁî®‰∫éÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÂáèÂ∞ëÂÜó‰ΩôÂíåÊó†ÊïàÊï∞ÊçÆÁöÑ‰ΩøÁî®„ÄÇDELIFTÈÄöËøá‰ºòÂåñÊï∞ÊçÆÈÄâÊã©ÔºåÁ≥ªÁªüÊÄßÂú∞ÊîπËøõ‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÁöÑÂæÆË∞ÉÈò∂ÊÆµÔºöÊåá‰ª§ÂæÆË∞É„ÄÅ‰ªªÂä°ÁâπÂÆöÂæÆË∞ÉÂíåÊåÅÁª≠ÂæÆË∞É„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®‰∫Ü‰∏ÄÁßçÊàêÂØπÊïàÁî®Â∫¶ÈáèÔºåÈáèÂåñÊï∞ÊçÆÊ†∑Êú¨ÂØπÊ®°ÂûãÂìçÂ∫îÂÖ∂‰ªñÊ†∑Êú¨ÁöÑÊîπÂñÑÁ®ãÂ∫¶Ôºå‰ªéËÄåÊúâÊïàËØÑ‰º∞‰ø°ÊÅØ‰ª∑ÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDELIFTËÉΩÂ§üÂú®‰∏çÈôç‰ΩéÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜÂæÆË∞ÉÊï∞ÊçÆÈáèÂáèÂ∞ëÂ§öËææ70%ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11Êúà8Êó•"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11Êúà12Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰ª£Á†ÅÁîüÊàê„ÄÅÊé®ÁêÜ‰ªªÂä°Âíå‰ª£ÁêÜÁ≥ªÁªü‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇËôΩÁÑ∂ÂºÄÊîæËÆøÈóÆÁöÑ‰ª£Á†ÅLLMsÊÄßËÉΩÊé•Ëøë‰∏ìÊúâÊ®°ÂûãÔºå‰ΩÜÈÄÇÂêà‰∏•Ê†ºÁßëÂ≠¶Á†îÁ©∂ÁöÑÈ´òË¥®ÈáèÊ®°Âûã‰ªçÁÑ∂ÊúâÈôê„ÄÇ‰∏∫‰∫ÜÂ°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºå‰ΩúËÄÖ‰ªãÁªç‰∫ÜOpenCoderÔºå‰∏Ä‰∏™È°∂Â∞ñÁöÑ‰ª£Á†ÅLLMÔºåÊÄßËÉΩÂ™≤ÁæéÈ¢ÜÂÖàÊ®°ÂûãÔºåÂπ∂Êèê‰æõËØ¶ÁªÜÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåÂçèËÆÆ„ÄÇÈÄöËøáËøôÁßçÂºÄÊîæÊÄßÔºå‰ΩúËÄÖÂ∏åÊúõÂä†ÈÄü‰ª£Á†ÅAIÁöÑÁ†îÁ©∂ÂíåÂèØÈáçÂ§çÁöÑËøõÂ±ï„ÄÇ",
        "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
        "pinyin": "Zh√® piƒÅn w√©nzhƒÅng t«éol√πn le d√†x√≠ng y«îy√°n m√≥x√≠ng (LLMs) z√†i d√†im«é shƒìngch√©ng, tuƒ´l«ê r√®nw√π h√© d√†il«ê x√¨t«íng zh≈çng de zh√≤ngy√†ox√¨ng. Suƒ´r√°n kƒÅif√†ng f«éngw√®n de d√†im«é LLMs x√¨ngn√©ng j√¨nk√® zhuƒÅny«íu m√≥x√≠ng, d√†n sh√¨h√© y√°nge kƒìxu√© y√°nji≈´ de gƒÅo zh√¨li√†ng m√≥x√≠ng r√©ngr√°n y«íuxi√†n. W√®ile ti√°nb«î zh√® yƒ´ k√≤ngb√°i, zu√≤zhƒõ ji√®sh√†o le OpenCoder, yƒ´g√® d«êngjiƒÅn de d√†im«é LLM, x√¨ngn√©ng j√¨m«ê l«êngxiƒÅn m√≥x√≠ng, b√¨ng t√≠g≈çng xi√°ngx√¨ de x√πnli√†n sh√πj√π h√© xi√©y√¨. T≈çnggu√≤ zh√® zh«íng kƒÅif√†ngx√¨ng, zu√≤zhƒõ xƒ´w√†ng jiƒÅs√π d√†im«é AI de y√°nji≈´ h√© kƒõ ch√≥ngf√π de j√¨nb√π.",
        "vocab": "[\n    {\"word\": \"ËÆ®ËÆ∫\", \"pinyin\": \"t«éo l√πn\", \"trans\": \"discuss\"},\n    {\"word\": \"Â§ßÂûã\", \"pinyin\": \"d√† x√≠ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"ËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"y«î y√°n m√≥ x√≠ng\", \"trans\": \"language model\"},\n    {\"word\": \"‰ª£Á†ÅÁîüÊàê\", \"pinyin\": \"d√†i m«é shƒìng ch√©ng\", \"trans\": \"code generation\"},\n    {\"word\": \"Êé®ÁêÜ‰ªªÂä°\", \"pinyin\": \"tuƒ´ l«ê r√®n w√π\", \"trans\": \"reasoning tasks\"},\n    {\"word\": \"‰ª£ÁêÜÁ≥ªÁªü\", \"pinyin\": \"d√†i l«ê x√¨ t«íng\", \"trans\": \"proxy system\"},\n    {\"word\": \"ÈáçË¶ÅÊÄß\", \"pinyin\": \"zh√≤ng y√†o x√¨ng\", \"trans\": \"importance\"},\n    {\"word\": \"ÂºÄÊîæËÆøÈóÆ\", \"pinyin\": \"kƒÅi f√†ng f«éng w√®n\", \"trans\": \"open access\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"Êé•Ëøë\", \"pinyin\": \"jiƒì j√¨n\", \"trans\": \"close to\"},\n    {\"word\": \"‰∏ìÊúâÊ®°Âûã\", \"pinyin\": \"zhuƒÅn y«íu m√≥ x√≠ng\", \"trans\": \"proprietary model\"},\n    {\"word\": \"ÈÄÇÂêà\", \"pinyin\": \"sh√¨ h√©\", \"trans\": \"suitable\"},\n    {\"word\": \"‰∏•Ê†º\", \"pinyin\": \"y√°n g√©\", \"trans\": \"strict\"},\n    {\"word\": \"ÁßëÂ≠¶Á†îÁ©∂\", \"pinyin\": \"kƒì xu√© y√°n ji≈´\", \"trans\": \"scientific research\"},\n    {\"word\": \"È´òË¥®Èáè\", \"pinyin\": \"gƒÅo zh√¨ li√†ng\", \"trans\": \"high quality\"},\n    {\"word\": \"ÊúâÈôê\", \"pinyin\": \"y«íu xi√†n\", \"trans\": \"limited\"},\n    {\"word\": \"Â°´Ë°•\", \"pinyin\": \"ti√°n b«î\", \"trans\": \"fill\"},\n    {\"word\": \"Á©∫ÁôΩ\", \"pinyin\": \"k√≤ng b√°i\", \"trans\": \"gap\"},\n    {\"word\": \"‰ΩúËÄÖ\", \"pinyin\": \"zu√≤ zhƒõ\", \"trans\": \"author\"},\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"OpenCoder\", \"pinyin\": \"OpenCoder\", \"trans\": \"OpenCoder\"},\n    {\"word\": \"È°∂Â∞ñ\", \"pinyin\": \"d«êng jiƒÅn\", \"trans\": \"top-notch\"},\n    {\"word\": \"Â™≤Áæé\", \"pinyin\": \"p√¨ mƒõi\", \"trans\": \"rival\"},\n    {\"word\": \"È¢ÜÂÖàÊ®°Âûã\", \"pinyin\": \"l«êng xiƒÅn m√≥ x√≠ng\", \"trans\": \"leading model\"},\n    {\"word\": \"ËØ¶ÁªÜ\", \"pinyin\": \"xi√°ng x√¨\", \"trans\": \"detailed\"},\n    {\"word\": \"ËÆ≠ÁªÉÊï∞ÊçÆ\", \"pinyin\": \"x√πn li√†n sh√π j√π\", \"trans\": \"training data\"},\n    {\"word\": \"ÂçèËÆÆ\", \"pinyin\": \"xi√© y√¨\", \"trans\": \"protocol\"},\n    {\"word\": \"ÂºÄÊîæÊÄß\", \"pinyin\": \"kƒÅi f√†ng x√¨ng\", \"trans\": \"openness\"},\n    {\"word\": \"Âä†ÈÄü\", \"pinyin\": \"jiƒÅ s√π\", \"trans\": \"accelerate\"},\n    {\"word\": \"ÂèØÈáçÂ§ç\", \"pinyin\": \"kƒõ ch√≥ng f√π\", \"trans\": \"reproducible\"},\n    {\"word\": \"ËøõÂ±ï\", \"pinyin\": \"j√¨n zh«én\", \"trans\": \"progress\"}\n]",
        "trans": "This article discusses the importance of large language models (LLMs) in code generation, reasoning tasks, and agent systems. While open-access code LLMs perform nearly as well as proprietary models, high-quality models suitable for rigorous scientific research remain limited. To fill this gap, the authors introduce OpenCoder, a top-tier code LLM that matches the performance of leading models and provides detailed training data and protocols. Through this openness, the authors aim to accelerate research and reproducible progress in code AI.",
        "update_ts": "2024-11-10 10:11"
    }
}