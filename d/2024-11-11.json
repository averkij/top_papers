{
    "date": {
        "ru": "11 ноября",
        "en": "November 11",
        "zh": "11月11日"
    },
    "time_utc": "2024-11-11 09:00",
    "weekday": 0,
    "issue_id": 519,
    "home_page_url": "https://huggingface.co/papers?date=2024-11-11",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.04997",
            "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation",
            "url": "https://huggingface.co/papers/2411.04997",
            "abstract": "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.",
            "score": 31,
            "issue_id": 513,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "5e5b851688791e8a",
            "authors": [
                "Weiquan Huang",
                "Aoqi Wu",
                "Yifan Yang",
                "Xufang Luo",
                "Yuqing Yang",
                "Liang Hu",
                "Qi Dai",
                "Xiyang Dai",
                "Dongdong Chen",
                "Chong Luo",
                "Lili Qiu"
            ],
            "affiliations": [
                "Tongji University",
                "Microsoft Corporation"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04997.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#games",
                    "#training"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Усиление CLIP с помощью LLM для лучшего понимания изображений и текста",
                    "desc": "LLM2CLIP - это новый подход к улучшению мультимодальной модели CLIP с помощью больших языковых моделей (LLM). Авторы предлагают дообучать LLM в пространстве подписей к изображениям с помощью контрастивного обучения, чтобы улучшить текстовые возможности CLIP. Затем обученная LLM выступает в роли учителя для визуального энкодера CLIP. Этот метод позволяет использовать более длинные и сложные подписи к изображениям, преодолевая ограничения стандартного текстового энкодера CLIP."
                },
                "en": {
                    "title": "Unlocking CLIP's Potential with LLMs",
                    "desc": "This paper introduces LLM2CLIP, a new method that enhances the CLIP model by integrating large language models (LLMs) like GPT-4. By fine-tuning the LLM in the caption space using contrastive learning, the model improves its ability to understand and generate complex image captions. The LLM acts as a teacher for CLIP's visual encoder, allowing it to process longer and more intricate texts than the original CLIP could handle. The results show significant advancements in cross-modal tasks, demonstrating the effectiveness of combining LLMs with multimodal representation learning."
                },
                "zh": {
                    "title": "利用大型语言模型提升CLIP的多模态学习能力",
                    "desc": "CLIP是一个重要的多模态基础模型，本文提出了一种新方法LLM2CLIP，旨在利用大型语言模型（LLMs）来增强CLIP的能力。通过对LLM进行微调并结合对比学习，我们能够提取其文本能力，从而显著提高CLIP在处理图像标题时的表现。LLM的强大文本理解能力使得CLIP能够处理更长和更复杂的文本，克服了传统CLIP的局限性。实验结果表明，这种方法在跨模态任务中带来了显著的改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04282",
            "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding",
            "url": "https://huggingface.co/papers/2411.04282",
            "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.",
            "score": 25,
            "issue_id": 518,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "e566070395107bc0",
            "authors": [
                "Haolin Chen",
                "Yihao Feng",
                "Zuxin Liu",
                "Weiran Yao",
                "Akshara Prabhakar",
                "Shelby Heinecke",
                "Ricky Ho",
                "Phil Mui",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04282.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие скрытого потенциала рассуждений в языковых моделях",
                    "desc": "Статья представляет новый метод LaTRO для улучшения способностей больших языковых моделей (LLM) к рассуждению. LaTRO формулирует процесс рассуждения как выборку из латентного распределения и оптимизирует его с помощью вариационных подходов. Метод позволяет LLM одновременно улучшать процесс рассуждения и способность оценивать качество рассуждений без необходимости внешней обратной связи. Эксперименты на наборах данных GSM8K и ARC-Challenge показали значительное улучшение точности по сравнению с базовыми моделями и обычной дообучением."
                },
                "en": {
                    "title": "Unlocking Reasoning Potential in Large Language Models with LaTRO",
                    "desc": "This paper presents LaTent Reasoning Optimization (LaTRO), a new framework designed to enhance the reasoning abilities of large language models (LLMs) during training. LaTRO treats reasoning as a process of sampling from a latent distribution and uses variational methods to optimize this process. The framework allows LLMs to improve their reasoning skills and assess the quality of their reasoning simultaneously, without needing external feedback. Experimental results show that LaTRO significantly boosts the performance of LLMs on reasoning tasks, indicating that pre-trained models have untapped reasoning potential that can be further developed through this method."
                },
                "zh": {
                    "title": "解锁大型语言模型的潜在推理能力",
                    "desc": "大型语言模型（LLMs）在处理复杂推理任务时仍然面临挑战。我们提出了一种新的框架，称为LaTent推理优化（LaTRO），它通过变分方法将推理过程视为从潜在分布中采样。LaTRO可以在训练过程中同时提高模型的推理能力和评估推理质量的能力，而无需外部反馈。实验结果表明，LaTRO显著提高了模型在GSM8K和ARC-Challenge数据集上的表现，证明了预训练LLMs的潜在推理能力可以通过我们的优化方法得到增强。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05288",
            "title": "Balancing Pipeline Parallelism with Vocabulary Parallelism",
            "url": "https://huggingface.co/papers/2411.05288",
            "abstract": "Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .",
            "score": 18,
            "issue_id": 509,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 ноября",
                "en": "November 8",
                "zh": "11月8日"
            },
            "hash": "19accdb712f507d9",
            "authors": [
                "Man Tsung Yeung",
                "Penghui Qi",
                "Min Lin",
                "Xinyi Wan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05288.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Эффективное распараллеливание словарных слоев для ускорения обучения больших языковых моделей",
                    "desc": "Статья предлагает новый метод распараллеливания обучения больших языковых моделей, фокусируясь на эффективном распределении слоев словаря. Авторы разработали алгоритмы для равномерного распределения вычислений и памяти между устройствами в конвейере, что позволяет уменьшить простои и оптимизировать использование памяти. Метод интегрируется с существующими схемами конвейерного параллелизма и особенно эффективен в сочетании с методами балансировки активационной памяти. Эксперименты показывают значительное улучшение пропускной способности и снижение пикового использования памяти, особенно для моделей с большими словарями."
                },
                "en": {
                    "title": "Balancing Memory and Computation in Pipeline Parallelism for Language Models",
                    "desc": "This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies."
                },
                "zh": {
                    "title": "优化词汇层以平衡计算与内存",
                    "desc": "本文探讨了在训练大型语言模型时，词汇层导致的计算和内存不平衡问题。我们提出了一种方法，将词汇层均匀分配到管道设备上，并将计算分组到管道传递中。为了减少激活内存开销，我们设计了几种算法来降低词汇层内的通信障碍。通过结合这些技术，我们的方法在计算和参数内存之间实现了有效平衡，并在大词汇场景下显著降低了峰值内存使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05738",
            "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
            "url": "https://huggingface.co/papers/2411.05738",
            "abstract": "We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io",
            "score": 13,
            "issue_id": 507,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 ноября",
                "en": "November 8",
                "zh": "11月8日"
            },
            "hash": "b23d3650ace21f86",
            "authors": [
                "Yuze He",
                "Yanning Zhou",
                "Wang Zhao",
                "Zhongkai Wu",
                "Kaiwen Xiao",
                "Wei Yang",
                "Yong-Jin Liu",
                "Xiao Han"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "Tsinghua University",
                "Beihang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05738.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "StdGEN: Революция в создании 3D-персонажей с семантическим разделением",
                    "desc": "StdGEN - это инновационный конвейер для генерации семантически декомпозированных трехмерных персонажей высокого качества из одиночных изображений. В основе StdGEN лежит предложенная авторами Semantic-aware Large Reconstruction Model (S-LRM), трансформер-модель, которая реконструирует геометрию, цвет и семантику из многоракурсных изображений. Конвейер включает дифференцируемую схему извлечения многослойной семантической поверхности, специализированную эффективную многоракурсную диффузионную модель и модуль итеративного уточнения многослойной поверхности. Эксперименты показывают значительное превосходство StdGEN над существующими методами в генерации 3D-персонажей аниме по качеству геометрии, текстур и возможностям декомпозиции."
                },
                "en": {
                    "title": "Revolutionizing 3D Character Generation with StdGEN",
                    "desc": "StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application."
                },
                "zh": {
                    "title": "StdGEN：高效生成可分解3D角色的创新管道",
                    "desc": "StdGEN是一种创新的管道，能够从单张图像生成语义分解的高质量3D角色，广泛应用于虚拟现实、游戏和电影制作等领域。与以往方法相比，StdGEN在可分解性、有效性和效率上表现出色，能够在三分钟内生成细致的3D角色，且各个语义组件如身体、衣服和头发分离。其核心是语义感知的大型重建模型（S-LRM），该模型基于变换器，能够从多视图图像中联合重建几何、颜色和语义。通过引入可微分的多层语义表面提取方案，StdGEN实现了高质量、可分解的3D角色生成，实验结果显示其在3D动漫角色生成方面的性能超越了现有基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02462",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "url": "https://huggingface.co/papers/2411.02462",
            "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.",
            "score": 9,
            "issue_id": 508,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "38beaabd86eeaa88",
            "authors": [
                "André Storhaug",
                "Jingyue Li"
            ],
            "affiliations": [
                "Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.02462.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#plp"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Эффективная настройка языковых моделей для генерации модульных тестов",
                    "desc": "Эта статья исследует применение методов эффективной настройки параметров (PEFT) для больших языковых моделей (LLM) в задаче генерации модульных тестов. Авторы сравнивают полную тонкую настройку с различными методами PEFT, включая LoRA, (IA)^3 и настройку промптов, на разных архитектурах и размерах моделей. Результаты показывают, что методы PEFT могут обеспечить производительность, сравнимую с полной тонкой настройкой, при этом значительно снижая вычислительные затраты. Особенно эффективной оказалась настройка промптов, а LoRA в некоторых случаях приближается к эффективности полной тонкой настройки."
                },
                "en": {
                    "title": "Unlocking Cost-Effective Fine-Tuning for Unit Test Generation",
                    "desc": "This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization."
                },
                "zh": {
                    "title": "参数高效微调：提升单元测试生成的经济性与有效性",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在单元测试生成中的应用，特别是参数高效微调（PEFT）方法。传统的全量微调虽然有效，但成本高昂，PEFT方法通过只微调部分参数来降低计算开销。研究表明，PEFT方法在单元测试生成中能够达到与全量微调相当的性能，尤其是提示微调在成本和资源利用上最为有效。论文还比较了不同模型架构和大小下的多种PEFT方法，展示了其在特定任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04425",
            "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
            "url": "https://huggingface.co/papers/2411.04425",
            "abstract": "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.",
            "score": 8,
            "issue_id": 509,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "397d7c5c26dfad0f",
            "authors": [
                "Ishika Agarwal",
                "Krishnateja Killamsetty",
                "Lucian Popa",
                "Marina Danilevksy"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "IBM Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04425.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективный файн-тюнинг языковых моделей с меньшими данными",
                    "desc": "Статья представляет новый алгоритм DELIFT для оптимизации выбора данных при файн-тюнинге больших языковых моделей (LLM). DELIFT использует попарную метрику полезности для оценки информативности образцов данных относительно текущих возможностей модели. Алгоритм эффективно работает на всех этапах файн-тюнинга: инструктирование, обучение специфичным задачам и непрерывное обучение. Эксперименты показали, что DELIFT может сократить объем данных для файн-тюнинга на 70% без ущерба для производительности."
                },
                "en": {
                    "title": "Optimize Data, Maximize Performance with DELIFT!",
                    "desc": "This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance."
                },
                "zh": {
                    "title": "高效微调：DELIFT算法的创新之路",
                    "desc": "本论文提出了一种名为DELIFT的新算法，用于提高大型语言模型（LLMs）在特定任务上的性能，同时减少冗余和无效数据的使用。DELIFT通过优化数据选择，系统性地改进了三个关键的微调阶段：指令微调、任务特定微调和持续微调。该方法使用了一种成对效用度量，量化数据样本对模型响应其他样本的改善程度，从而有效评估信息价值。实验结果表明，DELIFT能够在不降低性能的情况下，将微调数据量减少多达70%，显著提高了计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04954",
            "title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM",
            "url": "https://huggingface.co/papers/2411.04954",
            "abstract": "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/",
            "score": 7,
            "issue_id": 518,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "f3ddd073293c6b26",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "🏗️",
                "ru": {
                    "title": "CAD-MLLM: Революция в генерации CAD-моделей с помощью мультимодального ИИ",
                    "desc": "CAD-MLLM - это первая система, способная генерировать параметрические CAD-модели на основе мультимодальных входных данных, включая текст, изображения и облака точек. Система использует последовательности команд CAD-моделей и продвинутые большие языковые модели для согласования пространства признаков между различными модальностями и векторными представлениями CAD-моделей. Для обучения модели был создан набор данных Omni-CAD, содержащий около 450 тысяч CAD-моделей с соответствующими мультимодальными данными. Экспериментальные результаты показывают, что CAD-MLLM значительно превосходит существующие методы условной генерации и остается устойчивой к шумам и отсутствующим точкам."
                },
                "en": {
                    "title": "Revolutionizing CAD Generation with Multimodal Inputs",
                    "desc": "This paper presents CAD-MLLM, a novel system designed to generate Computer-Aided Design (CAD) models from various user inputs, including text, images, and point clouds. It utilizes large language models (LLMs) to effectively align different types of input data with the vectorized representations of CAD models. The authors introduce the Omni-CAD dataset, which is the first of its kind, containing around 450,000 instances of multimodal data paired with CAD construction sequences. The evaluation of the generated models includes new metrics for topology and surface quality, showing that CAD-MLLM outperforms existing methods and is resilient to data noise."
                },
                "zh": {
                    "title": "统一CAD生成系统：多模态输入的创新应用",
                    "desc": "本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，能够根据用户的文本描述、图像、点云或其组合轻松生成CAD模型。我们介绍了CAD-MLLM，这是第一个能够基于多模态输入生成参数化CAD模型的系统。该框架利用CAD模型的命令序列，并采用先进的大型语言模型（LLMs）对不同模态数据和CAD模型的向量表示进行特征空间对齐。我们构建了一个名为Omni-CAD的综合数据集，包含文本描述、多视图图像、点和命令序列，约有45万个实例，评估结果显示CAD-MLLM在生成质量上显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04097",
            "title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.04097",
            "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.",
            "score": 5,
            "issue_id": 516,
            "pub_date": "2024-11-06",
            "pub_date_card": {
                "ru": "6 ноября",
                "en": "November 6",
                "zh": "11月6日"
            },
            "hash": "62101343e5b62784",
            "authors": [
                "Maya Varma",
                "Jean-Benoit Delbrouck",
                "Zhihong Chen",
                "Akshay Chaudhari",
                "Curtis Langlotz"
            ],
            "affiliations": [
                "Stanford University",
                "Hugging Face"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04097.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#training",
                    "#healthcare",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Точное обнаружение и устранение ложных корреляций в мультимодальных моделях",
                    "desc": "Эта статья представляет метод RaVL для обнаружения и смягчения ложных корреляций в мультимодальных моделях компьютерного зрения и языка (VLM). RaVL использует кластеризацию на уровне регионов изображения для выявления конкретных визуальных признаков, вызывающих ошибки классификации. Затем применяется новая функция потерь, учитывающая регионы, чтобы модель фокусировалась на релевантных областях при дообучении. Эксперименты на 654 VLM показали значительное улучшение в обнаружении и смягчении ложных корреляций по сравнению с базовыми методами."
                },
                "en": {
                    "title": "Enhancing VLM Robustness by Targeting Spurious Correlations at the Local Level",
                    "desc": "This paper introduces RaVL, a method designed to improve the robustness of fine-tuned vision-language models (VLMs) by addressing spurious correlations between image features and text attributes. Unlike existing methods that focus on global image-level features, RaVL operates on fine-grained local image features to identify and mitigate these correlations. It employs a region-level clustering approach to pinpoint specific image features that lead to classification errors in zero-shot scenarios. The results demonstrate that RaVL significantly enhances the discovery and mitigation of spurious correlations, leading to improved classification accuracy across various VLM architectures and domains."
                },
                "zh": {
                    "title": "提升视觉语言模型鲁棒性的RaVL方法",
                    "desc": "本文提出了一种名为RaVL的模型，旨在提高视觉语言模型（VLM）的鲁棒性。RaVL通过局部图像特征来发现和减轻虚假相关性，而不是仅在全局图像层面进行干预。该方法利用区域级聚类来识别导致零-shot分类错误的精确图像特征，并通过新的区域感知损失函数来减轻这些虚假相关性。实验结果表明，RaVL在发现和减轻虚假相关性方面均显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04986",
            "title": "The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities",
            "url": "https://huggingface.co/papers/2411.04986",
            "abstract": "Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic \"hub\" which integrates information from various modality-specific \"spokes\" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model's dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.",
            "score": 4,
            "issue_id": 515,
            "pub_date": "2024-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "9a27aef11b630a04",
            "authors": [
                "Zhaofeng Wu",
                "Xinyan Velocity Yu",
                "Dani Yogatama",
                "Jiasen Lu",
                "Yoon Kim"
            ],
            "affiliations": [
                "MIT",
                "University of Southern California",
                "Allen Institute for AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.04986.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#multimodal",
                    "#transfer_learning",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единое семантическое пространство как ключ к мультимодальности ИИ",
                    "desc": "Статья рассматривает гипотезу семантического хаба в современных языковых моделях. Исследователи предполагают, что модели обрабатывают разнородные входные данные через общее семантическое пространство представлений. Эксперименты показывают, что семантически эквивалентные входы на разных языках имеют схожие представления в промежуточных слоях модели. Это свойство распространяется на различные типы данных, включая арифметические выражения, код и аудиовизуальные входы."
                },
                "en": {
                    "title": "Unlocking Multimodal Understanding: The Semantic Hub Hypothesis",
                    "desc": "This paper explores how modern language models can understand and process different languages and types of data by learning a shared representation space. The authors propose the 'semantic hub hypothesis', which suggests that these models organize information similarly to how the human brain integrates knowledge from various sources. They demonstrate that representations of semantically similar inputs, even from different languages or modalities, are closely aligned in the model's intermediate layers. Additionally, they show that changes in one type of data representation can influence outputs in other types, indicating that this shared space is actively used by the model rather than being a mere artifact of training."
                },
                "zh": {
                    "title": "共享表示空间：跨模态理解的关键",
                    "desc": "现代语言模型能够处理多种语言和模态的输入。我们假设模型通过学习一个共享的表示空间来获得这一能力，这个空间将语义相似的输入放在一起，即使它们来自不同的模态或语言。我们称之为语义中心假设，类似于神经科学中的中心-辐射模型，认为人脑中的语义知识是通过一个跨模态的语义“中心”组织的。研究表明，不同语言中语义等价的输入在模型的中间层具有相似的表示，这种共享表示空间在处理输入时被模型积极利用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.05457",
            "title": "Improving the detection of technical debt in Java source code with an enriched dataset",
            "url": "https://huggingface.co/papers/2411.05457",
            "abstract": "Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.",
            "score": 2,
            "issue_id": 510,
            "pub_date": "2024-11-08",
            "pub_date_card": {
                "ru": "8 ноября",
                "en": "November 8",
                "zh": "11月8日"
            },
            "hash": "bc9c84b19f317115",
            "authors": [
                "Nam Le Hai",
                "Anh M. T. Bui",
                "Phuong T. Nguyen",
                "Davide Di Ruscio",
                "Rick Kazman"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2411.05457.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Новый взгляд на технический долг: анализ кода и комментариев",
                    "desc": "Эта статья посвящена исследованию технического долга (ТД) в разработке программного обеспечения. Авторы создали первый датасет, содержащий примеры ТД, идентифицированного в комментариях к коду, вместе с соответствующим исходным кодом. Исследование показало, что включение классифицированного исходного кода значительно улучшает точность предсказания различных типов технического долга. Работа предлагает новый подход к обнаружению ТД, который может служить основой для будущих исследований в этой области."
                },
                "en": {
                    "title": "Bridging Comments and Code: Enhancing Technical Debt Detection",
                    "desc": "This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area."
                },
                "zh": {
                    "title": "技术债务识别的新视角",
                    "desc": "技术债务（TD）是指开发者为了快速解决问题而选择的简单方案所带来的额外工作和成本。自我承认的技术债务（SATD）是开发者通过文本注释主动记录和承认的一种特定类型的技术债务。本文通过分析来自974个Java项目的注释和相关源代码，创建了首个由代码注释识别的技术债务数据集，并发现这些注释能显著提高现有SATD检测模型的预测性能。我们的研究不仅为技术债务的识别提供了新的数据集，还提出了可作为基线的分类器，推动未来相关研究的发展。"
                }
            }
        }
    ],
    "link_prev": "2024-11-08.html",
    "link_next": "2024-11-12.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "08.11",
        "en": "11/08",
        "zh": "11月8日"
    },
    "short_date_next": {
        "ru": "12.11",
        "en": "11/12",
        "zh": "11月12日"
    },
    "zh": {
        "text": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。",
        "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
        "pinyin": "这篇文章介绍了StdGEN，一种创新的从单张图像生成高质量3D角色的方法。它能在三分钟内生成具有分离的语义组件（如身体、衣服和头发）的详细3D角色。StdGEN的核心是提出的语义感知大型重建模型（S-LRM），能够从多视图图像中重建几何、颜色和语义。实验证明，StdGEN在3D动漫角色生成方面表现出色，优于现有方法。它为虚拟现实、游戏和电影制作等提供了灵活的定制化3D角色。\n\nzhè piān wén zhāng jiè shào le StdGEN, yī zhǒng chuàng xīn de cóng dān zhāng tú xiàng shēng chéng gāo zhì liàng 3D jué sè de fāng fǎ. tā néng zài sān fēn zhōng nèi shēng chéng jù yǒu fēn lì de yǔ yì zǔ jìn (rú shēn tǐ, yī fú hé tóu fà) de xiáng xì 3D jué sè. StdGEN de hé xīn shì tí chū de yǔ yì gǎn jué dà xíng chóng jiàn mó xíng (S-LRM), néng gòu cóng duō shì jì tú xiàng zhōng chóng jiàn jǐ hé, yán sè hé yǔ yì. shí yàn zhèng míng, StdGEN zài 3D dòng màn jué sè shēng chéng fāng miàn biǎo xiǎn chū sè, yōu yú xiàn yǒu fāng fǎ. tā wèi xū nǐ xiàn shí, yóu xì hé diàn yǐng zhì zuò děng ti gōng gěi le línghuó de dìng zhì huà 3D jué sè.",
        "vocab": "[\n    {\"word\": \"StdGEN\", \"pinyin\": \"sītīdī jīn\", \"trans\": \"a method for generating high-quality 3D characters from a single image\"},\n    {\"word\": \"创新\", \"pinyin\": \"chuàngxīn\", \"trans\": \"innovative\"},\n    {\"word\": \"角色\", \"pinyin\": \"juésè\", \"trans\": \"character\"},\n    {\"word\": \"语义\", \"pinyin\": \"yǔyì\", \"trans\": \"semantic\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔjiàn\", \"trans\": \"component\"},\n    {\"word\": \"几何\", \"pinyin\": \"jǐhé\", \"trans\": \"geometry\"},\n    {\"word\": \"重建\", \"pinyin\": \"chóngjiàn\", \"trans\": \"reconstruct\"},\n    {\"word\": \"多视图\", \"pinyin\": \"duōshìtú\", \"trans\": \"multi-view\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"灵活\", \"pinyin\": \"línghuó\", \"trans\": \"flexible\"},\n    {\"word\": \"定制化\", \"pinyin\": \"dìngzhìhuà\", \"trans\": \"customized\"},\n    {\"word\": \"虚拟现实\", \"pinyin\": \"xūnǐ xiànshí\", \"trans\": \"virtual reality\"},\n    {\"word\": \"游戏\", \"pinyin\": \"yóuxì\", \"trans\": \"game\"},\n    {\"word\": \"电影制作\", \"pinyin\": \"diànyǐng zhìzuò\", \"trans\": \"film production\"}\n]",
        "trans": "This article introduces StdGEN, an innovative method for generating high-quality 3D characters from a single image. It can produce detailed 3D characters with separate semantic components (such as body, clothing, and hair) in just three minutes. The core of StdGEN is the proposed semantic-aware large reconstruction model (S-LRM), which can reconstruct geometry, color, and semantics from multi-view images. Experiments have shown that StdGEN performs excellently in generating 3D animated characters, outperforming existing methods. It provides flexible customization of 3D characters for virtual reality, gaming, and film production.",
        "update_ts": "2024-11-11 10:13"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}