
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 10 papers. September 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 —Å–µ–Ω—Ç—è–±—Ä—è</span> | <span id="title-articles-count">10 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-12.html">‚¨ÖÔ∏è <span id="prev-date">12.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-16.html">‚û°Ô∏è <span id="next-date">16.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 13', 'zh': '9Êúà13Êó•'};
        let feedDateNext = {'ru': '16.09', 'en': '09/16', 'zh': '9Êúà16Êó•'};
        let feedDatePrev = {'ru': '12.09', 'en': '09/12', 'zh': '9Êúà12Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.07703', 'title': 'DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?', 'url': 'https://huggingface.co/papers/2409.07703', 'abstract': 'Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.', 'score': 66, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': '8b2f2eaf3883ea5d', 'data': {'categories': ['#science', '#reasoning', '#cv', '#long_context', '#data', '#agents', '#benchmark', '#multimodal'], 'emoji': 'üìä', 'ru': {'title': 'DSBench: —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö', 'desc': 'DSBench - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 466 –∑–∞–¥–∞—á –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ 74 –∑–∞–¥–∞—á–∏ –ø–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ Eloquence –∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π Kaggle. DSBench –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è, –≤–∫–ª—é—á–∞—è –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ä–∞–±–æ—Ç—É —Å –±–æ–ª—å—à–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –º–Ω–æ–≥–æ—Ç–∞–±–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏. –û—Ü–µ–Ω–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –æ–Ω–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –ª–∏—à—å —Å 34,12% –∑–∞–¥–∞—á –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –±–æ–ª–µ–µ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã—Ö –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è data science.'}, 'en': {'title': 'Bridging the Gap: Realistic Benchmarks for Data Science Agents', 'desc': 'This paper introduces DSBench, a new benchmark aimed at evaluating the performance of data science agents in realistic scenarios. Unlike previous benchmarks, DSBench includes a wide range of tasks, such as data analysis and modeling, that reflect real-world challenges faced by data scientists. The evaluation of current state-of-the-art Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) reveals that they struggle significantly, with the best agent only solving about one-third of the tasks. This highlights the necessity for further improvements in creating more capable and autonomous data science agents.'}, 'zh': {'title': 'ÊûÑÂª∫Êõ¥Êô∫ËÉΩÁöÑÊï∞ÊçÆÁßëÂ≠¶Êô∫ËÉΩ‰Ωì', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ËØ≠Ë®ÄÂíåËßÜËßâÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊé®Âä®‰∫ÜÈíàÂØπÁâπÂÆöÂ∫îÁî®ÔºàÂ¶ÇË¥≠Áâ©Âä©ÊâãÊàñAIËΩØ‰ª∂Â∑•Á®ãÂ∏àÔºâÁöÑÊô∫ËÉΩ‰ΩìÂºÄÂèë„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞Ëøô‰∫õÊô∫ËÉΩ‰ΩìÂú®Êï∞ÊçÆÁßëÂ≠¶È¢ÜÂüüÁöÑË°®Áé∞ÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÊèêÂá∫‰∫ÜËÆ∏Â§öÊï∞ÊçÆÁßëÂ≠¶Âü∫ÂáÜÔºå‰ΩÜÁé∞ÊúâÂü∫ÂáÜ‰∏éÁúüÂÆûÊï∞ÊçÆÁßëÂ≠¶Â∫îÁî®Áõ∏ÊØî‰ªçÊòæ‰∏çË∂≥„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜDSBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Êï∞ÊçÆÁßëÂ≠¶Êô∫ËÉΩ‰ΩìÂú®Áé∞ÂÆû‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂåÖÂê´466‰∏™Êï∞ÊçÆÂàÜÊûê‰ªªÂä°Âíå74‰∏™Êï∞ÊçÆÂª∫Ê®°‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Â§ßÂ§öÊï∞‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂº∫Ë∞É‰∫ÜÂºÄÂèëÊõ¥ÂÆûÁî®„ÄÅÊô∫ËÉΩÂíåËá™‰∏ªÁöÑÊï∞ÊçÆÁßëÂ≠¶Êô∫ËÉΩ‰ΩìÁöÑÂøÖË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.04109', 'title': 'Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers', 'url': 'https://huggingface.co/papers/2409.04109', 'abstract': 'Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.', 'score': 43, 'issue_id': 1, 'pub_date': '2024-09-06', 'pub_date_card': {'ru': '6 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 6', 'zh': '9Êúà6Êó•'}, 'hash': 'b1fccf9709fd9871', 'data': {'categories': ['#science', '#reasoning', '#hallucinations', '#rl', '#agents', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'LLM vs –ß–µ–ª–æ–≤–µ–∫: –ö—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–∏?', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏–¥–µ–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM), —Å –∏–¥–µ—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏–¥–µ–∏ LLM –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –∫–∞–∫ –±–æ–ª–µ–µ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∏–µ, –Ω–æ –Ω–µ–º–Ω–æ–≥–æ –º–µ–Ω–µ–µ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–µ. –í—ã—è–≤–ª–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–∞–º–æ–æ—Ü–µ–Ω–∫–µ LLM –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–¥–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –¥–∏–∑–∞–π–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–π –ø—É—Ç–µ–º –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã.'}, 'en': {'title': 'LLMs Outshine Humans in Novelty of Research Ideas!', 'desc': 'This paper investigates the ability of large language models (LLMs) to generate novel research ideas compared to human experts in natural language processing (NLP). The authors conducted an experimental study where over 100 NLP researchers generated ideas and reviewed both LLM-generated and human-generated ideas. The results showed that LLM-generated ideas were considered more novel than those from human experts, although they were rated slightly lower in feasibility. The study highlights challenges in evaluating LLMs, such as their self-evaluation capabilities and diversity in idea generation, and suggests further research to assess the impact of these ideas on actual research outcomes.'}, 'zh': {'title': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Á†îÁ©∂ÂàõÊÑèÁîüÊàê‰∏≠ÁöÑÊΩúÂäõ‰∏éÊåëÊàò', 'desc': 'ÊúÄËøëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËøõÂ±ïÂºïÂèë‰∫Ü‰∫∫‰ª¨ÂØπÂÖ∂Âä†ÈÄüÁßëÂ≠¶ÂèëÁé∞ÊΩúÂäõÁöÑ‰πêËßÇ„ÄÇÊú¨ÊñáÈÄöËøáÂÆûÈ™åËÆæËÆ°ËØÑ‰º∞Á†îÁ©∂ÂàõÊÑèÁîüÊàêÔºåÈ¶ñÊ¨°ÂØπÊØî‰∫Ü‰∏ìÂÆ∂NLPÁ†îÁ©∂‰∫∫Âëò‰∏éLLMÂàõÊÑè‰ª£ÁêÜÁöÑË°®Áé∞„ÄÇÁªìÊûúÊòæÁ§∫ÔºåLLMÁîüÊàêÁöÑÂàõÊÑèÂú®Êñ∞È¢ñÊÄß‰∏äË¢´ËØÑÂà§‰∏∫‰ºò‰∫é‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑÂàõÊÑèÔºå‰ΩÜÂú®ÂèØË°åÊÄß‰∏äÁï•Êòæ‰∏çË∂≥„ÄÇÊàë‰ª¨ËøòÂèëÁé∞‰∫ÜÊûÑÂª∫ÂíåËØÑ‰º∞Á†îÁ©∂‰ª£ÁêÜÁöÑÂºÄÊîæÈóÆÈ¢òÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÁ†îÁ©∂ËÆæËÆ°Ôºå‰ª•Ëøõ‰∏ÄÊ≠•È™åËØÅÂàõÊÑèÁöÑÂÆûÈôÖÁ†îÁ©∂ÊàêÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08264', 'title': 'Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale', 'url': 'https://huggingface.co/papers/2409.08264', 'abstract': "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena", 'score': 43, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': 'e7d193394c84841c', 'data': {'categories': ['#science', '#reasoning', '#cv', '#training', '#agents', '#benchmark', '#open_source', '#multimodal'], 'emoji': 'üñ•Ô∏è', 'ru': {'title': 'Windows Agent Arena: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å—Ä–µ–¥–µ Windows', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Windows Agent Arena - –Ω–æ–≤—É—é —Å—Ä–µ–¥—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ Windows. –°—Ä–µ–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 150 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–∫—Ä–∞–Ω–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ Navi, –∫–æ—Ç–æ—Ä—ã–π –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 19.5% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á –Ω–∞ Windows. Benchmark –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –∏ –±—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —Å—Ä–µ–¥–µ Windows.'}, 'en': {'title': 'Empowering AI Agents in Real-World Windows Tasks', 'desc': 'This paper presents the Windows Agent Arena, a new benchmark designed to evaluate the performance of large language models (LLMs) as computer agents in a realistic Windows operating system environment. The arena allows agents to perform over 150 diverse tasks that require skills in planning, screen understanding, and tool usage, addressing the limitations of existing benchmarks that are often modality-specific and slow to evaluate. The introduced multi-modal agent, Navi, demonstrates a success rate of 19.5% in completing tasks, highlighting the challenges faced by AI agents compared to human performance. The benchmark is scalable and can be evaluated quickly, paving the way for future research in agent development and data generation.'}, 'zh': {'title': 'Windows‰ª£ÁêÜÁ´ûÊäÄÂú∫ÔºöÊèêÂçá‰ª£ÁêÜÊÄßËÉΩÁöÑÊñ∞Âπ≥Âè∞', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫‰Ωú‰∏∫ËÆ°ÁÆóÊú∫‰ª£ÁêÜÁöÑÂ∑®Â§ßÊΩúÂäõÔºåËÉΩÂ§üÊèêÂçá‰∫∫Á±ªÁöÑÁîü‰∫ßÂäõÂíåËΩØ‰ª∂ÁöÑÂèØËÆøÈóÆÊÄß„ÄÇÁÑ∂ËÄåÔºåÂú®Áé∞ÂÆûÁéØÂ¢É‰∏≠ËØÑ‰º∞‰ª£ÁêÜÊÄßËÉΩ‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåÂõ†‰∏∫Â§ßÂ§öÊï∞Âü∫ÂáÜÊµãËØï‰ªÖÈôê‰∫éÁâπÂÆöÁöÑÊ®°ÊÄÅÊàñÈ¢ÜÂüüÔºåÂπ∂‰∏îÂÆåÊï¥ÁöÑÂü∫ÂáÜËØÑ‰º∞ÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜWindows‰ª£ÁêÜÁ´ûÊäÄÂú∫ÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éWindowsÊìç‰ΩúÁ≥ªÁªüÁöÑÂèØÈáçÂ§çÁéØÂ¢ÉÔºå‰ª£ÁêÜÂèØ‰ª•Âú®ÂÖ∂‰∏≠Ëá™Áî±Êìç‰ΩúÔºå‰ΩøÁî®ÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÂíåÂ∑•ÂÖ∑„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü150Â§ö‰∏™Â§öÊ†∑ÂåñÁöÑWindows‰ªªÂä°ÔºåË¶ÅÊ±Ç‰ª£ÁêÜÂÖ∑Â§áËßÑÂàí„ÄÅÂ±èÂπïÁêÜËß£ÂíåÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑËÉΩÂäõÔºåÂπ∂‰∏îÊàë‰ª¨ÁöÑÂü∫ÂáÜÊµãËØïÂèØ‰ª•Âú®Azure‰∏äÊó†ÁºùÂπ∂Ë°åÂåñÔºåÂø´ÈÄüÂÆåÊàêËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08240', 'title': 'IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2409.08240', 'abstract': "While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.", 'score': 17, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': 'b63d6f4e9ec2890a', 'data': {'categories': ['#cv', '#graphs', '#benchmark', '#open_source', '#diffusion', '#architecture'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–¢–æ—á–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ - Instance Feature Generation (IFG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Instance Feature Adapter (IFAdapter), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. IFAdapter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∫–∞—Ä—Ç—É –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ IFAdapter –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö.'}, 'en': {'title': 'Enhancing Image Generation with Instance Feature Control', 'desc': 'This paper addresses the limitations of Text-to-Image (T2I) diffusion models in generating multiple instances with accurate positioning and detailed features. It introduces the Instance Feature Generation (IFG) task, which focuses on improving both the spatial accuracy and the fidelity of features in generated images. To tackle this task, the authors propose the Instance Feature Adapter (IFAdapter), which uses additional appearance tokens and an Instance Semantic Map to better align features with their spatial locations. The paper also presents a benchmark for evaluating the IFG task and shows that the IFAdapter significantly outperforms existing models in generating instances with precise positioning and enhanced features.'}, 'zh': {'title': 'ÊèêÂçáÂõæÂÉèÁîüÊàêÁöÑÂÆû‰æãÁâπÂæÅ‰∏éÂÆö‰ΩçÁ≤æÂ∫¶', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ªªÂä°ÔºåÁß∞‰∏∫ÂÆû‰æãÁâπÂæÅÁîüÊàêÔºàIFGÔºâÔºåÊó®Âú®ÊèêÈ´òÁîüÊàêÂõæÂÉè‰∏≠Â§ö‰∏™ÂÆû‰æãÁöÑÂÆö‰ΩçÂáÜÁ°ÆÊÄßÂíåÁâπÂæÅ‰øùÁúüÂ∫¶„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏Ä‰ªªÂä°Ôºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜÂÆû‰æãÁâπÂæÅÈÄÇÈÖçÂô®ÔºàIFAdapterÔºâÔºåËØ•Ê®°ÂùóÈÄöËøáÂ¢ûÂä†Â§ñËßÇÊ†áËÆ∞Âíå‰ΩøÁî®ÂÆû‰æãËØ≠‰πâÂõæÊù•Â¢ûÂº∫ÁâπÂæÅË°®Áé∞„ÄÇIFAdapter‰Ωú‰∏∫‰∏Ä‰∏™ÂèØÊèíÊãîÊ®°ÂùóÔºåËÉΩÂ§üÈÄÇÂ∫î‰∏çÂêåÁöÑÁ§æÂå∫Ê®°ÂûãÔºåÂπ∂ÊåáÂØºÊâ©Êï£ËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIFAdapterÂú®ÂÆöÈáèÂíåÂÆöÊÄßËØÑ‰º∞‰∏≠Âùá‰ºò‰∫éÂÖ∂‰ªñÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08239', 'title': 'Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources', 'url': 'https://huggingface.co/papers/2409.08239', 'abstract': 'Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.', 'score': 16, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': '7b45c82ece8d90d6', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#data', '#transfer_learning', '#benchmark', '#synthetic'], 'emoji': 'üß†', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Source2Synth –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö. Source2Synth —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–±—Ä–∞—Å—ã–≤–∞—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ –¥–≤—É—Ö —Å–ª–æ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö: –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.'}, 'en': {'title': 'Empowering LLMs with Synthetic Data for Enhanced Reasoning Skills', 'desc': 'This paper introduces Source2Synth, a novel method designed to enhance the capabilities of Large Language Models (LLMs) in complex tasks involving structured data and reasoning. The approach generates synthetic data points from a custom data source, incorporating intermediate reasoning steps that are based on real-world information. By filtering out low-quality outputs based on their answerability, Source2Synth significantly improves the quality of the training dataset. The effectiveness of this method is demonstrated through substantial performance gains in multi-hop question answering and tabular question answering tasks, achieving improvements of over 22% in accuracy compared to traditional fine-tuning methods.'}, 'zh': {'title': 'Source2SynthÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïSource2SynthÔºåÁî®‰∫éÊïôÂØºÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊñ∞ÊäÄËÉΩÔºåËÄåÊó†ÈúÄ‰æùËµñÊòÇË¥µÁöÑ‰∫∫Á±ªÊ†áÊ≥®„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËæìÂÖ•Ëá™ÂÆö‰πâÊï∞ÊçÆÊ∫êÔºåÁîüÊàêÂ∏¶Êúâ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÁöÑÂêàÊàêÊï∞ÊçÆÁÇπÔºåËøô‰∫õÊ≠•È™§Âü∫‰∫éÁúüÂÆû‰∏ñÁïåÁöÑÊù•Ê∫ê„ÄÇSource2SynthÈÄöËøá‰∏¢ÂºÉ‰ΩéË¥®ÈáèÁîüÊàêÁöÑÁ≠îÊ°àÊù•ÊèêÈ´òÊï∞ÊçÆÈõÜÁöÑË¥®Èáè„ÄÇÊàë‰ª¨Âú®‰∏§‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈ¢ÜÂüüËøõË°å‰∫ÜÊµãËØïÔºåÁªìÊûúÊòæÁ§∫ËØ•ÊñπÊ≥ïÂú®Ë°®Ê†ºÈóÆÁ≠îÔºàTQAÔºâÂíåÂ§öË∑≥ÈóÆÁ≠îÔºàMHQAÔºâ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08248', 'title': 'TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder', 'url': 'https://huggingface.co/papers/2409.08248', 'abstract': 'Recent breakthroughs in text-to-image models have opened up promising research avenues in personalized image generation, enabling users to create diverse images of a specific subject using natural language prompts. However, existing methods often suffer from performance degradation when given only a single reference image. They tend to overfit the input, producing highly similar outputs regardless of the text prompt. This paper addresses the challenge of one-shot personalization by mitigating overfitting, enabling the creation of controllable images through text prompts. Specifically, we propose a selective fine-tuning strategy that focuses on the text encoder. Furthermore, we introduce three key techniques to enhance personalization performance: (1) augmentation tokens to encourage feature disentanglement and alleviate overfitting, (2) a knowledge-preservation loss to reduce language drift and promote generalizability across diverse prompts, and (3) SNR-weighted sampling for efficient training. Extensive experiments demonstrate that our approach efficiently generates high-quality, diverse images using only a single reference image while significantly reducing memory and storage requirements.', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': 'c7e040a619639ae3', 'data': {'categories': ['#training', '#diffusion', '#optimization', '#cv'], 'emoji': 'üé®', 'ru': {'title': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–æ—á–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –≤–≤–æ–¥—è—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: —Ç–æ–∫–µ–Ω—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ SNR-—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ. –≠—Ç–∏ –ø–æ–¥—Ö–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–Ω–∏–∑–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —É–ª—É—á—à–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—é.'}, 'en': {'title': 'Enhancing One-Shot Personalization in Image Generation', 'desc': 'This paper presents a novel approach to improve personalized image generation from text prompts using only one reference image. It tackles the issue of overfitting, which leads to similar outputs regardless of the input text. The authors propose a selective fine-tuning strategy for the text encoder and introduce techniques like augmentation tokens, knowledge-preservation loss, and SNR-weighted sampling to enhance performance. Experimental results show that their method generates diverse, high-quality images efficiently while minimizing memory and storage needs.'}, 'zh': {'title': '‰∏ÄÂõæÂ§öÊ†∑Ôºå‰∏™ÊÄßÂåñÁîüÊàêÊñ∞Á™ÅÁ†¥', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÂú®‰∏™ÊÄßÂåñÂõæÂÉèÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂ¶Ç‰ΩïÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊèêÁ§∫ÁîüÊàêÁâπÂÆö‰∏ªÈ¢òÁöÑÂ§öÊ†∑ÂåñÂõæÂÉè„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®‰ªÖ‰ΩøÁî®Âçï‰∏ÄÂèÇËÄÉÂõæÂÉèÊó∂ÔºåÂ∏∏Â∏∏Âá∫Áé∞ÊÄßËÉΩ‰∏ãÈôçÂíåËøáÊãüÂêàÁöÑÈóÆÈ¢òÔºåÂØºËá¥ËæìÂá∫ÂõæÂÉè‰∏éËæìÂÖ•ÊèêÁ§∫È´òÂ∫¶Áõ∏‰ºº„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄâÊã©ÊÄßÂæÆË∞ÉÁ≠ñÁï•ÔºåÈáçÁÇπÂÖ≥Ê≥®ÊñáÊú¨ÁºñÁ†ÅÂô®ÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏âÁßçÂÖ≥ÈîÆÊäÄÊúØÊù•Â¢ûÂº∫‰∏™ÊÄßÂåñÊÄßËÉΩ„ÄÇÈÄöËøáËøô‰∫õÊäÄÊúØÔºåÁ†îÁ©∂Ë°®ÊòéÂèØ‰ª•ÊúâÊïàÁîüÊàêÈ´òË¥®Èáè„ÄÅÂ§öÊ†∑ÂåñÁöÑÂõæÂÉèÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ëÂÜÖÂ≠òÂíåÂ≠òÂÇ®ÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.07239', 'title': 'PiTe: Pixel-Temporal Alignment for Large Video-Language Model', 'url': 'https://huggingface.co/papers/2409.07239', 'abstract': 'Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform adequately due to the complexity of the relationship between language and spatial-temporal data structure. Recent Large Video-Language Models (LVidLMs) align feature of static visual data like image into latent space of language feature, by general multi-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we explore fine-grained alignment approach via object trajectory for different modalities across both spatial and temporal dimensions simultaneously. Thus, we propose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed PiTe, that exhibits promising applicable model property. To achieve fine-grained video-language alignment, we curate a multi-modal pre-training dataset PiTe-143k, the dataset provision of moving trajectories in pixel level for all individual objects, that appear and mention in the video and caption both, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates astounding capabilities on myriad video-related multi-modal tasks through beat the state-of-the-art methods by a large margin.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 11', 'zh': '9Êúà11Êó•'}, 'hash': '63af38e7f029dd60', 'data': {'categories': ['#video', '#dataset', '#cv', '#training', '#graphs', '#optimization', '#games', '#synthetic', '#multimodal'], 'emoji': 'üé•', 'ru': {'title': 'PiTe: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±—ä–µ–∫—Ç–æ–≤', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å PiTe, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç PiTe-143k —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π. –ú–æ–¥–µ–ª—å PiTe –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–∏–¥–µ–æ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Bridging Video and Language with Trajectory-Guided Alignment', 'desc': "This paper introduces a new model called PiTe, which stands for trajectory-guided Pixel-Temporal Alignment, aimed at improving the connection between video and language. It addresses the challenges faced by Large Video-Language Models (LVidLMs) in understanding the complex relationships between language and the dynamic nature of video data. The authors present a unique dataset, PiTe-143k, which includes detailed moving trajectories of objects in videos, enhancing the model's ability to align visual and textual information. PiTe outperforms existing models in various multi-modal tasks, showcasing its effectiveness in video-language alignment."}, 'zh': {'title': 'ËßÜÈ¢ë‰∏éËØ≠Ë®ÄÁöÑÁ≤æÁªÜÂØπÈΩêÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºåÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÊàê‰∏∫‰∫ÜËøûÊé•ÂõæÂÉèÂíåÊñáÊú¨ÁöÑÈáçË¶ÅËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËßÜÈ¢ëÁöÑÂ§çÊùÇÊÄß‰ΩøÂæóLVLMsÂú®Â§ÑÁêÜÊó∂Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑLVidLMÊ®°ÂûãÔºåÈÄöËøáËΩ®ËøπÂºïÂØºÁöÑÂÉèÁ¥†Êó∂Èó¥ÂØπÈΩêÔºàPiTeÔºâÔºåÂÆûÁé∞‰∫ÜËßÜÈ¢ëÂíåËØ≠Ë®ÄÁöÑÁ≤æÁªÜÂØπÈΩê„ÄÇÊàë‰ª¨ËøòÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜPiTe-143kÔºåÊèê‰æõ‰∫ÜËßÜÈ¢ë‰∏≠ÊâÄÊúâÂØπË±°ÁöÑÁßªÂä®ËΩ®ËøπÔºå‰ª•ÊîØÊåÅÂ§öÁßçËßÜÈ¢ëÁõ∏ÂÖ≥ÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08278', 'title': 'DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors', 'url': 'https://huggingface.co/papers/2409.08278', 'abstract': 'We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': '5e63dc8bd9635183', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': 'DreamHOI - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∞—Ä—Ç–∏–∫—É–ª—è—Ü–∏—é 3D-–º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–º–æ—â—å—é Score Distillation Sampling. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –¥–≤–æ–π–Ω–æ–µ –Ω–µ—è–≤–Ω–æ-—è–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏ —Å –∞–Ω–∏–º–∞—Ü–∏–µ–π, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Ä–∞–¥–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (NeRF) —Å —è–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–µ–π —Å–∫–µ–ª–µ—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏.'}, 'en': {'title': 'Realistic Human-Object Interactions from Text Descriptions', 'desc': "DreamHOI is a new method that allows a 3D human model to interact with various objects based on text descriptions, even when there is no prior data for those specific interactions. It addresses the challenge of limited datasets by using text-to-image diffusion models that have learned from a vast number of image-caption pairs. The method optimizes the movement of a human model's mesh by using Score Distillation Sampling (SDS) to guide the edits needed for realistic interactions. By combining neural radiance fields with traditional mesh articulation, DreamHOI effectively generates realistic human-object interactions in a zero-shot manner."}, 'zh': {'title': 'DreamHOIÔºöÂÆûÁé∞‰∫∫Êú∫‰∫§‰∫íÁöÑÈõ∂Ê†∑Êú¨ÂêàÊàê', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïDreamHOIÔºåÁî®‰∫éÈõ∂Ê†∑Êú¨ÂêàÊàê‰∫∫‰Ωì‰∏éÁâ©‰ΩìÁöÑ‰∫§‰∫íÔºàHOIsÔºâ„ÄÇËØ•ÊñπÊ≥ïÂÖÅËÆ∏3D‰∫∫Á±ªÊ®°ÂûãÊ†πÊçÆÊñáÊú¨ÊèèËø∞‰∏é‰ªª‰ΩïÁªôÂÆöÁâ©‰ΩìËøõË°åÈÄºÁúüÁöÑ‰∫§‰∫í„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨Âà©Áî®‰∫ÜÂú®Êï∞ÂçÅ‰∫øÂõæÂÉè-ÊñáÊú¨ÂØπ‰∏äËÆ≠ÁªÉÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ÂèåÈáçÈöêÂºè-ÊòæÂºèË°®Á§∫ÔºåÁªìÂêàÁ•ûÁªèËæêÂ∞ÑÂú∫ÔºàNeRFÔºâÂíåÈ™®Êû∂È©±Âä®ÁöÑÁΩëÊ†ºÂÖ≥ËäÇÔºå‰ºòÂåñ‰∫Ü‰∫∫Á±ªÊ®°ÂûãÁöÑÂÖ≥ËäÇÂä®‰Ωú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.08270', 'title': 'FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally', 'url': 'https://huggingface.co/papers/2409.08270', 'abstract': 'This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50times faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 12', 'zh': '9Êúà12Êó•'}, 'hash': 'a61a3bf3d33858ce', 'data': {'categories': ['#cv', '#math', '#optimization', '#benchmark', '#open_source', '#3d'], 'emoji': 'üéØ', 'ru': {'title': '–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è 3D Gaussian Splatting —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ 3D Gaussian Splatting –∏–∑ 2D –º–∞—Å–æ–∫. –í–º–µ—Å—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –ª–∏–Ω–µ–π–Ω–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª—å—Ñ–∞-—Å–º–µ—à–∏–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –æ–¥–∏–Ω —à–∞–≥. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ 50 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö —É–¥–∞–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞.'}, 'en': {'title': 'Fast and Robust 3D Segmentation with Linear Programming', 'desc': 'This paper presents a new method for segmenting 3D Gaussian Splatting (3D-GS) from 2D masks, addressing the inefficiencies of traditional iterative gradient descent approaches. The authors introduce a globally optimal solver that leverages the linear relationship between 2D mask rendering and Gaussian labels, allowing for a closed-form solution through linear programming. By incorporating background bias into the objective function, the method enhances robustness against noise in 3D segmentation tasks. The proposed optimization is significantly faster, completing in about 30 seconds, and demonstrates superior performance in various applications, including object removal and inpainting.'}, 'zh': {'title': 'È´òÊïàÈ≤ÅÊ£íÁöÑ3DÈ´òÊñØÂàÜÂâ≤ÊñπÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂Ëß£ÂÜ≥‰∫Ü‰ªé2DÊé©ËÜú‰∏≠ÂáÜÁ°ÆÂàÜÂâ≤3DÈ´òÊñØÁÇπ‰∫ëÁöÑÊåëÊàò„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñËø≠‰ª£Ê¢ØÂ∫¶‰∏ãÈôç‰∏∫ÊØè‰∏™È´òÊñØÂàÜÈÖçÂîØ‰∏ÄÊ†áÁ≠æÔºåÂØºËá¥‰ºòÂåñËøáÁ®ãÊº´Èïø‰∏îÁªìÊûú‰∏çÁêÜÊÉ≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰ΩÜÂÖ®Â±ÄÊúÄ‰ºòÁöÑ3D-GSÂàÜÂâ≤Ê±ÇËß£Âô®ÔºåÂà©Áî®Á∫øÊÄßËßÑÂàíÂú®Â∞ÅÈó≠ÂΩ¢Âºè‰∏≠Ëß£ÂÜ≥ÊúÄ‰ºòÊ†áÁ≠æÂàÜÈÖç„ÄÇÈÄöËøáÂ∞ÜËÉåÊôØÂÅèÂ∑ÆÁ∫≥ÂÖ•ÁõÆÊ†áÂáΩÊï∞ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®3DÂàÜÂâ≤‰∏≠ÂØπÂô™Â£∞Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ºòÂåñËøáÁ®ã‰ªÖÈúÄ30ÁßíÔºåÈÄüÂ∫¶ÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÂø´Á∫¶50ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2409.05162', 'title': 'Can OOD Object Detectors Learn from Foundation Models?', 'url': 'https://huggingface.co/papers/2409.05162', 'abstract': 'Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-08', 'pub_date_card': {'ru': '8 —Å–µ–Ω—Ç—è–±—Ä—è', 'en': 'September 8', 'zh': '9Êúà8Êó•'}, 'hash': '74ea126cddc6e29e', 'data': {'categories': ['#cv', '#training', '#data', '#optimization', '#benchmark', '#diffusion', '#synthetic'], 'emoji': 'üîç', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SyncOOD –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (OOD). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ OOD. SyncOOD –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ OOD –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—Ä–∞–∑—Ü—ã OOD –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ OOD, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏–π –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ (ID) –∏ –≤–Ω–µ –µ–≥–æ (OOD).'}, 'en': {'title': 'Enhancing OOD Detection with Synthetic Data from Generative Models', 'desc': 'This paper addresses the challenge of detecting out-of-distribution (OOD) objects, which is difficult due to the lack of available OOD data. The authors propose SyncOOD, a method that uses generative models like Stable Diffusion to create synthetic OOD samples from large-scale open-set data. By leveraging these synthetic samples, the method enhances the training of a lightweight OOD detector, improving its ability to distinguish between in-distribution (ID) and OOD data. The results show that SyncOOD achieves superior performance compared to existing techniques, setting new benchmarks with minimal reliance on synthetic data.'}, 'zh': {'title': 'Âà©Áî®ÁîüÊàêÊ®°ÂûãÊèêÂçáË∂ÖÂá∫ÂàÜÂ∏ÉÁâ©‰ΩìÊ£ÄÊµãÁöÑËÉΩÂäõ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ÁîüÊàêÊ®°ÂûãÊù•ÊîπÂñÑË∂ÖÂá∫ÂàÜÂ∏ÉÔºàOODÔºâÁâ©‰ΩìÊ£ÄÊµã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SyncOODÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ªéÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÊ®°Âûã‰∏≠ÊèêÂèñÊúâÊÑè‰πâÁöÑOODÊï∞ÊçÆÔºåÊù•ÂêàÊàêOODÊ†∑Êú¨„ÄÇËøôÊ†∑ÂèØ‰ª•Âà©Áî®Â§ßÂûãÂü∫Á°ÄÊ®°ÂûãÁöÑÂºÄÊîæ‰∏ñÁïåÁü•ËØÜÔºåÂ¢ûÂº∫OODÁâ©‰ΩìÊ£ÄÊµãÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSyncOODÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊÄßËÉΩ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment', '#architecture (2)', '#audio', '#benchmark (7)', '#cv (8)', '#data (3)', '#dataset (2)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs (3)', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (3)', '#optimization (5)', '#plp', '#rag', '#reasoning (4)', '#rl (1)', '#rlhf', '#robotics', '#science (3)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (4)', '#training (5)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-09-13 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-13 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-13 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    