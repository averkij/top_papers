
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. July 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 Ğ¸ÑĞ»Ñ</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-07.html">â¬…ï¸ <span id="prev-date">07.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-09.html">â¡ï¸ <span id="next-date">09.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'};
        let feedDateNext = {'ru': '09.07', 'en': '07/09', 'zh': '7æœˆ9æ—¥'};
        let feedDatePrev = {'ru': '07.07', 'en': '07/07', 'zh': '7æœˆ7æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.03724', 'title': 'MemOS: A Memory OS for AI System', 'url': 'https://huggingface.co/papers/2507.03724', 'abstract': 'MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.', 'score': 78, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': '5a64c779be945671', 'authors': ['Zhiyu Li', 'Shichao Song', 'Chenyang Xi', 'Hanyu Wang', 'Chen Tang', 'Simin Niu', 'Ding Chen', 'Jiawei Yang', 'Chunyu Li', 'Qingchen Yu', 'Jihao Zhao', 'Yezhaohui Wang', 'Peng Liu', 'Zehao Lin', 'Pengyuan Wang', 'Jiahao Huo', 'Tianyi Chen', 'Kai Chen', 'Kehang Li', 'Zhen Tao', 'Junpeng Ren', 'Huayi Lai', 'Hao Wu', 'Bo Tang', 'Zhenren Wang', 'Zhaoxin Fan', 'Ningyu Zhang', 'Linfeng Zhang', 'Junchi Yan', 'Mingchuan Yang', 'Tong Xu', 'Wei Xu', 'Huajun Chen', 'Haofeng Wang', 'Hongkang Yang', 'Wentao Zhang', 'Zhi-Qin John Xu', 'Siheng Chen', 'Feiyu Xiong'], 'affiliations': ['Beihang University', 'Institute for Advanced Algorithms Research, Shanghai', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Peking University', 'Renmin University of China', 'Research Institute of China Telecom', 'Shanghai Jiao Tong University', 'Tongji University', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.03724.jpg', 'data': {'categories': ['#training', '#agi', '#rag', '#long_context', '#optimization', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MemOS: Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MemOS - ÑÑ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. MemOS Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ MemCube ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¹ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM.'}, 'en': {'title': 'MemOS: Revolutionizing Memory Management for LLMs', 'desc': 'MemOS is a proposed memory operating system designed to improve memory management in Large Language Models (LLMs). It addresses the limitations of existing models that rely on static parameters and short-term context by introducing a structured memory layer that enhances storage and retrieval capabilities. The system utilizes MemCubes, which encapsulate memory content along with metadata, allowing for flexible transitions between different memory types. This approach not only increases computational efficiency but also supports continual learning and personalized modeling by managing knowledge across various temporal scales.'}, 'zh': {'title': 'MemOSï¼šä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ™ºèƒ½å†…å­˜ç®¡ç†', 'desc': 'MemOSæ˜¯ä¸€ç§ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„å†…å­˜æ“ä½œç³»ç»Ÿï¼Œæ—¨åœ¨æ”¹å–„å†…å­˜ç®¡ç†ã€‚å®ƒé€šè¿‡ç»Ÿä¸€è¡¨ç¤ºã€è°ƒåº¦å’Œæ¼”å˜ä¸åŒç±»å‹çš„å†…å­˜ï¼Œæ”¯æŒé«˜æ•ˆçš„å­˜å‚¨å’Œæ£€ç´¢ã€‚MemOSå¼•å…¥äº†MemCubeä½œä¸ºåŸºæœ¬å•å…ƒï¼Œå°è£…äº†å†…å­˜å†…å®¹å’Œå…ƒæ•°æ®ï¼Œå…è®¸çµæ´»çš„å†…å­˜ç±»å‹è½¬æ¢ã€‚è¯¥ç³»ç»Ÿä¸ºLLMsæä¾›äº†å¯æ§æ€§ã€å¯å¡‘æ€§å’Œå¯æ¼”åŒ–æ€§ï¼Œä¿ƒè¿›äº†æŒç»­å­¦ä¹ å’Œä¸ªæ€§åŒ–å»ºæ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00994', 'title': 'Should We Still Pretrain Encoders with Masked Language Modeling?', 'url': 'https://huggingface.co/papers/2507.00994', 'abstract': 'Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.', 'score': 56, 'issue_id': 4700, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': '3445ad02ac25de31', 'authors': ['Hippolyte Gisserot-Boukhlef', 'Nicolas Boizard', 'Manuel Faysse', 'Duarte M. Alves', 'Emmanuel Malherbe', 'AndrÃ© F. T. Martins', 'CÃ©line Hudelot', 'Pierre Colombo'], 'affiliations': ['Artefact Research Center', 'Diabolocom', 'Equall', 'Illuin Technology', 'Instituto Superior TÃ©cnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de TelecomunicaÃ§Ãµes', 'MICS, CentraleSupÃ©lec, UniversitÃ© Paris-Saclay', 'Unbabel'], 'pdf_title_img': 'assets/pdf/title_img/2507.00994.jpg', 'data': {'categories': ['#survey', '#optimization', '#training', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ CLM Ğ¸ MLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (MLM) Ğ¸ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (CLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLM Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼ Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ CLM Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ CLM Ğ¸ MLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ.'}, 'en': {'title': 'Unlocking Text Representation: CLM Meets MLM for Optimal Performance', 'desc': 'This paper investigates the effectiveness of Causal Language Modeling (CLM) compared to traditional Masked Language Modeling (MLM) for training text representations in natural language processing (NLP). The authors conduct extensive experiments with various model sizes and training strategies, revealing that while MLM generally performs better, CLM models are more efficient with data and offer greater stability during fine-tuning. They propose a biphasic training approach that first uses CLM and then MLM, which optimizes performance within a limited computational budget. Additionally, leveraging existing pretrained CLM models can significantly reduce the resources needed to achieve high-quality encoder models.'}, 'zh': {'title': 'åŒç›¸è®­ç»ƒç­–ç•¥ï¼šCLMä¸MLMçš„æœ€ä½³ç»“åˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMï¼‰é¢„è®­ç»ƒçš„è§£ç å™¨æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºç¼–ç å™¨ï¼Œä¸”åœ¨æ–‡æœ¬è¡¨ç¤ºåŸºå‡†æµ‹è¯•ä¸­å¸¸å¸¸è¶…è¶Šä¼ ç»Ÿçš„ç¼–ç å™¨ã€‚å°½ç®¡ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰è®­ç»ƒé€šå¸¸åœ¨æ–‡æœ¬è¡¨ç¤ºä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œä½†CLMè®­ç»ƒçš„æ¨¡å‹åœ¨æ•°æ®æ•ˆç‡å’Œå¾®è°ƒç¨³å®šæ€§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒç›¸è®­ç»ƒç­–ç•¥ï¼Œå…ˆåº”ç”¨CLMå†åº”ç”¨MLMï¼Œåœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—ä¸‹å®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒCLMæ¨¡å‹åˆå§‹åŒ–æ—¶ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®­ç»ƒé¡¶çº§ç¼–ç å™¨æ¨¡å‹çš„è®¡ç®—è´Ÿæ‹…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05163', 'title': '4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture', 'url': 'https://huggingface.co/papers/2507.05163', 'abstract': 'A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.', 'score': 32, 'issue_id': 4693, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'e1f4c8e83495db53', 'authors': ['Yutian Chen', 'Shi Guo', 'Tianshuo Yang', 'Lihe Ding', 'Xiuyuan Yu', 'Jinwei Gu', 'Tianfan Xue'], 'affiliations': ['NVIDIA', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.05163.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#3d', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ°Ñ 4D-ÑÑŠĞµĞ¼ĞºĞ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ 4D-ÑÑŠĞµĞ¼ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ¾ 100-200 FPS Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑÑŠĞµĞ¼ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€. Ğ”Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ¼Ñƒ Ñ‡Ğ¸ÑĞ»Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑŠĞµĞ¼ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Revolutionizing 4D Capture: High-Speed Reconstruction with Low FPS Cameras', 'desc': 'This paper presents a novel system for capturing high-speed 4D scenes using low frame rate cameras. It introduces an asynchronous capture technique that effectively increases the frame rate by staggering the start times of multiple cameras, achieving rates of 100-200 FPS from a base of 25 FPS. Additionally, the authors propose a video-diffusion-based generative model to correct artifacts in the sparse 4D reconstruction, ensuring better detail and temporal consistency. Experimental results show that this approach significantly improves the quality of high-speed 4D reconstructions compared to traditional synchronous methods.'}, 'zh': {'title': 'ä½å¸§ç‡ç›¸æœºå®ç°é«˜é€Ÿåº¦4Dé‡å»ºçš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜é€Ÿåº¦çš„4Dæ•æ‰ç³»ç»Ÿï¼Œåˆ©ç”¨ä½å¸§ç‡ç›¸æœºè¿›è¡Œå¼‚æ­¥æ•æ‰å’Œè§†é¢‘æ‰©æ•£åŸºç¡€çš„ä¼ªå½±ä¿®æ­£ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡ã€‚ä¼ ç»Ÿçš„4Dæ•æ‰ç³»ç»Ÿé€šå¸¸å¸§ç‡ä½äº30 FPSï¼Œç›´æ¥ä»ä½å¸§ç‡è¾“å…¥è¿›è¡Œé«˜é€Ÿåº¦è¿åŠ¨çš„4Dé‡å»ºä¼šå¯¼è‡´ä¸ç†æƒ³çš„ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼‚æ­¥æ•æ‰æ–¹æ¡ˆï¼Œå°†ç›¸æœºçš„å¯åŠ¨æ—¶é—´é”™å¼€ï¼Œæå‡äº†æœ‰æ•ˆå¸§ç‡ï¼Œè¾¾åˆ°100-200 FPSçš„æ•ˆæœã€‚å¤„ç†æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¿®å¤4Dç¨€ç–è§†å›¾é‡å»ºä¸­äº§ç”Ÿçš„ä¼ªå½±ï¼Œæ˜¾è‘—æ”¹å–„äº†é‡å»ºçš„ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04447', 'title': 'DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge', 'url': 'https://huggingface.co/papers/2507.04447', 'abstract': 'DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.', 'score': 29, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ»Ñ', 'en': 'July 6', 'zh': '7æœˆ6æ—¥'}, 'hash': '8fbe1a8248768baa', 'authors': ['Wenyao Zhang', 'Hongsi Liu', 'Zekun Qi', 'Yunnan Wang', 'XinQiang Yu', 'Jiazhao Zhang', 'Runpei Dong', 'Jiawei He', 'He Wang', 'Zhizheng Zhang', 'Li Yi', 'Wenjun Zeng', 'Xin Jin'], 'affiliations': ['EIT', 'Galbot', 'PKU', 'SJTU', 'THU', 'UIUC', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2507.04447.jpg', 'data': {'categories': ['#robotics', '#reasoning', '#training', '#optimization', '#multimodal', '#diffusion', '#agents', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DreamVLA: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'DreamVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DreamVLA Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Robot Manipulation with DreamVLA: Clear Action Planning through World Knowledge', 'desc': 'DreamVLA is a new framework designed to enhance robot manipulation by integrating world knowledge with a vision-language-action (VLA) model. It addresses the limitations of existing methods by providing a clear and organized representation of dynamic, spatial, and semantic information, which is crucial for effective action planning. The framework uses a dynamic-region-guided approach to predict world knowledge, allowing robots to form abstract reasoning chains before executing actions. Additionally, a diffusion-based transformer is employed to ensure that action representations remain distinct and free from interference during training, leading to improved performance in both real-world and simulated tasks.'}, 'zh': {'title': 'DreamVLAï¼šæå‡æœºå™¨äººæ“ä½œçš„æ™ºèƒ½æ¡†æ¶', 'desc': 'DreamVLA æ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå…¨é¢çš„ä¸–ç•ŒçŸ¥è¯†æ¥æ”¹å–„æœºå™¨äººæ“ä½œã€‚å®ƒå¼•å…¥äº†åŠ¨æ€åŒºåŸŸå¼•å¯¼çš„ä¸–ç•ŒçŸ¥è¯†é¢„æµ‹ï¼Œç»“åˆç©ºé—´å’Œè¯­ä¹‰çº¿ç´¢ï¼Œä¸ºåŠ¨ä½œè§„åˆ’æä¾›äº†ç´§å‡‘è€Œå…¨é¢çš„è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é€šè¿‡å—çŠ¶ç»“æ„æ³¨æ„æœºåˆ¶ï¼Œå‡å°‘åŠ¨æ€ã€ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ä¹‹é—´çš„å¹²æ‰°ï¼Œç¡®ä¿æ¯ä¸ªè¡¨ç¤ºçš„æ¸…æ™°å’Œåˆ†ç¦»ã€‚æ­¤å¤–ï¼ŒDreamVLA é‡‡ç”¨æ‰©æ•£å¼å˜æ¢å™¨æ¥å»ºæ¨¡æœªæ¥åŠ¨ä½œçš„æ¡ä»¶åˆ†å¸ƒï¼Œä»è€Œæé«˜äº†æœºå™¨äººä»»åŠ¡çš„æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05197', 'title': 'Pre-Trained Policy Discriminators are General Reward Models', 'url': 'https://huggingface.co/papers/2507.05197', 'abstract': 'A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  \t\t\t\t\tAI-generated summary \t\t\t\t We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.', 'score': 27, 'issue_id': 4694, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '88d62db0ed894120', 'authors': ['Shihan Dou', 'Shichun Liu', 'Yuming Yang', 'Yicheng Zou', 'Yunhua Zhou', 'Shuhao Xing', 'Chenhao Huang', 'Qiming Ge', 'Demin Song', 'Haijun Lv', 'Songyang Gao', 'Chengqi Lv', 'Enyu Zhou', 'Honglin Guo', 'Zhiheng Xi', 'Wenwei Zhang', 'Qipeng Guo', 'Qi Zhang', 'Xipeng Qiu', 'Xuanjing Huang', 'Tao Gui', 'Kai Chen'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.05197.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'POLAR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Policy Discriminative Learning (POLAR). POLAR Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ POLAR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'POLAR: Revolutionizing Reward Modeling through Policy Comparison', 'desc': 'The paper introduces Policy Discriminative Learning (POLAR), a new method for reward modeling in reinforcement learning that focuses on comparing policies rather than relying on absolute preferences. By treating reward modeling as a policy discriminator, POLAR effectively generates reward signals that guide the training policy towards a target policy with desired behaviors. This approach allows for scalable pre-training of reward models, which can discern between similar and different policies, enhancing their performance significantly. Empirical results demonstrate that POLAR outperforms traditional methods, showing improved accuracy and robust generalization in various tasks.'}, 'zh': {'title': 'POLARï¼šæå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'POLARæ˜¯ä¸€ç§å¯æ‰©å±•çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡ç­–ç•¥æ¯”è¾ƒæ¥å¢å¼ºå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½å¹¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚å®ƒå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºç­–ç•¥é‰´åˆ«å™¨ï¼Œé‡åŒ–ä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼ŒæŒ‡å¯¼è®­ç»ƒç­–ç•¥æœå‘å…·æœ‰æœŸæœ›è¡Œä¸ºçš„ç›®æ ‡ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ç»å¯¹åå¥½æ–¹æ³•ä¸åŒï¼ŒPOLARæ•æ‰ä¸€ä¸ªç­–ç•¥ä¸ä»»æ„ç›®æ ‡ç­–ç•¥ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œé€‚åˆäºå»ºæ¨¡é€šç”¨çš„æ’åå…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPOLARæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„éé¢„è®­ç»ƒæ–¹æ³•ï¼Œæå‡äº†å¥–åŠ±æ¨¡å‹çš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03483', 'title': 'BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset', 'url': 'https://huggingface.co/papers/2507.03483', 'abstract': "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.", 'score': 20, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': 'a916ca78a2bd6196', 'authors': ['Zhiheng Xi', 'Guanyu Li', 'Yutao Fan', 'Honglin Guo', 'Yufang Liu', 'Xiaoran Fan', 'Jiaqi Liu', 'Jingchao Ding', 'Wangmeng Zuo', 'Zhenfei Yin', 'Lei Bai', 'Tao Ji', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['East China Normal University', 'Fudan University', 'Harbin Institute of Technology', 'Oxford', 'Shanghai AI Laboratory', 'University of Sydney', 'Yimudata'], 'pdf_title_img': 'assets/pdf/title_img/2507.03483.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#dataset', '#benchmark', '#open_source', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BMMR - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 110 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾ 300 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµÑÑ‚Ñ‹, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ BMMR-Verifier Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Multimodal Reasoning with BMMR Dataset', 'desc': "This paper presents BMMR, a comprehensive dataset designed to enhance the reasoning abilities of large multimodal models (LMMs) across various disciplines. It includes 110,000 college-level questions from 300 subjects, formatted in multiple-choice, fill-in-the-blank, and open-ended styles, sourced from both print and digital media. The dataset is divided into BMMR-Eval for evaluation and BMMR-Train for training, with a focus on improving reasoning in diverse domains beyond just mathematics. Additionally, the authors introduce BMMR-Verifier, a tool for detailed assessment of reasoning paths, revealing significant gaps in current models' performance and highlighting the need for further research in multidisciplinary reasoning."}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„è·¨å­¦ç§‘æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†BMMRï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘æ¨ç†æ•°æ®é›†ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘å’Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚è¯¥æ•°æ®é›†åŒ…å«110,000ä¸ªå¤§å­¦æ°´å¹³çš„é—®é¢˜ï¼Œæ¶µç›–300ä¸ªè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡å®šä¹‰çš„å­¦ç§‘ï¼Œé—®é¢˜å½¢å¼å¤šæ ·ï¼ŒåŒ…æ‹¬é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜å’Œå¼€æ”¾å¼é—®ç­”ã€‚æ•°æ®ç»è¿‡äººå·¥ç­›é€‰å’Œè¿‡æ»¤ï¼Œå¹¶ä¸ºæ¯ä¸ªå®ä¾‹é…å¤‡é«˜è´¨é‡çš„æ¨ç†è·¯å¾„ï¼Œåˆ†ä¸ºBMMR-Evalå’ŒBMMR-Trainä¸¤éƒ¨åˆ†ï¼Œä»¥æ”¯æŒå¤šå­¦ç§‘çŸ¥è¯†å’Œæ¨ç†çš„è¯„ä¼°ä¸ç ”ç©¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºè¿‡ç¨‹çš„å¤šå­¦ç§‘éªŒè¯å™¨ï¼ˆBMMR-Verifierï¼‰ï¼Œç”¨äºå¯¹æ¨ç†è·¯å¾„è¿›è¡Œå‡†ç¡®å’Œç»†è‡´çš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.02029', 'title': 'RoboBrain 2.0 Technical Report', 'url': 'https://huggingface.co/papers/2507.02029', 'abstract': 'RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.', 'score': 18, 'issue_id': 4698, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': '5d72b9df64404714', 'authors': ['BAAI RoboBrain Team', 'Mingyu Cao', 'Huajie Tan', 'Yuheng Ji', 'Minglan Lin', 'Zhiyu Li', 'Zhou Cao', 'Pengwei Wang', 'Enshen Zhou', 'Yi Han', 'Yingbo Tang', 'Xiangqi Xu', 'Wei Guo', 'Yaoxu Lyu', 'Yijie Xu', 'Jiayu Shi', 'Mengfei Du', 'Cheng Chi', 'Mengdi Zhao', 'Xiaoshuai Hao', 'Junkai Zhao', 'Xiaojie Zhang', 'Sh/anyu Rong', 'Huaihai Lyu', 'Zhengliang Cai', 'Yankai Fu', 'Ning Chen', 'Bolun Zhang', 'Lingfeng Zhang', 'Shuyi Zhang', 'Xi Feng', 'Songjing Wang', 'Xiaodan Liu', 'Yance Jiao', 'Mengsi Lyu', 'Zhuo Chen', 'Chenrui He', 'Yulong Ao', 'Xue Sun', 'Zheqi He', 'Jingshu Zheng', 'Xi Yang', 'Donghai Shi', 'Kunchang Xie', 'Bochao Zhang', 'Shaokai Nie', 'Chunlei Men', 'Yonghua Lin', 'Zhongyuan Wang', 'Tiejun Huang', 'Shanghang Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.02029.jpg', 'data': {'categories': ['#agi', '#reasoning', '#architecture', '#benchmark', '#training', '#agents', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboBrain 2.0: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'RoboBrain 2.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… 7B Ğ¸ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². RoboBrain 2.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'RoboBrain 2.0: Unifying Vision and Language for Advanced Embodied AI', 'desc': 'RoboBrain 2.0 is a cutting-edge vision-language foundation model that integrates perception, reasoning, and planning for complex tasks in physical environments. It features a heterogeneous architecture with both a vision encoder and a language model, available in two sizes: a lightweight 7B model and a powerful 32B model. The 32B variant excels in various embodied reasoning tasks, outperforming previous models in spatial and temporal benchmarks. This model aims to enhance embodied AI research and facilitate the development of generalist embodied agents, with resources available for further exploration.'}, 'zh': {'title': 'RoboBrain 2.0ï¼šæ¨åŠ¨å®ä½“AIçš„æœªæ¥', 'desc': 'RoboBrain 2.0 æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’ï¼Œä»¥åº”å¯¹å¤æ‚çš„å®ä½“ä»»åŠ¡ã€‚å®ƒæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šè½»é‡çº§çš„7Bæ¨¡å‹å’Œå…¨è§„æ¨¡çš„32Bæ¨¡å‹ï¼Œé‡‡ç”¨å¼‚æ„æ¶æ„ï¼Œç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ã€‚å°½ç®¡ä½“ç§¯å°ï¼ŒRoboBrain 2.0 åœ¨å¤šç§å®ä½“æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´å’Œæ—¶é—´åŸºå‡†æµ‹è¯•ä¸­ï¼Œ32Bç‰ˆæœ¬çš„æ€§èƒ½é¢†å…ˆäºä¹‹å‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ”¯æŒå…³é”®çš„ç°å®ä¸–ç•Œå®ä½“AIèƒ½åŠ›ï¼Œå¦‚ç©ºé—´ç†è§£å’Œæ—¶é—´å†³ç­–ï¼Œæ¨åŠ¨äº†å®ä½“AIç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03253', 'title': 'RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs', 'url': 'https://huggingface.co/papers/2507.03253', 'abstract': 'RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose RefineX, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines.', 'score': 14, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': '6f3d1aa17a4188e7', 'authors': ['Baolong Bi', 'Shenghua Liu', 'Xingzhang Ren', 'Dayiheng Liu', 'Junyang Lin', 'Yiwei Wang', 'Lingrui Mei', 'Junfeng Fang', 'Jiafeng Guo', 'Xueqi Cheng'], 'affiliations': ['Alibaba Group', 'Institute of Computing Technology, Chinese Academy of Sciences', 'National University of Singapore', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2507.03253.jpg', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'RefineX: Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'RefineX - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. RefineX Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RefineX, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'RefineX: Precision Editing for Superior Language Model Training', 'desc': 'RefineX is a new framework designed to enhance the quality of pre-training data for large language models (LLMs) through targeted programmatic editing. It addresses the challenge of improving data quality at scale by allowing for precise modifications rather than broad document-level changes. This method preserves the diversity and naturalness of the text while ensuring efficient processing. Evaluations show that models trained with RefineX consistently outperform those trained on raw or traditionally refined data across various tasks, demonstrating its effectiveness in optimizing pre-training data.'}, 'zh': {'title': 'RefineXï¼šæå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡çš„å¯æ‰©å±•æ¡†æ¶', 'desc': 'RefineXæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç¨‹åºåŒ–ç¼–è¾‘æé«˜å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†æ•°æ®è´¨é‡æå‡ä¸å¤„ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œèƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆä¸”ç»†è‡´çš„æ•°æ®ç²¾ç‚¼ã€‚RefineXé€šè¿‡æœ€å°åŒ–ç¼–è¾‘çš„åˆ é™¤ç¨‹åºï¼Œæç‚¼å‡ºé«˜è´¨é‡çš„ä¸“å®¶æŒ‡å¯¼çš„ç«¯åˆ°ç«¯ç²¾ç‚¼ç»“æœï¼Œä»è€Œç³»ç»Ÿæ€§åœ°æ”¹å–„è¯­æ–™åº“ä¸­çš„æ¯ä¸ªå®ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼ŒRefineXåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä½¿ç”¨åŸå§‹ã€è¿‡æ»¤æˆ–å…¶ä»–ç²¾ç‚¼æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04009', 'title': 'Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents', 'url': 'https://huggingface.co/papers/2507.04009', 'abstract': 'A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.', 'score': 13, 'issue_id': 4698, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ»Ñ', 'en': 'July 5', 'zh': '7æœˆ5æ—¥'}, 'hash': 'fd1930ff40937fac', 'authors': ['Ziyang Miao', 'Qiyu Sun', 'Jingyuan Wang', 'Yuchen Gong', 'Yaowei Zheng', 'Shiqi Li', 'Richong Zhang'], 'affiliations': ['Independent Researcher', 'School of Computer Science and Engineering, Beihang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.04009.jpg', 'data': {'categories': ['#synthetic', '#data', '#dataset', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Easy Dataset', 'desc': 'Easy Dataset - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹. Ğ—Ğ°Ñ‚ĞµĞ¼ Easy Dataset Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming Unstructured Data into Domain-Specific Knowledge', 'desc': "The paper introduces Easy Dataset, a framework designed to create fine-tuning data from unstructured documents using a user-friendly graphical interface and large language models (LLMs). It addresses the challenge of obtaining high-quality domain-specific data by allowing users to configure text extraction and chunking methods easily. The framework employs a persona-driven prompting technique to generate varied question-answer pairs, enhancing the dataset's richness. Experiments demonstrate that fine-tuning LLMs with this synthesized data significantly boosts their performance in specific domains while retaining their general knowledge capabilities."}, 'zh': {'title': 'è½»æ¾åˆæˆæ•°æ®ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'Easy Datasetæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–æ–‡æ¡£ä¸­åˆæˆå¾®è°ƒæ•°æ®ï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨çŸ¥è¯†ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›´è§‚çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾é…ç½®æ–‡æœ¬æå–æ¨¡å‹å’Œåˆ†å—ç­–ç•¥ï¼Œå°†åŸå§‹æ–‡æ¡£è½¬åŒ–ä¸ºè¿è´¯çš„æ–‡æœ¬å—ã€‚æ¥ç€ï¼ŒEasy Datasetåˆ©ç”¨åŸºäºè§’è‰²çš„æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œä½¿ç”¨å…¬å¼€å¯ç”¨çš„LLMsã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒLLMsæ˜¾è‘—æé«˜äº†å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03745', 'title': 'StreamDiT: Real-Time Streaming Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2507.03745', 'abstract': 'A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href="https://cumulo-autumn.github.io/StreamDiT/">this https URL.</a>', 'score': 13, 'issue_id': 4696, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': '8ca0a425a1ddc352', 'authors': ['Akio Kodaira', 'Tingbo Hou', 'Ji Hou', 'Masayoshi Tomizuka', 'Yue Zhao'], 'affiliations': ['Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.03745.jpg', 'data': {'categories': ['#diffusion', '#video', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'StreamDiT: Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'StreamDiT - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ StreamDiT Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ flow matching Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰ĞµĞ³Ğ¾ÑÑ Ğ±ÑƒÑ„ĞµÑ€Ğ° Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ±ÑƒÑ„ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ 16 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 512p.'}, 'en': {'title': 'Real-Time Video Generation with StreamDiT', 'desc': 'StreamDiT is a novel streaming video generation model that leverages transformer-based diffusion techniques to create high-quality videos in real-time. Unlike traditional models that generate short clips offline, StreamDiT can produce continuous video streams, making it suitable for interactive applications. The model employs flow matching with a moving buffer and mixed training strategies to enhance both content consistency and visual fidelity. With 4 billion parameters, StreamDiT achieves impressive performance, generating videos at 16 frames per second while maintaining a resolution of 512p.'}, 'zh': {'title': 'å®æ—¶è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šStreamDiT', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStreamDiTçš„æµåª’ä½“è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºå˜æ¢å™¨çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒé«˜å†…å®¹ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚é€šè¿‡å¼•å…¥ç§»åŠ¨ç¼“å†²åŒºï¼ŒStreamDiTçš„è®­ç»ƒé‡‡ç”¨æµåŒ¹é…çš„æ–¹æ³•ï¼Œè®¾è®¡äº†ä¸åŒåˆ†åŒºæ–¹æ¡ˆçš„æ··åˆè®­ç»ƒï¼Œä»¥æå‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†å…·æœ‰ä¸åŒæ—¶é—´åµŒå…¥å’Œçª—å£æ³¨æ„åŠ›çš„adaLN DiTæ¶æ„ï¼Œå¹¶é€šè¿‡å¤šæ­¥è’¸é¦æ–¹æ³•ä¼˜åŒ–æ€§èƒ½ã€‚æœ€ç»ˆï¼Œç»è¿‡è’¸é¦çš„æ¨¡å‹åœ¨å•ä¸ªGPUä¸Šä»¥16å¸§æ¯ç§’çš„é€Ÿåº¦å®ç°äº†å®æ—¶æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆ512påˆ†è¾¨ç‡çš„è§†é¢‘æµã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05108', 'title': 'Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration', 'url': 'https://huggingface.co/papers/2507.05108', 'abstract': "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement to 94.25\\% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.", 'score': 10, 'issue_id': 4698, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '1fe680b6a13d9932', 'authors': ['Yuyi Zhang', 'Peirong Zhang', 'Zhenhua Yang', 'Pengyu Yan', 'Yongxin Shi', 'Pengwei Liu', 'Fengjun Guo', 'Lianwen Jin'], 'affiliations': ['INTSIG-SCUT Joint Lab on Document Analysis and Recognition', 'Intsig Information Co., Ltd.', 'SCUT-Zhuhai Institute of Modern Industrial Innovation', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2507.05108.jpg', 'data': {'categories': ['#architecture', '#cv', '#multimodal', '#dataset', '#data'], 'emoji': 'ğŸ“œ', 'ru': {'title': 'AutoHDR: Ğ’Ğ¾Ğ·Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AutoHDR. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FPHDR, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. AutoHDR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ OCR, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ OCR Ğ´Ğ»Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 84.05% Ğ±ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ 94.25% Ğ¿Ñ€Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Historical Document Restoration with AutoHDR', 'desc': 'This paper introduces a new approach to restoring historical documents that have been damaged over time. It presents a dataset called FPHDR, which includes thousands of images with detailed annotations for various damage levels. The proposed method, AutoHDR, uses a three-stage process that combines damage detection, text prediction, and appearance restoration, mimicking the work of historians. The results show a significant improvement in OCR accuracy, demonstrating the effectiveness of this automated solution in preserving cultural heritage.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–å†å²æ–‡çŒ®ä¿®å¤çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'å†å²æ–‡çŒ®æ˜¯å®è´µçš„æ–‡åŒ–é—äº§ï¼Œä½†ç”±äºæ’•è£‚ã€æ°´ä¾µèš€å’Œæ°§åŒ–ç­‰åŸå› ï¼Œç»å†äº†ä¸¥é‡çš„é€€åŒ–ã€‚ç°æœ‰çš„å†å²æ–‡çŒ®ä¿®å¤æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¨¡æ€æˆ–æœ‰é™è§„æ¨¡çš„ä¿®å¤ï¼Œæ— æ³•æ»¡è¶³å®é™…éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¡µå†å²æ–‡çŒ®ä¿®å¤æ•°æ®é›†ï¼ˆFPHDRï¼‰å’Œä¸€ç§æ–°é¢–çš„è‡ªåŠ¨åŒ–ä¿®å¤è§£å†³æ–¹æ¡ˆï¼ˆAutoHDRï¼‰ã€‚AutoHDRé€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ¨¡æ‹Ÿå†å²å­¦å®¶çš„ä¿®å¤å·¥ä½œæµç¨‹ï¼Œæ˜¾è‘—æé«˜äº†ä¸¥é‡æŸåæ–‡æ¡£çš„OCRå‡†ç¡®ç‡ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ–å†å²æ–‡çŒ®ä¿®å¤çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.02659', 'title': 'OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding', 'url': 'https://huggingface.co/papers/2507.02659', 'abstract': "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the ``one drafter for all'' paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.", 'score': 10, 'issue_id': 4694, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ»Ñ', 'en': 'July 3', 'zh': '7æœˆ3æ—¥'}, 'hash': '356734d41c5a5e65', 'authors': ['Ramchalam Kinattinkara Ramakrishnan', 'Zhaocong Yuan', 'Shaojie Zhuo', 'Chen Feng', 'Yicheng Lin', 'Chenzheng Su', 'Xiaopeng Zhang'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.02659.jpg', 'data': {'categories': ['#optimization', '#games', '#inference', '#training', '#reasoning', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ´Ğ¸Ğ½ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ…: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'OmniDraft - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ĞºÑÑˆ n-Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ. OmniDraft Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ LLM-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ²Ğ°Ğ¶Ğ½Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'One Draft Model for All Target Models!', 'desc': 'OmniDraft is a new framework designed to solve the problem of cross-vocabulary mismatch between draft models and target models in machine learning applications. It allows a single draft model to work with different target models dynamically, improving decoding speed and efficiency. The framework uses an online n-gram cache and hybrid distillation fine-tuning to adapt to user data and enhance performance. OmniDraft is particularly beneficial for on-device large language model (LLM) applications, where it can significantly reduce latency and improve user customization.'}, 'zh': {'title': 'ä¸€ä¸ªè‰ç¨¿æ¨¡å‹ï¼Œé€‚é…æ‰€æœ‰ç›®æ ‡æ¨¡å‹', 'desc': 'OmniDraftæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨è¯æ±‡ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶é€šè¿‡å…è®¸å•ä¸€è‰ç¨¿æ¨¡å‹ä¸å¤šç§ç›®æ ‡æ¨¡å‹åŠ¨æ€äº¤äº’æ¥æé«˜è§£ç é€Ÿåº¦ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºåœ¨çº¿éƒ¨ç½²ç¯å¢ƒï¼Œèƒ½å¤Ÿä½¿å•ä¸€è‰ç¨¿æ¨¡å‹ä¸ä»»ä½•ç›®æ ‡æ¨¡å‹å…¼å®¹ï¼Œå¹¶æ ¹æ®ç”¨æˆ·æ•°æ®åŠ¨æ€è°ƒæ•´ã€‚é€šè¿‡å¼•å…¥åœ¨çº¿n-gramç¼“å­˜å’Œæ··åˆè’¸é¦å¾®è°ƒï¼ŒOmniDraftæœ‰æ•ˆè§£å†³äº†è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ã€ç¼–ç å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†è§£ç æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03683', 'title': 'On the rankability of visual embeddings', 'url': 'https://huggingface.co/papers/2507.03683', 'abstract': "Visual embedding models often capture continuous, ordinal attributes along specific axes, enabling effective image ranking with minimal supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term _rank axes_. We define a model as _rankable_ for an attribute if projecting embeddings onto such an axis preserves the attribute's order. Across 7 popular encoders and 9 datasets with attributes like age, crowd count, head pose, aesthetics, and recency, we find that many embeddings are inherently rankable. Surprisingly, a small number of samples, or even just two extreme examples, often suffice to recover meaningful rank axes, without full-scale supervision. These findings open up new use cases for image ranking in vector databases and motivate further study into the structure and learning of rankable embeddings. Our code is available at https://github.com/aktsonthalia/rankable-vision-embeddings.", 'score': 9, 'issue_id': 4707, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': 'ddaccaf820821b2e', 'authors': ['Ankit Sonthalia', 'Arnas Uselis', 'Seong Joon Oh'], 'affiliations': ['TÃ¼bingen AI Center, UniversitÃ¤t TÃ¼bingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2507.03683.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ¾Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾ÑÑĞ¼Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ 'Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹' Ğ´Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°, ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ğ°ĞºÑƒÑ Ğ¾ÑÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· 7 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ 9 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ²Ğ¾ĞµĞ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ²ÑƒÑ… ĞºÑ€Ğ°Ğ¹Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¾ÑĞµĞ¹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼."}, 'en': {'title': 'Unlocking Image Ranking with Minimal Supervision', 'desc': "This paper investigates how visual embedding models can represent continuous and ordinal attributes along specific directions, referred to as 'rank axes'. It defines a model as 'rankable' if projecting its embeddings onto these axes maintains the order of the attributes. The study reveals that many popular encoders can produce rankable embeddings across various datasets, and interestingly, only a few samples are needed to identify these rank axes effectively. This research highlights the potential for improved image ranking in vector databases and encourages further exploration of rankable embeddings."}, 'zh': {'title': 'è§†è§‰åµŒå…¥æ¨¡å‹çš„æ’åèƒ½åŠ›æ¢ç´¢', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰åµŒå…¥æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ²¿ç€çº¿æ€§æ–¹å‘æ•æ‰è¿ç»­çš„ã€æœ‰åºçš„å±æ€§ï¼Œè¿™äº›æ–¹å‘è¢«ç§°ä¸º_æ’åè½´_ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ¨¡å‹ä¸º_å¯æ’å_ï¼Œå¦‚æœå°†åµŒå…¥æŠ•å½±åˆ°è¿™æ ·çš„è½´ä¸Šèƒ½å¤Ÿä¿æŒå±æ€§çš„é¡ºåºã€‚é€šè¿‡å¯¹7ç§æµè¡Œç¼–ç å™¨å’Œ9ä¸ªæ•°æ®é›†çš„åˆ†æï¼Œå‘ç°è®¸å¤šåµŒå…¥æœ¬è´¨ä¸Šæ˜¯å¯æ’åçš„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°‘é‡æ ·æœ¬ï¼Œç”šè‡³ä»…ä»…ä¸¤ä¸ªæç«¯ä¾‹å­ï¼Œé€šå¸¸å°±è¶³ä»¥æ¢å¤æœ‰æ„ä¹‰çš„æ’åè½´ï¼Œè€Œæ— éœ€å…¨é¢ç›‘ç£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04952', 'title': 'ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation', 'url': 'https://huggingface.co/papers/2507.04952', 'abstract': 'ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  \t\t\t\t\tAI-generated summary \t\t\t\t The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.', 'score': 8, 'issue_id': 4695, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '8eed78adf2fbda3a', 'authors': ['Chenchen Zhang', 'Yuhang Li', 'Can Xu', 'Jiaheng Liu', 'Ao Liu', 'Shihui Hu', 'Dengpeng Wu', 'Guanhua Huang', 'Kejiao Li', 'Qi Yi', 'Ruibin Xiong', 'Haotian Zhu', 'Yuanxing Zhang', 'Yuhao Jiang', 'Yue Zhang', 'Zenan Xu', 'Bohui Zhai', 'Guoxiang He', 'Hebin Li', 'Jie Zhao', 'Le Zhang', 'Lingyun Tan', 'Pengyu Guo', 'Xianshu Pang', 'Yang Ruan', 'Zhifeng Zhang', 'Zhonghu Wang', 'Ziyan Xu', 'Zuopu Yin', 'Wiggin Zhou', 'Chayse Zhou', 'Fengzong Lian'], 'affiliations': ['Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2507.04952.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#open_source'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ArtifactsBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1825 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 30 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ArtifactsBench Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 94.4% ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ¼ WebDev Arena Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Automating Quality Assessment in Visual Code Generation', 'desc': 'ArtifactsBench is a new framework designed to evaluate the quality of visual code generation by using temporal screenshots and a multimodal language model as a judge. It addresses the limitations of existing benchmarks that only focus on algorithmic correctness, ignoring the visual and interactive aspects crucial for user experiences. By programmatically rendering artifacts and capturing their dynamic behavior, ArtifactsBench provides a comprehensive assessment through a detailed checklist. The framework has been tested on 1,825 tasks and shows high consistency with human evaluations, making it a valuable tool for improving generative models in web development.'}, 'zh': {'title': 'ArtifactsBenchï¼šè‡ªåŠ¨åŒ–è§†è§‰ä»£ç ç”Ÿæˆè¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'ArtifactsBenchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†è§‰ä»£ç ç”Ÿæˆçš„è´¨é‡ã€‚å®ƒé€šè¿‡æ—¶é—´æˆªå›¾å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„åˆ¤ï¼Œæ•æ‰ç”Ÿæˆçš„è§†è§‰å·¥ä»¶çš„åŠ¨æ€è¡Œä¸ºã€‚è¯¥æ¡†æ¶ä½¿ç”¨ç»†è‡´çš„ä»»åŠ¡æ¸…å•æ¥ç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå¯é‡å¤æ€§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«1825ä¸ªå¤šæ ·åŒ–ä»»åŠ¡çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°ä¸äººç±»ä¸“å®¶çš„è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œæ ‡å¿—ç€ArtifactsBenchæˆä¸ºå¯é çš„è‡ªåŠ¨åŒ–è¯„ä¼°äººç±»æ„ŸçŸ¥è´¨é‡çš„é¦–ä¸ªæ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05257', 'title': 'Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions', 'url': 'https://huggingface.co/papers/2507.05257', 'abstract': 'Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.', 'score': 7, 'issue_id': 4711, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'bdaecd0c6c4258a2', 'authors': ['Yuanzhe Hu', 'Yu Wang', 'Julian McAuley'], 'affiliations': ['UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2507.05257.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#agents', '#benchmark', '#rag', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MemoryAgentBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²ÑĞµ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµÑ‚ÑŒ Ğ²ÑĞµĞ¼Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Memory in Language Models: Introducing MemoryAgentBench', 'desc': "This paper addresses the gap in evaluating memory capabilities of Large Language Model (LLM) agents, which are crucial for tasks involving long-term information management. It introduces the concept of 'memory agents' and identifies four key competencies: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. The authors present MemoryAgentBench, a new benchmark that combines existing datasets with new ones to comprehensively assess these competencies in memory agents. The findings indicate that current memory methods are inadequate, highlighting the necessity for improved memory mechanisms in LLMs."}, 'zh': {'title': 'è®°å¿†ä»£ç†çš„æ–°åŸºå‡†è¯„ä¼°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è®°å¿†æœºåˆ¶ï¼Œæå‡ºäº†è®°å¿†ä»£ç†çš„æ¦‚å¿µã€‚æˆ‘ä»¬è¯†åˆ«å‡ºå››ä¸ªè®°å¿†ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ï¼šå‡†ç¡®æ£€ç´¢ã€æµ‹è¯•æ—¶å­¦ä¹ ã€é•¿ç¨‹ç†è§£å’Œå†²çªè§£å†³ã€‚ç°æœ‰çš„æ•°æ®é›†æ— æ³•å…¨é¢è¯„ä¼°è¿™äº›èƒ½åŠ›ï¼Œå› æ­¤æˆ‘ä»¬æ¨å‡ºäº†MemoryAgentBenchï¼Œä¸€ä¸ªä¸“é—¨ä¸ºè®°å¿†ä»£ç†è®¾è®¡çš„æ–°åŸºå‡†ã€‚é€šè¿‡ç»“åˆç°æœ‰æ•°æ®é›†å’Œæ–°æ„å»ºçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¸ºè¯„ä¼°è®°å¿†è´¨é‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21884', 'title': 'UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields', 'url': 'https://huggingface.co/papers/2506.21884', 'abstract': 'A framework combining NeRF with spectral unmixing yields accurate material segmentation and editing through hyperspectral synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF.', 'score': 7, 'issue_id': 4707, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 Ğ¸ÑĞ½Ñ', 'en': 'June 27', 'zh': '6æœˆ27æ—¥'}, 'hash': 'f4e1f24bc4c740b2', 'authors': ['Fabian Perez', 'Sara Rojas', 'Carlos Hinojosa', 'Hoover Rueda-ChacÃ³n', 'Bernard Ghanem'], 'affiliations': ['KAUST', 'Universidad Industrial de Santander'], 'pdf_title_img': 'assets/pdf/title_img/2506.21884.jpg', 'data': {'categories': ['#robotics', '#cv', '#3d'], 'emoji': 'ğŸŒˆ', 'ru': {'title': 'Ğ“Ğ¸Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ NeRF Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnMix-NeRF - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ (NeRF) ÑĞ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ñ‹Ğµ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². UnMix-NeRF Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'UnMix-NeRF: Revolutionizing Material Segmentation with Spectral Insights', 'desc': 'This paper presents UnMix-NeRF, a novel framework that enhances Neural Radiance Fields (NeRF) by incorporating spectral unmixing for improved material segmentation and editing. Traditional NeRF methods rely on RGB data, which limits their ability to accurately perceive material properties essential for various applications like robotics and augmented reality. UnMix-NeRF addresses this by modeling spectral reflectance through diffuse and specular components, using a learned dictionary of global endmembers to represent pure material signatures. The framework allows for unsupervised material clustering and scene editing by modifying endmember dictionaries, resulting in superior performance in spectral reconstruction and material segmentation compared to existing techniques.'}, 'zh': {'title': 'ç»“åˆNeRFä¸å…‰è°±è§£æ··åˆï¼Œå®ç°ç²¾å‡†ææ–™åˆ†å‰²ä¸ç¼–è¾‘', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUnMix-NeRFçš„æ¡†æ¶ï¼Œå°†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¸å…‰è°±è§£æ··åˆç›¸ç»“åˆï¼Œå®ç°äº†å‡†ç¡®çš„ææ–™åˆ†å‰²å’Œç¼–è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡è”åˆè¶…å…‰è°±æ–°è§†å›¾åˆæˆå’Œæ— ç›‘ç£ææ–™åˆ†å‰²ï¼Œå…‹æœäº†ä¼ ç»ŸNeRFæ–¹æ³•ä»…ä¾èµ–RGBæ•°æ®çš„å±€é™æ€§ã€‚UnMix-NeRFé€šè¿‡å­¦ä¹ çš„å…¨å±€ç«¯å…ƒå­—å…¸è¡¨ç¤ºçº¯ææ–™ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ¯ä¸ªç‚¹çš„ä¸°åº¦æ•æ‰ææ–™åˆ†å¸ƒï¼Œä»è€Œå®ç°ææ–™çš„æ— ç›‘ç£èšç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…‰è°±é‡å»ºå’Œææ–™åˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04590', 'title': 'VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents', 'url': 'https://huggingface.co/papers/2507.04590', 'abstract': 'A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.', 'score': 5, 'issue_id': 4694, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'a417297c3b4c5459', 'authors': ['Rui Meng', 'Ziyan Jiang', 'Ye Liu', 'Mingyi Su', 'Xinyi Yang', 'Yuepeng Fu', 'Can Qin', 'Zeyuan Chen', 'Ran Xu', 'Caiming Xiong', 'Yingbo Zhou', 'Wenhu Chen', 'Semih Yavuz'], 'affiliations': ['Salesforce Research', 'Tsinghua University', 'UC Santa Barbara', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2507.04590.jpg', 'data': {'categories': ['#games', '#rag', '#survey', '#benchmark', '#transfer_learning', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²', 'desc': 'VLM2Vec-V2 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. VLM2Vec-V2 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMEB-V2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unified Embeddings for All Visual Forms!', 'desc': 'The paper introduces VLM2Vec-V2, a new framework designed to learn embeddings for various visual forms, including videos and documents. This model enhances the capabilities of existing multimodal embedding models, which have primarily focused on natural images. By establishing a comprehensive benchmark called MMEB-V2, the authors evaluate the model on new tasks such as video retrieval and visual document retrieval. The results show that VLM2Vec-V2 not only excels in these new tasks but also outperforms previous models on traditional image benchmarks, demonstrating its versatility and effectiveness in real-world applications.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶VLM2Vec-V2ï¼Œç”¨äºå­¦ä¹ å¤šç§è§†è§‰å½¢å¼ï¼ˆå¦‚è§†é¢‘å’Œæ–‡æ¡£ï¼‰çš„åµŒå…¥ã€‚è¯¥æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å›¾åƒçš„ç°æœ‰åŸºå‡†ä¸Šæœ‰æ‰€æå‡ã€‚æˆ‘ä»¬å¼•å…¥äº†MMEB-V2åŸºå‡†ï¼Œæ‰©å±•äº†äº”ç§æ–°ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬è§†è§‰æ–‡æ¡£æ£€ç´¢å’Œè§†é¢‘åˆ†ç±»ç­‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒVLM2Vec-V2å±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ ä¸­çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œå®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03607', 'title': 'VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification', 'url': 'https://huggingface.co/papers/2507.03607', 'abstract': 'A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descriptions. Built on RoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and achieves over 82% accuracy in predicting severity categories, enabling faster and more consistent triage ahead of manual CVSS scoring. The model and dataset are open-source and integrated into the Vulnerability-Lookup service.', 'score': 5, 'issue_id': 4698, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': '7fd058ff4a7cb43a', 'authors': ['CÃ©dric Bonhomme', 'Alexandre Dulaunoy'], 'affiliations': ['Computer Incident Response Center Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2507.03607.jpg', 'data': {'categories': ['#security', '#architecture', '#dataset', '#data', '#training', '#open_source'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ ĞŸĞ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VLAI Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. VLAI Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ RoBERTa Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 600 000 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 82% Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿ĞµÑ€ĞµĞ´ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ CVSS. VLAI Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² ÑĞµÑ€Ğ²Ğ¸Ñ Vulnerability-Lookup.'}, 'en': {'title': 'Transforming Vulnerability Assessment with VLAI', 'desc': 'This paper introduces VLAI, a transformer-based model designed to predict the severity levels of software vulnerabilities from textual descriptions. Utilizing the RoBERTa architecture, VLAI has been fine-tuned on a large dataset of over 600,000 real-world vulnerabilities, achieving an impressive accuracy of over 82% in classifying severity categories. This model significantly improves the efficiency and consistency of vulnerability triage processes, allowing for quicker assessments before manual Common Vulnerability Scoring System (CVSS) evaluations. Additionally, both the model and the dataset are made open-source and are integrated into the Vulnerability-Lookup service for broader accessibility.'}, 'zh': {'title': 'æ™ºèƒ½é¢„æµ‹è½¯ä»¶æ¼æ´ä¸¥é‡æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„æ¨¡å‹VLAIï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»æ–‡æœ¬æè¿°ä¸­é¢„æµ‹è½¯ä»¶æ¼æ´çš„ä¸¥é‡æ€§ç­‰çº§ã€‚VLAIåŸºäºRoBERTaæ¨¡å‹ï¼Œç»è¿‡å¯¹è¶…è¿‡60ä¸‡ä¸ªçœŸå®æ¼æ´çš„å¾®è°ƒï¼Œè¾¾åˆ°äº†è¶…è¿‡82%çš„ä¸¥é‡æ€§åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™ä¸€æ¨¡å‹çš„åº”ç”¨å¯ä»¥æé«˜æ¼æ´åˆ†ç±»çš„æ•ˆç‡å’Œä¸€è‡´æ€§ï¼Œå¸®åŠ©åœ¨æ‰‹åŠ¨CVSSè¯„åˆ†ä¹‹å‰è¿›è¡Œæ›´å¿«é€Ÿçš„è¯„ä¼°ã€‚è¯¥æ¨¡å‹å’Œæ•°æ®é›†éƒ½æ˜¯å¼€æºçš„ï¼Œå¹¶å·²é›†æˆåˆ°æ¼æ´æŸ¥è¯¢æœåŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04036', 'title': 'PresentAgent: Multimodal Agent for Presentation Video Generation', 'url': 'https://huggingface.co/papers/2507.04036', 'abstract': 'A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.', 'score': 4, 'issue_id': 4693, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ»Ñ', 'en': 'July 5', 'zh': '7æœˆ5æ—¥'}, 'hash': '79b10f5eed3bd7e4', 'authors': ['Jingwei Shi', 'Zeyu Zhang', 'Biao Wu', 'Yanjie Liang', 'Meng Fang', 'Ling Chen', 'Yang Zhao'], 'affiliations': ['AI Geeks, Australia', 'Australian Artificial Intelligence Institute, Australia', 'La Trobe University, Australia', 'University of Liverpool, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.04036.jpg', 'data': {'categories': ['#cv', '#multimodal', '#agents', '#optimization', '#dataset', '#benchmark', '#games', '#interpretability'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PresentAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ·Ğ²ÑƒÑ‡ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PresentEval Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PresentAgent Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Transforming Text into Engaging Videos with PresentAgent', 'desc': 'PresentAgent is a multimodal agent designed to convert long documents into engaging presentation videos with synchronized audio. Unlike traditional methods that only create static slides or text summaries, this approach generates dynamic visual and spoken content that resembles human presentations. It utilizes a modular pipeline for document segmentation, slide rendering, and narration generation, ensuring high-quality audio-visual alignment. The effectiveness of PresentAgent is evaluated using PresentEval, a framework that assesses video quality based on content fidelity, visual clarity, and audience comprehension, demonstrating its potential to enhance the accessibility of information.'}, 'zh': {'title': 'å°†æ–‡æ¡£è½¬åŒ–ä¸ºç”ŸåŠ¨æ¼”ç¤ºçš„æ™ºèƒ½ä½“', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPresentAgentçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œå®ƒèƒ½å¤Ÿå°†é•¿ç¯‡æ–‡æ¡£è½¬åŒ–ä¸ºå¸¦æœ‰æ—ç™½çš„æ¼”ç¤ºè§†é¢‘ã€‚ä¸ç°æœ‰æ–¹æ³•ä»…èƒ½ç”Ÿæˆé™æ€å¹»ç¯ç‰‡æˆ–æ–‡æœ¬æ‘˜è¦ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸äººç±»æ¼”ç¤ºé£æ ¼ç›¸ä¼¼çš„åŒæ­¥è§†è§‰å’Œè¯­éŸ³å†…å®¹ã€‚PresentAgenté‡‡ç”¨æ¨¡å—åŒ–æµç¨‹ï¼Œç³»ç»Ÿåœ°å¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œåˆ†æ®µï¼Œè§„åˆ’å’Œæ¸²æŸ“å¹»ç¯ç‰‡é£æ ¼çš„è§†è§‰æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„æ—ç™½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†PresentEvalè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘è¿›è¡Œå…¨é¢è¯„åˆ†ï¼ŒéªŒè¯äº†PresentAgentåœ¨å†…å®¹çœŸå®æ€§ã€è§†è§‰æ¸…æ™°åº¦å’Œè§‚ä¼—ç†è§£åŠ›ç­‰æ–¹é¢æ¥è¿‘äººç±»æ°´å¹³çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03033', 'title': 'Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation', 'url': 'https://huggingface.co/papers/2507.03033', 'abstract': 'A fine-tuned Llama 3.2 1B model using PEFT with LoRA in the browser improves medical transcription accuracy and reduces privacy and computational concerns.  \t\t\t\t\tAI-generated summary \t\t\t\t Background: Clinical documentation represents a significant burden for healthcare providers, with physicians spending up to 2 hours daily on administrative tasks. Recent advances in large language models (LLMs) offer promising solutions, but privacy concerns and computational requirements limit their adoption in healthcare settings. Objective: To develop and evaluate a privacy-preserving, on-device medical transcription system using a fine-tuned Llama 3.2 1B model capable of generating structured medical notes from medical transcriptions while maintaining complete data sovereignty entirely in the browser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical transcription-to-structured note pairs. The model was evaluated against the base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140 modified ACI benchmark cases. Evaluation employed both statistical metrics (ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple clinical quality dimensions. Results: The fine-tuned OnDevice model demonstrated substantial improvements over the base model. On the ACI benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1 improved from 0.832 to 0.866. Clinical quality assessments showed marked reduction in major hallucinations (from 85 to 35 cases) and enhanced factual correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed on the internal evaluation dataset, with composite scores increasing from 3.13 to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical transcription yields clinically meaningful improvements while enabling complete on-device browser deployment. This approach addresses key barriers to AI adoption in healthcare: privacy preservation, cost reduction, and accessibility for resource-constrained environments.', 'score': 4, 'issue_id': 4708, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ»Ñ', 'en': 'July 3', 'zh': '7æœˆ3æ—¥'}, 'hash': '0ea33e70360e744c', 'authors': ['Johnson Thomas', 'Ayush Mudgal', 'Wendao Liu', 'Nisten Tahiraj', 'Zeeshaan Mohammed', 'Dhruv Diddi'], 'affiliations': ['Department of Endocrinology, Mercy Hospital, Springfield, Missouri, USA', 'Solo Tech', 'Starfishdata.ai', 'alignmentlab.ai'], 'pdf_title_img': 'assets/pdf/title_img/2507.03033.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#synthetic', '#inference', '#training', '#low_resource', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 1B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ PEFT Ñ LoRA. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞ»Ğ¾ÑÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ROUGE, BERTScore Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´.'}, 'en': {'title': 'Enhancing Medical Transcription with Privacy-Preserving AI', 'desc': 'This paper presents a method to enhance medical transcription accuracy using a fine-tuned Llama 3.2 1B model with Parameter-Efficient Fine-Tuning (PEFT) and LoRA. The model operates entirely in the browser, ensuring data privacy and reducing computational demands, which are critical in healthcare settings. Evaluation results show significant improvements in transcription quality, with reductions in errors and hallucinations, as well as better factual correctness. This approach not only improves clinical documentation efficiency but also addresses privacy and accessibility challenges in AI adoption for healthcare.'}, 'zh': {'title': 'æå‡åŒ»ç–—è½¬å½•å‡†ç¡®æ€§ï¼Œä¿æŠ¤éšç§ä¸è®¡ç®—æ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºLlama 3.2 1Bæ¨¡å‹çš„åŒ»ç–—è½¬å½•ç³»ç»Ÿï¼Œé‡‡ç”¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å’ŒLoRAæŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨æµè§ˆå™¨ä¸­ç”Ÿæˆç»“æ„åŒ–çš„åŒ»ç–—ç¬”è®°ï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®éšç§å’Œå®‰å…¨ã€‚é€šè¿‡å¯¹1500å¯¹åˆæˆåŒ»ç–—è½¬å½•å’Œç»“æ„åŒ–ç¬”è®°çš„å¾®è°ƒï¼Œæ¨¡å‹åœ¨ä¸´åºŠè´¨é‡è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘é”™è¯¯å’Œæé«˜å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè§£å†³äº†åŒ»ç–—é¢†åŸŸä¸­AIåº”ç”¨çš„éšç§å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05259', 'title': 'Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing', 'url': 'https://huggingface.co/papers/2507.05259', 'abstract': 'X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.', 'score': 3, 'issue_id': 4696, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '2db1a2de292203d6', 'authors': ['Chun-Hsiao Yeh', 'Yilin Wang', 'Nanxuan Zhao', 'Richard Zhang', 'Yuheng Li', 'Yi Ma', 'Krishna Kumar Singh'], 'affiliations': ['Adobe', 'HKU', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.05259.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'X-Planner: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'X-Planner - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². X-Planner Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'X-Planner: Simplifying Complex Image Edits with Precision', 'desc': 'X-Planner is a planning system that uses a multimodal large language model to improve text-guided image editing. It breaks down complex editing instructions into simpler sub-instructions, which helps in making precise edits while preserving the identity of the images. This system reduces the need for manual masks and minimizes unintended changes by generating accurate edit types and segmentation masks automatically. X-Planner has shown to achieve top performance on established benchmarks, demonstrating its effectiveness in handling intricate editing tasks.'}, 'zh': {'title': 'X-Plannerï¼šç²¾å‡†åˆ†è§£å¤æ‚æŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ç³»ç»Ÿ', 'desc': 'X-Planner æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§„åˆ’ç³»ç»Ÿï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºç²¾ç¡®çš„å­æŒ‡ä»¤ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç¼–è¾‘çš„å±€éƒ¨æ€§å’Œèº«ä»½ä¿ç•™ï¼Œé¿å…äº†ä¸å¿…è¦çš„ç¼–è¾‘é”™è¯¯ã€‚X-Planner é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼Œç³»ç»Ÿåœ°å°†å¤æ‚æŒ‡ä»¤ç®€åŒ–ä¸ºæ›´æ¸…æ™°çš„å­æŒ‡ä»¤ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆç¼–è¾‘ç±»å‹å’Œåˆ†å‰²æ©ç ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„ã€‚è¯¥ç³»ç»Ÿåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæ–°å¼•å…¥çš„å¤æ‚ç¼–è¾‘åŸºå‡†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04642', 'title': 'R1-RE: Cross-Domain Relationship Extraction with RLVR', 'url': 'https://huggingface.co/papers/2507.04642', 'abstract': "R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.", 'score': 3, 'issue_id': 4712, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '26a997cc2412dd19', 'authors': ['Runpeng Dai', 'Tong Zheng', 'Run Yang', 'Hongtu Zhu'], 'affiliations': ['BiliBili', 'University of Maryland, College Park', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2507.04642.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#small_models', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ R1-RE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. R1-RE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ R1-RE-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¾ĞºĞ¾Ğ»Ğ¾ 70%, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Relationship Extraction with Reinforcement Learning and Reasoning', 'desc': 'The paper introduces R1-RE, a novel framework that applies reinforcement learning with verifiable rewards to improve relationship extraction (RE) in natural language processing. Traditional RE methods struggle with generalizing to new, unseen data, but R1-RE reframes the task as a reasoning challenge, similar to how human annotators work. By utilizing the reasoning capabilities of small language models, this approach enhances robustness against out-of-domain scenarios. The results show that R1-RE achieves competitive accuracy on benchmark datasets, providing valuable insights into the training dynamics of reinforcement learning in RE tasks.'}, 'zh': {'title': 'æå‡å…³ç³»æå–çš„åŸŸå¤–é²æ£’æ€§', 'desc': 'R1-REæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å…³ç³»æå–ä»»åŠ¡åœ¨åŸŸå¤–çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå°†å…³ç³»æå–é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªæ¨ç†ä»»åŠ¡ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼ŒR1-REæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒR1-RE-7Bæ¨¡å‹åœ¨Sem-2010å’ŒMDKGæ•°æ®é›†ä¸Šè¾¾åˆ°äº†çº¦70%çš„åŸŸå¤–å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03336', 'title': 'Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky', 'url': 'https://huggingface.co/papers/2507.03336', 'abstract': "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.", 'score': 3, 'issue_id': 4694, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': 'a22f17539601dde1', 'authors': ['Ashutosh Hathidara', 'Julien Yu', 'Sebastian Schreiber'], 'affiliations': ['SAP Labs'], 'pdf_title_img': 'assets/pdf/title_img/2507.03336.jpg', 'data': {'categories': ['#optimization', '#open_source', '#dataset', '#training', '#data', '#alignment', '#benchmark', '#agents'], 'emoji': 'ğŸ”§', 'ru': {'title': 'DiaFORGE: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ API Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM', 'desc': 'DiaFORGE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ API Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ DiaBENCH Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DiaFORGE, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 27 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4 Ğ¸ Ğ½Ğ° 49 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Claude-3.5-Sonnet. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 5000 ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… API-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering LLMs to Accurately Invoke APIs with DiaFORGE', 'desc': "DiaFORGE is a framework designed to improve how large language models (LLMs) interact with enterprise APIs by resolving ambiguities in user requests. It consists of a three-stage process that includes generating multi-turn dialogues to help the model differentiate between similar tools, fine-tuning the model with supervised learning using reasoning traces, and evaluating the model's performance in real-world scenarios. The results show that models trained with DiaFORGE significantly outperform existing models like GPT-4o and Claude-3.5-Sonnet in successfully invoking tools. Additionally, DiaFORGE provides a valuable resource by releasing a corpus of enterprise API specifications and validated dialogues to aid future research."}, 'zh': {'title': 'æå‡APIè°ƒç”¨å‡†ç¡®æ€§çš„å¯¹è¯æ¡†æ¶', 'desc': 'DiaFORGEæ˜¯ä¸€ä¸ªæ¶ˆæ­§ä¹‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­å‡†ç¡®è°ƒç”¨ä¼ä¸šAPIçš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆåˆæˆä»¥è§’è‰²ä¸ºé©±åŠ¨çš„å¤šè½®å¯¹è¯ï¼Œå¸®åŠ©åŠ©æ‰‹åŒºåˆ†ç›¸ä¼¼å·¥å…·ï¼›å…¶æ¬¡å¯¹å¼€æºæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨3Båˆ°70Bå‚æ•°çš„æ¨ç†è½¨è¿¹ï¼›æœ€åé€šè¿‡åŠ¨æ€è¯„ä¼°å¥—ä»¶æµ‹è¯•æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡DiaFORGEè®­ç»ƒçš„æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨æˆåŠŸç‡ä¸Šæ¯”GPT-4oæé«˜äº†27ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¯”Claude-3.5-Sonnetæé«˜äº†49ä¸ªç™¾åˆ†ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04562', 'title': 'Evaluating LLMs on Real-World Forecasting Against Human Superforecasters', 'url': 'https://huggingface.co/papers/2507.04562', 'abstract': 'State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against human superforecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of superforecasters.', 'score': 2, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ»Ñ', 'en': 'July 6', 'zh': '7æœˆ6æ—¥'}, 'hash': '27146571110c725d', 'authors': ['Janna Lu'], 'affiliations': ['Department of Economics, George Mason University, Fairfax, VA 22030'], 'pdf_title_img': 'assets/pdf/title_img/2507.04562.jpg', 'data': {'categories': ['#benchmark', '#reasoning'], 'emoji': 'ğŸ”®', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ vs ÑÑƒĞ¿ĞµÑ€Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸ÑÑ‚Ñ‹: Ğ±Ğ¸Ñ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ‘Ñ€Ğ°Ğ¹ĞµÑ€Ğ°, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ»ÑĞ´Ğ¸, Ğ½Ğ¾ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ ÑÑƒĞ¿ĞµÑ€Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸ÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° 464 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Metaculus. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµ ĞµÑ‰Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'LLMs Lag Behind Humans in Forecasting Accuracy', 'desc': 'This paper evaluates the forecasting abilities of state-of-the-art large language models (LLMs) against human superforecasters. Despite LLMs showing impressive performance in various tasks, their accuracy in predicting future events is still lacking. The study uses Brier scores to compare the predictions of LLMs on 464 questions from Metaculus with those made by human experts. The results indicate that while LLMs may achieve scores that seem better than average human predictions, they still fall short when compared to the best human forecasters.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹ä¸­çš„å±€é™æ€§', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å…¶å‡†ç¡®æ€§ä½äºäººç±»è¶…çº§é¢„æµ‹è€…ã€‚å°½ç®¡LLMsåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨é¢„æµ‹æœªæ¥äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡å¯¹Metaculusçš„464ä¸ªé¢„æµ‹é—®é¢˜è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°å‰æ²¿æ¨¡å‹çš„Brieråˆ†æ•°è¡¨é¢ä¸Šè¶…è¿‡äº†äººç±»ç¾¤ä½“ï¼Œä½†ä»æ˜¾è‘—ä½äºè¶…çº§é¢„æµ‹è€…çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMsåœ¨é¢„æµ‹ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†äººç±»é¢„æµ‹è€…çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04376', 'title': 'MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents', 'url': 'https://huggingface.co/papers/2507.04376', 'abstract': "As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.", 'score': 2, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ»Ñ', 'en': 'July 6', 'zh': '7æœˆ6æ—¥'}, 'hash': 'a7ed25fb7089c612', 'authors': ['Georgios Ioannides', 'Christos Constantinou', 'Vinija Jain', 'Aman Chadha', 'Aaron Elkins'], 'affiliations': ['Amazon GenAI, USA', 'James Silberrad Brown Center for Artificial Intelligence, Carnegie Mellon University', 'James Silberrad Brown Center for Artificial Intelligence, USA', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2507.04376.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#agi', '#agents', '#architecture'], 'emoji': 'ğŸ”€', 'ru': {'title': 'MOD-X: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MOD-X - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. MOD-X Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞ¸Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞºĞ¸, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¹ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'MOD-X: Bridging AI Agents for Seamless Collaboration', 'desc': 'This paper presents MOD-X, a new framework designed to improve communication between different AI agents. It introduces a layered architecture that includes a Universal Message Bus and blockchain security, allowing diverse agents to work together seamlessly. MOD-X enhances interoperability by using a publish-subscribe model and dynamic workflow orchestration, making it easier for agents with different capabilities to collaborate. The framework aims to create decentralized ecosystems of specialized agents that can scale without central control.'}, 'zh': {'title': 'MOD-Xï¼šæ„å»ºæ™ºèƒ½ä»£ç†çš„äº’æ“ä½œæ€§æ–°æ¡†æ¶', 'desc': 'éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿä»å•ä¸€æ¨¡å‹æ¼”å˜ä¸ºä¸“é—¨åŒ–ä»£ç†çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæ ‡å‡†åŒ–é€šä¿¡åè®®çš„éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡æå‡ºäº†MOD-Xï¼ˆæ¨¡å—åŒ–å¼€æ”¾å»ä¸­å¿ƒåŒ–äº¤æ¢ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¶æ„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åè®®çš„å…³é”®é™åˆ¶ã€‚MOD-Xé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œé…å¤‡é€šç”¨æ¶ˆæ¯æ€»çº¿ã€å…¨é¢çš„çŠ¶æ€ç®¡ç†ã€ç¿»è¯‘èƒ½åŠ›å’ŒåŸºäºåŒºå—é“¾çš„å®‰å…¨æœºåˆ¶ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºå‘å¸ƒ-è®¢é˜…é€šä¿¡æ¨¡å‹ã€è¯­ä¹‰èƒ½åŠ›å‘ç°å’ŒåŠ¨æ€å·¥ä½œæµç¼–æ’ï¼Œæä¾›äº†ä¸€ä¸ªè¿æ¥ç†è®ºå½¢å¼ä¸å®é™…å®æ–½çš„æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04285', 'title': 'SeqTex: Generate Mesh Textures in Video Sequence', 'url': 'https://huggingface.co/papers/2507.04285', 'abstract': 'SeqTex leverages pretrained video foundation models to directly generate high-fidelity UV texture maps through a sequence generation approach, enhancing 3D texture generation with superior consistency and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.', 'score': 1, 'issue_id': 4701, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ»Ñ', 'en': 'July 6', 'zh': '7æœˆ6æ—¥'}, 'hash': 'be8c2e50f0eeca2f', 'authors': ['Ze Yuan', 'Xin Yu', 'Yangtian Sun', 'Yuan-Chen Guo', 'Yan-Pei Cao', 'Ding Liang', 'Xiaojuan Qi'], 'affiliations': ['HKU', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2507.04285.jpg', 'data': {'categories': ['#architecture', '#3d', '#synthetic', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'SeqTex: Ğ¿Ñ€ÑĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ UV-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SeqTex - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ UV-Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. SeqTex Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ UV-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'SeqTex: Direct UV Texture Generation with Video Model Power', 'desc': 'SeqTex is a new framework that uses pretrained video models to create high-quality UV texture maps directly, improving the process of 3D texture generation. Traditional methods often struggle with limited datasets and generate textures in two stages, which can lead to errors and inconsistencies. By treating the generation of UV textures as a sequence problem, SeqTex learns to connect multi-view images and UV textures more effectively. This approach, along with innovative design features, allows SeqTex to produce detailed and consistent textures without needing additional processing steps.'}, 'zh': {'title': 'SeqTexï¼šç›´æ¥ç”Ÿæˆé«˜ä¿çœŸ UV çº¹ç†å›¾çš„åˆ›æ–°æ¡†æ¶', 'desc': 'SeqTex æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹ç›´æ¥ç”Ÿæˆé«˜ä¿çœŸ UV çº¹ç†å›¾ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒSeqTex å°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œä»è€Œå­¦ä¹ å¤šè§†å›¾æ¸²æŸ“å’Œ UV çº¹ç†çš„è”åˆåˆ†å¸ƒã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°å°†è§†é¢‘åŸºç¡€æ¨¡å‹ä¸­çš„ä¸€è‡´æ€§å›¾åƒç©ºé—´å…ˆéªŒè½¬ç§»åˆ° UV é¢†åŸŸã€‚é€šè¿‡å¤šç§æ¶æ„åˆ›æ–°ï¼ŒSeqTex åœ¨ç”Ÿæˆé«˜è´¨é‡ 3D çº¹ç†æ—¶å®ç°äº†æ›´å¥½çš„ä¸€è‡´æ€§å’Œå¯¹é½ï¼Œä¸”æ— éœ€åå¤„ç†ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (6)', '#agi (3)', '#alignment (2)', '#architecture (5)', '#audio', '#benchmark (9)', '#cv (4)', '#data (7)', '#dataset (11)', '#diffusion (4)', '#ethics', '#games (6)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (8)', '#open_source (6)', '#optimization (12)', '#plp', '#rag (3)', '#reasoning (8)', '#rl (2)', '#rlhf (1)', '#robotics (2)', '#science', '#security (1)', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (3)', '#training (11)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-08 23:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-08 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-08 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    