
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 28 papers. July 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 Ğ¸ÑĞ»Ñ</span> | <span id="title-articles-count">28 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-08.html">â¬…ï¸ <span id="prev-date">08.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-10.html">â¡ï¸ <span id="next-date">10.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 Ğ¸ÑĞ»Ñ', 'en': 'July 9', 'zh': '7æœˆ9æ—¥'};
        let feedDateNext = {'ru': '10.07', 'en': '07/10', 'zh': '7æœˆ10æ—¥'};
        let feedDatePrev = {'ru': '08.07', 'en': '07/08', 'zh': '7æœˆ8æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.05566', 'title': 'SingLoRA: Low Rank Adaptation Using a Single Matrix', 'url': 'https://huggingface.co/papers/2507.05566', 'abstract': 'SingLoRA, a reformulated low-rank adaptation method, enhances parameter-efficient fine-tuning by learning a single low-rank matrix update, ensuring stable optimization and reduced parameter count.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SingLoRA, which reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SingLoRA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+ (90.2%) - while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SingLoRA significantly improves image fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively.', 'score': 58, 'issue_id': 4720, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'b4bac7e0cf74ddfa', 'authors': ['David BensaÃ¯d', 'Noam Rotstein', 'Roy Velich', 'Daniel BensaÃ¯d', 'Ron Kimmel'], 'affiliations': ['Technion - IIT Haifa, Israel', 'University Paris Dauphine Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.05566.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#architecture'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'SingLoRA: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SingLoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ LoRA, Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. SingLoRA Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ²Ğ´Ğ²Ğ¾Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SingLoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LoRA Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'SingLoRA: Simplifying Fine-Tuning with Stable Low-Rank Updates', 'desc': 'SingLoRA is a new method for fine-tuning large machine learning models efficiently by using a single low-rank matrix update. This approach simplifies the training process by avoiding issues related to scale differences between multiple matrices, which can lead to unstable learning. By reformulating the adaptation process, SingLoRA reduces the number of parameters needed, making it more efficient while maintaining performance. Experiments show that SingLoRA outperforms previous methods in tasks like common sense reasoning and image generation, achieving higher accuracy and better image quality with fewer parameters.'}, 'zh': {'title': 'SingLoRAï¼šç¨³å®šé«˜æ•ˆçš„ä½ç§©é€‚åº”æ–¹æ³•', 'desc': 'SingLoRAæ˜¯ä¸€ç§é‡æ–°æ„å»ºçš„ä½ç§©é€‚åº”æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ å•ä¸ªä½ç§©çŸ©é˜µçš„æ›´æ–°æ¥å¢å¼ºå‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚å®ƒé€šè¿‡å°†ä¸¤ä¸ªè¾ƒå°çŸ©é˜µçš„ä¹˜ç§¯å½¢æˆä½ç§©çŸ©é˜µæ›´æ–°ï¼Œè§£å†³äº†ä¼ ç»ŸLoRAæ–¹æ³•ä¸­çŸ©é˜µé—´å°ºåº¦å·®å¼‚å¯¼è‡´çš„ä¸ç¨³å®šè®­ç»ƒé—®é¢˜ã€‚SingLoRAçš„è®¾è®¡æ¶ˆé™¤äº†è¿™äº›å†²çªï¼Œç¡®ä¿äº†ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå¹¶å¤§å¹…å‡å°‘äº†å‚æ•°æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSingLoRAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¸¸è¯†æ¨ç†å’Œå›¾åƒç”Ÿæˆæ–¹é¢ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„LoRAæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06229', 'title': 'Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving', 'url': 'https://huggingface.co/papers/2507.06229', 'abstract': "As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.", 'score': 54, 'issue_id': 4733, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'fe7edacae46166de', 'authors': ['Xiangru Tang', 'Tianrui Qin', 'Tianhao Peng', 'Ziyang Zhou', 'Daniel Shao', 'Tingting Du', 'Xinming Wei', 'Peng Xia', 'Fang Wu', 'He Zhu', 'Ge Zhang', 'Jiaheng Liu', 'Xingyao Wang', 'Sirui Hong', 'Chenglin Wu', 'Hao Cheng', 'Chi Wang', 'Wangchunshu Zhou'], 'affiliations': ['All Hands AI', 'Bytedance', 'Google DeepMind', 'MetaGPT X', 'Microsoft Research', 'Nanjing University', 'OPPO', 'Stanford University', 'UNC Chapel Hill', 'UW-Madison', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06229.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: Agent KB Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent KB - Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Reason-Retrieve-Refine, Agent KB ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğµ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA Ğ´Ğ¾ 16.28 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². Agent KB Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° SWE-bench Ğ´Ğ»Ñ Claude-3 Ñ 41.33% Ğ´Ğ¾ 53.33%.'}, 'en': {'title': 'Empowering Agents with Shared Knowledge for Better Learning', 'desc': 'This paper presents Agent KB, a new framework designed to enhance the learning capabilities of language agents by allowing them to share and reuse experiences. It introduces a Reason-Retrieve-Refine pipeline that helps agents learn from both high-level strategies and detailed execution logs. By creating a shared knowledge base, Agent KB facilitates knowledge transfer between agents, overcoming the traditional limitation of isolated learning. The framework shows significant improvements in task success rates across various benchmarks, demonstrating its effectiveness in enabling agents to generalize successful strategies to new challenges.'}, 'zh': {'title': 'Agent KBï¼šæå‡ä»£ç†å­¦ä¹ ä¸ç»éªŒå…±äº«çš„æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgent KBçš„å±‚æ¬¡åŒ–ç»éªŒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¯­è¨€ä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é”™è¯¯ä¿®æ­£å’Œç»éªŒé‡ç”¨èƒ½åŠ›ã€‚Agent KBé€šè¿‡ä¸€ç§æ–°é¢–çš„æ¨ç†-æ£€ç´¢-ç²¾ç‚¼ç®¡é“ï¼Œè§£å†³äº†ä»£ç†ä¹‹é—´æ— æ³•ç›¸äº’å­¦ä¹ ç»éªŒçš„æ ¸å¿ƒé™åˆ¶ã€‚è¯¥æ¡†æ¶æ•æ‰é«˜å±‚ç­–ç•¥å’Œè¯¦ç»†æ‰§è¡Œæ—¥å¿—ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…±äº«çŸ¥è¯†åº“ï¼Œä¿ƒè¿›äº†è·¨ä»£ç†çš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent KBæ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼ŒClaude-3å’ŒGPT-4åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06203', 'title': 'A Survey on Latent Reasoning', 'url': 'https://huggingface.co/papers/2507.06203', 'abstract': "Latent reasoning enhances large language models by performing multi-step inference in continuous hidden states, improving efficiency and expressiveness beyond token-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/.", 'score': 52, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '9b37c1970657d866', 'authors': ['Rui-Jie Zhu', 'Tianhao Peng', 'Tianhao Cheng', 'Xingwei Qu', 'Jinfa Huang', 'Dawei Zhu', 'Hao Wang', 'Kaiwen Xue', 'Xuanliang Zhang', 'Yong Shan', 'Tianle Cai', 'Taylor Kergan', 'Assel Kembay', 'Andrew Smith', 'Chenghua Lin', 'Binh Nguyen', 'Yuqi Pan', 'Yuhong Chou', 'Zefan Cai', 'Zhenhe Wu', 'Yongchi Zhao', 'Tianyu Liu', 'Jian Yang', 'Wangchunshu Zhou', 'Chujie Zheng', 'Chongxuan Li', 'Yuyin Zhou', 'Zhoujun Li', 'Zhaoxiang Zhang', 'Jiaheng Liu', 'Ge Zhang', 'Wenhao Huang', 'Jason Eshraghian'], 'affiliations': ['FDU', 'M-A-P', 'NJU', 'PKU', 'PolyU', 'RUC', 'UCSC', 'UW-Madison', 'UoM'], 'pdf_title_img': 'assets/pdf/title_img/2507.06203.jpg', 'data': {'categories': ['#reasoning', '#rl', '#survey', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking LLM Potential with Latent Reasoning', 'desc': "This paper discusses how latent reasoning can improve large language models (LLMs) by allowing them to perform multi-step inference in their hidden states, rather than relying on token-level supervision. It highlights the limitations of traditional chain-of-thought reasoning, which, while effective, restricts the model's ability to express complex ideas. The authors review various methodologies for latent reasoning, such as activation-based recurrence and hidden state propagation, which enhance the model's reasoning capabilities. They also introduce advanced techniques like masked diffusion models that facilitate more sophisticated and reversible reasoning processes."}, 'zh': {'title': 'æ½œåœ¨æ¨ç†ï¼šè¶…è¶Šæ ‡è®°çš„æ¨ç†æ–°å¢ƒç•Œ', 'desc': 'æ½œåœ¨æ¨ç†é€šè¿‡åœ¨è¿ç»­çš„éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†åŸºäºæ ‡è®°çš„ç›‘ç£ï¼Œæé«˜äº†æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚è™½ç„¶é“¾å¼æ¨ç†ï¼ˆCoTï¼‰å¯ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å…¶å¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„éšè—çŠ¶æ€ä¸­è¿›è¡Œæ¨ç†ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°çº§ç›‘ç£çš„éœ€æ±‚ã€‚æœ¬æ–‡ç»¼è¿°äº†æ½œåœ¨æ¨ç†çš„åŸºç¡€ã€æ–¹æ³•å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨é˜æ˜è¿™ä¸€æ–°å…´é¢†åŸŸçš„æ¦‚å¿µæ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06165', 'title': 'OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion', 'url': 'https://huggingface.co/papers/2507.06165', 'abstract': 'OmniPart generates part-aware 3D objects with high semantic decoupling and robust structural cohesion using an autoregressive structure planning module and a spatially-conditioned rectified flow model.  \t\t\t\t\tAI-generated summary \t\t\t\t The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, a novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into two synergistic stages: (1) an autoregressive structure planning module generates a controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) a spatially-conditioned rectified flow model, efficiently adapted from a pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content.', 'score': 41, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '3acb1f1fb28acbf6', 'authors': ['Yunhan Yang', 'Yufan Zhou', 'Yuan-Chen Guo', 'Zi-Xin Zou', 'Yukun Huang', 'Ying-Tian Liu', 'Hao Xu', 'Ding Liang', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['Harbin Institute of Technology, China', 'The University of Hong Kong, China', 'VAST, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.06165.jpg', 'data': {'categories': ['#optimization', '#3d', '#games'], 'emoji': 'ğŸ§©', 'ru': {'title': 'OmniPart: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'OmniPart - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. OmniPart Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing 3D Object Generation with Part Awareness', 'desc': 'OmniPart is a framework designed to generate 3D objects that are aware of their individual parts, allowing for better control and editing. It uses an autoregressive structure planning module to create a sequence of bounding boxes for each part, guided by 2D masks for intuitive control. Additionally, it employs a spatially-conditioned rectified flow model to synthesize all parts together in a coherent manner. This method enhances the usability of 3D assets for interactive applications by providing high semantic decoupling and structural integrity.'}, 'zh': {'title': 'OmniPartï¼šå¯ç¼–è¾‘çš„ä¸‰ç»´å¯¹è±¡ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'OmniPart æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰æ˜ç¡®å¯ç¼–è¾‘éƒ¨ä»¶ç»“æ„çš„ä¸‰ç»´å¯¹è±¡ã€‚å®ƒé€šè¿‡è‡ªå›å½’ç»“æ„è§„åˆ’æ¨¡å—å’Œç©ºé—´æ¡ä»¶ä¿®æ­£æµæ¨¡å‹ï¼Œå®ç°äº†éƒ¨ä»¶ä¹‹é—´çš„é«˜è¯­ä¹‰è§£è€¦å’Œç¨³å¥çš„ç»“æ„å‡èšåŠ›ã€‚è¯¥æ–¹æ³•å°†å¤æ‚ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªååŒé˜¶æ®µï¼Œé¦–å…ˆç”Ÿæˆå¯æ§çš„ä¸‰ç»´éƒ¨ä»¶è¾¹ç•Œæ¡†åºåˆ—ï¼Œç„¶ååŒæ—¶åˆæˆæ‰€æœ‰ä¸‰ç»´éƒ¨ä»¶ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniPart åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸ºæ›´å¯è§£é‡Šã€å¯ç¼–è¾‘å’Œå¤šåŠŸèƒ½çš„ä¸‰ç»´å†…å®¹é“ºå¹³äº†é“è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04103', 'title': 'How to Train Your LLM Web Agent: A Statistical Diagnosis', 'url': 'https://huggingface.co/papers/2507.04103', 'abstract': 'A study on compute allocation for post-training LLM-based web agents finds that combining supervised fine-tuning with on-policy reinforcement learning improves performance and reduces computational costs compared to either method alone.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.', 'score': 35, 'issue_id': 4716, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 Ğ¸ÑĞ»Ñ', 'en': 'July 5', 'zh': '7æœˆ5æ—¥'}, 'hash': '64d72d8a50ee925a', 'authors': ['Dheeraj Vattikonda', 'Santhoshi Ravichandran', 'Emiliano Penaloza', 'Hadi Nekoei', 'Megh Thakkar', 'Thibault Le Sellier de Chezelles', 'Nicolas Gontier', 'Miguel MuÃ±oz-MÃ¡rmol', 'Sahar Omidi Shayegan', 'Stefania Raimondo', 'Xue Liu', 'Alexandre Drouin', 'Laurent Charlin', 'Alexandre PichÃ©', 'Alexandre Lacoste', 'Massimo Caccia'], 'affiliations': ['HEC MontrÃ©al', 'McGill University', 'MilaQuebec AI Institute', 'Polytechnique MontrÃ©al', 'ServiceNow Research', 'UniveristÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2507.04103.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#training', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ SFT Ğ¸ RL', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WorkArena Ğ¸ MiniWob++. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 45% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¼ SFT Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing LLM Training: Combining SFT and RL for Efficiency', 'desc': 'This paper investigates how to allocate computing resources effectively for post-training large language model (LLM)-based web agents. It demonstrates that integrating supervised fine-tuning (SFT) with on-policy reinforcement learning (RL) leads to better performance and lower computational costs than using either method separately. The study employs a two-stage training process, where a smaller model learns from a larger one, and it carefully samples hyperparameters to optimize the training process. The findings indicate that this combined approach not only enhances performance but also significantly reduces the computational resources needed, making it a viable alternative to closed-source systems.'}, 'zh': {'title': 'ç»“åˆå¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæå‡LLMä»£ç†æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½‘ç»œä»£ç†çš„è®¡ç®—åˆ†é…é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œå‘ç°è¿™ç§ç»„åˆåœ¨æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¸Šå‡ä¼˜äºå•ç‹¬ä½¿ç”¨ä»»ä¸€æ–¹æ³•ã€‚é€šè¿‡å¯¹1,370ç§é…ç½®è¿›è¡Œé‡‡æ ·å¹¶ä½¿ç”¨è‡ªåŠ©æ³•ä¼°è®¡æœ‰æ•ˆè¶…å‚æ•°ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç­–ç•¥åœ¨MiniWob++ä¸Šä»…éœ€55%çš„è®¡ç®—èµ„æºå³å¯è¾¾åˆ°çº¯SFTçš„æœ€ä½³æ€§èƒ½ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆç¼©å°äº†ä¸é—­æºæ¨¡å‹çš„å·®è·ï¼Œæ¨åŠ¨äº†è®¡ç®—ä¸æ€§èƒ½çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06181', 'title': 'CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization', 'url': 'https://huggingface.co/papers/2507.06181', 'abstract': "CriticLean, a reinforcement learning framework with CriticLeanGPT and CriticLeanBench, enhances semantic evaluation in automated theorem proving by actively learning to distinguish correct from incorrect formalizations.  \t\t\t\t\tAI-generated summary \t\t\t\t Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning.", 'score': 34, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'ad86fa3f7ff162ee', 'authors': ['Zhongyuan Peng', 'Yifan Yao', 'Kaijing Ma', 'Shuyue Guo', 'Yizhe Li', 'Yichi Zhang', 'Chenchen Zhang', 'Yifan Zhang', 'Zhouliang Yu', 'Luming Li', 'Minghao Liu', 'Yihang Xia', 'Jiawei Shen', 'Yuchen Wu', 'Yixin Cao', 'Zhaoxiang Zhang', 'Wenhao Huang', 'Jiaheng Liu', 'Ge Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.06181.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#reasoning', '#rl', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸Ğº ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼', 'desc': 'CriticLean - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ CriticLeanGPT - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ CriticLeanBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ ĞµĞ³Ğ¾ Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ· Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ°Ğ·Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Theorem Proving with Active Semantic Evaluation', 'desc': 'CriticLean is a reinforcement learning framework designed to improve the evaluation of formalizations in automated theorem proving. It introduces CriticLeanGPT, a model that actively learns to assess the semantic accuracy of mathematical statements translated into formal code. The framework also includes CriticLeanBench, a benchmark for measuring the effectiveness of models in distinguishing correct from incorrect formalizations. By optimizing the critic phase, CriticLean aims to enhance the reliability of formalizations and contribute to advancements in formal mathematical reasoning.'}, 'zh': {'title': 'ä¼˜åŒ–è¯„åˆ¤é˜¶æ®µï¼Œæå‡è‡ªåŠ¨å®šç†è¯æ˜çš„å¯é æ€§', 'desc': 'CriticLeanæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨å®šç†è¯æ˜ä¸­çš„è¯­ä¹‰è¯„ä¼°ã€‚å®ƒé€šè¿‡CriticLeanGPTå’ŒCriticLeanBenchï¼Œä¸»åŠ¨å­¦ä¹ åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯çš„å½¢å¼åŒ–è¡¨è¾¾ã€‚CriticLeanGPTç»è¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿä¸¥æ ¼è¯„ä¼°Lean 4å½¢å¼åŒ–çš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–è¯„åˆ¤é˜¶æ®µå¯¹äºç”Ÿæˆå¯é çš„å½¢å¼åŒ–è¡¨è¾¾è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05240', 'title': 'StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling', 'url': 'https://huggingface.co/papers/2507.05240', 'abstract': 'StreamVLN, a streaming VLN framework, uses a hybrid slow-fast context modeling strategy to balance fine-grained visual understanding, long-term context modeling, and computational efficiency in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: https://streamvln.github.io/{https://streamvln.github.io/}.', 'score': 34, 'issue_id': 4718, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '89c364a9ab7df29e', 'authors': ['Meng Wei', 'Chenyang Wan', 'Xiqian Yu', 'Tai Wang', 'Yuqiang Yang', 'Xiaohan Mao', 'Chenming Zhu', 'Wenzhe Cai', 'Hanqing Wang', 'Yilun Chen', 'Xihui Liu', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05240.jpg', 'data': {'categories': ['#video', '#agents', '#multimodal', '#games', '#benchmark', '#long_context'], 'emoji': 'ğŸ§­', 'ru': {'title': 'StreamVLN: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'StreamVLN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLN) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾-Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. StreamVLN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StreamVLN Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'StreamVLN: Efficient Navigation with Hybrid Context Modeling', 'desc': 'StreamVLN is a new framework designed for Vision-and-Language Navigation (VLN) that effectively processes continuous visual data while following language instructions. It uses a hybrid slow-fast context modeling approach to balance detailed visual understanding with long-term context and computational efficiency. The fast-streaming dialogue context allows for quick action responses, while the slow-updating memory context efficiently manages historical visual information. This innovative design leads to improved performance in real-world applications, achieving state-of-the-art results with low latency and robust operation.'}, 'zh': {'title': 'StreamVLNï¼šé«˜æ•ˆçš„æµå¼è§†è§‰ä¸è¯­è¨€å¯¼èˆª', 'desc': 'StreamVLNæ˜¯ä¸€ä¸ªæµå¼è§†è§‰ä¸è¯­è¨€å¯¼èˆªæ¡†æ¶ï¼Œé‡‡ç”¨æ··åˆçš„æ…¢é€Ÿ-å¿«é€Ÿä¸Šä¸‹æ–‡å»ºæ¨¡ç­–ç•¥ï¼Œä»¥å¹³è¡¡ç»†ç²’åº¦è§†è§‰ç†è§£ã€é•¿æœŸä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šæ¨¡æ€æ¨ç†ï¼Œèƒ½å¤Ÿå¤„ç†äº¤é”™çš„è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œè¾“å…¥ã€‚å¿«é€Ÿæµå¼å¯¹è¯ä¸Šä¸‹æ–‡é€šè¿‡æ»‘åŠ¨çª—å£å®ç°å“åº”å¼åŠ¨ä½œç”Ÿæˆï¼Œè€Œæ…¢é€Ÿæ›´æ–°çš„è®°å¿†ä¸Šä¸‹æ–‡åˆ™åˆ©ç”¨3Dæ„ŸçŸ¥çš„æ ‡è®°ä¿®å‰ªç­–ç•¥å‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamVLNåœ¨VLN-CEåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡ç¨³å®šçš„ä½å»¶è¿Ÿï¼Œç¡®ä¿åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03112', 'title': 'RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents', 'url': 'https://huggingface.co/papers/2507.03112', 'abstract': "An end-to-end reinforcement learning framework using simulated user emotion rewards enhances emotional intelligence in large language models while maintaining cognitive skills.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.", 'score': 27, 'issue_id': 4715, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 Ğ¸ÑĞ»Ñ', 'en': 'July 3', 'zh': '7æœˆ3æ—¥'}, 'hash': 'c1368a26272d7e57', 'authors': ['Peisong Wang', 'Ruotian Ma', 'Bang Zhang', 'Xingyu Chen', 'Zhiwei He', 'Kang Luo', 'Qingsong Lv', 'Qingxuan Jiang', 'Zheng Xie', 'Shanyi Wang', 'Yuan Li', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2507.03112.jpg', 'data': {'categories': ['#reasoning', '#rl', '#alignment', '#agents', '#rlhf', '#training'], 'emoji': 'ğŸ¤–ğŸ’•', 'ru': {'title': 'Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ˜Ğ˜: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RLVER - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ RLVER Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-7B-Instruct Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ĞµÑ‘ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RLVER Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Emotional Intelligence in Language Models with RLVER', 'desc': 'This paper presents RLVER, a novel reinforcement learning framework designed to enhance emotional intelligence in large language models (LLMs) by using simulated user emotion rewards. The framework employs reinforcement learning from verifiable rewards (RLVR) to train LLMs in dialogue settings, focusing on developing empathetic abilities. By fine-tuning the Qwen2.5-7B-Instruct model with Proximal Policy Optimization (PPO), the authors demonstrate significant improvements in emotional understanding while maintaining cognitive skills. The findings indicate that RLVER effectively boosts dialogue capabilities and suggests that moderate training environments can lead to better outcomes than more challenging ones.'}, 'zh': {'title': 'æƒ…æ„Ÿæ™ºèƒ½ä¸è®¤çŸ¥èƒ½åŠ›çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶RLVERï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·çš„æƒ…æ„Ÿå¥–åŠ±æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿæ™ºèƒ½ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„æƒ…æ„Ÿæ™ºèƒ½ä»ç„¶ä¸è¶³ã€‚RLVERåˆ©ç”¨å¯éªŒè¯çš„æƒ…æ„Ÿå¥–åŠ±ï¼ŒæŒ‡å¯¼æ¨¡å‹å­¦ä¹ æ›´é«˜å±‚æ¬¡çš„åŒç†å¿ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLVERæ˜¾è‘—æé«˜äº†å¯¹è¯èƒ½åŠ›ï¼Œå¹¶åœ¨ä¿æŒæ•°å­¦å’Œç¼–ç èƒ½åŠ›çš„åŒæ—¶ï¼Œæå‡äº†æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05675', 'title': 'MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos', 'url': 'https://huggingface.co/papers/2507.05675', 'abstract': 'MedGen, a model trained on the large-scale MedVideoCap-55K dataset, achieves top performance in medical video generation by balancing visual quality and medical accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen', 'score': 22, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '06c0aadc1cba572e', 'authors': ['Rongsheng Wang', 'Junying Chen', 'Ke Ji', 'Zhenyang Cai', 'Shunian Chen', 'Yunjin Yang', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2507.05675.jpg', 'data': {'categories': ['#science', '#video', '#healthcare', '#open_source', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MedGen: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ MedGen, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MedVideoCap-55K, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. MedVideoCap-55K - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 55 000 Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². MedGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ¿ĞµÑ€Ğ½Ğ¸Ñ‡Ğ°ĞµÑ‚ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¼ Ğ¸ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Medical Video Generation with MedGen', 'desc': 'MedGen is a cutting-edge model designed for generating medical videos, trained on the extensive MedVideoCap-55K dataset. This dataset is the first of its kind, featuring over 55,000 high-quality video clips that accurately depict real-world medical scenarios. MedGen excels in producing videos that not only look visually appealing but also maintain strict adherence to medical accuracy, addressing a significant challenge in the field. By providing this innovative model and dataset, the authors aim to advance research in medical video generation and improve applications in clinical training and education.'}, 'zh': {'title': 'åŒ»å­¦è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'MedGenæ˜¯ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡MedVideoCap-55Kæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸“æ³¨äºåŒ»å­¦è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¨¡å‹åœ¨è§†è§‰è´¨é‡å’ŒåŒ»å­¦å‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œè¡¨ç°å‡ºè‰²ã€‚åŒ»å­¦è§†é¢‘åœ¨ä¸´åºŠåŸ¹è®­ã€æ•™è‚²å’Œæ¨¡æ‹Ÿä¸­è‡³å…³é‡è¦ï¼Œå› æ­¤éœ€è¦é«˜è§†è§‰ä¿çœŸåº¦å’Œä¸¥æ ¼çš„åŒ»å­¦å‡†ç¡®æ€§ã€‚MedVideoCap-55Kæ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä¸”å¯Œå«å­—å¹•çš„åŒ»å­¦è§†é¢‘ç”Ÿæˆæ•°æ®é›†ï¼Œä¸ºè®­ç»ƒåŒ»å­¦è§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†åšå®åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06219', 'title': 'Is Diversity All You Need for Scalable Robotic Manipulation?', 'url': 'https://huggingface.co/papers/2507.06219', 'abstract': 'Investigation into data diversity in robotic manipulation reveals that task diversity is crucial, multi-embodiment data is optional, and expert diversity can be confounding, leading to a distribution debiasing method for improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.', 'score': 17, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'd4781dc7e2730cb8', 'authors': ['Modi Shi', 'Li Chen', 'Jin Chen', 'Yuxiang Lu', 'Chiming Liu', 'Guanghui Ren', 'Ping Luo', 'Di Huang', 'Maoqing Yao', 'Hongyang Li'], 'affiliations': ['AgiBot', 'Beihang University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.06219.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#robotics', '#data', '#dataset', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ¼ĞµĞµÑ‚ Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GO-1-Pro, ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² 15%.'}, 'en': {'title': 'Diversity in Data: Key to Better Robot Learning', 'desc': 'This paper explores the importance of data diversity in robotic manipulation, focusing on three key aspects: task diversity, embodiment diversity, and expert diversity. It finds that having a variety of tasks is more beneficial than simply increasing the number of demonstrations for each task. The study also shows that using data from a single robot can be just as effective as using data from multiple robots for training. Additionally, it highlights that differences in how experts demonstrate tasks can complicate learning, leading to the development of a new method to reduce confusion caused by these variations, resulting in significant performance improvements.'}, 'zh': {'title': 'ä»»åŠ¡å¤šæ ·æ€§æ˜¯æœºå™¨äººæ“ä½œçš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ•°æ®å¤šæ ·æ€§åœ¨æœºå™¨äººæ“ä½œä¸­çš„é‡è¦æ€§ï¼Œå‘ç°ä»»åŠ¡å¤šæ ·æ€§æ˜¯å…³é”®ï¼Œè€Œå¤šç§æœºå™¨äººå½¢æ€çš„æ•°æ®æ˜¯å¯é€‰çš„ã€‚é€šè¿‡å¯¹ä¸åŒæœºå™¨äººå¹³å°çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä»»åŠ¡å¤šæ ·æ€§æ¯”æ¯ä¸ªä»»åŠ¡çš„æ¼”ç¤ºæ•°é‡æ›´ä¸ºé‡è¦ï¼Œæœ‰åŠ©äºä»å¤šæ ·çš„é¢„è®­ç»ƒä»»åŠ¡è½¬ç§»åˆ°æ–°çš„ä¸‹æ¸¸åœºæ™¯ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ†å¸ƒå»åæ–¹æ³•ï¼Œä»¥å‡å°‘é€Ÿåº¦æ¨¡ç³Šï¼Œä»è€Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚æ•´ä½“è€Œè¨€ï¼Œè¿™äº›å‘ç°ä¸ºæœ‰æ•ˆæ‰©å±•æœºå™¨äººæ“ä½œæ•°æ®é›†æä¾›äº†æ–°çš„è§†è§’å’Œå®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04569', 'title': 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts', 'url': 'https://huggingface.co/papers/2507.04569', 'abstract': 'Nile-Chat models, using Branch-Train-MiX strategy, outperform existing multilingual and Arabic LLMs on Egyptian dialect benchmarks in both Arabic and Latin scripts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for Egyptian dialect, uniquely designed to understand and generate texts written in both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we introduce a novel language adaptation approach by leveraging the Branch-Train-MiX strategy to merge script-specialized experts, into a single MoE model. Our Nile-Chat models significantly outperform leading multilingual and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced Egyptian evaluation benchmarks, which span both understanding and generative tasks. Notably, our 12B model yields a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly available. We believe this work presents a comprehensive methodology for adapting LLMs to dual-script languages, addressing an often overlooked aspect in modern LLM development.', 'score': 15, 'issue_id': 4722, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 Ğ¸ÑĞ»Ñ', 'en': 'July 6', 'zh': '7æœˆ6æ—¥'}, 'hash': '40595bd58ae7eb1b', 'authors': ['Guokan Shang', 'Hadi Abdine', 'Ahmad Chamma', 'Amr Mohamed', 'Mohamed Anwar', 'Abdelaziz Bounhar', 'Omar El Herraoui', 'Preslav Nakov', 'Michalis Vazirgiannis', 'Eric Xing'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2507.04569.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multilingual', '#training', '#low_resource'], 'emoji': 'ğŸ‡ªğŸ‡¬', 'ru': {'title': 'Nile-Chat: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµĞ³Ğ¸Ğ¿ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ° Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Nile-Chat Ğ´Ğ»Ñ ĞµĞ³Ğ¸Ğ¿ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ»Ğ°Ñ‚Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Branch-Train-MiX, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Nile-Chat Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ ĞµĞ³Ğ¸Ğ¿ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°.'}, 'en': {'title': 'Nile-Chat: Bridging Scripts for Egyptian Dialect Mastery', 'desc': 'The paper presents Nile-Chat, a series of large language models (LLMs) specifically designed for the Egyptian dialect, capable of processing both Arabic and Latin scripts. It introduces the Branch-Train-MiX strategy, which combines specialized models into a single mixture of experts (MoE) framework, enhancing language adaptation. The Nile-Chat models demonstrate superior performance on newly established benchmarks compared to existing multilingual and Arabic LLMs, achieving significant improvements in both understanding and generation tasks. This work highlights a novel approach to adapting LLMs for dual-script languages, filling a gap in current LLM research.'}, 'zh': {'title': 'Nile-Chatï¼šåŒè„šæœ¬è¯­è¨€çš„çªç ´æ€§é€‚åº”', 'desc': 'Nile-Chatæ¨¡å‹é‡‡ç”¨äº†Branch-Train-MiXç­–ç•¥ï¼Œåœ¨åŸƒåŠæ–¹è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šè¯­è¨€å’Œé˜¿æ‹‰ä¼¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æˆ‘ä»¬æ¨å‡ºäº†Nile-Chat-4Bã€3x4B-A6Bå’Œ12Bï¼Œè¿™äº›æ¨¡å‹ä¸“é—¨è®¾è®¡ç”¨äºç†è§£å’Œç”Ÿæˆé˜¿æ‹‰ä¼¯å’Œæ‹‰ä¸æ–‡ä¹¦å†™çš„æ–‡æœ¬ã€‚ç‰¹åˆ«æ˜¯Nile-Chat-3x4B-A6Bï¼Œé€šè¿‡åˆå¹¶è„šæœ¬ä¸“é—¨åŒ–çš„ä¸“å®¶ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­è¨€é€‚åº”æ–¹æ³•ï¼Œå½¢æˆäº†ä¸€ä¸ªå•ä¸€çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å¤šè¯­è¨€å’Œé˜¿æ‹‰ä¼¯LLMsï¼Œå±•ç¤ºäº†å¦‚ä½•å°†LLMsé€‚åº”åŒè„šæœ¬è¯­è¨€çš„å…¨é¢æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06138', 'title': 'Coding Triangle: How Does Large Language Model Understand Code?', 'url': 'https://huggingface.co/papers/2507.06138', 'abstract': 'The Code Triangle framework evaluates large language models across editorial analysis, code implementation, and test case generation, revealing limitations in diversity and robustness compared to human programmers and suggesting enhancements through human-generated content and model mixtures.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models.', 'score': 14, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '1eb6710e161b9c74', 'authors': ['Taolin Zhang', 'Zihan Ma', 'Maosong Cao', 'Junnan Liu', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06138.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#training', '#benchmark'], 'emoji': 'ğŸ”º', 'ru': {'title': 'Ğ¢Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸Ğº ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Code Triangle Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ñ€ĞµĞ´Ğ°ĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing LLMs: Bridging the Gap with Human Insight', 'desc': 'The Code Triangle framework assesses large language models (LLMs) by examining their performance in three key areas: editorial analysis, code implementation, and test case generation. The study finds that while LLMs can create coherent outputs, they often lack the diversity and robustness seen in human programming. It highlights a gap between how models understand coding tasks and the expertise of human programmers, primarily due to biases in training data. The paper suggests that integrating human-generated content and using a mix of models can significantly improve the capabilities and reliability of LLMs in coding tasks.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼–ç èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Code Triangleæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–è¾‘åˆ†æã€ä»£ç å®ç°å’Œæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆä¸‰ä¸ªæ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨è¿™äº›ç»´åº¦ä¸Šèƒ½å¤Ÿå½¢æˆè‡ªæ´½çš„ç³»ç»Ÿï¼Œä½†å…¶è§£å†³æ–¹æ¡ˆçš„å¤šæ ·æ€§å’Œé²æ£’æ€§å¾€å¾€ä¸åŠäººç±»ç¨‹åºå‘˜ã€‚æˆ‘ä»¬æŒ‡å‡ºæ¨¡å‹è®¤çŸ¥ä¸äººç±»ä¸“ä¸šçŸ¥è¯†ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„åˆ†å¸ƒå·®å¼‚ï¼Œæ¨¡å‹é”™è¯¯å¾€å¾€å› è®­ç»ƒæ•°æ®åå·®å’Œæœ‰é™çš„æ¨ç†è½¬ç§»è€Œèšé›†ã€‚é€šè¿‡å¼•å…¥äººç±»ç”Ÿæˆçš„å†…å®¹å’Œæ¨¡å‹æ··åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡LLMsçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„ç¼–ç æ¨¡å‹æä¾›äº†æ½œåœ¨æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05791', 'title': 'GTA1: GUI Test-time Scaling Agent', 'url': 'https://huggingface.co/papers/2507.05791', 'abstract': 'GTA1 addresses task planning ambiguity and visual grounding in GUI interactions using test-time scaling and reinforcement learning, achieving state-of-the-art performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets.   This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements.   Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here.', 'score': 14, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'e20f930b7b567221', 'authors': ['Yan Yang', 'Dongxu Li', 'Yutong Dai', 'Yuhao Yang', 'Ziyang Luo', 'Zirui Zhao', 'Zhiyuan Hu', 'Junzhe Huang', 'Amrita Saha', 'Zeyuan Chen', 'Ran Xu', 'Liyuan Pan', 'Caiming Xiong', 'Junnan Li'], 'affiliations': ['Salesforce AI Research', 'The Australian National University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.05791.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#games', '#rl', '#agents', '#open_source'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'GTA1: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GTA1 - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. GTA1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'GTA1: Mastering GUI Interactions with Smart Planning and Learning', 'desc': 'The paper presents GTA1, a novel approach to enhance task planning and visual grounding in graphical user interface (GUI) interactions using reinforcement learning. It addresses the challenges of ambiguity in action proposals by employing a test-time scaling method that samples multiple candidates and selects the best one through a judge model. Additionally, it improves the accuracy of grounding actions to visual elements by leveraging reinforcement learning, which aligns objectives with successful interactions. The results demonstrate that GTA1 achieves state-of-the-art performance on various benchmarks, showcasing its effectiveness in autonomous GUI task execution.'}, 'zh': {'title': 'GTA1ï¼šæå‡GUIäº¤äº’çš„æ™ºèƒ½å†³ç­–ä¸è§†è§‰å®šä½', 'desc': 'GTA1æ˜¯ä¸€ç§å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œæ—¨åœ¨è§£å†³ä»»åŠ¡è§„åˆ’ä¸­çš„æ¨¡ç³Šæ€§å’Œè§†è§‰å®šä½é—®é¢˜ã€‚å®ƒé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾å’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä¼˜åŒ–äº†åœ¨å¤æ‚ç•Œé¢ä¸­ä¸è§†è§‰å…ƒç´ çš„äº¤äº’ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡‡æ ·å¤šä¸ªå€™é€‰åŠ¨ä½œææ¡ˆï¼Œå¹¶åˆ©ç”¨è¯„åˆ¤æ¨¡å‹é€‰æ‹©æœ€åˆé€‚çš„ææ¡ˆï¼Œä»è€Œæé«˜å†³ç­–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGTA1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05169', 'title': 'Critiques of World Models', 'url': 'https://huggingface.co/papers/2507.05169', 'abstract': 'World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.', 'score': 12, 'issue_id': 4729, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'a6ec55259ef20f8f', 'authors': ['Eric Xing', 'Mingkai Deng', 'Jinyu Hou', 'Zhiting Hu'], 'affiliations': ['HalÄ±cÄ±oglu Data Science Institute, UC San Diego', 'Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence', 'School of Computer Science, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05169.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#agents', '#agi'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ ĞœĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾-Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ (PAN) ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ˜Ğ˜ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Building Smarter Agents with Advanced World Models', 'desc': "This paper discusses the concept of a 'world model', which is a representation of the environment that artificial agents use to understand and interact with the world. It critiques existing approaches to building and evaluating these models, emphasizing the need for a system that can simulate all possible actions in a realistic manner. The authors propose a new architecture that combines hierarchical structures and mixed representations to enhance the model's capabilities. Ultimately, they envision a Physical, Agentic, and Nested (PAN) AGI system that leverages this advanced world model for better reasoning and decision-making."}, 'zh': {'title': 'æ„å»ºæ™ºèƒ½ä»£ç†çš„ä¸–ç•Œæ¨¡å‹', 'desc': 'ä¸–ç•Œæ¨¡å‹æ˜¯ç”Ÿç‰©ä½“åœ¨çœŸå®ç¯å¢ƒä¸­ä½“éªŒå’Œè¡ŒåŠ¨çš„ç®—æ³•æ›¿ä»£å“ï¼Œè¿‘å¹´æ¥å› å¼€å‘å…·æœ‰äººå·¥æ™ºèƒ½çš„è™šæ‹Ÿä»£ç†çš„éœ€æ±‚è€Œå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹çš„å®šä¹‰ã€æ„å»ºã€ä½¿ç”¨å’Œè¯„ä¼°ç­‰é—®é¢˜ï¼Œå¹¶æ‰¹è¯„äº†å‡ ç§å…³äºä¸–ç•Œå»ºæ¨¡çš„ç†è®ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸–ç•Œæ¨¡å‹çš„ä¸»è¦ç›®æ ‡æ˜¯æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­æ‰€æœ‰å¯æ“ä½œçš„å¯èƒ½æ€§ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰ç›®çš„çš„æ¨ç†å’Œè¡ŒåŠ¨ã€‚åŸºäºè¿™äº›æ‰¹è¯„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨ä¸–ç•Œæ¨¡å‹æ¶æ„ï¼Œé‡‡ç”¨åˆ†å±‚ã€å¤šçº§å’Œæ··åˆè¿ç»­/ç¦»æ•£è¡¨ç¤ºï¼Œå¹¶ç»“åˆç”Ÿæˆå’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œå±•æœ›åŸºäºæ­¤æ¨¡å‹çš„ç‰©ç†ã€ä»£ç†å’ŒåµŒå¥—çš„äººå·¥é€šç”¨æ™ºèƒ½ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06223', 'title': 'Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers', 'url': 'https://huggingface.co/papers/2507.06223', 'abstract': 'E\\textsuperscript{2}R-FLOPs evaluates LLM-based rerankers by measuring relevance and throughput per PetaFLOP, providing a hardware-agnostic metric for efficiency and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E2R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.', 'score': 11, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '6073d3ee8c07d225', 'authors': ['Zhiyuan Peng', 'Ting-ruen Wei', 'Tingyu Song', 'Yilun Zhao', 'Yi Fang'], 'affiliations': ['Independent Researcher, Beijing, China', 'Santa Clara University, Santa Clara, CA', 'Yale University, New Haven, CT'], 'pdf_title_img': 'assets/pdf/title_img/2507.06223.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#interpretability', '#architecture', '#inference'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'E2R-FLOPs: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LLM-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ E2R-FLOPs Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞŸĞµÑ‚Ğ°Ğ¤Ğ›ĞĞŸ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ¤Ğ›ĞĞŸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ², Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'E2R-FLOPs: A New Standard for Evaluating LLM Efficiency', 'desc': 'The paper introduces E2R-FLOPs, a new metric for evaluating the efficiency of Large Language Model (LLM)-based rerankers in information retrieval. It measures relevance and throughput per PetaFLOP, providing a hardware-agnostic way to assess performance. Traditional metrics like latency and token counts are limited by hardware dependencies and do not adequately reflect model size, making comparisons challenging. By using E2R-FLOPs, researchers can better understand the trade-offs between efficiency and effectiveness in LLM-based reranking tasks.'}, 'zh': {'title': 'E2R-FLOPsï¼šé«˜æ•ˆè¯„ä¼°LLMé‡æ’åºå™¨çš„å·¥å…·', 'desc': 'E2R-FLOPs æ˜¯ä¸€ç§è¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡æ’åºå™¨çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ¯ PetaFLOP çš„ç›¸å…³æ€§å’Œååé‡æ¥è¡¡é‡å…¶æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¾èµ–äºç¡¬ä»¶å’Œè¿è¡Œæ—¶é—´é€‰æ‹©çš„é—®é¢˜ï¼Œä½¿å¾—è¯„ä¼°æ›´åŠ é€šç”¨å’Œæ˜“äºç†è§£ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ FLOPs ä¼°ç®—å™¨ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œå®éªŒçš„æƒ…å†µä¸‹ä¼°ç®— LLM é‡æ’åºå™¨çš„ FLOPsã€‚é€šè¿‡è¿™äº›æ–°æŒ‡æ ‡ï¼Œæˆ‘ä»¬å¯¹å¤šç§ä¸åŒæ¶æ„çš„ LLM é‡æ’åºå™¨è¿›è¡Œäº†å…¨é¢å®éªŒï¼Œç ”ç©¶äº†æ•ˆç‡ä¸æœ‰æ•ˆæ€§ä¹‹é—´çš„æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05101', 'title': 'PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs', 'url': 'https://huggingface.co/papers/2507.05101', 'abstract': "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.", 'score': 10, 'issue_id': 4715, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'c987593bed9476c8', 'authors': ['Xinzhe Zheng', 'Hao Du', 'Fanding Xu', 'Jinzhe Li', 'Zhiyuan Liu', 'Wenkang Wang', 'Tao Chen', 'Wanli Ouyang', 'Stan Z. Li', 'Yan Lu', 'Nanqing Dong', 'Yang Zhang'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong', 'Westlake University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05101.jpg', 'data': {'categories': ['#benchmark', '#data', '#open_source', '#dataset', '#graphs', '#leakage'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'PRING: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PRING - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ»Ğ¾Ğº-Ğ±ĞµĞ»Ğ¾Ğº (PPI) Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². PRING Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PPI-ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 21,484 Ğ±ĞµĞ»ĞºĞ° Ğ¸ 186,818 Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞµÑ‚Ğ¸, Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PPI Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² PPI-ÑĞµÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing PPI Prediction with PRING: A Graph-Level Benchmark', 'desc': 'This paper introduces PRING, a new benchmark for evaluating protein-protein interaction (PPI) prediction models from a graph-level perspective. Unlike previous benchmarks that focused on pairwise evaluations, PRING assesses the ability of models to reconstruct meaningful PPI networks, which is essential for biological research. The benchmark includes a comprehensive dataset of 21,484 proteins and 186,818 interactions, addressing issues like data redundancy and leakage. The evaluation framework consists of topology-oriented and function-oriented tasks, revealing limitations in current PPI models and guiding future improvements in PPI prediction.'}, 'zh': {'title': 'PRINGï¼šè›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢„æµ‹çš„æ–°åŸºå‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·PRINGï¼Œç”¨äºè¯„ä¼°è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰é¢„æµ‹æ¨¡å‹çš„èƒ½åŠ›ã€‚ä¸ä»¥å¾€çš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒPRINGä»å›¾çº§åˆ«çš„è§’åº¦å‡ºå‘ï¼Œå…³æ³¨æ¨¡å‹é‡å»ºç”Ÿç‰©å­¦æ„ä¹‰çš„PPIç½‘ç»œã€‚è¯¥åŸºå‡†æ•°æ®é›†åŒ…å«21,484ä¸ªè›‹ç™½è´¨å’Œ186,818ä¸ªç›¸äº’ä½œç”¨ï¼Œæ—¨åœ¨è§£å†³æ•°æ®å†—ä½™å’Œæ³„æ¼é—®é¢˜ã€‚é€šè¿‡æ‹“æ‰‘å¯¼å‘å’ŒåŠŸèƒ½å¯¼å‘çš„è¯„ä¼°ä»»åŠ¡ï¼ŒPRINGå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£PPIç½‘ç»œçš„ç»“æ„å’ŒåŠŸèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03698', 'title': 'SAMed-2: Selective Memory Enhanced Medical Segment Anything Model', 'url': 'https://huggingface.co/papers/2507.03698', 'abstract': 'SAMed-2, an adaptation of SAM-2 for medical image segmentation, incorporates a temporal adapter and confidence-driven memory to improve performance across diverse medical datasets and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent "segment anything" efforts show promise by learning from large-scale data, but adapting such models directly to medical images remains challenging due to the complexity of medical data, noisy annotations, and continual learning requirements across diverse modalities and anatomical structures. In this work, we propose SAMed-2, a new foundation model for medical image segmentation built upon the SAM-2 architecture. Specifically, we introduce a temporal adapter into the image encoder to capture image correlations and a confidence-driven memory mechanism to store high-certainty features for later retrieval. This memory-based strategy counters the pervasive noise in large-scale medical datasets and mitigates catastrophic forgetting when encountering new tasks or modalities. To train and evaluate SAMed-2, we curate MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21 medical segmentation tasks. Our experiments on both internal benchmarks and 10 external datasets demonstrate superior performance over state-of-the-art baselines in multi-task scenarios. The code is available at: https://github.com/ZhilingYan/Medical-SAM-Bench.', 'score': 10, 'issue_id': 4715, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': '6eb9d67bc0e6c585', 'authors': ['Zhiling Yan', 'Sifan Song', 'Dingjie Song', 'Yiwei Li', 'Rong Zhou', 'Weixiang Sun', 'Zhennong Chen', 'Sekeun Kim', 'Hui Ren', 'Tianming Liu', 'Quanzheng Li', 'Xiang Li', 'Lifang He', 'Lichao Sun'], 'affiliations': ['Lehigh University, Bethlehem, PA, USA', 'Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA', 'University of Georgia, Athens, GA, USA', 'University of Notre Dame, Notre Dame, IN, USA'], 'pdf_title_img': 'assets/pdf/title_img/2507.03698.jpg', 'data': {'categories': ['#cv', '#benchmark', '#healthcare', '#data', '#dataset', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'SAMed-2: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'SAMed-2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ SAM-2. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MedBank-100k, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¼ 7 Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ 21 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ SAMed-2 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Medical Image Segmentation with SAMed-2', 'desc': 'SAMed-2 is a new model designed for medical image segmentation, enhancing the original SAM-2 framework. It introduces a temporal adapter to the image encoder, which helps in understanding relationships between images over time. Additionally, a confidence-driven memory mechanism is implemented to retain important features, addressing issues like noisy data and preventing loss of knowledge when learning new tasks. The model is trained on a large dataset called MedBank-100k, showing improved performance in various medical imaging tasks compared to existing methods.'}, 'zh': {'title': 'åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°çªç ´ï¼šSAMed-2', 'desc': 'SAMed-2æ˜¯é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„SAM-2æ¨¡å‹çš„æ”¹è¿›ç‰ˆæœ¬ã€‚å®ƒå¼•å…¥äº†æ—¶é—´é€‚é…å™¨å’ŒåŸºäºä¿¡å¿ƒçš„è®°å¿†æœºåˆ¶ï¼Œä»¥æé«˜åœ¨ä¸åŒåŒ»å­¦æ•°æ®é›†å’Œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ—¶é—´é€‚é…å™¨å¸®åŠ©æ•æ‰å›¾åƒä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè€Œè®°å¿†æœºåˆ¶åˆ™å­˜å‚¨é«˜ç½®ä¿¡åº¦ç‰¹å¾ï¼Œä»¥åº”å¯¹åŒ»å­¦æ•°æ®ä¸­çš„å™ªå£°å’Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚é€šè¿‡æ„å»ºMedBank-100kæ•°æ®é›†å¹¶è¿›è¡Œå®éªŒï¼ŒSAMed-2åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05963', 'title': 'Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation', 'url': 'https://huggingface.co/papers/2507.05963', 'abstract': 'Tora2 enhances motion-guided video generation by introducing a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss, enabling simultaneous multi-entity customization and advanced motion control.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://github.com/alibaba/Tora .', 'score': 8, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '0230e47a99a4f7d8', 'authors': ['Zhenghao Zhang', 'Junchao Liao', 'Xiangyu Meng', 'Long Qin', 'Weizhi Wang'], 'affiliations': ['Alibaba Group China'], 'pdf_title_img': 'assets/pdf/title_img/2507.05963.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Tora2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Tora2 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Tora Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. Tora2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Tora2: Revolutionizing Multi-Entity Video Customization', 'desc': 'Tora2 is an advanced model for generating videos that can be customized for multiple entities at the same time. It uses a decoupled personalization extractor to create detailed embeddings that capture the unique features of each entity. The model also incorporates a gated self-attention mechanism to effectively combine different types of information, such as motion and text descriptions, which helps improve the alignment during training. Additionally, Tora2 employs a contrastive loss to ensure that the generated motion is consistent with the personalized features, making it a significant step forward in motion-guided video generation.'}, 'zh': {'title': 'Tora2ï¼šå¤šå®ä½“ä¸ªæ€§åŒ–ä¸è¿åŠ¨æ§åˆ¶çš„çªç ´', 'desc': 'Tora2 æ˜¯ä¸€ç§å¢å¼ºçš„è¿åŠ¨å¼•å¯¼è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨äº†åˆ†ç¦»çš„ä¸ªæ€§åŒ–æå–å™¨å’Œé—¨æ§è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒèƒ½å¤ŸåŒæ—¶å¯¹å¤šä¸ªå®ä½“è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ï¼Œå¹¶å®ç°æ›´é«˜çº§çš„è¿åŠ¨æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”æŸå¤±ï¼ŒTora2 ä¼˜åŒ–äº†è¿åŠ¨åŠ¨æ€å’Œå®ä½“ä¸€è‡´æ€§ï¼Œæå‡äº†å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTora2 åœ¨ä¸ªæ€§åŒ–å®šåˆ¶æ–¹é¢çš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¼ºçš„è¿åŠ¨æ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04723', 'title': 'LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework', 'url': 'https://huggingface.co/papers/2507.04723', 'abstract': "Long-context processing has become a fundamental capability for large language models~(LLMs). To assess model's long-context performance, numerous long-context evaluation benchmarks have been proposed. However, variations in evaluation settings across these benchmarks lead to inconsistent results, making it difficult to draw reliable comparisons. Besides, the high computational cost of long-context evaluation poses a significant barrier for the community to conduct comprehensive assessments of long-context models. In this paper, we propose LOOM-Scope, a comprehensive and efficient framework for long-context evaluation. LOOM-Scope standardizes evaluation settings across diverse benchmarks, supports deployment of efficient long-context inference acceleration methods, and introduces a holistic yet lightweight benchmark suite to evaluate models comprehensively. Homepage: https://loomscope.github.io", 'score': 8, 'issue_id': 4718, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '14c1c5cb4e9b0c49', 'authors': ['Zecheng Tang', 'Haitian Wang', 'Quantong Qiu', 'Baibei Ji', 'Ruoxi Sun', 'Keyan Zhou', 'Juntao Li', 'Min Zhang'], 'affiliations': ['Key Laboratory of Data Intelligence and Advanced Computing, Soochow University', 'Soochow University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.04723.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#inference'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'LOOM-Scope: ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'LOOM-Scope - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞĞ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LOOM-Scope Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹, Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Standardizing Long-Context Evaluation for Better Comparisons', 'desc': 'This paper introduces LOOM-Scope, a new framework designed to improve the evaluation of long-context performance in large language models (LLMs). It addresses the inconsistencies in results caused by varying evaluation settings across existing benchmarks. LOOM-Scope not only standardizes these settings but also incorporates efficient inference acceleration methods to reduce computational costs. Additionally, it provides a lightweight benchmark suite that allows for comprehensive assessments of long-context models, facilitating better comparisons within the research community.'}, 'zh': {'title': 'LOOM-Scopeï¼šé«˜æ•ˆçš„é•¿æ–‡æœ¬è¯„ä¼°æ¡†æ¶', 'desc': 'é•¿æ–‡æœ¬å¤„ç†å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºæœ¬èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°ï¼Œæå‡ºäº†è®¸å¤šé•¿æ–‡æœ¬è¯„ä¼°åŸºå‡†ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†çš„è¯„ä¼°è®¾ç½®å·®å¼‚å¯¼è‡´ç»“æœä¸ä¸€è‡´ï¼Œä½¿å¾—å¯é æ¯”è¾ƒå˜å¾—å›°éš¾ã€‚æ­¤å¤–ï¼Œé•¿æ–‡æœ¬è¯„ä¼°çš„é«˜è®¡ç®—æˆæœ¬ä¹Ÿæˆä¸ºç¤¾åŒºè¿›è¡Œå…¨é¢è¯„ä¼°çš„é‡å¤§éšœç¢ã€‚æœ¬æ–‡æå‡ºäº†LOOM-Scopeï¼Œä¸€ä¸ªå…¨é¢ä¸”é«˜æ•ˆçš„é•¿æ–‡æœ¬è¯„ä¼°æ¡†æ¶ï¼Œæ ‡å‡†åŒ–äº†ä¸åŒåŸºå‡†çš„è¯„ä¼°è®¾ç½®ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„é•¿æ–‡æœ¬æ¨ç†åŠ é€Ÿæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06204', 'title': 'Differential Mamba', 'url': 'https://huggingface.co/papers/2507.06204', 'abstract': 'A novel differential mechanism for Mamba, a selective state-space layer architecture, improves retrieval capabilities and performance by addressing overallocation issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available.', 'score': 7, 'issue_id': 4720, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '69109b333dac5628', 'authors': ['Nadav Schneider', 'Itamar Zimerman', 'Eliya Nachmani'], 'affiliations': ['Ben-Gurion University', 'IBM Research', 'School of Electrical and Computer Engineering, Ben Gurion University of the Negev', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06204.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#optimization', '#long_context', '#hallucinations', '#architecture'], 'emoji': 'ğŸ', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Mamba: Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, ÑƒĞ¼Ğ½ĞµĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mamba, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Mamba. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ±Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Mamba: A New Approach to Mitigate Overallocation in Language Models', 'desc': 'This paper presents a new differential mechanism for the Mamba architecture, which uses selective state-space layers to enhance performance in language modeling. The authors identify that traditional sequence models, like Transformers, often focus too much on irrelevant information, leading to poor results. By adapting differential design techniques to Mamba, they show that careful modifications can significantly improve retrieval capabilities and overall performance. The results from their experiments indicate that this new approach effectively reduces the overallocation problem, making Mamba models more robust and efficient.'}, 'zh': {'title': 'æå‡Mambaæ¶æ„æ€§èƒ½çš„æ–°æœºåˆ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å·®åˆ†æœºåˆ¶ï¼Œç”¨äºMambaæ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å±‚çš„è®¾è®¡ï¼Œæ—¨åœ¨æ”¹å–„æ£€ç´¢èƒ½åŠ›å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„åºåˆ—æ¨¡å‹å¦‚Transformerå’ŒRNNå¸¸å¸¸å¯¹æ— å…³ä¸Šä¸‹æ–‡è¿‡åº¦å…³æ³¨ï¼Œå¯¼è‡´ä¸­é—´è¡¨ç¤ºå™ªå£°å¢å¤šï¼Œä»è€Œå½±å“å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•åœ°å°†å·®åˆ†è®¾è®¡åº”ç”¨äºMambaå¹¶ä¸è¶³å¤Ÿï¼Œéœ€è¦è¿›è¡Œä»”ç»†çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–°æœºåˆ¶æœ‰æ•ˆç¼“è§£äº†Mambaæ¨¡å‹ä¸­çš„è¿‡åº¦åˆ†é…é—®é¢˜ï¼Œæå‡äº†å…¶æ£€ç´¢èƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05920', 'title': 'High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.05920', 'abstract': "MGPO, an end-to-end reinforcement learning framework, enhances large multi-modal models' ability to focus on key visual regions without requiring additional grounding annotations, improving performance on both in-distribution and out-of-distribution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art large multi-modal models (LMMs) face challenges when processing high-resolution images, as these inputs are converted into enormous visual tokens, many of which are irrelevant to the downstream task. In this paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an end-to-end reinforcement learning (RL) framework that enables LMMs to iteratively focus on key visual regions by automatically cropping sub-images, based on model-predicted grounding coordinates within a multi-turn conversation framework. Compared to supervised fine-tuning (SFT), which requires costly additional grounding annotations, our approach highlights that LMMs can emerge robust grounding abilities during the RL training process, leveraging only a binary reward function derived from the correctness of the final answer. Additionally, we observe that LMMs struggle to autonomously trigger visual grounding during the rollout process. To address this cold start problem, we design a multi-turn conversational template and restrict policy loss computation to model outputs generated across multiple dialogue rounds, thereby promoting stable optimization. Extensive experiments demonstrate that, when trained on standard visual-question-short answering data without grounding annotations, MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and 5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench. Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at https://github.com/EvolvingLMMs-Lab/MGPO.", 'score': 7, 'issue_id': 4717, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': 'be1d59008c501b7d', 'authors': ['Xinyu Huang', 'Yuhao Dong', 'Weiwei Tian', 'Bo Li', 'Rui Feng', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05920.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#rl', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'MGPO - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. MGPO Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MGPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¸ out-of-distribution Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering LMMs with Reinforcement Learning for Visual Grounding', 'desc': "The paper introduces MGPO, a novel reinforcement learning framework designed to enhance the performance of large multi-modal models (LMMs) by enabling them to focus on important visual areas without needing extra grounding annotations. MGPO uses a multi-turn conversational approach to iteratively crop sub-images based on predicted grounding coordinates, improving the model's ability to ground visual information effectively. Unlike traditional supervised fine-tuning methods, MGPO relies on a simple binary reward system derived from the accuracy of the final output, allowing LMMs to develop grounding skills during training. The results show significant improvements in both in-distribution and out-of-distribution benchmarks, demonstrating MGPO's effectiveness in enhancing visual grounding capabilities in LMMs."}, 'zh': {'title': 'MGPOï¼šæ— éœ€æ ‡æ³¨çš„å¼ºåŒ–å­¦ä¹ èšç„¦å…³é”®è§†è§‰åŒºåŸŸ', 'desc': 'MGPOæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶çš„å…³é”®è§†è§‰åŒºåŸŸèšç„¦èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„æ ‡æ³¨ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å¤šè½®å¯¹è¯æ¡†æ¶ä¸­è‡ªåŠ¨è£å‰ªå­å›¾åƒï¼Œåˆ©ç”¨æ¨¡å‹é¢„æµ‹çš„å®šä½åæ ‡ï¼Œå¸®åŠ©æ¨¡å‹é€æ­¥èšç„¦äºé‡è¦åŒºåŸŸã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒMGPOä»…ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§æ¥æä¾›äºŒå…ƒå¥–åŠ±ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŸ¹å…»å‡ºæ›´å¼ºçš„å®šä½èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMGPOåœ¨æ²¡æœ‰æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨æ ‡å‡†è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05578', 'title': 'The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation', 'url': 'https://huggingface.co/papers/2507.05578', 'abstract': 'The paper reviews recent studies on memorization in Large Language Models, exploring factors that influence memorization, detection methodologies, and mitigation strategies, while addressing privacy and ethical implications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the minimization of harmful memorization with utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.', 'score': 4, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '9fd6f105854c8570', 'authors': ['Alexander Xiong', 'Xuandong Zhao', 'Aneesh Pappu', 'Dawn Song'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.05578.jpg', 'data': {'categories': ['#healthcare', '#hallucinations', '#survey', '#data', '#training', '#ethics'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ² LLM: Ğ¾Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ¾ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, ĞºĞ°Ğº Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² LLM.'}, 'en': {'title': 'Understanding and Mitigating Memorization in Large Language Models', 'desc': 'This paper reviews how Large Language Models (LLMs) memorize information from their training data, which can lead to privacy concerns. It discusses factors that contribute to this memorization, such as data duplication and training methods. The paper also evaluates various techniques for detecting memorized data, like membership inference and adversarial prompting. Finally, it suggests strategies to reduce harmful memorization while maintaining model performance, highlighting the need for further research in this area.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹çš„è®°å¿†ç°è±¡ä¸æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡å›é¡¾äº†å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®°å¿†ç°è±¡çš„æœ€æ–°ç ”ç©¶ï¼Œæ¢è®¨äº†å½±å“è®°å¿†çš„å› ç´ ã€æ£€æµ‹æ–¹æ³•å’Œç¼“è§£ç­–ç•¥ï¼ŒåŒæ—¶å…³æ³¨éšç§å’Œä¼¦ç†é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ä¼šè®°ä½è®­ç»ƒæ•°æ®ï¼Œè¿™å¼•å‘äº†å…³äºæ¨¡å‹è¡Œä¸ºå’Œéšç§é£é™©çš„å…³é”®é—®é¢˜ã€‚è®ºæ–‡åˆ†æäº†è®­ç»ƒæ•°æ®é‡å¤ã€è®­ç»ƒåŠ¨æ€å’Œå¾®è°ƒè¿‡ç¨‹ç­‰å…³é”®é©±åŠ¨å› ç´ ï¼Œå¹¶è¯„ä¼°äº†å‰ç¼€æå–ã€æˆå‘˜æ¨æ–­å’Œå¯¹æŠ—æ€§æç¤ºç­‰æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè®ºæ–‡è®¨è®ºäº†æ•°æ®æ¸…ç†ã€å·®åˆ†éšç§å’Œåè®­ç»ƒé—å¿˜ç­‰ç¼“è§£ç­–ç•¥ï¼Œå¼ºè°ƒåœ¨å‡å°‘æœ‰å®³è®°å¿†ä¸ä¿æŒæ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04610', 'title': 'any4: Learned 4-bit Numeric Representation for LLMs', 'url': 'https://huggingface.co/papers/2507.04610', 'abstract': 'any4 is a learned 4-bit weight quantization method for LLMs that achieves high accuracy without preprocessing and uses a GPU-efficient lookup table strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present any4, a learned 4-bit weight quantization solution for large language models (LLMs) providing arbitrary numeric representations without requiring pre-processing of weights or activations. any4 yields higher accuracy compared to other related 4-bit numeric representation types: int4, fp4 and nf4, as evaluated on a range of model sizes, generations and families (Llama 2, Llama 3, Mistral and Mixtral). While any4 does not require preprocessing of weights or activations, it is also competitive with orthogonal techniques that require such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3 and any2 and show competitiveness at lower bits. Additionally, we show that we can calibrate using a single curated diverse sample rather than hundreds of samples from a dataset as done in most quantization approaches. We also open source tinygemm, a latency optimized GPU matrix multiplication library for LLMs, that implements any4 using a GPU-efficient lookup table strategy along with other common quantization methods. We open source our code at https://github.com/facebookresearch/any4 .', 'score': 4, 'issue_id': 4715, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '677d34e801c63489', 'authors': ['Mostafa Elhoushi', 'Jeff Johnson'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2507.04610.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'any4: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ any4 - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ 4-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. any4 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ»Ñ GPU ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ any3 Ğ¸ any2, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ±Ğ¸Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'any4: Efficient 4-Bit Weight Quantization for High-Accuracy LLMs', 'desc': 'The paper introduces any4, a novel method for quantizing weights in large language models (LLMs) to 4 bits, which maintains high accuracy without the need for preprocessing. This method outperforms existing 4-bit representations like int4, fp4, and nf4 across various model sizes and families. Additionally, any4 allows for calibration using just one diverse sample, contrasting with traditional methods that require many samples. The authors also provide an open-source GPU-optimized library, tinygemm, to implement this quantization technique efficiently.'}, 'zh': {'title': 'any4ï¼šé«˜æ•ˆçš„4ä½æƒé‡é‡åŒ–æ–¹æ³•', 'desc': 'any4æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å­¦ä¹ å‹4ä½æƒé‡é‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢„å¤„ç†çš„æƒ…å†µä¸‹å®ç°é«˜ç²¾åº¦çš„æ•°å€¼è¡¨ç¤ºã€‚ä¸å…¶ä»–4ä½æ•°å€¼è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚int4ã€fp4å’Œnf4ï¼‰ç›¸æ¯”ï¼Œany4åœ¨å¤šç§æ¨¡å‹è§„æ¨¡å’Œç±»å‹ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥ä½¿ç”¨å•ä¸ªå¤šæ ·åŒ–æ ·æœ¬è¿›è¡Œæ ¡å‡†ï¼Œè€Œä¸æ˜¯åƒå¤§å¤šæ•°é‡åŒ–æ–¹æ³•é‚£æ ·éœ€è¦æ•°ç™¾ä¸ªæ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€æºäº†tinygemmï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹LLMsä¼˜åŒ–çš„GPUçŸ©é˜µä¹˜æ³•åº“ï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„æŸ¥æ‰¾è¡¨ç­–ç•¥æ¥å®ç°any4ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06230', 'title': 'Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion', 'url': 'https://huggingface.co/papers/2507.06230', 'abstract': 'SceneDINO achieves state-of-the-art segmentation accuracy in unsupervised semantic scene completion by leveraging self-supervised representation learning and 2D unsupervised scene understanding techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.', 'score': 3, 'issue_id': 4725, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '12d51a9f4c4400e9', 'authors': ['Aleksandar JevtiÄ‡', 'Christoph Reich', 'Felix Wimbauer', 'Oliver Hahn', 'Christian Rupprecht', 'Stefan Roth', 'Daniel Cremers'], 'affiliations': ['ELIZA', 'MCML', 'TU Darmstadt', 'TU Munich', 'University of Oxford', 'hessian.AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.06230.jpg', 'data': {'categories': ['#optimization', '#cv', '#transfer_learning', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğµ 3D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'SceneDINO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ (SSC). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 2D ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ SSC. SceneDINO Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ 3D Ğ¸ 2D ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Unsupervised 3D Scene Understanding with SceneDINO', 'desc': 'SceneDINO is a novel method for unsupervised semantic scene completion (SSC) that excels in accurately inferring 3D geometry and semantics from single images. It utilizes self-supervised representation learning and 2D scene understanding techniques, avoiding the need for expensive ground-truth annotations. The model leverages multi-view consistency self-supervision to train, allowing it to generate 3D features and semantics without any labeled data. SceneDINO achieves state-of-the-art segmentation accuracy, demonstrating its effectiveness in both 3D and 2D scene understanding tasks.'}, 'zh': {'title': 'æ— ç›‘ç£è¯­ä¹‰åœºæ™¯è¡¥å…¨çš„æ–°çªç ´', 'desc': 'SceneDINOæ˜¯ä¸€ç§æ— ç›‘ç£çš„è¯­ä¹‰åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­æ¨æ–­å‡ºåœºæ™¯çš„3Då‡ ä½•å½¢çŠ¶å’Œè¯­ä¹‰ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ å’Œ2Dæ— ç›‘ç£åœºæ™¯ç†è§£æŠ€æœ¯ï¼Œé¿å…äº†å¯¹æ˜‚è´µçš„çœŸå®æ ‡æ³¨çš„ä¾èµ–ã€‚é€šè¿‡å¤šè§†å›¾ä¸€è‡´æ€§çš„è‡ªæˆ‘ç›‘ç£è®­ç»ƒï¼ŒSceneDINOåœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ã€‚è¯¥æ–¹æ³•çš„3Dç‰¹å¾è’¸é¦æŠ€æœ¯ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè·å¾—æ— ç›‘ç£çš„3Dè¯­ä¹‰ï¼Œä¸ºå•å›¾åƒ3Dåœºæ™¯ç†è§£å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05201', 'title': 'MedGemma Technical Report', 'url': 'https://huggingface.co/papers/2507.05201', 'abstract': "MedGemma, a collection of medical vision-language foundation models, demonstrates advanced understanding and reasoning in healthcare applications, improving performance across various tasks and maintaining general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.", 'score': 2, 'issue_id': 4728, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': '7f5f922ddf886fbb', 'authors': ['Andrew Sellergren', 'Sahar Kazemzadeh', 'Tiam Jaroensri', 'Atilla Kiraly', 'Madeleine Traverse', 'Timo Kohlberger', 'Shawn Xu', 'Fayaz Jamil', 'CÃ­an Hughes', 'Charles Lau', 'Justin Chen', 'Fereshteh Mahvar', 'Liron Yatziv', 'Tiffany Chen', 'Bram Sterling', 'Stefanie Anna Baby', 'Susanna Maria Baby', 'Jeremy Lai', 'Samuel Schmidgall', 'Lu Yang', 'Kejia Chen', 'Per Bjornsson', 'Shashir Reddy', 'Ryan Brush', 'Kenneth Philbrick', 'Howard Hu', 'Howard Yang', 'Richa Tiwari', 'Sunny Jansen', 'Preeti Singh', 'Yun Liu', 'Shekoofeh Azizi', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre RamÃ©', 'Morgane Riviere', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Dmitry Lepikhin', 'Vlad Feinberg', 'Sebastian Borgeaud', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'LÃ©onard Hussenot', 'Armand Joulin', 'Olivier Bachem', 'Yossi Matias', 'Katherine Chou', 'Avinatan Hassidim', 'Kavi Goel', 'Clement Farabet', 'Joelle Barral', 'Tris Warkentin', 'Jonathon Shlens', 'David Fleet', 'Victor Cotruta', 'Omar Sanseviero', 'Gus Martins', 'Phoebe Kirk', 'Anand Rao', 'Shravya Shetty', 'David F. Steiner', 'Can Kirmizibayrak', 'Rory Pilgrim', 'Daniel Golden', 'Lin Yang'], 'affiliations': ['Google DeepMind', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.05201.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#multimodal', '#science', '#dataset', '#training', '#healthcare'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MedGemma: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'MedGemma - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Gemma 3. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MedGemma ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 3, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'MedGemma: Revolutionizing Healthcare AI with Vision-Language Models', 'desc': "MedGemma is a set of advanced medical vision-language foundation models designed to enhance AI applications in healthcare. It excels in understanding and reasoning with medical images and text, outperforming similar models and nearing the effectiveness of specialized systems. The models show significant improvements in various tasks, such as medical question answering and chest X-ray classification, while also reducing errors in electronic health record retrieval. Additionally, MedSigLIP, a vision encoder, boosts MedGemma's capabilities, making it a valuable resource for accelerating medical research and application development."}, 'zh': {'title': 'MedGemmaï¼šåŠ é€ŸåŒ»ç–—AIåº”ç”¨çš„åŸºç¡€æ¨¡å‹', 'desc': 'MedGemmaæ˜¯ä¸€ä¸ªåŒ»ç–—è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹çš„é›†åˆï¼Œå±•ç¤ºäº†åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„é«˜çº§ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¿æŒäº†Gemma 3åŸºç¡€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚MedGemmaåœ¨åŒ»å­¦å¤šæ¨¡æ€é—®ç­”ã€èƒ¸éƒ¨Xå…‰å‘ç°åˆ†ç±»ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå¤–ä»»åŠ¡æ—¶ã€‚é€šè¿‡å¾®è°ƒï¼ŒMedGemmaåœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°è¿›ä¸€æ­¥æå‡ï¼Œé”™è¯¯ç‡é™ä½äº†50%ï¼Œå¹¶ä¸ç°æœ‰çš„ä¸“ä¸šæ–¹æ³•ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.03728', 'title': 'FAROS: Fair Graph Generation via Attribute Switching Mechanisms', 'url': 'https://huggingface.co/papers/2507.03728', 'abstract': "FAROS is a framework that enhances fairness in graph diffusion models by strategically switching node attributes during generation to balance accuracy and fairness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in graph diffusion models (GDMs) have enabled the synthesis of realistic network structures, yet ensuring fairness in the generated data remains a critical challenge. Existing solutions attempt to mitigate bias by re-training the GDMs with ad-hoc fairness constraints. Conversely, with this work, we propose FAROS, a novel FAir graph geneRatiOn framework leveraging attribute Switching mechanisms and directly running in the generation process of the pre-trained GDM. Technically, our approach works by altering nodes' sensitive attributes during the generation. To this end, FAROS calculates the optimal fraction of switching nodes, and selects the diffusion step to perform the switch by setting tailored multi-criteria constraints to preserve the node-topology profile from the original distribution (a proxy for accuracy) while ensuring the edge independence on the sensitive attributes for the generated graph (a proxy for fairness). Our experiments on benchmark datasets for link prediction demonstrate that the proposed approach effectively reduces fairness discrepancies while maintaining comparable (or even higher) accuracy performance to other similar baselines. Noteworthy, FAROS is also able to strike a better accuracy-fairness trade-off than other competitors in some of the tested settings under the Pareto optimality concept, demonstrating the effectiveness of the imposed multi-criteria constraints.", 'score': 1, 'issue_id': 4723, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 Ğ¸ÑĞ»Ñ', 'en': 'July 4', 'zh': '7æœˆ4æ—¥'}, 'hash': 'a0d6f2a3eb887939', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#graphs', '#ethics', '#benchmark', '#multimodal', '#dataset'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'FAROS - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² ÑƒĞ·Ğ»Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑˆĞ°Ğ³ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ±ĞµÑ€ Ğ¾Ñ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FAROS ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Balancing Fairness and Accuracy in Graph Generation with FAROS', 'desc': 'FAROS is a framework designed to improve fairness in graph diffusion models (GDMs) by modifying node attributes during the generation process. It addresses the challenge of bias in generated data by strategically switching sensitive attributes of nodes while preserving the overall network structure. The framework calculates the optimal number of nodes to switch and selects the appropriate diffusion steps to maintain accuracy and fairness. Experimental results show that FAROS not only reduces fairness discrepancies but also achieves competitive accuracy compared to existing methods, highlighting its effectiveness in balancing these two important aspects.'}, 'zh': {'title': 'FAROSï¼šå¹³è¡¡å‡†ç¡®æ€§ä¸å…¬å¹³æ€§çš„å›¾ç”Ÿæˆæ¡†æ¶', 'desc': 'FAROSæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°åˆ‡æ¢èŠ‚ç‚¹å±æ€§æ¥å¢å¼ºå›¾æ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé¢„è®­ç»ƒå›¾æ‰©æ•£æ¨¡å‹æ—¶ï¼ŒåŠ¨æ€è°ƒæ•´èŠ‚ç‚¹çš„æ•æ„Ÿå±æ€§ï¼Œä»¥å¹³è¡¡ç”Ÿæˆæ•°æ®çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ã€‚FAROSé€šè¿‡è®¡ç®—æœ€ä½³åˆ‡æ¢èŠ‚ç‚¹çš„æ¯”ä¾‹ï¼Œå¹¶è®¾ç½®å¤šæ ‡å‡†çº¦æŸï¼Œç¡®ä¿ç”Ÿæˆå›¾çš„è¾¹ç¼˜ç‹¬ç«‹æ€§å’ŒèŠ‚ç‚¹æ‹“æ‰‘ç‰¹å¾çš„ä¿ç•™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAROSåœ¨å‡å°‘å…¬å¹³æ€§å·®å¼‚çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸å…¶ä»–åŸºçº¿ç›¸å½“æˆ–æ›´é«˜çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.06137', 'title': 'NeoBabel: A Multilingual Open Tower for Visual Generation', 'url': 'https://huggingface.co/papers/2507.06137', 'abstract': 'NeoBabel, a multilingual image generation framework, achieves state-of-the-art performance across six languages while maintaining efficiency and cultural alignment, outperforming existing multilingual models.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.', 'score': 0, 'issue_id': 4733, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 Ğ¸ÑĞ»Ñ', 'en': 'July 8', 'zh': '7æœˆ8æ—¥'}, 'hash': '15b4ba4c927c0d0b', 'authors': ['Mohammad Mahdi Derakhshani', 'Dheeraj Varghese', 'Marzieh Fadaee', 'Cees G. M. Snoek'], 'affiliations': ['Cohere Labs', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2507.06137.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#alignment', '#benchmark', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'NeoBabel: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'NeoBabel - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑˆĞµÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. NeoBabel Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ¼, Ğ° ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Multilingual Image Generation with NeoBabel', 'desc': 'NeoBabel is a multilingual image generation framework that excels in creating images from text in six different languages, achieving top performance while being efficient and culturally relevant. Unlike previous models that relied on translation, which often led to inaccuracies and inefficiencies, NeoBabel uses a unique training approach that combines multilingual pretraining with high-resolution instruction tuning. It has been evaluated using new multilingual benchmarks and shows superior performance compared to existing models, particularly in multilingual tasks. The framework is designed to be smaller and more efficient, proving that supporting multiple languages can enhance the overall quality and robustness of AI-generated content.'}, 'zh': {'title': 'NeoBabelï¼šå¤šè¯­è¨€ç”Ÿæˆçš„æœªæ¥', 'desc': 'NeoBabelæ˜¯ä¸€ä¸ªå¤šè¯­è¨€å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å…­ç§è¯­è¨€ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§å’Œæ–‡åŒ–ä¸€è‡´æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šè¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡çš„å¤šè¯­è¨€é¢„è®­ç»ƒå’Œé«˜åˆ†è¾¨ç‡çš„æŒ‡ä»¤è°ƒä¼˜è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒè‹±è¯­ã€ä¸­æ–‡ã€è·å…°è¯­ã€æ³•è¯­ã€å°åœ°è¯­å’Œæ³¢æ–¯è¯­ã€‚NeoBabelåœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è‹±è¯­ä»»åŠ¡ä¸Šä¸é¢†å…ˆæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨å¤šè¯­è¨€åŸºå‡†ä¸Šè¶…å‡ºå®ƒä»¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šè¯­è¨€èƒ½åŠ›ä¸ä»…ä¸æ˜¯ä¸€ç§æƒè¡¡ï¼Œè€Œæ˜¯ç”ŸæˆAIä¸­æé«˜é²æ£’æ€§ã€é«˜æ•ˆæ€§å’Œæ–‡åŒ–å¿ å®åº¦çš„å‚¬åŒ–å‰‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.05411', 'title': 'AXLearn: Modular Large Model Training on Heterogeneous Infrastructure', 'url': 'https://huggingface.co/papers/2507.05411', 'abstract': "AXLearn is a modular deep learning system designed for scalable training on heterogeneous hardware, maintaining performance and modularity through efficient code integration methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We design and implement AXLearn, a production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-the-art deep learning systems, AXLearn has a unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn.", 'score': 0, 'issue_id': 4722, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'b9bc54d8cca9de71', 'authors': ['Mark Lee', 'Tom Gunter', 'Chang Lan', 'John Peebles', 'Hanzhi Zhou', 'Kelvin Zou', 'Sneha Bangalore', 'Chung-Cheng Chiu', 'Nan Du', 'Xianzhi Du', 'Philipp Dufter', 'Ruixuan Hou', 'Haoshuo Huang', 'Dongseong Hwang', 'Xiang Kong', 'Jinhao Lei', 'Tao Lei', 'Meng Li', 'Li Li', 'Jiarui Lu', 'Zhiyun Lu', 'Yiping Ma', 'David Qiu', 'Vivek Rathod', 'Senyu Tong', 'Zhucheng Tu', 'Jianyu Wang', 'Yongqiang Wang', 'Zirui Wang', 'Floris Weers', 'Sam Wiseman', 'Guoli Yin', 'Bowen Zhang', 'Xiyou Zhou', 'Danyang Zhuo', 'Cheng Leong', 'Ruoming Pang'], 'affiliations': ['Apple', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05411.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸', 'desc': 'AXLearn - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. AXLearn Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞ°Ñ… ĞºĞ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Modular Deep Learning for Scalable Performance', 'desc': 'AXLearn is a deep learning system that allows for efficient training of large models on various types of hardware. It emphasizes modularity, meaning different parts of the system can be easily combined and reused, which speeds up the development process. The system uses a unique way to measure modularity, showing that it keeps complexity low even as more components are added. This design enables quick integration of new features while maintaining high performance, making AXLearn a competitive choice among existing deep learning frameworks.'}, 'zh': {'title': 'AXLearnï¼šé«˜æ•ˆæ¨¡å—åŒ–çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿ', 'desc': 'AXLearnæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒåœ¨å¼‚æ„ç¡¬ä»¶ä¸Šè¿›è¡Œå¯æ‰©å±•çš„è®­ç»ƒã€‚å®ƒé€šè¿‡é«˜æ•ˆçš„ä»£ç é›†æˆæ–¹æ³•ï¼Œä¿æŒäº†æ€§èƒ½å’Œæ¨¡å—åŒ–çš„ç‰¹ç‚¹ã€‚AXLearnçš„å†…éƒ¨æ¥å£éµå¾ªä¸¥æ ¼çš„å°è£…åŸåˆ™ï¼Œä½¿å¾—ä¸åŒç»„ä»¶å¯ä»¥å¿«é€Ÿç»„è£…ï¼Œä¾¿äºåœ¨å¼‚æ„è®¡ç®—åŸºç¡€è®¾æ–½ä¸Šè¿›è¡Œæ¨¡å‹å¼€å‘å’Œå®éªŒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ¨¡å—åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡ä»£ç è¡Œå¤æ‚åº¦ï¼ˆLoCå¤æ‚åº¦ï¼‰æ¥å±•ç¤ºç³»ç»Ÿåœ¨æ‰©å±•ç»„ä»¶æ—¶ä¿æŒæ’å®šå¤æ‚åº¦çš„èƒ½åŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (1)', '#alignment (2)', '#architecture (7)', '#audio', '#benchmark (13)', '#cv (2)', '#data (4)', '#dataset (9)', '#diffusion (1)', '#ethics (2)', '#games (4)', '#graphs (2)', '#hallucinations (2)', '#healthcare (4)', '#inference (4)', '#interpretability (1)', '#leakage (1)', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math', '#multilingual (2)', '#multimodal (5)', '#open_source (8)', '#optimization (13)', '#plp (1)', '#rag', '#reasoning (7)', '#rl (6)', '#rlhf (1)', '#robotics (1)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (2)', '#synthetic', '#training (12)', '#transfer_learning (4)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-09 20:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-09 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-09 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    