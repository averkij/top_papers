{
    "date": {
        "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 3",
        "zh": "4æœˆ3æ—¥"
    },
    "time_utc": "2025-04-03 14:11",
    "weekday": 3,
    "issue_id": 3052,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.00999",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "url": "https://huggingface.co/papers/2504.00999",
            "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.",
            "score": 54,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "bb6506ffd72aed19",
            "authors": [
                "Siyuan Li",
                "Luyuan Zhang",
                "Zedong Wang",
                "Juanxi Tian",
                "Cheng Tan",
                "Zicheng Liu",
                "Chang Yu",
                "Qingsong Xie",
                "Haonan Lu",
                "Haoqian Wang",
                "Zhen Lei"
            ],
            "affiliations": [
                "CAIR, HKISI-CAS",
                "MAIS CASIA",
                "OPPO AI Center",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (VQ) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. MergeVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MergeAR, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MergeVQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "MergeVQ: Bridging Image Generation and Representation Learning Efficiently",
                    "desc": "This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed."
                },
                "zh": {
                    "title": "MergeVQï¼šæå‡å›¾åƒç”Ÿæˆä¸è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MergeVQï¼Œæ—¨åœ¨æ”¹å–„åŸºäºå‘é‡é‡åŒ–çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨ç¼–ç å™¨ä¸­å¼•å…¥ä»¤ç‰Œåˆå¹¶æŠ€æœ¯ï¼ŒMergeVQèƒ½å¤Ÿåœ¨è‡ªæ³¨æ„åŠ›å—åè§£è€¦æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¢å¤ç»†èŠ‚ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨KVç¼“å­˜å‹ç¼©æ¥æé«˜é¢„æµ‹æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeVQåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00883",
            "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
            "url": "https://huggingface.co/papers/2504.00883",
            "abstract": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.",
            "score": 40,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "43fba84cc49adfad",
            "authors": [
                "Zhenyi Liao",
                "Qingsong Xie",
                "Yanhao Zhang",
                "Zijian Kong",
                "Haonan Lu",
                "Zhenyu Yang",
                "Zhijie Deng"
            ],
            "affiliations": [
                "OPPO AI Center",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00883.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#video",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ R1-Zero. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2-VL Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GRPO Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VSI-100k Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vsGRPO-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2-VL-7B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ LLaVA-NeXT-Video-72B."
                },
                "en": {
                    "title": "Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training",
                    "desc": "This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶èšç„¦äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åŸºç¡€çš„è§†è§‰ç©ºé—´æ™ºèƒ½ï¼ˆVSIï¼‰æ–¹é¢ã€‚æˆ‘ä»¬å‘ç°å°åˆ°ä¸­å‹çš„Qwen2-VLæ¨¡å‹æ— æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¿€æ´»å…¶è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤å¼•å…¥äº†GRPOè®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„VSI-100kæ•°æ®é›†è¿›è¡Œæ”¹è¿›ã€‚ç»è¿‡120ä¸ªGPUå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„vsGRPO-2Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºç¡€æ¨¡å‹12.1%ï¼Œå¹¶ä¸”vsGRPO-7Bæ¨¡å‹çš„è¡¨ç°ä¸æœ€ä½³å¼€æºæ¨¡å‹LLaVA-NeXT-Video-72Bç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¿æŒKLæƒ©ç½šåœ¨GRPOä¸­æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”vsGRPOåœ¨ä¸ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–åŸºçº¿çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01014",
            "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
            "url": "https://huggingface.co/papers/2504.01014",
            "abstract": "Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.",
            "score": 25,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "98efa783105c3173",
            "authors": [
                "Junhao Cheng",
                "Yuying Ge",
                "Yixiao Ge",
                "Jing Liao",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "City University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01014.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "AnimeGamer: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
                    "desc": "AnimeGamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². AnimeGamer Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Anime into Interactive Gaming with Dynamic AI",
                    "desc": "This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations."
                },
                "zh": {
                    "title": "AnimeGamerï¼šåŠ¨æ€äº’åŠ¨çš„æ— é™åŠ¨æ¼«æ¸¸æˆä½“éªŒ",
                    "desc": "æœ€è¿‘åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢çš„è¿›å±•ä¸ºç”Ÿæˆæ¸¸æˆå¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAnimeGamerçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŠ¨æ€åŠ¨ç”»é•œå¤´ï¼Œä»¥å¢å¼ºæ¸¸æˆçš„äº’åŠ¨æ€§å’Œæ²‰æµ¸æ„Ÿã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œæ„ŸçŸ¥çš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒAnimeGamerèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€å˜åŒ–çš„æ¸¸æˆçŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnimeGameråœ¨æ¸¸æˆä½“éªŒçš„å„ä¸ªæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01956",
            "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
            "url": "https://huggingface.co/papers/2504.01956",
            "abstract": "Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene",
            "score": 21,
            "issue_id": 3042,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "44f1db8ef8cc244a",
            "authors": [
                "Hanyang Wang",
                "Fangfu Liu",
                "Jiawei Chi",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01956.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VideoScene: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoScene - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ 3D-aware leap flow Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. VideoScene Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D."
                },
                "en": {
                    "title": "Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models",
                    "desc": "This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆ3Dåœºæ™¯çš„VideoScene",
                    "desc": "ä»ç¨€ç–è§†å›¾æ¢å¤3Dåœºæ™¯æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡å‡ ä½•æ­£åˆ™åŒ–æˆ–å‰é¦ˆç¡®å®šæ€§æ¨¡å‹ç­‰ä¸“é—¨è§£å†³æ–¹æ¡ˆæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨è¾“å…¥è§†å›¾é‡å è¾ƒå°‘ä¸”è§†è§‰ä¿¡æ¯ä¸è¶³æ—¶ï¼Œæ€§èƒ½ä»ç„¶ä¸‹é™ã€‚æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºå‡ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åˆç†3Dç»“æ„çš„è§†é¢‘ç‰‡æ®µã€‚æœ¬æ–‡æå‡ºäº†VideoSceneï¼Œé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹æç‚¼ç”Ÿæˆ3Dåœºæ™¯ï¼Œè®¾è®¡äº†3Dæ„ŸçŸ¥çš„è·ƒè¿æµè’¸é¦ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01724",
            "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
            "url": "https://huggingface.co/papers/2504.01724",
            "abstract": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.",
            "score": 20,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "d59102a274145730",
            "authors": [
                "Yuxuan Luo",
                "Zhengkun Rong",
                "Lizhen Wang",
                "Longhao Zhang",
                "Tianshu Hu",
                "Yongming Zhu"
            ],
            "affiliations": [
                "Bytedance Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01724.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DreamActor-M1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, 3D-ÑÑ„ĞµÑ€Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ 3D-ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼. DreamActor-M1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency",
                    "desc": "The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations."
                },
                "zh": {
                    "title": "çªç ´åŠ¨ç”»ç”Ÿæˆçš„å±€é™æ€§ï¼ŒDreamActor-M1å¼•é¢†æ–°æ½®æµï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶DreamActor-M1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåŸºç¡€çš„äººä½“åŠ¨ç”»æ–¹æ³•åœ¨ç»†ç²’åº¦æ•´ä½“å¯æ§æ€§ã€å¤šå°ºåº¦é€‚åº”æ€§å’Œé•¿æœŸæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡æ··åˆæ§åˆ¶ä¿¡å·ï¼Œç»“åˆéšå¼é¢éƒ¨è¡¨ç¤ºã€3Då¤´éƒ¨çƒä½“å’Œ3Dèº«ä½“éª¨æ¶ï¼Œå®ç°äº†å¯¹é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œçš„å¼ºå¤§æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒåŠ¨ç”»çš„è¡¨ç°åŠ›å’Œèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„èº«ä½“å§¿åŠ¿å’Œå›¾åƒå°ºåº¦ï¼Œé‡‡ç”¨äº†æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡å’Œå°ºåº¦çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚–åƒã€ä¸ŠåŠèº«å’Œå…¨èº«ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æœŸä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20783",
            "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
            "url": "https://huggingface.co/papers/2503.20783",
            "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.",
            "score": 20,
            "issue_id": 3044,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "c5971e424bc52a6b",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… ÑƒĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². Ğ‘Ñ‹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Group Relative Policy Optimization (GRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dr. GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Unbiased RL Optimization",
                    "desc": "This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) without needing supervised fine-tuning. It examines the impact of different base models, particularly DeepSeek-V3-Base and Qwen2.5, on RL performance, revealing that pretraining characteristics can lead to inherent biases in reasoning capabilities. The authors also identify an optimization bias in the Group Relative Policy Optimization (GRPO) method, which can inflate response lengths during training. To counter this, they propose Dr. GRPO, a new optimization technique that enhances token efficiency while preserving reasoning accuracy, achieving a notable 43.3% accuracy on the AIME 2024 benchmark with a 7B base model."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æ–°çªç ´",
                    "desc": "DeepSeek-R1-Zeroå±•ç¤ºäº†å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†R1-Zeroè®­ç»ƒçš„ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼šåŸºç¡€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¤šç§åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-V3-Baseï¼Œä»¥äº†è§£é¢„è®­ç»ƒç‰¹æ€§å¦‚ä½•å½±å“RLæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼ŒDeepSeek-V3-Baseå·²ç»å±•ç°å‡ºâ€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»ï¼Œè€ŒQwen2.5åŸºç¡€æ¨¡å‹å³ä½¿åœ¨æ²¡æœ‰æç¤ºæ¨¡æ¿çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæš—ç¤ºäº†æ½œåœ¨çš„é¢„è®­ç»ƒåå·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23368",
            "title": "Towards Physically Plausible Video Generation via VLM Planning",
            "url": "https://huggingface.co/papers/2503.23368",
            "abstract": "Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.",
            "score": 19,
            "issue_id": 3050,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "f3087a720104ea83",
            "authors": [
                "Xindi Yang",
                "Baolu Li",
                "Yiming Zhang",
                "Zhenfei Yin",
                "Lei Bai",
                "Liqian Ma",
                "Zhiyong Wang",
                "Jianfei Cai",
                "Tien-Tsin Wong",
                "Huchuan Lu",
                "Xu Jia"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Monash University",
                "Oxford University",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Sydney",
                "ZMO AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23368.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#diffusion",
                    "#video",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒ (VLM) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ’Ğ¸Ğ´ĞµĞ¾ (VDM) Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ VDM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Bringing Physics to Video Generation: A Two-Stage Approach",
                    "desc": "This paper introduces a new two-stage framework for generating videos that are more physically realistic using video diffusion models (VDMs). The first stage uses a Vision Language Model (VLM) to create rough motion trajectories that consider real-world physics, ensuring that the generated video maintains consistency between frames. In the second stage, these trajectories guide the VDM in producing detailed video content, with added noise to allow for creative freedom in motion generation. The results show that this approach significantly improves the physical plausibility of the generated videos compared to existing methods."
                },
                "zh": {
                    "title": "å¼•å…¥ç‰©ç†çŸ¥è¯†çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰åœ¨ç”Ÿæˆé«˜åº¦çœŸå®çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¸¸å¸¸ç¼ºä¹å¯¹ç‰©ç†çš„ç†è§£ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸Šä¸å¤Ÿåˆç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜ç¡®åœ°èå…¥äº†ç‰©ç†çŸ¥è¯†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç•¥çš„è¿åŠ¨è§„åˆ’å™¨ï¼Œç»“åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†ï¼Œé¢„æµ‹æ¥è¿‘çœŸå®ç‰©ç†åŠ¨æ€çš„ç²—ç•¥è¿åŠ¨è½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆï¼Œä»è€Œå®ç°æ›´ç»†è‡´çš„è¿åŠ¨è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01848",
            "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
            "url": "https://huggingface.co/papers/2504.01848",
            "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.",
            "score": 16,
            "issue_id": 3041,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "60923777325e85cc",
            "authors": [
                "Giulio Starace",
                "Oliver Jaffe",
                "Dane Sherburn",
                "James Aung",
                "Jun Shern Chan",
                "Leon Maksin",
                "Rachel Dias",
                "Evan Mays",
                "Benjamin Kinsella",
                "Wyatt Thompson",
                "Johannes Heidecke",
                "Amelia Glaese",
                "Tejal Patwardhan"
            ],
            "affiliations": [
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01848.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PaperBench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "PaperBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 20 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· ICML 2024, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Claude 3.5 Sonnet (New) Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ 21.0%."
                },
                "en": {
                    "title": "Evaluating AI's Research Replication Skills with PaperBench",
                    "desc": "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."
                },
                "zh": {
                    "title": "PaperBenchï¼šè¯„ä¼°AIå¤åˆ¶ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†å¤åˆ¶æœ€å…ˆè¿›AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†ã€‚ä»£ç†éœ€è¦ä»å¤´å¼€å§‹å¤åˆ¶20ç¯‡ICML 2024çš„äº®ç‚¹å’Œå£å¤´è®ºæ–‡ï¼ŒåŒ…æ‹¬ç†è§£è®ºæ–‡è´¡çŒ®ã€å¼€å‘ä»£ç åº“å’ŒæˆåŠŸæ‰§è¡Œå®éªŒã€‚ä¸ºäº†è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†åˆ†å±‚çš„è¯„åˆ†æ ‡å‡†ï¼Œå°†æ¯ä¸ªå¤åˆ¶ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶è®¾å®šæ˜ç¡®çš„è¯„åˆ†æ ‡å‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜åŒ…æ‹¬ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„å®¡è€…è‡ªåŠ¨è¯„åˆ†ï¼Œå¹¶ä¸é¡¶å°–çš„æœºå™¨å­¦ä¹ åšå£«è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°ç›®å‰çš„æ¨¡å‹å°šæœªè¶…è¶Šäººç±»åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00824",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
            "url": "https://huggingface.co/papers/2504.00824",
            "abstract": "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.",
            "score": 15,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "b135f3f003dcaaff",
            "authors": [
                "Yubo Wang",
                "Xueguang Ma",
                "Ping Nie",
                "Huaye Zeng",
                "Zhiheng Lyu",
                "Yuxuan Zhang",
                "Benjamin Schneider",
                "Yi Lu",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#rag",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°",
                    "desc": "ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½ [RET], Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScholarCopilot ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· arXiv, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ top-1 Ğ² 40.1% Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Academic Writing with ScholarCopilot",
                    "desc": "This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience."
                },
                "zh": {
                    "title": "ScholarCopilotï¼šæå‡å­¦æœ¯å†™ä½œçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ScholarCopilotï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸“ä¸šå­¦æœ¯æ–‡ç« æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ£€ç´¢æ ‡è®°[RET]ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨å…¶è¡¨ç¤ºä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸å…³å¼•ç”¨ã€‚ScholarCopilotåœ¨ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡ã€‚ç»è¿‡åœ¨500Kç¯‡arXivè®ºæ–‡ä¸Šçš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†40.1%çš„é¡¶çº§æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬çš„ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01934",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
            "url": "https://huggingface.co/papers/2504.01934",
            "abstract": "We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.",
            "score": 13,
            "issue_id": 3042,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "a50e19f04d94405f",
            "authors": [
                "Runhui Huang",
                "Chunwei Wang",
                "Junwei Yang",
                "Guansong Lu",
                "Yunlong Yuan",
                "Jianhua Han",
                "Lu Hou",
                "Wei Zhang",
                "Lanqing Hong",
                "Hengshuang Zhao",
                "Hang Xu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01934.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ILLUME+: Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ILLUME+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ILLUME+ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ DualViTok, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "ILLUME+: Unifying Understanding, Generation, and Editing in One Model",
                    "desc": "ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications."
                },
                "zh": {
                    "title": "ILLUME+: å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "ILLUME+ æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆäº†åŒé‡è§†è§‰æ ‡è®°å’Œæ‰©æ•£è§£ç å™¨ï¼Œæ—¨åœ¨æå‡æ·±å±‚è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ç›¸æ¯”ï¼ŒILLUME+ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘è¿™ä¸‰ç§åŸºæœ¬èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ DualViTokï¼ŒILLUME+ ä¿ç•™äº†ç»†è‡´çš„çº¹ç†å’Œæ–‡æœ¬å¯¹é½çš„è¯­ä¹‰ï¼ŒåŒæ—¶é‡‡ç”¨ç²—åˆ°ç»†çš„å›¾åƒè¡¨ç¤ºç­–ç•¥ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒILLUME+ åœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†ä¸ç°æœ‰æ¨¡å‹çš„ç«äº‰åŠ›ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01204",
            "title": "Articulated Kinematics Distillation from Video Diffusion Models",
            "url": "https://huggingface.co/papers/2504.01204",
            "abstract": "We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/",
            "score": 11,
            "issue_id": 3041,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "c8630e7ca691cef3",
            "authors": [
                "Xuan Li",
                "Qianli Ma",
                "Tsung-Yi Lin",
                "Yongxin Chen",
                "Chenfanfu Jiang",
                "Ming-Yu Liu",
                "Donglai Xiang"
            ],
            "affiliations": [
                "NVIDIA",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01204.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#3d"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Articulated Kinematics Distillation (AKD) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AKD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. AKD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 4D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with AKD",
                    "desc": "Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible."
                },
                "zh": {
                    "title": "å…³èŠ‚è¿åŠ¨è’¸é¦ï¼šé«˜ä¿çœŸåŠ¨ç”»çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå…³èŠ‚è¿åŠ¨è’¸é¦ï¼ˆAKDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸè§’è‰²åŠ¨ç”»ã€‚AKDé€šè¿‡ä½¿ç”¨åŸºäºéª¨éª¼çš„è¡¨ç¤ºï¼Œæ˜¾è‘—å‡å°‘äº†è‡ªç”±åº¦ï¼Œä¸“æ³¨äºå…³èŠ‚çº§æ§åˆ¶ï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ä¸€è‡´çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç»“åˆçš„å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰ï¼ŒAKDèƒ½å¤Ÿè’¸é¦å¤æ‚çš„å…³èŠ‚è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒç»“æ„å®Œæ•´æ€§ï¼Œå…‹æœäº†4Dç¥ç»å˜å½¢åœºåœ¨ä¿æŒå½¢çŠ¶ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒAKDåœ¨3Dä¸€è‡´æ€§å’Œè¿åŠ¨è´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ°4Dç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01308",
            "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
            "url": "https://huggingface.co/papers/2504.01308",
            "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
            "score": 10,
            "issue_id": 3040,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "315938d70f25095e",
            "authors": [
                "Jiawei Wang",
                "Yushen Zuo",
                "Yuanjun Chai",
                "Zhendong Liu",
                "Yichen Fu",
                "Yichun Feng",
                "Kin-man Lam"
            ],
            "affiliations": [
                "Nanjing University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China",
                "University of Washington",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01308.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robust-VLGuard, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiffPure-VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº."
                },
                "en": {
                    "title": "Strengthening Vision-Language Models Against Noise Attacks",
                    "desc": "This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality."
                },
                "zh": {
                    "title": "å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å¯¹å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„è„†å¼±æ€§å´æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œç»“åˆäº†å¯¹é½å’Œä¸å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºçš„å¾®è°ƒæ¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»ç‰¹æ€§ä¸æˆ‘ä»¬å¾®è°ƒçš„VLMså¾ˆå¥½åœ°å¯¹é½ï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ‰°åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23573",
            "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
            "url": "https://huggingface.co/papers/2503.23573",
            "abstract": "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.",
            "score": 9,
            "issue_id": 3049,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "cd9c0f9c9392d470",
            "authors": [
                "Maximilian Augustin",
                "Yannic Neuhaus",
                "Matthias Hein"
            ],
            "affiliations": [
                "Tubingen AI Center, University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23573.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#hallucinations",
                    "#benchmark",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "DASH: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DASH - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. DASH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ DASH Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DASH: Detecting and Mitigating Object Hallucinations in Vision-Language Models",
                    "desc": "This paper addresses the issue of object hallucinations in vision-language models (VLMs), where these models incorrectly identify objects in images. The authors introduce DASH, a new automated pipeline that detects and assesses systematic hallucinations in VLMs using large-scale, real-world image datasets. DASH includes a component called DASH-OPT, which generates misleading images to evaluate the VLM's performance on the 'natural image manifold'. The study demonstrates that fine-tuning VLMs with images identified by DASH can significantly reduce the occurrence of object hallucinations."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå¹»è§‰",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®¹æ˜“å‡ºç°ç‰©ä½“å¹»è§‰ï¼Œå³é”™è¯¯åœ°æŒ‡ç¤ºå›¾åƒä¸­å­˜åœ¨æŸäº›ç‰©ä½“ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„æ ‡è®°æ•°æ®é›†æ¥é‡åŒ–å¹»è§‰ï¼Œä½†è¿™ç§æ–¹æ³•ä¸è¶³ä»¥è¯„ä¼°åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å‡ºç°çš„å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†DASHï¼ˆç³»ç»Ÿå¹»è§‰çš„æ£€æµ‹ä¸è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤§è§„æ¨¡ç®¡é“ï¼Œæ—¨åœ¨è¯†åˆ«VLMsåœ¨çœŸå®å›¾åƒä¸­çš„ç³»ç»Ÿå¹»è§‰ã€‚é€šè¿‡ä¼˜åŒ–â€œè‡ªç„¶å›¾åƒæµå½¢â€ï¼ŒDASHèƒ½å¤Ÿç”Ÿæˆè¯¯å¯¼VLMçš„å›¾åƒï¼Œå¹¶è¯†åˆ«å‡ºå¤§é‡çš„çœŸå®å’Œè¯­ä¹‰ç›¸ä¼¼å›¾åƒçš„èšç±»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2405.20216",
            "title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback",
            "url": "https://huggingface.co/papers/2405.20216",
            "abstract": "The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.",
            "score": 9,
            "issue_id": 3046,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "8ffe2bddf2c0ee58",
            "authors": [
                "Sanghyeon Na",
                "Yonggyu Kim",
                "Hyunjoon Lee"
            ],
            "affiliations": [
                "Kakao"
            ],
            "pdf_title_img": "assets/pdf/title_img/2405.20216.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ§‘â€ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DPO, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Human Image Generation with Direct Preference Optimization",
                    "desc": "This paper addresses the challenges of generating high-quality human images from text descriptions using text-to-image (T2I) methods. It highlights the importance of meeting specific criteria such as human pose and anatomy, which are crucial for realistic image synthesis. The authors propose a novel approach that employs Direct Preference Optimization (DPO) to create a specialized dataset for training without requiring expensive human feedback. Additionally, they introduce a modified loss function that reduces artifacts and enhances image quality, leading to significant improvements in generating human images that align well with textual prompts."
                },
                "zh": {
                    "title": "ä¼˜åŒ–äººç±»å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡äººç±»å›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ä¸€èˆ¬å›¾åƒç”Ÿæˆä¸åŒï¼Œäººç±»å›¾åƒåˆæˆéœ€è¦æ»¡è¶³ä¸¥æ ¼çš„äººä½“å§¿åŠ¿ã€è§£å‰–ç»“æ„å’Œä¸æ–‡æœ¬æç¤ºå¯¹é½çš„æ ‡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸“é—¨é’ˆå¯¹äººç±»å›¾åƒç”Ÿæˆï¼Œæ„å»ºé«˜æ•ˆçš„DPOæ•°æ®é›†ä»¥è®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘å¯¹æ˜‚è´µäººç±»åé¦ˆçš„ä¾èµ–ã€‚é€šè¿‡ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹èƒ½å¤Ÿå‡å°‘ä¼ªå½±å¹¶æé«˜å›¾åƒçš„çœŸå®æ„Ÿï¼Œæ˜¾è‘—æå‡äººç±»å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23135",
            "title": "LSNet: See Large, Focus Small",
            "url": "https://huggingface.co/papers/2503.23135",
            "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.",
            "score": 4,
            "issue_id": 3040,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "d2ac65a2356c89c3",
            "authors": [
                "Ao Wang",
                "Hui Chen",
                "Zijia Lin",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist, Tsinghua University",
                "Department of Automation, Tsinghua University",
                "School of Software, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23135.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾' Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ LS-ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LS-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LSNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSNet Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "See Large, Focus Small: Efficient Vision Networks",
                    "desc": "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."
                },
                "zh": {
                    "title": "è½»é‡çº§è§†è§‰ç½‘ç»œçš„æ–°ç­–ç•¥ï¼šå¤§è§†é‡ï¼Œå°èšç„¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§è§†è§‰ç½‘ç»œè®¾è®¡ç­–ç•¥ï¼Œç§°ä¸ºâ€œSee Large, Focus Smallâ€ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤§æ ¸æ„ŸçŸ¥å’Œå°æ ¸èšåˆçš„LSå·ç§¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¹¿æ³›çš„æ„ŸçŸ¥ä¿¡æ¯å¹¶å®ç°ç²¾ç¡®çš„ç‰¹å¾èšåˆã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLSNetåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…‹æœäº†ç°æœ‰è½»é‡çº§æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™æƒ…å†µä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSNetåœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01201",
            "title": "Medical large language models are easily distracted",
            "url": "https://huggingface.co/papers/2504.01201",
            "abstract": "Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.",
            "score": 1,
            "issue_id": 3052,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "cfc7db001feb908d",
            "authors": [
                "Krithik Vishwanath",
                "Anton Alyakin",
                "Daniel Alexander Alber",
                "Jin Vivian Lee",
                "Douglas Kondziolka",
                "Eric Karl Oermann"
            ],
            "affiliations": [
                "Center for Data Science, New York University, New York, New York, 10016",
                "Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712",
                "Department of Mathematics, The University of Texas at Austin, Austin, Texas, 78712",
                "Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016",
                "Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110",
                "Department of Radiology, NYU Langone Medical Center, New York, New York, 10016"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01201.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#healthcare",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·ĞµÑ€ĞµĞ½ Ğ¾Ñ‚ Ğ¿Ğ»ĞµĞ²ĞµĞ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MedDistractQA, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ USMLE Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° 17.9%. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº RAG Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ LLM Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs: Tackling Noise in Medical Contexts",
                    "desc": "This paper explores the challenges faced by large language models (LLMs) in medical settings, particularly when they encounter irrelevant information during clinical scenarios. The authors created a benchmark called MedDistractQA, which includes USMLE-style questions with distractions that mimic real-world clinical noise. Their research found that such distractions can significantly decrease the accuracy of LLMs, by as much as 17.9%. Additionally, common strategies to improve model performance, like retrieval-augmented generation and medical fine-tuning, did not alleviate the issue and sometimes worsened it, indicating a fundamental limitation in LLMs' ability to filter relevant clinical data."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦ä¸­çš„æŠ—å¹²æ‰°èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰å˜é©æ½œåŠ›ï¼Œä½†ç°å®ä¸´åºŠåœºæ™¯ä¸­å­˜åœ¨çš„å¤šä½™ä¿¡æ¯å¯èƒ½ä¼šå½±å“å…¶è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†MedDistractQAåŸºå‡†ï¼Œä½¿ç”¨åµŒå…¥æ¨¡æ‹Ÿç°å®å¹²æ‰°çš„USMLEé£æ ¼é—®é¢˜æ¥è¯„ä¼°LLMsè¿‡æ»¤ç›¸å…³æ•°æ®çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¹²æ‰°æ€§é™ˆè¿°ä¼šä½¿LLMçš„å‡†ç¡®æ€§é™ä½å¤šè¾¾17.9%ã€‚è¿™è¡¨æ˜LLMsåœ¨åŒºåˆ†ç›¸å…³ä¸æ— å…³ä¸´åºŠä¿¡æ¯æ–¹é¢ç¼ºä¹å¿…è¦çš„é€»è¾‘æœºåˆ¶ï¼Œç»™å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00406",
            "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
            "url": "https://huggingface.co/papers/2504.00406",
            "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
            "score": 1,
            "issue_id": 3046,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "d4ef7ad5ea6d5aad",
            "authors": [
                "Jiuzhou Han",
                "Wray Buntine",
                "Ehsan Shareghi"
            ],
            "affiliations": [
                "College of Engineering and Computer Science, VinUniversity",
                "Department of Data Science & AI, Monash University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00406.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#agents",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "VerifiAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerifiAgent - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VerifiAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼ĞµÑ‚Ğ°-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VerifiAgent Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "VerifiAgent: Enhancing Reliability in Language Model Reasoning",
                    "desc": "This paper introduces VerifiAgent, a novel verification system designed to improve the reliability of large language models' responses. It combines two verification levels: meta-verification for checking the completeness and consistency of answers, and tool-based adaptive verification that selects the best verification tools based on the reasoning type. This approach enhances efficiency and robustness across various reasoning tasks, outperforming traditional verification methods. Additionally, VerifiAgent improves reasoning accuracy by utilizing feedback from its verification processes and is more cost-effective in mathematical reasoning applications."
                },
                "zh": {
                    "title": "VerifiAgentï¼šæ™ºèƒ½éªŒè¯ï¼Œæå‡æ¨ç†å‡†ç¡®æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¸¸å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›ç­”ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹æˆ–é¢†åŸŸï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VerifiAgentï¼Œä¸€ä¸ªç»Ÿä¸€çš„éªŒè¯ä»£ç†ï¼Œé›†æˆäº†ä¸¤çº§éªŒè¯ï¼šå…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å›ç­”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Œå·¥å…·è‡ªé€‚åº”éªŒè¯åˆ™æ ¹æ®æ¨ç†ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„éªŒè¯å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿éªŒè¯æ–¹æ³•ï¼Œå¹¶èƒ½é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœçš„åé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18817",
            "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations",
            "url": "https://huggingface.co/papers/2503.18817",
            "abstract": "Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.",
            "score": 1,
            "issue_id": 3046,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "fa652fd57c6312f8",
            "authors": [
                "Jeonghyeon Kim",
                "Sangheum Hwang"
            ],
            "affiliations": [
                "Seoul National University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18817.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (OoDD) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (MMFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ OoDD. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ±Ğ»Ğ¸Ğ¶Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet-1k OoD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾ÑÑ‚-Ñ…Ğ¾Ğº Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ OoDD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Out-of-Distribution Detection with Multi-Modal Fine-Tuning",
                    "desc": "This paper focuses on improving out-of-distribution detection (OoDD) using multi-modal fine-tuning (MMFT) with vision-language models like CLIP. The authors argue that traditional methods often freeze or partially tune pretrained weights, which limits performance on new datasets. They identify a key issue with na\"ive fine-tuning methods, which fail to fully utilize the pretrained knowledge due to a modality gap in embeddings. To overcome this, they propose a new training objective that aligns image and text embeddings more effectively, leading to significant improvements in OoDD performance and overall accuracy on benchmark datasets."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€å¾®è°ƒæå‡åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰ä¸­çš„åº”ç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¨¡æ€æ¨¡å‹ä¸Šï¼Œè€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é€šè¿‡å¢å¼ºå›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†OoDDæ€§èƒ½ã€‚æˆ‘ä»¬åˆ†æäº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†ã€‚é€šè¿‡åœ¨ImageNet-1k OoDåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨OoDDæ€§èƒ½å’ŒIDå‡†ç¡®ç‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-02.html",
    "link_next": "2025-04-04.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "02.04",
        "en": "04/02",
        "zh": "4æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 8,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 8,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de tÃºxiÃ ng shÄ“ngchÃ©ng mÃ³xÃ­ng, jiÃ ozuÃ² MergeVQ. TÄ jiÄ“hÃ© le xiÃ ngliÃ ng liÃ nggÃ©huÃ  hÃ© token hÃ©bÃ¬ng jÃ¬shÃ¹, zhÇyÃº jiÄ›juÃ© tÃºxiÃ ng shÄ“ngchÃ©ng zhÃ¬liÃ ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ xiÃ oyÃ¬ zhÄ«jiÄn de pÃ­nghÃ©ng wÃ¨ntÃ­. MergeVQ zÃ i yÃ¹xÃ¹nliÃ n jiÄ“duÃ n tÅngguÃ² token hÃ©bÃ¬ng mÃ³kuÃ i tÃ­qÇ” yÇ”yÃ¬ xÃ¬nxÄ«, bÃ¬ng zÃ i jiÄ›mÇ jiÄ“duÃ n huÄ«fÃ¹ xÃ¬jiÃ¨. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, MergeVQ zÃ i tÃºxiÃ ng shÄ“ngchÃ©ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ rÃ¨nwÃ¹ zhÅng dÅu biÇoxiÃ n chÅ«sÃ¨, qiÄ› xiÃ oyÃ¬ gÄo. DÃ imÇ hÃ© mÃ³xÃ­ng jiÄng zÃ i wÇngshÃ ng gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"å‘é‡é‡åŒ–\", \"pinyin\": \"xiÃ ngliÃ ng liÃ ngzhÃ¬\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"tÅukÃ¨n\", \"trans\": \"token\"},\n    {\"word\": \"åˆå¹¶\", \"pinyin\": \"hÃ©bÃ¬ng\", \"trans\": \"merge\"},\n    {\"word\": \"æ—¨åœ¨\", \"pinyin\": \"zhÇzÃ i\", \"trans\": \"aim to\"},\n    {\"word\": \"å¹³è¡¡\", \"pinyin\": \"pÃ­nghÃ©ng\", \"trans\": \"balance\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹nliÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"è§£ç \", \"pinyin\": \"jiÄ›mÇ\", \"trans\": \"decode\"},\n    {\"word\": \"æ¢å¤\", \"pinyin\": \"huÄ«fÃ¹\", \"trans\": \"recover\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬jiÃ©\", \"trans\": \"detail\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ olÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅngkÄi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.",
        "update_ts": "2025-04-03 09:11"
    }
}