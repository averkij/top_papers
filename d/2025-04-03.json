{
    "date": {
        "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 3",
        "zh": "4æœˆ3æ—¥"
    },
    "time_utc": "2025-04-03 02:21",
    "weekday": 3,
    "issue_id": 3040,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.00999",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "url": "https://huggingface.co/papers/2504.00999",
            "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.",
            "score": 16,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "bb6506ffd72aed19",
            "authors": [
                "Siyuan Li",
                "Luyuan Zhang",
                "Zedong Wang",
                "Juanxi Tian",
                "Cheng Tan",
                "Zicheng Liu",
                "Chang Yu",
                "Qingsong Xie",
                "Haonan Lu",
                "Haoqian Wang",
                "Zhen Lei"
            ],
            "affiliations": [
                "CAIR, HKISI-CAS",
                "MAIS CASIA",
                "OPPO AI Center",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Chinese Academy of Sciences",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00999.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (VQ) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. MergeVQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MergeAR, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MergeVQ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "MergeVQ: Bridging Image Generation and Representation Learning Efficiently",
                    "desc": "This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed."
                },
                "zh": {
                    "title": "MergeVQï¼šæå‡å›¾åƒç”Ÿæˆä¸è¡¨ç¤ºå­¦ä¹ çš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MergeVQï¼Œæ—¨åœ¨æ”¹å–„åŸºäºå‘é‡é‡åŒ–çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡åœ¨ç¼–ç å™¨ä¸­å¼•å…¥ä»¤ç‰Œåˆå¹¶æŠ€æœ¯ï¼ŒMergeVQèƒ½å¤Ÿåœ¨è‡ªæ³¨æ„åŠ›å—åè§£è€¦æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¢å¤ç»†èŠ‚ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨KVç¼“å­˜å‹ç¼©æ¥æé«˜é¢„æµ‹æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeVQåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ä»¤ç‰Œæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00824",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
            "url": "https://huggingface.co/papers/2504.00824",
            "abstract": "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.",
            "score": 5,
            "issue_id": 3040,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "b135f3f003dcaaff",
            "authors": [
                "Yubo Wang",
                "Xueguang Ma",
                "Ping Nie",
                "Huaye Zeng",
                "Zhiheng Lyu",
                "Yuxuan Zhang",
                "Benjamin Schneider",
                "Yi Lu",
                "Xiang Yue",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#rag",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°",
                    "desc": "ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½ [RET], Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScholarCopilot ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· arXiv, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ top-1 Ğ² 40.1% Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Academic Writing with ScholarCopilot",
                    "desc": "This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience."
                },
                "zh": {
                    "title": "ScholarCopilotï¼šæå‡å­¦æœ¯å†™ä½œçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ScholarCopilotï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸“ä¸šå­¦æœ¯æ–‡ç« æ—¶çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç”Ÿæˆæ£€ç´¢æ ‡è®°[RET]ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨å…¶è¡¨ç¤ºä»æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸å…³å¼•ç”¨ã€‚ScholarCopilotåœ¨ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡ã€‚ç»è¿‡åœ¨500Kç¯‡arXivè®ºæ–‡ä¸Šçš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†40.1%çš„é¡¶çº§æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¸”åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬çš„ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01308",
            "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
            "url": "https://huggingface.co/papers/2504.01308",
            "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
            "score": 4,
            "issue_id": 3040,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "315938d70f25095e",
            "authors": [
                "Jiawei Wang",
                "Yushen Zuo",
                "Yuanjun Chai",
                "Zhendong Liu",
                "Yichen Fu",
                "Yichun Feng",
                "Kin-man Lam"
            ],
            "affiliations": [
                "Nanjing University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "University of Science and Technology of China",
                "University of Washington",
                "University of the Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01308.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#cv",
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Robust-VLGuard, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¼. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiffPure-VLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ VLM Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº."
                },
                "en": {
                    "title": "Strengthening Vision-Language Models Against Noise Attacks",
                    "desc": "This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality."
                },
                "zh": {
                    "title": "å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰ä¿¡æ¯æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å™ªå£°æˆ–æŸåçš„å›¾åƒæ—¶ä»ç„¶å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç°æœ‰çš„VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡å–äº†å®‰å…¨æªæ–½ä»¥å‡è½»è¿™äº›æ”»å‡»ï¼Œä½†å¯¹å™ªå£°å¢å¼ºè§†è§‰è¾“å…¥çš„è„†å¼±æ€§å´æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æˆ‘ä»¬æå‡ºäº†Robust-VLGuardï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œç»“åˆäº†å¯¹é½å’Œä¸å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºçš„å¾®è°ƒæ¥é™ä½æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒVLMçš„åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒè½¬ç§»ç‰¹æ€§ä¸æˆ‘ä»¬å¾®è°ƒçš„VLMså¾ˆå¥½åœ°å¯¹é½ï¼Œæ˜¾è‘—å‡è½»äº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ‰°åŠ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23135",
            "title": "LSNet: See Large, Focus Small",
            "url": "https://huggingface.co/papers/2503.23135",
            "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.",
            "score": 2,
            "issue_id": 3040,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "d2ac65a2356c89c3",
            "authors": [
                "Ao Wang",
                "Hui Chen",
                "Zijia Lin",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist, Tsinghua University",
                "Department of Automation, Tsinghua University",
                "School of Software, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23135.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾' Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ LS-ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LS-ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LSNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSNet Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "See Large, Focus Small: Efficient Vision Networks",
                    "desc": "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."
                },
                "zh": {
                    "title": "è½»é‡çº§è§†è§‰ç½‘ç»œçš„æ–°ç­–ç•¥ï¼šå¤§è§†é‡ï¼Œå°èšç„¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§è§†è§‰ç½‘ç»œè®¾è®¡ç­–ç•¥ï¼Œç§°ä¸ºâ€œSee Large, Focus Smallâ€ã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¤§æ ¸æ„ŸçŸ¥å’Œå°æ ¸èšåˆçš„LSå·ç§¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¹¿æ³›çš„æ„ŸçŸ¥ä¿¡æ¯å¹¶å®ç°ç²¾ç¡®çš„ç‰¹å¾èšåˆã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLSNetåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå…‹æœäº†ç°æœ‰è½»é‡çº§æ¨¡å‹åœ¨è®¡ç®—é¢„ç®—æœ‰é™æƒ…å†µä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSNetåœ¨å®æ—¶åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-02.html",
    "link_next": "2025-04-04.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "02.04",
        "en": "04/02",
        "zh": "4æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.04",
        "en": "04/04",
        "zh": "4æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚",
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºAny2Captionçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†é¢‘ç”Ÿæˆç¤¾åŒºä¸­å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å„ç§æ¡ä»¶è§£é‡Šæ­¥éª¤ä¸è§†é¢‘åˆæˆæ­¥éª¤åˆ†ç¦»ã€‚é€šè¿‡åˆ©ç”¨ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒAny2Captionèƒ½å¤Ÿå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç‰¹å®šæç¤ºï¼ˆå¦‚åŒºåŸŸã€è¿åŠ¨å’Œæ‘„åƒå¤´å§¿æ€ï¼‰è½¬æ¢ä¸ºå¯†é›†ã€ç»“æ„åŒ–çš„å­—å¹•ï¼Œä»è€Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›æ›´å¥½çš„æŒ‡å¯¼ã€‚æ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Any2CapInsï¼ŒåŒ…å«337Kä¸ªå®ä¾‹å’Œ407Kä¸ªæ¡ä»¶ï¼Œç”¨äºä»»æ„æ¡ä»¶åˆ°å­—å¹•çš„æŒ‡ä»¤è°ƒä¼˜ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯æ§æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng mÃ­ng wÃ¨i Any2Caption de xÄ«n kuÃ ng jiÃ , zhÇ yÃº jiÄ› juÃ© dÄng qiÃ¡n shÃ¬ pÇn shÄ“ng chÃ©ng shÃ¨ qÅ« zhÅng zhÇ”n quÃ¨ lÇ jiÄ› yÃ²ng hÃ¹ de pÃ­ng yÇ wÃ¨n ti. gÇi kuÃ ng jiÃ  de hÃ© xÄ«n sÄ« xiÇng shÃ¬ jiÄng gÃ¨ zhÇ’ng tiÃ¡o jiÃ n jiÄ› shÃ¬ bÃ¹ zhÃ²u yÇ” shÃ¬ pÇn hÃ© chÃ©ng bÃ¹ zhÃ²u fÄ“n lÃ­. tÅng guÃ² lÃ¬ yÃ²ng xiÃ n dÃ i duÅ mÃ³ shÃ¬ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs), Any2Caption nÃ©ng gÃ²u jiÄng wÃ©n bÄ›n, tÃº xiÃ ng, shÃ¬ pÇn hÃ© tÃ¨ dÃ¬ng tÃ­ shÃ¬ (rÃº qÅ« yÃ¹, yÃ¹n dÃ²ng hÃ© shÃ¨ xiÃ ng tÃ³u zÄ« tÃ i) zhuÇn huÃ n wÃ¨i mÃ¬ jÄ«, jiÃ© gÃ²u huÃ  de zÃ¬ mÃ¹, cÃ³ng Ã©r wÃ©i shÃ¬ pÇn shÄ“ng chÃ©ng tÃ­ gÅng gÄ›ng hÇo de zhÇ dÇo. wÃ©n zhÄng hÃ¡i yÇn rÃ¹ le yÄ« gÃ¨ dÃ  guÄ« mÃ³ shÃ¹ jÃ¹ Any2CapIns, bÄo hÃ¡n 337K gÃ¨ shÃ­ lÃ¬ hÃ© 407K gÃ¨ tiÃ¡o jiÃ n, yÃ²ng yÃº rÃ¨n yÃ¬ tiÃ¡o jiÃ n dÃ o zÃ¬ mÃ¹ de zhÇ lÃ¬ng tiÃ¡o yÅu. zÃ²ng hÃ© pÃ­ng gÇ” biÇo mÃ­ng, gÇi xÃ¬ tÇ’ng zÃ i kÃ¨ kÃ²ng xÃ¬ng hÃ© shÃ¬ pÇn zhÃ¬ liÃ ng fÄng miÃ n xiÇn zhÃ¹ yÅu xiÃ n yÇ’u de shÃ¬ pÇn shÄ“ng chÃ©ng mÃ³ xÃ­ng.",
        "vocab": "[{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'ç“¶é¢ˆ', 'pinyin': 'pÃ­ng jÇng', 'trans': 'bottleneck'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'æ ¸å¿ƒ', 'pinyin': 'hÃ© xÄ«n', 'trans': 'core'},\n{'word': 'æ€æƒ³', 'pinyin': 'sÄ« xiÇng', 'trans': 'idea'},\n{'word': 'è§£é‡Š', 'pinyin': 'jiÄ› shÃ¬', 'trans': 'interpretation'},\n{'word': 'æ­¥éª¤', 'pinyin': 'bÃ¹ zhÃ²u', 'trans': 'step'},\n{'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesis'},\n{'word': 'åˆ†ç¦»', 'pinyin': 'fÄ“n lÃ­', 'trans': 'separation'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'},\n{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ shuÃ i', 'trans': 'multimodal'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'è½¬æ¢', 'pinyin': 'zhuÇn huÃ n', 'trans': 'convert'},\n{'word': 'å¯†é›†', 'pinyin': 'mÃ¬ jÃ­', 'trans': 'dense'},\n{'word': 'ç»“æ„åŒ–', 'pinyin': 'jiÃ© gÃ²u huÃ ', 'trans': 'structured'},\n{'word': 'å­—å¹•', 'pinyin': 'zÃ¬ mÃ¹', 'trans': 'subtitle'},\n{'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guidance'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å®ä¾‹', 'pinyin': 'shÃ­ lÃ¬', 'trans': 'instance'},\n{'word': 'è°ƒä¼˜', 'pinyin': 'tiÃ¡o yÅu', 'trans': 'tuning'},\n{'word': 'ç»¼åˆ', 'pinyin': 'zÃ²ng hÃ©', 'trans': 'comprehensive'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'},\n{'word': 'å¯æ§æ€§', 'pinyin': 'kÄ› kÃ²ng xÃ¬ng', 'trans': 'controllability'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]",
        "trans": "This article introduces a new framework called Any2Caption, aimed at addressing the bottleneck issue of accurately understanding user intent in the current video generation community. The core idea of the framework is to separate various condition interpretation steps from video synthesis steps. By leveraging modern multimodal large language models (MLLMs), Any2Caption can convert text, images, videos, and specific prompts (such as regions, motion, and camera poses) into dense, structured captions, providing better guidance for video generation. The article also introduces a large-scale dataset, Any2CapIns, containing 337K instances and 407K conditions, for instruction tuning from any condition to captions. Comprehensive evaluations indicate that the system significantly outperforms existing video generation models in terms of controllability and video quality.",
        "update_ts": "2025-04-02 09:11"
    }
}