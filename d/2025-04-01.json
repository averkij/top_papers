{
    "date": {
        "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 1",
        "zh": "4æœˆ1æ—¥"
    },
    "time_utc": "2025-04-01 07:11",
    "weekday": 1,
    "issue_id": 2999,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.23307",
            "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
            "url": "https://huggingface.co/papers/2503.23307",
            "abstract": "Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.",
            "score": 23,
            "issue_id": 2994,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "6ce9b3642bf3ace3",
            "authors": [
                "Cong Wei",
                "Bo Sun",
                "Haoyu Ma",
                "Ji Hou",
                "Felix Juefei-Xu",
                "Zecheng He",
                "Xiaoliang Dai",
                "Luxin Zhang",
                "Kunpeng Li",
                "Tingbo Hou",
                "Animesh Sinha",
                "Peter Vajda",
                "Wenhu Chen"
            ],
            "affiliations": [
                "GenAI, Meta",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23307.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#video",
                    "#benchmark",
                    "#story_generation"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MoCha: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MoCha Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ°Ñ€Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Character Animation with MoCha",
                    "desc": "This paper presents MoCha, a novel approach for generating talking character animations from speech and text, focusing on full character portrayal rather than just facial movements. It introduces a speech-video window attention mechanism to ensure that the generated video aligns accurately with the spoken words. To tackle the challenge of limited speech-labeled video datasets, the authors propose a joint training strategy that utilizes both speech and text-labeled data, enhancing the model's ability to generalize across various character actions. Additionally, structured prompt templates with character tags allow for multi-character dialogues, enabling AI-generated characters to engage in coherent conversations, thus advancing the field of automated cinematic storytelling."
                },
                "zh": {
                    "title": "ä¼šè¯´è¯çš„è§’è‰²ï¼šAIç”Ÿæˆç”µå½±å™äº‹çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œç§°ä¸ºâ€œä¼šè¯´è¯çš„è§’è‰²â€ï¼Œæ—¨åœ¨ä»è¯­éŸ³å’Œæ–‡æœ¬ç›´æ¥ç”Ÿæˆè§’è‰²åŠ¨ç”»ã€‚ä¸ä¼ ç»Ÿçš„â€œè¯´è¯å¤´â€ä¸åŒï¼Œè¿™ç§æ–¹æ³•ç”Ÿæˆçš„ä¸ä»…ä»…æ˜¯é¢éƒ¨è¡¨æƒ…ï¼Œè€Œæ˜¯å®Œæ•´çš„è§’è‰²å½¢è±¡ã€‚æˆ‘ä»¬æå‡ºäº†MoChaï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆä¼šè¯´è¯è§’è‰²çš„æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è¯­éŸ³-è§†é¢‘çª—å£æ³¨æ„æœºåˆ¶ï¼Œä»¥ç¡®ä¿è§†é¢‘ä¸è¯­éŸ³çš„ç²¾ç¡®åŒæ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ç»“æ„åŒ–çš„æç¤ºæ¨¡æ¿ï¼Œä½¿å¾—å¤šä¸ªè§’è‰²èƒ½å¤Ÿè¿›è¡ŒåŸºäºå›åˆçš„å¯¹è¯ï¼Œä»è€Œå®ç°æ›´å…·ç”µå½±æ„Ÿçš„æƒ…å¢ƒå¯¹è¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24388",
            "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
            "url": "https://huggingface.co/papers/2503.24388",
            "abstract": "Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.",
            "score": 18,
            "issue_id": 2995,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "98b80967d4757be2",
            "authors": [
                "Zhonghan Zhao",
                "Wenwei Zhang",
                "Haian Huang",
                "Kuikun Liu",
                "Jianfei Gao",
                "Gaoang Wang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24388.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RIG (Reasoning and Imagination in Generalist policy). RIG Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° RIG ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¾Ğ¿ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Synergizing Reasoning and Imagination for Enhanced Agent Performance",
                    "desc": "This paper introduces RIG, a novel end-to-end Generalist policy that combines reasoning and imagination for embodied agents in complex environments. By integrating these two capabilities, RIG enhances learning efficiency and generalization, overcoming limitations of previous models that focused on only one aspect. The authors developed a data pipeline that enriches trajectories from existing agents, allowing for joint learning of reasoning and next image generation. Experimental results demonstrate that RIG significantly improves sample efficiency and robustness, enabling agents to self-correct actions based on imagined outcomes before execution."
                },
                "zh": {
                    "title": "æ¨ç†ä¸æƒ³è±¡çš„ååŒæå‡æ™ºèƒ½ä½“èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRIGçš„é€šç”¨ç­–ç•¥ï¼Œé¦–æ¬¡å°†æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç»“åˆåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ä¸­ã€‚é€šè¿‡æ„å»ºæ•°æ®ç®¡é“ï¼Œé€æ­¥æ•´åˆå’Œä¸°å¯Œä»ç°æœ‰æ™ºèƒ½ä½“æ”¶é›†çš„è½¨è¿¹ä¸­çš„æ¨ç†å’Œæƒ³è±¡å†…å®¹ï¼ŒRIGå®ç°äº†æ›´é«˜çš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è”åˆå­¦ä¹ æ¨ç†å’Œä¸‹ä¸€å›¾åƒç”Ÿæˆï¼Œæ˜ç¡®å»ºæ¨¡äº†æ¨ç†ã€è¡ŒåŠ¨å’Œç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œä½¿å¾—æ ·æœ¬æ•ˆç‡æé«˜äº†17å€ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†ä¸æƒ³è±¡çš„ååŒä½œç”¨ä¸ä»…å¢å¼ºäº†é€šç”¨ç­–ç•¥çš„é²æ£’æ€§å’Œäº’æ“ä½œæ€§ï¼Œè¿˜æå‡äº†æ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24235",
            "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2503.24235",
            "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.",
            "score": 18,
            "issue_id": 2995,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "21674468fcc8c7d5",
            "authors": [
                "Qiyuan Zhang",
                "Fuyuan Lyu",
                "Zexu Sun",
                "Lei Wang",
                "Weixu Zhang",
                "Zhihan Guo",
                "Yufei Wang",
                "Irwin King",
                "Xue Liu",
                "Chen Ma"
            ],
            "affiliations": [
                "Chinese University of Hong Kong",
                "City University of Hong Kong",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Macquarie University",
                "McGill University & MILA",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24235.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ TTS-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ğ³Ğ´Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ TTS. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ TTS Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Potential: The Power of Test-Time Scaling in LLMs",
                    "desc": "This paper discusses the emerging focus on test-time scaling (TTS) in machine learning, particularly for large language models (LLMs). TTS enhances the problem-solving abilities of LLMs in various tasks, including specialized reasoning and open-ended questions. The authors propose a comprehensive framework that categorizes TTS research into four dimensions: what, how, where, and how well to scale. They also review existing methods and applications, highlight challenges, and suggest future research directions to improve TTS effectiveness."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶æ‰©å±•ï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›",
                    "desc": "éšç€å¯¹é¢„è®­ç»ƒæ—¶ä»£è®¡ç®—è§„æ¨¡ï¼ˆæ•°æ®å’Œå‚æ•°ï¼‰çš„çƒ­æƒ…é€æ¸å‡é€€ï¼Œæµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶ç„¦ç‚¹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒTTSå¯ä»¥è¿›ä¸€æ­¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰ä¸“ä¸šæ¨ç†ä»»åŠ¡ä»¥åŠå¼€æ”¾å¼é—®ç­”ç­‰ä¸€èˆ¬ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çªç ´ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿…é€Ÿå¢åŠ ï¼Œä½†ä»è¿«åˆ‡éœ€è¦ä¸€é¡¹å…¨é¢çš„è°ƒæŸ¥ï¼Œä»¥æä¾›ç³»ç»Ÿçš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç»´æ¡†æ¶ï¼Œæ¶µç›–TTSç ”ç©¶çš„å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶å¯¹æ–¹æ³•ã€åº”ç”¨åœºæ™¯å’Œè¯„ä¼°æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„å›é¡¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23461",
            "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
            "url": "https://huggingface.co/papers/2503.23461",
            "abstract": "This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.",
            "score": 18,
            "issue_id": 2995,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "00cccb2000a01b76",
            "authors": [
                "Nikai Du",
                "Zhennan Chen",
                "Zhizhou Chen",
                "Shan Gao",
                "Xi Chen",
                "Zhengkai Jiang",
                "Jian Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "China Mobile",
                "Nanjing University",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23461.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "TextCrafter: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ TextCrafter Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ĞµĞ¼. TextCrafter Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¾ĞºÑƒÑĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CVTG-2K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… CVTG."
                },
                "en": {
                    "title": "TextCrafter: Mastering Complex Visual Text Generation",
                    "desc": "This paper introduces Complex Visual Text Generation (CVTG), which involves creating detailed text that appears in various parts of images. The authors identify common issues in existing models, such as producing unclear or incomplete visual text. To improve this, they present TextCrafter, a new method that breaks down complex text into manageable parts and ensures that the text aligns well with the images. Additionally, TextCrafter enhances the visibility of the text during generation and is evaluated using a new dataset called CVTG-2K, showing superior performance compared to current methods."
                },
                "zh": {
                    "title": "TextCrafterï¼šæå‡å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆçš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤æ‚è§†è§‰æ–‡æœ¬ç”Ÿæˆï¼ˆCVTGï¼‰ä»»åŠ¡ï¼Œä¸»è¦å…³æ³¨åœ¨è§†è§‰å›¾åƒä¸­ç”Ÿæˆåˆ†å¸ƒåœ¨ä¸åŒåŒºåŸŸçš„å¤æ‚æ–‡æœ¬å†…å®¹ã€‚åœ¨CVTGä¸­ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹å¸¸å¸¸ä¼šæ¸²æŸ“å‡ºæ‰­æ›²ã€æ¨¡ç³Šçš„è§†è§‰æ–‡æœ¬æˆ–é—æ¼æŸäº›è§†è§‰æ–‡æœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TextCrafterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤šè§†è§‰æ–‡æœ¬æ¸²æŸ“æ–¹æ³•ã€‚TextCrafteré€šè¿‡é€æ­¥ç­–ç•¥å°†å¤æ‚è§†è§‰æ–‡æœ¬åˆ†è§£ä¸ºä¸åŒç»„ä»¶ï¼ŒåŒæ—¶ç¡®ä¿æ–‡æœ¬å†…å®¹ä¸å…¶è§†è§‰è½½ä½“ä¹‹é—´çš„å¼ºå¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24370",
            "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
            "url": "https://huggingface.co/papers/2503.24370",
            "abstract": "Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.",
            "score": 9,
            "issue_id": 2994,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "5f218f08538c601f",
            "authors": [
                "Tong Wu",
                "Chong Xiang",
                "Jiachen T. Wang",
                "Prateek Mittal"
            ],
            "affiliations": [
                "NVIDIA",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24370.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Thinking Intervention'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Thinking Intervention",
                    "desc": "This paper introduces a new method called Thinking Intervention, which enhances the reasoning capabilities of large language models (LLMs) by allowing explicit control over their internal thought processes. By inserting or modifying specific reasoning tokens, the model can generate more accurate and contextually relevant answers. The authors conducted extensive tests on various tasks, showing that this approach leads to significant improvements in performance, such as higher accuracy in following instructions and better handling of unsafe prompts. Overall, the findings suggest that Thinking Intervention provides a valuable framework for refining how LLMs reason and respond to complex queries."
                },
                "zh": {
                    "title": "æ€ç»´å¹²é¢„ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€ç»´å¹²é¢„ï¼ˆThinking Interventionï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ’å…¥æˆ–ä¿®æ”¹ç‰¹å®šçš„æ€ç»´æ ‡è®°æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­èƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ€ç»´å¹²é¢„æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æç¤ºæ–¹æ³•ï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†å±‚æ¬¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ§åˆ¶æ¨ç†è¿‡ç¨‹ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24115",
            "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
            "url": "https://huggingface.co/papers/2503.24115",
            "abstract": "The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.",
            "score": 7,
            "issue_id": 2994,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "61845428f5c3d9df",
            "authors": [
                "Zhiming Ma",
                "Peidong Wang",
                "Minhua Huang",
                "Jingpeng Wang",
                "Kai Wu",
                "Xiangzhao Lv",
                "Yachun Pang",
                "Yin Yang",
                "Wenjie Tang",
                "Yuchen Kang"
            ],
            "affiliations": [
                "China Mobile Internet Company Ltd. Guangzhou, Guangdong, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24115.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#synthetic",
                    "#open_source",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TeleAntiFraud-28k - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞ»ĞµĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ASR Ğ¸ TTS, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 28,511 Ğ¿Ğ°Ñ€ Ñ€ĞµÑ‡ÑŒ-Ñ‚ĞµĞºÑÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¾ÑˆĞµĞ½Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ TeleAntiFraud-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SFT."
                },
                "en": {
                    "title": "Revolutionizing Telecom Fraud Detection with TeleAntiFraud-28k",
                    "desc": "This paper introduces TeleAntiFraud-28k, a novel dataset designed to enhance telecom fraud detection by combining audio signals with textual analysis. The dataset is created using three innovative strategies, including privacy-preserved text generation from speech recordings and semantic enhancement through large language models. It consists of over 28,000 annotated speech-text pairs, enabling tasks like scenario classification and fraud detection. Additionally, the authors provide a benchmark for evaluating model performance and a fine-tuned model for practical applications, promoting further research in multimodal anti-fraud techniques."
                },
                "zh": {
                    "title": "æ„å»ºç”µä¿¡æ¬ºè¯ˆæ£€æµ‹çš„æ–°åŸºçŸ³",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†TeleAntiFraud-28kæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„å¼€æºéŸ³é¢‘-æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸‰ç§ç­–ç•¥æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®çš„éšç§ä¿æŠ¤å’ŒçœŸå®åœºæ™¯çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†TeleAntiFraud-Benchè¯„ä¼°åŸºå‡†ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°æµ‹è¯•æ¨¡å‹åœ¨ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ­¤é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€åæ¬ºè¯ˆç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18809",
            "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
            "url": "https://huggingface.co/papers/2503.18809",
            "abstract": "In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.",
            "score": 7,
            "issue_id": 2994,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "28288adc69a019ac",
            "authors": [
                "Augusto B. CorrÃªa",
                "AndrÃ© G. Pereira",
                "Jendrik Seipp"
            ],
            "affiliations": [
                "Federal University of Rio Grande do Sul",
                "LinkÃ¶ping University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18809.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. LLM ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ² Ğ²Ğ¸Ğ´Ğµ Python-ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering LLMs with Domain-Specific Heuristics for Better Planning",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in planning tasks, which often lead to incorrect and non-generalizable plans. The authors propose a novel approach where LLMs generate domain-specific heuristic functions in Python code, which are then evaluated using a greedy best-first search algorithm. Their method demonstrates that LLM-generated heuristics can outperform traditional domain-independent heuristics and compete with advanced learning algorithms in planning. The results indicate that these heuristics are not only efficient but also provide more informative guidance in certain planning domains."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§äººå·¥æ™ºèƒ½é—®é¢˜ä¸Šå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨è¯¦ç»†å®šä¹‰è§„åˆ’ä»»åŠ¡çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨è§„åˆ’æ–¹é¢ä»ç„¶ä¸å¯é ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨LLMsç”Ÿæˆæ­£ç¡®çš„è§„åˆ’ï¼Œå³ä½¿å¯¹äºè¶Šæ¥è¶Šå¤§çš„åˆ†å¸ƒå¤–ä»»åŠ¡ã€‚é€šè¿‡ç”Ÿæˆé¢†åŸŸç›¸å…³çš„å¯å‘å¼å‡½æ•°å¹¶åœ¨è´ªå©ªä¼˜å…ˆæœç´¢ä¸­è¯„ä¼°ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼å‡½æ•°åœ¨è§£å†³æœªè§æµ‹è¯•ä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„é¢†åŸŸæ— å…³å¯å‘å¼æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23284",
            "title": "SketchVideo: Sketch-based Video Generation and Editing",
            "url": "https://huggingface.co/papers/2503.23284",
            "abstract": "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.",
            "score": 6,
            "issue_id": 2996,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 30",
                "zh": "3æœˆ30æ—¥"
            },
            "hash": "d968c1da27effa84",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#games",
                    "#video"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞµÑ‚Ñ‡Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞºĞµÑ‚Ñ‡ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ DiT. Ğ”Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑĞºĞµÑ‚Ñ‡Ğ° Ğ½Ğ° Ğ²ÑĞµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Sketch Your Way to Better Video Control!",
                    "desc": "This paper presents a novel approach to video generation and editing using sketch-based controls. It addresses the challenges of accurately managing global layout and motion details by introducing a memory-efficient control structure that utilizes sketch control blocks. The method allows users to draw sketches on keyframes, which are then analyzed through an inter-frame attention mechanism to ensure consistency across all video frames. The proposed system, SketchVideo, enhances the ability to generate and edit videos while preserving the integrity of unedited regions, demonstrating superior performance in controllable video tasks."
                },
                "zh": {
                    "title": "è‰å›¾é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºè‰å›¾çš„ç©ºé—´å’Œè¿åŠ¨æ§åˆ¶åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å’Œå›¾åƒæ¡ä»¶ä¸‹çš„å¸ƒå±€å’Œå‡ ä½•ç»†èŠ‚æ§åˆ¶é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ§åˆ¶ç»“æ„ï¼Œåˆ©ç”¨è‰å›¾æ§åˆ¶å—é¢„æµ‹è·³è¿‡çš„DiTå—çš„æ®‹å·®ç‰¹å¾ã€‚é€šè¿‡åœ¨å…³é”®å¸§ä¸Šç»˜åˆ¶è‰å›¾ï¼Œå¹¶ä½¿ç”¨è·¨å¸§æ³¨æ„æœºåˆ¶åˆ†æå…³é”®å¸§ä¸æ¯ä¸ªè§†é¢‘å¸§ä¹‹é—´çš„å…³ç³»ï¼Œå®ç°äº†æ—¶é—´ä¸Šç¨€ç–çš„è‰å›¾æ¡ä»¶ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SketchVideoåœ¨å¯æ§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24364",
            "title": "Query and Conquer: Execution-Guided SQL Generation",
            "url": "https://huggingface.co/papers/2503.24364",
            "abstract": "We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.",
            "score": 5,
            "issue_id": 2997,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "2af26722887e77ac",
            "authors": [
                "Åukasz Borchmann",
                "Marek Wydmuch"
            ],
            "affiliations": [
                "Snowflake AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24364.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#small_models",
                    "#dataset",
                    "#inference",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº o1, o3-mini Ğ¸ DeepSeek R1, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 30 Ñ€Ğ°Ğ·. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL."
                },
                "en": {
                    "title": "Efficient SQL Generation: Small Models, Big Results!",
                    "desc": "This paper introduces a new method for generating SQL queries from natural language, enhancing accuracy in text-to-SQL tasks. The approach utilizes execution results to choose the best query from several options based on semantic consistency. It allows smaller models to outperform larger, more complex models while also cutting down inference costs significantly. This method is designed to work well with current models, making it a practical solution for improving SQL generation."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”ŸæˆSQLï¼Œæå‡æ–‡æœ¬åˆ°SQLçš„å‡†ç¡®æ€§",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤æ‚è¾“å‡ºï¼Œæ˜¾è‘—æé«˜æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰§è¡Œç»“æœï¼Œä»å¤šä¸ªå€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹©æœ€ç¬¦åˆè¯­ä¹‰çš„ä¸€é¡¹ï¼Œä½¿å¾—è¾ƒå°ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè®¡ç®—å¯†é›†å‹çš„æ¨ç†æ–¹æ³•ï¼Œå¦‚o1ã€o3-miniå’ŒDeepSeek R1ï¼ŒåŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½å¤šè¾¾30å€ã€‚å®ƒä¸ç°æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œæä¾›äº†ä¸€æ¡å®ç”¨ä¸”å¯æ‰©å±•çš„é€šå¾€æœ€å…ˆè¿›SQLç”Ÿæˆçš„è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.24290",
            "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
            "url": "https://huggingface.co/papers/2503.24290",
            "abstract": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.",
            "score": 3,
            "issue_id": 2998,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "c3cd649c5eb9d423",
            "authors": [
                "Jingcheng Hu",
                "Yinmin Zhang",
                "Qi Han",
                "Daxin Jiang",
                "Xiangyu Zhang",
                "Heung-Yeung Shum"
            ],
            "affiliations": [
                "StepFun",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.24290.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#open_source",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Open-Reasoner-Zero - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ vanilla PPO Ğ¸ GAE, Ğ±ĞµĞ· KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° DeepSeek-R1-Zero Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… AIME2024, MATH500 Ğ¸ GPQA Diamond, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ĞºĞ¾Ğ´, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Simplifying Large-Scale Reasoning in RL with Open-Reasoner-Zero",
                    "desc": "Open-Reasoner-Zero is an innovative open-source framework designed for large-scale reasoning-oriented reinforcement learning (RL) training. It utilizes a simple approach with vanilla Proximal Policy Optimization (PPO) and Generalized Advantage Estimation (GAE), achieving impressive results without complex regularization techniques. The framework demonstrates that minimalistic strategies can effectively enhance response length and benchmark performance, outperforming previous models like DeepSeek-R1-Zero. By releasing all components of the project, including source code and model weights, Open-Reasoner-Zero promotes accessibility and collaboration in the machine learning community."
                },
                "zh": {
                    "title": "å¼€æºæ¨ç†å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå®ç°",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Open-Reasoner-Zeroï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„å¤§è§„æ¨¡æ¨ç†å¯¼å‘å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°ï¼Œé‡ç‚¹å…³æ³¨å¯æ‰©å±•æ€§ã€ç®€æ´æ€§å’Œå¯è®¿é—®æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨ç®€å•çš„PPOç®—æ³•å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å“åº”é•¿åº¦å’ŒåŸºå‡†æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°ä¸DeepSeek-R1-Zeroä½¿ç”¨ç›¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨AIME2024ã€MATH500å’ŒGPQA DiamondåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡æ˜¾è‘—æé«˜ï¼Œä»…éœ€DeepSeek-R1-Zeroç®¡é“çš„ååˆ†ä¹‹ä¸€è®­ç»ƒæ­¥éª¤ã€‚ä¸ºäº†æ”¯æŒå¼€æºç²¾ç¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æºä»£ç ã€å‚æ•°è®¾ç½®ã€è®­ç»ƒæ•°æ®å’Œä¸åŒè§„æ¨¡çš„æ¨¡å‹æƒé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23077",
            "title": "Efficient Inference for Large Reasoning Models: A Survey",
            "url": "https://huggingface.co/papers/2503.23077",
            "abstract": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
            "score": 3,
            "issue_id": 2995,
            "pub_date": "2025-03-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 29",
                "zh": "3æœˆ29æ—¥"
            },
            "hash": "f76d7ead3d85b37e",
            "authors": [
                "Yue Liu",
                "Jiaying Wu",
                "Yufei He",
                "Hongcheng Gao",
                "Hongyu Chen",
                "Baolong Bi",
                "Jiaheng Zhang",
                "Zhiqi Huang",
                "Bryan Hooi"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Moonshot",
                "National University of Singapore",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23077.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#survey",
                    "#interpretability",
                    "#optimization",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: ÑĞ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Efficiency in Large Reasoning Models",
                    "desc": "This paper discusses Large Reasoning Models (LRMs) that enhance the reasoning capabilities of Large Language Models (LLMs) but face challenges like inefficient token usage and high memory consumption. It reviews various efficient inference methods aimed at reducing these inefficiencies while maintaining reasoning quality. The authors categorize these methods into explicit and implicit Chain-of-Thought approaches, analyzing their strengths and weaknesses. Additionally, the paper identifies ongoing challenges in the field and suggests strategies for improving inference efficiency in LRMs."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ•ˆç‡ï¼Œä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å­¦ä¹ æ¨ç†æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´äº†ä»¤ç‰Œä½¿ç”¨ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´çš„ä½æ•ˆã€‚å› æ­¤ï¼Œæœ¬æ–‡ç»¼è¿°äº†ä¸“é—¨ä¸ºLRMsè®¾è®¡çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºå‡è½»ä»¤ç‰Œä½æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨ç†è´¨é‡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ†ç±»æ³•ï¼Œå°†æœ€è¿‘çš„æ–¹æ³•åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ˜¾å¼ç´§å‡‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œéšå¼æ½œåœ¨çš„æ€ç»´é“¾ï¼Œè®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åˆ†æäº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23829",
            "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
            "url": "https://huggingface.co/papers/2503.23829",
            "abstract": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.",
            "score": 2,
            "issue_id": 2998,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "2c875f8335892dfd",
            "authors": [
                "Yi Su",
                "Dian Yu",
                "Linfeng Song",
                "Juntao Li",
                "Haitao Mi",
                "Zhaopeng Tu",
                "Min Zhang",
                "Dong Yu"
            ],
            "affiliations": [
                "Soochow University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23829.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RLVR: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°, Ñ…Ğ¸Ğ¼Ğ¸Ñ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ² Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµĞ¶Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Expanding RLVR: From Coding to Real-World Applications",
                    "desc": "This paper explores the use of Reinforcement Learning with Verifiable Rewards (RLVR) in diverse fields like medicine and economics, where traditional structured answers may not be available. The authors find that different large language models (LLMs) often agree on binary judgments when objective reference answers are present, suggesting that extensive annotations may not be necessary for training reward models. To enhance RLVR's effectiveness, they introduce model-based soft scoring to better handle unstructured reference answers. Their experiments demonstrate that a distilled generative reward model can effectively verify rewards across domains, leading to improved performance of RL policies compared to existing state-of-the-art models."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ï¼šè·¨é¢†åŸŸåº”ç”¨çš„æ–°å¯èƒ½",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨å°šæœªæ·±å…¥æ¢ç´¢ã€‚æˆ‘ä»¬ç ”ç©¶äº†RLVRåœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦å’Œç»æµå­¦ç­‰å¤šæ ·åŒ–é¢†åŸŸçš„æ‰©å±•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å­˜åœ¨å®¢è§‚å‚è€ƒç­”æ¡ˆæ—¶ï¼Œä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äºŒå…ƒåˆ¤æ–­ä¸Šé«˜åº¦ä¸€è‡´ï¼Œè¿™è¡¨æ˜è®­ç»ƒç‰¹å®šé¢†åŸŸå¥–åŠ±æ¨¡å‹ä¸ä¸€å®šéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨ã€‚é€šè¿‡å°†åŸºäºæ¨¡å‹çš„è½¯è¯„åˆ†çº³å…¥RLVRï¼Œæˆ‘ä»¬æé«˜äº†å…¶çµæ´»æ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è’¸é¦ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨è·¨é¢†åŸŸéªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23730",
            "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
            "url": "https://huggingface.co/papers/2503.23730",
            "abstract": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA",
            "score": 2,
            "issue_id": 2995,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 31",
                "zh": "3æœˆ31æ—¥"
            },
            "hash": "1d3d53298afe6cce",
            "authors": [
                "Yoonshik Kim",
                "Jaeyoon Jung"
            ],
            "affiliations": [
                "MAUM AI Inc. / Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23730.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#low_resource",
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡°ğŸ‡·",
                "ru": {
                    "title": "KOFFVQA: Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KOFFVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 275 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ 10 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ VLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶ĞµĞ½, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ VLM."
                },
                "en": {
                    "title": "KOFFVQA: Reliable Evaluation for Korean Vision-Language Models",
                    "desc": "This paper introduces KOFFVQA, a new benchmark for evaluating Large Vision-Language Models (VLMs) specifically in the Korean language. It addresses the limitations of current evaluation methods that either restrict responses or rely on subjective judge models. KOFFVQA features 275 visual question-answering tasks with clear grading criteria that enhance reliability and objectivity in assessments. The authors demonstrate that their approach provides a more dependable evaluation of VLMs compared to existing methods, making it suitable for both large and small models."
                },
                "zh": {
                    "title": "KOFFVQAï¼šéŸ©è¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é è¯„ä¼°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„ä¼°åŸºå‡†ï¼Œåä¸ºKOFFVQAï¼Œä¸“é—¨é’ˆå¯¹éŸ©è¯­ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢„è®¾çš„å›ç­”é€‰é¡¹æˆ–è¯„åˆ¤æ¨¡å‹ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸»è§‚ä¸”ä¸å¯é ã€‚KOFFVQAåŒ…å«275ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰å›¾åƒå’Œæ¶µç›–VLMæ€§èƒ½çš„10ä¸ªè¯„ä¼°æ ‡å‡†ã€‚é€šè¿‡å®¢è§‚çš„è¯„ä¼°æ ‡å‡†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼°æ¨¡å‹ï¼Œå³ä½¿æ˜¯å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½æœ‰æ•ˆä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20286",
            "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
            "url": "https://huggingface.co/papers/2503.20286",
            "abstract": "Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.",
            "score": 1,
            "issue_id": 2994,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 26",
                "zh": "3æœˆ26æ—¥"
            },
            "hash": "bf1debfaa462fca8",
            "authors": [
                "Zhenyu Liang",
                "Hao Li",
                "Naiwei Yu",
                "Kebin Sun",
                "Ran Cheng"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
                "Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20286.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ EMO: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (EMO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ½Ğ° GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğº Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ EMO: NSGA-III, MOEA/D Ğ¸ HypE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 1113 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ CPU Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ½Ğ° GPU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Accelerating EMO with GPU Tensorization for Enhanced Performance",
                    "desc": "This paper addresses the limitations of traditional evolutionary multiobjective optimization (EMO) algorithms when faced with complex and large-scale problems. It highlights the lack of hardware acceleration in existing EMO approaches and proposes a novel method to parallelize these algorithms using GPUs through tensorization. By transforming EMO data structures into tensor representations, the authors enable efficient GPU computing, resulting in significant performance improvements. The proposed tensorized EMO algorithms demonstrate remarkable speedups while maintaining solution quality in multiobjective robot control tasks."
                },
                "zh": {
                    "title": "å¼ é‡åŒ–æå‡EMOç®—æ³•æ€§èƒ½ï¼ŒGPUåŠ é€Ÿæ˜¾è‘—",
                    "desc": "è¿›åŒ–å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆEMOï¼‰åœ¨è¿‡å»äºŒåå¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éšç€é—®é¢˜è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œä¼ ç»Ÿçš„EMOç®—æ³•é¢ä¸´æ€§èƒ½é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ é‡åŒ–æ–¹æ³•åœ¨GPUä¸Šå¹¶è¡ŒåŒ–EMOç®—æ³•çš„æ–¹æ¡ˆï¼Œä»¥è§£å†³ä¼ ç»Ÿç®—æ³•çš„å¹¶è¡Œæ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼ é‡åŒ–ï¼ŒEMOç®—æ³•çš„æ•°æ®ç»“æ„å’Œæ“ä½œè¢«è½¬åŒ–ä¸ºç®€æ´çš„å¼ é‡è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†GPUè®¡ç®—çš„è‡ªåŠ¨åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ é‡åŒ–çš„EMOç®—æ³•åœ¨é€Ÿåº¦ä¸Šæ¯”åŸºäºCPUçš„ç®—æ³•å¿«äº†å¤šè¾¾1113å€ï¼ŒåŒæ—¶ä¿æŒäº†è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œå¹¶æœ‰æ•ˆå¤„ç†å¤æ‚çš„å¤šç›®æ ‡æœºå™¨äººæ§åˆ¶ä»»åŠ¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-31.html",
    "link_next": "2025-04-02.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "31.03",
        "en": "03/31",
        "zh": "3æœˆ31æ—¥"
    },
    "short_date_next": {
        "ru": "02.04",
        "en": "04/02",
        "zh": "4æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨èç³»ç»Ÿæ¡†æ¶ ReaRecã€‚å®ƒæ—¨åœ¨é€šè¿‡å¤šæ­¥æ¨ç†æ¥å¢å¼ºç”¨æˆ·è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·åå¥½çš„å¤æ‚å˜åŒ–ã€‚ReaRec ä½¿ç”¨è‡ªå›å½’çš„æ–¹å¼å°†åºåˆ—çš„æœ€åéšè—çŠ¶æ€åé¦ˆåˆ°æ¨èç³»ç»Ÿä¸­ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§è½»é‡çº§çš„æ¨ç†å­¦ä¹ æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReaRec èƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šç§é¡ºåºæ¨èæ¨¡å‹çš„æ€§èƒ½ã€‚ä½œè€…ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¸ºæœªæ¥çš„æ¨èç³»ç»Ÿç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚",
        "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨èç³»ç»Ÿæ¡†æ¶ ReaRecã€‚å®ƒæ—¨åœ¨é€šè¿‡å¤šæ­¥æ¨ç†æ¥å¢å¼ºç”¨æˆ·è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·åå¥½çš„å¤æ‚å˜åŒ–ã€‚ReaRec ä½¿ç”¨è‡ªå›å½’çš„æ–¹å¼å°†åºåˆ—çš„æœ€åéšè—çŠ¶æ€åé¦ˆåˆ°æ¨èç³»ç»Ÿä¸­ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§è½»é‡çº§çš„æ¨ç†å­¦ä¹ æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReaRec èƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šç§é¡ºåºæ¨èæ¨¡å‹çš„æ€§èƒ½ã€‚ä½œè€…ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¸ºæœªæ¥çš„æ¨èç³»ç»Ÿç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de tuÄ« jiÃ n xÃ¬ tÇ’ng kuÃ ng jiÃ  ReaRecã€‚tÄ zhÇ yÃº tÅng guÃ² duÅ bÃ¹ tuÄ« lÇ lÃ¡i zÄ“ng qiÃ¡ng yÃ²ng hÃ¹ biÇo shÃ¬, cÃ³ng Ã©r gÃ¨ng hÇo de bÄ«ng zhuÅ yÃ²ng hÃ¹ piÄn hÃ o de fÃº zÃ  biÃ n huÃ ã€‚ReaRec shÇ yÃ²ng zÃ¬ huÃ­ guÄ« de fÄng shÃ¬ jiÄng xÃ¹ liÃ¨ de zuÃ¬ hÃ²u yÇn cÃ¡ng zhuÃ ng tÃ i fÇn kuÃ¬ dÃ o tuÄ« jiÃ n xÃ¬ tÇ’ng zhÅng, bÃ¬ng yÇn rÃ¹ le liÇng zhÇ’ng qÄ«ng liÃ ng jÃ­ de tuÄ« lÇ xuÃ© xÃ­ fÄng fÇã€‚shÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, ReaRec nÃ©ng gÃ²u xiÇn zhÃ¹ tÃ­ gÄo duÅ zhÇ’ng shÃ¹n xÃ¹ tuÄ« jiÃ n mÃ³ xÃ­ng de xÃ¬ng nÃ©ngã€‚zuÃ² zhÄ› xiÄng xÃ¬n zhÃ¨ xiÃ ng gÅng zuÃ² wÃ¨i wÃ¨i lÃ¡i de tuÄ« jiÃ n xÃ¬ tÇ’ng yÃ¡n jiÅ« kÄi pÃ¬ le xÄ«n de fÄng xiÃ ngã€‚",
        "vocab": "[\n    {\"word\": \"æ¨èç³»ç»Ÿ\", \"pinyin\": \"tuÄ« jiÃ n xÃ¬ tÇ’ng\", \"trans\": \"recommendation system\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"æ—¨åœ¨\", \"pinyin\": \"zhÇ zÃ i\", \"trans\": \"aim to\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"å¤šæ­¥\", \"pinyin\": \"duÅ bÃ¹\", \"trans\": \"multi-step\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ng qiÃ¡ng\", \"trans\": \"enhance\"},\n    {\"word\": \"ç”¨æˆ·\", \"pinyin\": \"yÃ²ng hÃ¹\", \"trans\": \"user\"},\n    {\"word\": \"è¡¨ç¤º\", \"pinyin\": \"biÇo shÃ¬\", \"trans\": \"representation\"},\n    {\"word\": \"ä»è€Œ\", \"pinyin\": \"cÃ³ng Ã©r\", \"trans\": \"thus\"},\n    {\"word\": \"æ•æ‰\", \"pinyin\": \"bÇ” zhuÅ\", \"trans\": \"capture\"},\n    {\"word\": \"åå¥½\", \"pinyin\": \"piÄn hÃ o\", \"trans\": \"preference\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹ zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"å˜åŒ–\", \"pinyin\": \"biÃ n huÃ \", \"trans\": \"change\"},\n    {\"word\": \"è‡ªå›å½’\", \"pinyin\": \"zÃ¬ huÃ­ guÄ«\", \"trans\": \"autoregressive\"},\n    {\"word\": \"æ–¹å¼\", \"pinyin\": \"fÄng shÃ¬\", \"trans\": \"manner\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"éšè—çŠ¶æ€\", \"pinyin\": \"yÇn cÃ¡ng zhuÃ ng tÃ i\", \"trans\": \"hidden state\"},\n    {\"word\": \"åé¦ˆ\", \"pinyin\": \"fÇn kuÃ¬\", \"trans\": \"feedback\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è½»é‡çº§\", \"pinyin\": \"qÄ«ng liÃ ng jÃ­\", \"trans\": \"lightweight\"},\n    {\"word\": \"å­¦ä¹ æ–¹æ³•\", \"pinyin\": \"xuÃ© xÃ­ fÄng fÇ\", \"trans\": \"learning method\"},\n    {\"word\": \"å®éªŒç»“æœ\", \"pinyin\": \"shÃ­ yÃ n jiÃ© guÇ’\", \"trans\": \"experimental results\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"é¡ºåº\", \"pinyin\": \"shÃ¹n xÃ¹\", \"trans\": \"sequential\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ² zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"ç›¸ä¿¡\", \"pinyin\": \"xiÄng xÃ¬n\", \"trans\": \"believe\"},\n    {\"word\": \"å·¥ä½œ\", \"pinyin\": \"gÅng zuÃ²\", \"trans\": \"work\"},\n    {\"word\": \"æœªæ¥\", \"pinyin\": \"wÃ¨i lÃ¡i\", \"trans\": \"future\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"å¼€è¾Ÿ\", \"pinyin\": \"kÄi pÃ¬\", \"trans\": \"open up\"},\n    {\"word\": \"æ–¹å‘\", \"pinyin\": \"fÄng xiÃ ng\", \"trans\": \"direction\"}\n]",
        "trans": "This article introduces a new recommendation system framework called ReaRec. It aims to enhance user representation through multi-step reasoning, thereby better capturing the complex changes in user preferences. ReaRec employs an autoregressive approach to feed the final hidden state of the sequence back into the recommendation system and introduces two lightweight reasoning learning methods. Experimental results demonstrate that ReaRec can significantly improve the performance of various sequential recommendation models. The authors believe that this work opens up new directions for future research in recommendation systems.",
        "update_ts": "2025-03-31 09:12"
    }
}