
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 46 papers. June 6.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">6 июня</span> | <span id="title-articles-count">46 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-05.html">⬅️ <span id="prev-date">05.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-09.html">➡️ <span id="next-date">09.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'};
        let feedDateNext = {'ru': '09.06', 'en': '06/09', 'zh': '6月9日'};
        let feedDatePrev = {'ru': '05.06', 'en': '06/05', 'zh': '6月5日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.05301', 'title': 'SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training', 'url': 'https://huggingface.co/papers/2506.05301', 'abstract': 'SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.', 'score': 43, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b4eef08c89e4a692', 'authors': ['Jianyi Wang', 'Shanchuan Lin', 'Zhijie Lin', 'Yuxi Ren', 'Meng Wei', 'Zongsheng Yue', 'Shangchen Zhou', 'Hao Chen', 'Yang Zhao', 'Ceyuan Yang', 'Xuefeng Xiao', 'Chen Change Loy', 'Lu Jiang'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05301.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Эффективное восстановление видео за один шаг', 'desc': 'SeedVR2 - это однопроходная модель восстановления видео на основе диффузии. Она использует адаптивное оконное внимание и функцию сопоставления признаков для достижения высокого визуального качества при меньших вычислительных затратах. Модель применяет состязательное обучение на реальных данных для решения сложной задачи восстановления видео высокого разрешения за один шаг. Эксперименты показывают, что SeedVR2 достигает сравнимых или лучших результатов по сравнению с существующими подходами к восстановлению видео, выполняя обработку за один проход.'}, 'en': {'title': 'Efficient Video Restoration with SeedVR2: One-Step Diffusion Redefined', 'desc': 'SeedVR2 is a novel one-step diffusion-based model designed for video restoration that enhances visual quality while minimizing computational costs. It introduces an adaptive window attention mechanism that dynamically adjusts to the output resolution, addressing issues of window inconsistency in high-resolution video. The model also employs a feature matching loss to stabilize adversarial training, ensuring effective performance without compromising efficiency. Experimental results indicate that SeedVR2 outperforms or matches existing video restoration methods, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'SeedVR2：高效视频修复的新选择', 'desc': 'SeedVR2是一种基于扩散的一步视频修复模型，采用自适应窗口注意力机制和特征匹配损失，能够在降低计算成本的同时实现高视觉质量。该模型通过对真实数据进行对抗性训练，解决了高分辨率视频修复的挑战。为了适应不同输出分辨率，SeedVR2动态调整窗口大小，避免了高分辨率视频修复中窗口不一致的问题。此外，实验结果表明，SeedVR2在单步修复中能够达到或超过现有视频修复方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.05010', 'title': 'ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development', 'url': 'https://huggingface.co/papers/2506.05010', 'abstract': 'ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.', 'score': 43, 'issue_id': 4157, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '5d1aa2eb9189bc56', 'authors': ['Zhenran Xu', 'Xue Yang', 'Yiyu Wang', 'Qingli Hu', 'Zijiao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce', 'Harbin Institute of Technology (Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05010.jpg', 'data': {'categories': ['#multimodal', '#agents', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'ИИ-ассистент для творчества: ComfyUI-Copilot упрощает создание цифрового искусства', 'desc': 'ComfyUI-Copilot - это плагин на основе большой языковой модели, разработанный для улучшения удобства использования и эффективности платформы ComfyUI для создания искусства с помощью ИИ. Система использует иерархическую мультиагентную структуру с центральным агентом-ассистентом и специализированными рабочими агентами. ComfyUI-Copilot предлагает интеллектуальные рекомендации по узлам и моделям, а также автоматизированное построение рабочего процесса в один клик. Эффективность системы подтверждена как офлайн-оценками, так и отзывами пользователей, показывающими, что она точно рекомендует узлы и ускоряет разработку рабочих процессов.'}, 'en': {'title': 'Empowering Art Creation with Intelligent Assistance', 'desc': 'ComfyUI-Copilot is a plugin that leverages a large language model and a multi-agent system to improve the ComfyUI platform for AI art creation. It addresses common challenges faced by users, such as limited documentation and complex workflows, by providing intelligent recommendations and automating workflow construction. The system features a hierarchical structure with a central assistant agent that delegates tasks to specialized worker agents, enhancing usability and efficiency. Validation through user feedback and quantitative evaluations demonstrates that ComfyUI-Copilot effectively lowers barriers for beginners while streamlining processes for experienced users.'}, 'zh': {'title': '智能助手，轻松创作艺术', 'desc': 'ComfyUI-Copilot 是一个基于大型语言模型的插件，旨在提升 ComfyUI 这一开源 AI 艺术创作平台的可用性和效率。该系统通过提供智能节点和模型推荐，以及一键式自动化工作流构建，解决了新手用户在使用 ComfyUI 时面临的挑战。它采用了分层的多代理框架，中央助手代理负责任务分配，而专门的工作代理则处理不同的使用场景。通过离线定量评估和在线用户反馈，我们验证了 ComfyUI-Copilot 的有效性，显示其能够准确推荐节点并加速工作流开发。'}}}, {'id': 'https://huggingface.co/papers/2506.05284', 'title': 'Video World Models with Long-term Spatial Memory', 'url': 'https://huggingface.co/papers/2506.05284', 'abstract': "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.", 'score': 36, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '313ae4b0ffca864d', 'authors': ['Tong Wu', 'Shuai Yang', 'Ryan Po', 'Yinghao Xu', 'Ziwei Liu', 'Dahua Lin', 'Gordon Wetzstein'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05284.jpg', 'data': {'categories': ['#video', '#dataset', '#3d', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Улучшение долгосрочной памяти в видео-моделях мира', 'desc': 'Новая система улучшает долгосрочную согласованность видео-моделей мира, интегрируя механизм долгосрочной пространственной памяти, основанный на геометрии. Эта система вдохновлена механизмами человеческой памяти и включает в себя методы хранения и извлечения информации из долгосрочной пространственной памяти. Авторы создали специальные наборы данных для обучения и оценки моделей мира с явно сохраняемыми 3D-механизмами памяти. Оценки показывают улучшение качества, согласованности и длины контекста по сравнению с соответствующими базовыми моделями.'}, 'en': {'title': 'Enhancing Video World Models with Long-Term Spatial Memory', 'desc': 'This paper presents a new framework that improves the long-term consistency of video world models by incorporating a geometry-grounded long-term spatial memory mechanism. Traditional autoregressive models often forget previously generated scenes due to limited temporal context, which affects their ability to maintain consistency during revisits. The proposed framework mimics human memory by allowing the model to store and retrieve spatial information effectively, enhancing the overall quality of generated video frames. Evaluations demonstrate that this approach leads to better scene consistency and longer context retention compared to existing methods.'}, 'zh': {'title': '增强视频世界模型的一致性', 'desc': '本文提出了一种新框架，通过集成基于几何的长期空间记忆机制，增强视频世界模型的长期一致性。现有的世界模型在生成视频帧时，因时间上下文窗口大小有限，常常难以保持场景的一致性，导致对先前生成环境的严重遗忘。我们借鉴人类记忆机制，设计了存储和检索长期空间记忆的信息机制，并使用定制数据集来训练和评估具有显式存储3D记忆机制的世界模型。评估结果显示，与相关基线相比，我们的方法在质量、一致性和上下文长度上都有所提升，为长期一致的世界生成铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2506.04308', 'title': 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics', 'url': 'https://huggingface.co/papers/2506.04308', 'abstract': 'Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 36, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ef5abd087929ed17', 'authors': ['Enshen Zhou', 'Jingkun An', 'Cheng Chi', 'Yi Han', 'Shanyu Rong', 'Chi Zhang', 'Pengwei Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beihang University', 'Beijing Academy of Artificial Intelligence', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#robotics', '#reasoning', '#dataset', '#3d', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'RoboRefer: Пространственный интеллект для роботов нового поколения', 'desc': 'RoboRefer - это модель пространственного понимания для роботов, основанная на зрении и языке. Она использует специальный энкодер глубины и обучение с подкреплением для точного понимания 3D-сцен и рассуждений о пространственных отношениях. Авторы также представили большой набор данных RefSpatial и бенчмарк RefSpatial-Bench для обучения и оценки модели. RoboRefer превзошла современные методы, включая Gemini-2.5-Pro, и может применяться для управления различными роботами в реальных условиях.'}, 'en': {'title': 'RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments', 'desc': 'The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks.'}, 'zh': {'title': 'RoboRefer：提升机器人空间理解与推理能力的创新模型', 'desc': '空间指向是具身机器人与三维物理世界互动的基本能力。尽管现有的预训练视觉语言模型（VLMs）很强大，但它们在理解复杂的三维场景和动态推理指示位置方面仍然存在不足。为此，我们提出了RoboRefer，这是一种具有三维感知能力的VLM，通过监督微调（SFT）集成了专门的深度编码器，实现了精确的空间理解。此外，RoboRefer通过强化微调（RFT）推进了多步骤空间推理，采用针对空间指向任务的度量敏感过程奖励函数。'}}}, {'id': 'https://huggingface.co/papers/2506.05229', 'title': 'Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts', 'url': 'https://huggingface.co/papers/2506.05229', 'abstract': 'Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications.', 'score': 33, 'issue_id': 4163, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c1c7fcda2cc6ed7a', 'authors': ['Danil Sivtsov', 'Ivan Rodkin', 'Gleb Kuzmin', 'Yuri Kuratov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Moscow, Russia', 'FRC CSC RAS, Moscow, Russia', 'MBZUAI, Abu Dhabi, UAE', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia', 'Skoltech, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.05229.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#training', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение обработки длинных последовательностей в рекуррентных трансформерах', 'desc': 'Статья представляет новый метод планирования вычислений для рекуррентных моделей трансформеров (RMT), называемый Diagonal Batching. Этот подход позволяет распараллелить обработку сегментов в RMT, сохраняя при этом точную рекуррентность. Diagonal Batching устраняет ограничения последовательного выполнения, что значительно ускоряет вывод на GPU для длинных последовательностей. Применение этого метода к модели LLaMA-1B ARMT показало ускорение в 3.3 раза по сравнению со стандартной моделью LLaMA-1B и в 1.8 раза по сравнению с последовательной реализацией RMT на последовательностях длиной 131 072 токена.'}, 'en': {'title': 'Unlocking Parallelism for Efficient Long-Context Inference', 'desc': 'This paper addresses the limitations of Transformer models in handling long-context inference due to their high computational costs. It introduces Recurrent Memory Transformers (RMTs) that improve efficiency by reducing time complexity to linear and memory usage to constant. The authors propose a novel scheduling method called Diagonal Batching, which allows for parallel processing of segments in RMTs, overcoming the sequential execution bottleneck. By implementing this technique, they achieve significant speedups in inference times for long sequences, making RMTs more viable for practical applications.'}, 'zh': {'title': '对角批处理：提升长上下文推理效率的创新方案', 'desc': '本文介绍了一种新的调度方案，称为对角批处理（Diagonal Batching），旨在解决递归记忆变换器（RMTs）在长上下文推理中的性能瓶颈。传统的RMT由于其内存更新机制，导致了顺序执行，从而影响了性能。对角批处理通过在RMT中实现并行处理，消除了顺序限制，使得在单个长上下文输入上也能高效推理。该技术无需重新训练现有模型，应用于LLaMA-1B ARMT模型时，速度提升达3.3倍，显著降低了推理成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.05176', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models', 'url': 'https://huggingface.co/papers/2506.05176', 'abstract': "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.", 'score': 31, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '90ebf52dd91334c2', 'authors': ['Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Xin Zhang', 'Huan Lin', 'Baosong Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Junyang Lin', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05176.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#multilingual', '#open_source', '#training', '#small_models', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Qwen3 Embedding: Новый стандарт многоязычных текстовых эмбеддингов', 'desc': 'В работе представлена серия моделей Qwen3 Embedding, улучшающая возможности текстовых эмбеддингов и ранжирования по сравнению с предшественником GTE-Qwen. Модели основаны на фундаментальных моделях Qwen3 и используют многоэтапный процесс обучения, включающий масштабную предварительную подготовку и тонкую настройку на качественных датасетах. Qwen3 Embedding предлагает модели различных размеров (0.6B, 4B, 8B) для задач эмбеддинга и ранжирования, что позволяет оптимизировать их под разные сценарии применения. Эмпирические оценки показывают, что серия Qwen3 Embedding достигает передовых результатов в различных бенчмарках, особенно в многоязычной оценке MTEB и задачах поиска.'}, 'en': {'title': 'Empowering Multilingual Text Understanding with Qwen3 Embeddings', 'desc': 'The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research.'}, 'zh': {'title': 'Qwen3嵌入系列：多语言文本处理的新突破', 'desc': '本文介绍了Qwen3嵌入系列，这是在文本嵌入和重排序能力上相较于GTE-Qwen系列的重大进展。该系列基于Qwen3基础模型，利用其强大的多语言文本理解和生成能力，采用多阶段训练流程，结合大规模无监督预训练和高质量数据集的监督微调。通过有效的模型合并策略，Qwen3嵌入系列确保了模型的鲁棒性和适应性，提供了多种模型规模以满足不同的部署场景。实证评估表明，Qwen3嵌入系列在多项基准测试中取得了最先进的结果，尤其在多语言评估基准MTEB上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.02865', 'title': 'Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights', 'url': 'https://huggingface.co/papers/2506.02865', 'abstract': 'Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.', 'score': 27, 'issue_id': 4162, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '18e38be0b8df6445', 'authors': ['Mathieu Andreux', 'Breno Baldas Skuk', 'Hamza Benchekroun', 'Emilien Biré', 'Antoine Bonnet', 'Riaz Bordie', 'Matthias Brunel', 'Pierre-Louis Cedoz', 'Antoine Chassang', 'Mickaël Chen', 'Alexandra D. Constantinou', "Antoine d'Andigné", 'Hubert de La Jonquière', 'Aurélien Delfosse', 'Ludovic Denoyer', 'Alexis Deprez', 'Augustin Derupti', 'Michael Eickenberg', 'Mathïs Federico', 'Charles Kantor', 'Xavier Koegler', 'Yann Labbé', 'Matthew C. H. Lee', 'Erwan Le Jumeau de Kergaradec', 'Amir Mahla', 'Avshalom Manevich', 'Adrien Maret', 'Charles Masson', 'Rafaël Maurin', 'Arturo Mena', 'Philippe Modard', 'Axel Moyal', 'Axel Nguyen Kerbel', 'Julien Revelle', 'Mats L. Richter', 'María Santos', 'Laurent Sifre', 'Maxime Theillard', 'Marc Thibault', 'Louis Thiry', 'Léo Tronchon', 'Nicolas Usunier', 'Tony Wu'], 'affiliations': ['Company'], 'pdf_title_img': 'assets/pdf/title_img/2506.02865.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#agents', '#data', '#benchmark'], 'emoji': '🏄', 'ru': {'title': 'Surfer-H и Holo1: Эффективная веб-навигация с помощью ИИ', 'desc': 'Статья представляет Surfer-H - эффективного веб-агента, использующего модели компьютерного зрения и обработки естественного языка (VLM) для выполнения задач в интернете. Surfer-H интегрирован с Holo1 - новым набором специализированных VLM для веб-навигации и извлечения информации. Holo1 показывает высокие результаты на общих тестах пользовательских интерфейсов и новом бенчмарке WebClick. В сочетании с Holo1, Surfer-H достигает 92.2% точности на тесте WebVoyager, оптимально балансируя между эффективностью и стоимостью.'}, 'en': {'title': 'Surfer-H: Efficient Web Navigation with Vision-Language Models', 'desc': 'The paper introduces Surfer-H, a web agent designed to efficiently navigate and perform tasks using Vision-Language Models (VLMs). It is paired with Holo1, a collection of open-weight VLMs that excel in web navigation and information extraction. Holo1 is trained on diverse data sources, ensuring high performance on various benchmarks, including a new web UI localization benchmark called WebClick. Surfer-H achieves impressive results, reaching 92.2% accuracy on the WebVoyager task, while maintaining cost-efficiency, and both the evaluation dataset and model weights are made available for further research.'}, 'zh': {'title': '高效网络代理，智能导航新选择', 'desc': 'Surfer-H是一种高性价比的网络代理，结合了视觉-语言模型（VLM）来执行用户定义的网络任务。它与Holo1配对，Holo1是一个新的开放权重VLM集合，专注于网络导航和信息提取。Holo1在经过精心策划的数据源上训练，包括开放访问的网络内容和合成示例，表现出色，尤其是在用户界面基准测试中。通过Holo1，Surfer-H在WebVoyager上达到了92.2%的最佳性能，实现了准确性和成本效率的帕累托最优平衡。'}}}, {'id': 'https://huggingface.co/papers/2506.05209', 'title': 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text', 'url': 'https://huggingface.co/papers/2506.05209', 'abstract': 'The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.', 'score': 26, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '18ac0c75007ddff5', 'authors': ['Nikhil Kandpal', 'Brian Lester', 'Colin Raffel', 'Sebastian Majstorovic', 'Stella Biderman', 'Baber Abbasi', 'Luca Soldaini', 'Enrico Shippole', 'A. Feder Cooper', 'Aviya Skowron', 'John Kirchenbauer', 'Shayne Longpre', 'Lintang Sutawika', 'Alon Albalak', 'Zhenlin Xu', 'Guilherme Penedo', 'Loubna Ben Allal', 'Elie Bakouch', 'John David Pressman', 'Honglu Fan', 'Dashiell Stander', 'Guangyu Song', 'Aaron Gokaslan', 'Tom Goldstein', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tyler Murray'], 'affiliations': ['CMU', 'Cornell University', 'EleutherAI', 'Hugging Face', 'Independent', 'Lawrence Livermore National', 'Lila Sciences', 'MIT', 'Teraflop AI', 'The Allen Institute for', 'University of Maryland, College Park', 'University of Toronto Artificial Intelligence', 'Vector Institute', 'poolside'], 'pdf_title_img': 'assets/pdf/title_img/2506.05209.jpg', 'data': {'categories': ['#dataset', '#ethics', '#open_source', '#data'], 'emoji': '📚', 'ru': {'title': 'Открытые данные для этичного ИИ', 'desc': 'Исследователи представили набор данных Common Pile v0.1 - 8-терабайтную коллекцию текстов с открытой лицензией для обучения языковых моделей. На основе этих данных были обучены две модели Comma v0.1 с 7 миллиардами параметров, показавшие результаты на уровне аналогичных моделей, обученных на нелицензированных текстах. Набор данных включает разнообразные источники: научные статьи, код, книги, энциклопедии и другие. Это первый шаг к решению этических проблем и вопросов интеллектуальной собственности при обучении больших языковых моделей.'}, 'en': {'title': 'Openly Licensed Text for Competitive LLM Training', 'desc': "The paper introduces the Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text aimed at training large language models (LLMs). This dataset addresses ethical concerns related to using unlicensed text by providing a high-quality, diverse source of data from various domains. The authors validate the dataset's effectiveness by training two competitive 7 billion parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, which demonstrate performance comparable to LLMs trained on unlicensed data. Additionally, the paper includes the release of the dataset, training code, and model checkpoints to support further research."}, 'zh': {'title': '开放许可文本助力大型语言模型的训练', 'desc': 'Common Pile v0.1 数据集是一个包含 8TB 开放许可文本的集合，旨在训练具有 70 亿参数的竞争性大型语言模型（LLM）。该数据集汇集了来自 30 个不同领域的内容，包括研究论文、代码、书籍、百科全书和教育材料等。通过在 Common Pile 上训练的 Comma v0.1-1T 和 Comma v0.1-2T 模型，验证了该数据集的有效性，这两个模型在性能上与使用未授权文本训练的 LLM 相当。此研究为解决知识产权和伦理问题提供了一个重要的第一步。'}}}, {'id': 'https://huggingface.co/papers/2505.23656', 'title': 'VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models', 'url': 'https://huggingface.co/papers/2505.23656', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.', 'score': 24, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'aa7bbc7378df2b3e', 'authors': ['Xiangdong Zhang', 'Jiaqi Liao', 'Shaofeng Zhang', 'Fanqing Meng', 'Xiangpeng Wan', 'Junchi Yan', 'Yu Cheng'], 'affiliations': ['Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University', 'NetMind.AI', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.23656.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Физически достоверное видео из текста: VideoREPA улучшает понимание физики в моделях T2V', 'desc': 'В статье представлен новый метод VideoREPA для улучшения физической достоверности видео, генерируемых моделями text-to-video (T2V). Авторы предлагают дистиллировать понимание физики из моделей видеопонимания в модели T2V путем выравнивания отношений на уровне токенов. Ключевым элементом метода является функция потерь Token Relation Distillation (TRD), которая обеспечивает мягкое руководство для дообучения предобученных моделей T2V. Эмпирические оценки показывают, что VideoREPA значительно улучшает физический здравый смысл базового метода CogVideoX.'}, 'en': {'title': 'Bridging the Physics Gap in Text-to-Video Generation', 'desc': 'This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation.'}, 'zh': {'title': 'VideoREPA：提升文本到视频模型的物理理解能力', 'desc': '最近，文本到视频（T2V）扩散模型的进展使得高保真和真实的视频合成成为可能。然而，当前的T2V模型在生成物理上合理的内容方面常常面临挑战，因为它们对物理的理解能力有限。我们提出了一种新框架，称为VideoREPA，通过对齐令牌级关系，将视频理解基础模型中的物理理解能力提炼到T2V模型中，从而缩小物理理解的差距。我们的实验表明，VideoREPA显著增强了基线方法CogVideoX的物理常识，能够生成与直观物理一致的视频。'}}}, {'id': 'https://huggingface.co/papers/2506.05240', 'title': 'Aligning Latent Spaces with Flow Priors', 'url': 'https://huggingface.co/papers/2506.05240', 'abstract': 'This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.', 'score': 23, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c776325b1f9f6966', 'authors': ['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05240.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#cv', '#math', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Выравнивание латентных пространств с помощью потоковых моделей', 'desc': 'Статья представляет новый подход к выравниванию обучаемых латентных пространств с произвольными целевыми распределениями, используя генеративные модели на основе потоков в качестве априорных. Метод сначала предобучает модель потока на целевых признаках, а затем использует ее для регуляризации латентного пространства через функцию потерь выравнивания. Авторы доказывают, что минимизация этой функции потерь является вычислительно эффективным суррогатом для максимизации вариационной нижней границы правдоподобия латентов. Эффективность подхода подтверждается экспериментами по генерации изображений на ImageNet с различными целевыми распределениями.'}, 'en': {'title': 'Aligning Latent Spaces with Flow-Based Models', 'desc': "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."}, 'zh': {'title': '潜在空间对齐的新方法', 'desc': '本文提出了一种新颖的框架，通过利用基于流的生成模型作为先验，将可学习的潜在空间对齐到任意目标分布。我们的方法首先在目标特征上预训练流模型，以捕捉潜在分布。然后，这个固定的流模型通过对齐损失来规范化潜在空间，重新构造流匹配目标，将潜在变量视为优化目标。我们正式证明，最小化这个对齐损失建立了一个计算上可处理的替代目标，用于最大化潜在变量在目标分布下的变分下界的对数似然。'}}}, {'id': 'https://huggingface.co/papers/2506.05349', 'title': 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos', 'url': 'https://huggingface.co/papers/2506.05349', 'abstract': "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA", 'score': 22, 'issue_id': 4159, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '7f9f8448e60cfdb4', 'authors': ['Hanoona Rasheed', 'Abdelrahman Shaker', 'Anqi Tang', 'Muhammad Maaz', 'Ming-Hsuan Yang', 'Salman Khan', 'Fahad Khan'], 'affiliations': ['Australian National University', 'Google Research', 'Linköping University', 'MBZUAI', 'University of California Merced'], 'pdf_title_img': 'assets/pdf/title_img/2506.05349.jpg', 'data': {'categories': ['#math', '#multimodal', '#benchmark', '#transfer_learning', '#reasoning', '#video'], 'emoji': '🧮', 'ru': {'title': 'VideoMathQA: Новый рубеж в оценке математических рассуждений ИИ на основе видео', 'desc': 'VideoMathQA - это новый бенчмарк для оценки способности моделей выполнять кросс-модальные рассуждения в математических задачах на основе видео. Он охватывает 10 различных математических областей и включает видео продолжительностью от 10 секунд до более чем часа. Бенчмарк оценивает три ключевых навыка: прямое решение задач, концептуальный перенос и глубокое понимание инструкций. VideoMathQA демонстрирует ограничения существующих подходов и устанавливает систематическую основу для оценки моделей, способных рассуждать в математических задачах с временной протяженностью и мультимодальным контекстом.'}, 'en': {'title': 'VideoMathQA: Advancing Cross-Modal Reasoning in Mathematics', 'desc': 'VideoMathQA is a benchmark that tests how well models can reason about mathematics in videos, which is more complex than in static images or text. It focuses on understanding visual information, reading text, and integrating spoken cues that are often spread out over time. The benchmark includes questions that require direct problem solving, applying learned concepts to new situations, and understanding detailed instructions. By analyzing model performance on these tasks, VideoMathQA aims to identify the strengths and weaknesses of current approaches in handling complex, multimodal reasoning in mathematical contexts.'}, 'zh': {'title': '视频数学推理的新挑战', 'desc': 'VideoMathQA 是一个评估模型在视频环境中进行跨模态推理能力的基准，特别是在数学领域。它要求模型能够理解复杂的视觉信息、手写或数字文本，并整合分散的语音提示。该基准涵盖了10个不同的数学领域，问题设计围绕直接问题解决、概念转移和深度教学理解三个核心挑战。通过这个基准，我们揭示了现有方法的局限性，并建立了一个系统的评估框架，以测试模型在复杂的数学问题设置中的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05328', 'title': 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs', 'url': 'https://huggingface.co/papers/2506.05328', 'abstract': "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.", 'score': 20, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '774ccf3fd01aa4ef', 'authors': ['Lidong Lu', 'Guo Chen', 'Zhiqi Li', 'Yicheng Liu', 'Tong Lu'], 'affiliations': ['Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#reasoning', '#dataset', '#long_context', '#training', '#video', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Продвинутый подсчет объектов в видео с помощью мультимодального ИИ', 'desc': 'Статья представляет новый бенчмарк CG-AV-Counting для задач подсчета объектов в длинных видео с использованием мультимодальных вопросов и аннотированных подсказок. Авторы предлагают модель AV-Reasoner, обученную с помощью обучения с подкреплением и куррикулярного обучения для улучшения способности подсчета. Модель достигает лучших результатов на нескольких бенчмарках, демонстрируя эффективность обучения с подкреплением. Однако эксперименты показывают, что рассуждения в языковом пространстве не приносят улучшений на бенчмарках вне домена обучения.'}, 'en': {'title': 'Enhancing Video Counting with CG-AV-Counting and AV-Reasoner', 'desc': 'This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts.'}, 'zh': {'title': '提升视频计数能力的新基准与模型', 'desc': '尽管视频理解取得了进展，但当前的多模态学习模型在计数任务上仍然存在困难。现有的基准测试受限于短视频、封闭式查询、缺乏线索注释和多模态覆盖不足。本文介绍了CG-AV-Counting，这是一个手动注释的线索基础计数基准，包含1,027个多模态问题和5,845个注释线索，覆盖497个长视频。我们提出的AV-Reasoner模型通过GRPO和课程学习进行训练，能够从相关任务中推广计数能力，并在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2506.05345', 'title': 'Inference-Time Hyper-Scaling with KV Cache Compression', 'url': 'https://huggingface.co/papers/2506.05345', 'abstract': 'Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8times compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets.', 'score': 19, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '172bab27eb20c036', 'authors': ['Adrian Łańcucki', 'Konrad Staniszewski', 'Piotr Nawrot', 'Edoardo M. Ponti'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.05345.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Больше токенов, выше точность: сжатие памяти для эффективного вывода языковых моделей', 'desc': 'Статья представляет метод динамического разреживания памяти (Dynamic Memory Sparsification, DMS) для сжатия кэша ключ-значение в трансформерных моделях большого языка. DMS позволяет генерировать больше токенов при том же объеме вычислений, что повышает точность вывода. Метод требует всего 1000 шагов обучения для достижения 8-кратного сжатия, сохраняя при этом лучшую точность, чем методы разреженного внимания без обучения. Эксперименты показывают значительное улучшение результатов на различных наборах данных для нескольких семейств языковых моделей.'}, 'en': {'title': 'Boosting Token Generation with Dynamic Memory Sparsification', 'desc': 'This paper presents a method called Dynamic Memory Sparsification (DMS) to improve the efficiency of Transformer large language models (LLMs) during inference. By compressing the key-value (KV) cache, DMS allows for the generation of more tokens without increasing the computational cost, thus enhancing reasoning accuracy. The method achieves significant compression while preserving important information, which is crucial for maintaining model performance. The results show that DMS can boost accuracy across various LLMs while keeping the inference runtime and memory usage stable.'}, 'zh': {'title': '动态内存稀疏化：推理时的超规模扩展', 'desc': '本文提出了一种在推理时进行超规模扩展的方法，利用动态内存稀疏化技术来压缩键值缓存，从而在相同的计算预算内生成更多的令牌，并提高推理的准确性。传统的推理扩展往往在效率与推理准确性之间进行权衡，而我们的研究表明，推理成本主要受限于键值缓存的大小。通过压缩键值缓存，我们能够在不增加计算负担的情况下，生成更长或更多的并行序列。我们的方法在多个大型语言模型上验证了其有效性，显示出在相似的推理运行时间和内存负载下，准确性得到了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.04633', 'title': 'Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations', 'url': 'https://huggingface.co/papers/2506.04633', 'abstract': 'Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.', 'score': 16, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '05d0ae7d805419c1', 'authors': ['Linjie Li', 'Mahtab Bigverdi', 'Jiawei Gu', 'Zixian Ma', 'Yinuo Yang', 'Ziang Li', 'Yejin Choi', 'Ranjay Krishna'], 'affiliations': ['Stanford University', 'Sun Yat-sen University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04633.jpg', 'data': {'categories': ['#multimodal', '#3d', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'STARE: Новый рубеж в оценке пространственного интеллекта ИИ', 'desc': 'Статья представляет новый бенчмарк STARE для оценки мультимодальных языковых моделей в задачах пространственного мышления и визуального моделирования. Бенчмарк включает 4000 заданий на геометрические преобразования, пространственное мышление и реальные пространственные задачи. Результаты показывают, что модели хорошо справляются с простыми 2D-преобразованиями, но близки к случайному угадыванию в сложных задачах, требующих многошаговых визуальных симуляций. Люди демонстрируют почти идеальную точность, но тратят значительное время на сложные задачи, существенно ускоряясь при использовании промежуточных визуальных симуляций.'}, 'en': {'title': 'STARE: Bridging the Gap in Spatial Reasoning for AI', 'desc': 'This paper presents STARE, a new benchmark for evaluating multimodal large language models on spatial reasoning tasks that require visual simulations. Unlike existing benchmarks that focus on verbal reasoning, STARE includes 4,000 tasks involving geometric transformations and real-world spatial challenges. The results show that while models perform well on simple 2D tasks, they struggle with complex 3D tasks that require multi-step reasoning. This indicates that current models have difficulty effectively using visual information to enhance their reasoning capabilities.'}, 'zh': {'title': 'STARE：评估空间推理的新基准', 'desc': '空间认知对人类智能至关重要，它使我们能够通过视觉模拟解决问题，而不仅仅依赖语言推理。现有的人工智能基准主要评估语言推理，忽视了非语言多步骤视觉模拟的复杂性。我们提出了STARE（空间变换与推理评估），这是一个旨在严格评估多模态大型语言模型在多步骤视觉模拟任务上的基准。评估结果显示，模型在简单的2D变换上表现良好，但在更复杂的任务上，如3D立方体展开和拼图，表现接近随机，表明模型可能无法有效利用中间视觉信息。'}}}, {'id': 'https://huggingface.co/papers/2506.05344', 'title': 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs', 'url': 'https://huggingface.co/papers/2506.05344', 'abstract': 'Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.', 'score': 15, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '0175b3788ebacf29', 'authors': ['Jiahui Wang', 'Zuyan Liu', 'Yongming Rao', 'Jiwen Lu'], 'affiliations': ['Tencent Hunyuan Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05344.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#architecture', '#open_source', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зрение: оптимизация визуального восприятия в мультимодальных ИИ', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) используют лишь небольшую часть (менее 5%) механизмов внимания для обработки визуальной информации. Авторы разработали метод SparseMM для оптимизации KV-кэша, который распределяет вычислительные ресурсы асимметрично, основываясь на визуальной релевантности головок внимания. Этот подход позволяет ускорить вывод MLLM в 1,38 раза и сократить использование памяти на 52% при сохранении производительности. Метод SparseMM показывает лучшее соотношение точности и эффективности по сравнению с существующими методами оптимизации KV-кэша.'}, 'en': {'title': 'Optimizing Visual Understanding in MLLMs with Sparse Attention', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks.'}, 'zh': {'title': '利用稀疏视觉头加速多模态模型推理', 'desc': '多模态大型语言模型（MLLMs）通过扩展预训练的大型语言模型（LLMs）来增加视觉能力。我们研究了MLLMs如何处理视觉输入，分析了它们的注意力机制。我们发现了一个惊人的稀疏现象：在LLMs中，只有少量（大约5%以下）的注意力头积极参与视觉理解，这些被称为视觉头。基于这一发现，我们提出了SparseMM，一种KV-Cache优化策略，根据视觉得分为LLMs中的头分配不对称的计算预算，从而加速MLLMs的推理。'}}}, {'id': 'https://huggingface.co/papers/2506.03077', 'title': 'StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs', 'url': 'https://huggingface.co/papers/2506.03077', 'abstract': "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  \t\t\t\t\tAI-generated summary \t\t\t\t Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.", 'score': 15, 'issue_id': 4156, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '14c578e79a6d095c', 'authors': ['Qijun Luo', 'Mengqi Li', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03077.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'StreamBP: Революция в обучении языковых моделей', 'desc': 'StreamBP - это эффективный метод обратного распространения ошибки для обучения языковых моделей. Он разлагает правило цепочки вдоль последовательности, что значительно снижает затраты памяти на хранение активаций и логитов. StreamBP позволяет обрабатывать более длинные последовательности и ускоряет обучение по сравнению с методом контрольных точек градиента. Метод применим к различным целевым функциям и эффективно масштабируется на несколько GPU.'}, 'en': {'title': 'StreamBP: Efficient Backpropagation for Long Sequences in Language Models', 'desc': 'StreamBP is a novel backpropagation method designed to efficiently handle long sequence lengths in language models. By decomposing the chain rule in a layer-wise manner, it significantly reduces the memory required for storing activation values during training. This method not only speeds up the backpropagation process but also allows for longer sequences compared to traditional gradient checkpointing techniques. Additionally, StreamBP can be easily integrated into existing transformer training pipelines and supports multi-GPU setups for enhanced performance.'}, 'zh': {'title': 'StreamBP：高效反向传播，助力长序列训练', 'desc': 'StreamBP是一种高效的反向传播方法，通过对链式法则进行线性分解，显著降低了内存消耗。这使得在训练语言模型时，可以处理更长的序列并加快训练速度。与传统的梯度检查点技术相比，StreamBP能够将反向传播的最大序列长度提高2.8到5.5倍，同时保持相似或更少的反向传播时间。此外，StreamBP还支持多GPU训练，增强了其在实际应用中的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2506.05287', 'title': 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?', 'url': 'https://huggingface.co/papers/2506.05287', 'abstract': "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.", 'score': 13, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '8b357b07e4b449cd', 'authors': ['Yuqian Yuan', 'Ronghao Dang', 'Long Li', 'Wentong Li', 'Dian Jiao', 'Xin Li', 'Deli Zhao', 'Fan Wang', 'Wenqiao Zhang', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05287.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#interpretability', '#benchmark', '#games'], 'emoji': '👁️', 'ru': {'title': 'EOC-Bench: Новый стандарт для оценки когнитивных способностей ИИ в эгоцентрическом зрении', 'desc': 'Статья представляет новый бенчмарк EOC-Bench для оценки понимания объектов в динамических эгоцентрических сценариях мультимодальными языковыми моделями. EOC-Bench включает 3277 аннотированных пар вопрос-ответ, охватывающих три временные категории и 11 измерений оценки. Авторы разработали систему аннотаций с участием человека и новую метрику для оценки временной точности. EOC-Bench позволяет комплексно оценивать возможности мультимодальных языковых моделей в понимании объектов в воплощенных системах.'}, 'en': {'title': 'EOC-Bench: Advancing Object Cognition in Dynamic Environments', 'desc': "This paper introduces EOC-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) in dynamic environments where users interact with objects. Unlike previous benchmarks that focus on static scenes, EOC-Bench assesses how well models understand objects in changing contexts by using 3,277 annotated question-answer pairs across different time frames. The benchmark includes a unique human-in-the-loop annotation framework and a multi-scale temporal accuracy metric to evaluate models' performance in real-time interactions. By providing a comprehensive evaluation tool, EOC-Bench aims to enhance the cognitive abilities of MLLMs in embodied applications."}, 'zh': {'title': '动态场景中的物体认知评估新基准', 'desc': '多模态大型语言模型（MLLMs）的出现推动了以自我为中心的视觉应用的突破。这些应用需要对物体进行持续的、上下文感知的理解，因为用户在动态和杂乱的环境中与工具互动。然而，现有的体现基准主要集中在静态场景探索上，强调物体的外观和空间属性，而忽视了用户互动所带来的动态变化评估。为了解决这个问题，我们引入了EOC-Bench，这是一个创新的基准，旨在系统地评估动态自我中心场景中的以物体为中心的体现认知。'}}}, {'id': 'https://huggingface.co/papers/2506.05331', 'title': 'MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.05331', 'abstract': 'Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT', 'score': 12, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b3e1648a048ac6b7', 'authors': ['Xinyan Chen', 'Renrui Zhang', 'Dongzhi Jiang', 'Aojun Zhou', 'Shilin Yan', 'Weifeng Lin', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05331.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#reasoning', '#math', '#games'], 'emoji': '🧮', 'ru': {'title': 'Визуальное рассуждение в математике: новый уровень с MINT-CoT', 'desc': 'Статья представляет MINT-CoT - новый подход к визуальному рассуждению в математических задачах с использованием цепочки размышлений (Chain-of-Thought). Метод адаптивно внедряет релевантные визуальные токены в текстовые шаги рассуждения, динамически выбирая визуальные области любой формы в математических фигурах. Авторы создали датасет MINT-CoT, содержащий 54 тысячи математических задач, где каждый шаг рассуждения согласован с визуальными областями на уровне токенов. Разработанная модель MINT-CoT-7B значительно превосходит базовые модели на нескольких бенчмарках математического визуального рассуждения.'}, 'en': {'title': 'Enhancing Math Reasoning with Visual Interleaving in LLMs', 'desc': 'This paper introduces MINT-CoT, a novel approach to enhance mathematical reasoning in Large Language Models (LLMs) by integrating visual information more effectively. The method uses Mathematical Interleaved Tokens to dynamically select and incorporate relevant visual regions into the reasoning process, overcoming limitations of previous models that relied on fixed image regions. The authors created a dataset with 54,000 math problems that align visual tokens with reasoning steps, enabling better training of the model. Experimental results show that MINT-CoT-7B significantly outperforms existing models in various math problem-solving tasks, demonstrating its effectiveness in multimodal reasoning.'}, 'zh': {'title': 'MINT-CoT：数学推理的新突破', 'desc': '本论文提出了一种新的方法MINT-CoT，用于在多模态领域中增强大型语言模型的数学推理能力。MINT-CoT通过引入数学交错标记，将相关的视觉信息动态地融入文本推理步骤中，从而解决了传统方法在数学问题解决中的局限性。我们构建了一个包含54K数学问题的数据集，确保每个推理步骤与视觉区域在标记级别上对齐。实验结果表明，MINT-CoT-7B模型在多个数学任务上显著优于基线模型，展示了其在视觉交错推理中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.05334', 'title': 'Search Arena: Analyzing Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.05334', 'abstract': "Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.", 'score': 11, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '531f22c16daba821', 'authors': ['Mihran Miroyan', 'Tsung-Han Wu', 'Logan King', 'Tianle Li', 'Jiayi Pan', 'Xinyan Hu', 'Wei-Lin Chiang', 'Anastasios N. Angelopoulos', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.05334.jpg', 'data': {'categories': ['#alignment', '#data', '#multilingual', '#dataset', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Глубокий анализ взаимодействия пользователей с ИИ-системами, дополненными поиском', 'desc': 'Search Arena - это масштабный набор данных о предпочтениях пользователей при взаимодействии с языковыми моделями, дополненными поиском. Исследование показывает, что количество цитат влияет на восприятие достоверности ответов, даже если цитируемый контент напрямую не поддерживает утверждения. Анализ выявил предпочтения пользователей к определенным источникам информации, причем платформы, основанные на сообществах, оказались более популярными. Набор данных охватывает разнообразные намерения и языки, содержит полные системные трассировки и около 12 000 оценок предпочтений пользователей.'}, 'en': {'title': 'Unveiling User Preferences in Search-Augmented Language Models', 'desc': 'Search Arena is a comprehensive dataset designed to study user interactions with search-augmented language models (LLMs). It includes over 24,000 multi-turn interactions and 12,000 human preference votes, providing insights into how users perceive citation influence and source credibility. The findings indicate that user preferences are swayed by the number of citations, regardless of their relevance, highlighting a disconnect between perceived and actual credibility. Additionally, the dataset reveals that users favor community-driven sources over static encyclopedic ones, and it demonstrates that web search can enhance performance in various contexts, challenging the notion that LLMs should rely solely on their internal knowledge.'}, 'zh': {'title': '揭示搜索增强模型中的用户偏好与可信度', 'desc': 'Search Arena是一个大规模的人类偏好数据集，分析用户与搜索增强语言模型的互动，揭示了引用影响和来源可信度的见解。该数据集包含超过24,000对多轮用户交互，涵盖多种意图和语言，并提供了约12,000个用户偏好投票的完整系统记录。分析结果显示，用户偏好受到引用数量的影响，即使被引用的内容并不直接支持所声称的观点。此外，用户对不同引用来源的偏好存在差异，表明社区驱动的平台通常更受欢迎，而静态的百科全书来源并不总是合适和可靠。'}}}, {'id': 'https://huggingface.co/papers/2506.05327', 'title': 'Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.05327', 'abstract': 'Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss', 'score': 11, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '374ddd58ce0653c6', 'authors': ['Duochao Shi', 'Weijie Wang', 'Donny Y. Chen', 'Zeyu Zhang', 'Jia-Wang Bian', 'Bohan Zhuang', 'Chunhua Shen'], 'affiliations': ['GigaAI', 'MBZUAI', 'Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.05327.jpg', 'data': {'categories': ['#optimization', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение 3D-реконструкции с помощью умной регуляризации границ объектов', 'desc': 'Статья представляет новый метод регуляризации для улучшения качества 3D-реконструкции в системах 3D Gaussian Splatting. Авторы предлагают PM-Loss - функцию потерь, основанную на предсказании карты точек с помощью предобученного трансформера. Этот подход позволяет сгладить геометрические разрывы на границах объектов, которые часто возникают при использовании карт глубины. Применение PM-Loss значительно улучшает качество рендеринга для различных архитектур и сцен.'}, 'en': {'title': 'Enhancing 3D Rendering with PM-Loss for Smooth Depth Representation', 'desc': 'This paper presents PM-Loss, a new regularization technique designed to enhance the quality of 3D point clouds generated from depth maps in 3D Gaussian Splatting (3DGS) systems. The authors address the common issue of depth discontinuities at object boundaries, which can lead to poor rendering quality due to fragmented point clouds. By utilizing a pointmap predicted by a pre-trained transformer, PM-Loss promotes geometric smoothness, improving the overall accuracy of the depth representation. The results demonstrate that incorporating PM-Loss leads to superior rendering outcomes across different architectures and scenes.'}, 'zh': {'title': '提升3D渲染质量的新方法', 'desc': '本文提出了一种新的正则化损失函数PM-Loss，用于改善基于深度图的3D高斯点云渲染。传统方法在物体边界处的深度不连续性会导致点云稀疏，从而影响渲染质量。PM-Loss利用预训练的变换器预测的点图，尽管其准确性不如深度图，但能有效增强几何平滑性。通过改进深度图，我们的方法在不同架构和场景中显著提升了3D高斯点云的渲染效果。'}}}, {'id': 'https://huggingface.co/papers/2506.02620', 'title': 'FlexPainter: Flexible and Multi-View Consistent Texture Generation', 'url': 'https://huggingface.co/papers/2506.02620', 'abstract': 'FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  \t\t\t\t\tAI-generated summary \t\t\t\t Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.', 'score': 11, 'issue_id': 4157, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '296d6abe1e32cfa9', 'authors': ['Dongyu Yan', 'Leyi Wu', 'Jiantao Lin', 'Luozhou Wang', 'Tianshuo Xu', 'Zhifei Chen', 'Zhen Yang', 'Lie Xu', 'Shunsi Zhang', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Quwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.02620.jpg', 'data': {'categories': ['#multimodal', '#3d', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'FlexPainter: гибкая и согласованная генерация текстур с мультимодальным управлением', 'desc': 'FlexPainter - это новый конвейер генерации текстур, использующий общее условное пространство вложений для гибкого мультимодального управления. Он обеспечивает высококачественную и согласованную генерацию карт текстур с помощью диффузионных моделей изображений и 3D-ориентированной модели. FlexPainter решает проблемы ограниченной гибкости управления и несогласованности между сгенерированными многоракурсными изображениями. Система включает метод декомпозиции структурной и стилевой информации, а также модуль синхронизации ракурсов для обеспечения локальной согласованности.'}, 'en': {'title': 'FlexPainter: Revolutionizing Texture Generation with Multi-Modal Guidance', 'desc': 'FlexPainter is a new pipeline designed for generating high-quality texture maps in 3D modeling. It utilizes a shared conditional embedding space to allow for flexible multi-modal guidance, which helps in producing consistent textures from various input types. By employing an image diffusion prior and a 3D-aware model, it generates multi-view images that maintain local consistency and enhance overall quality. The framework has been shown to outperform existing methods in terms of both flexibility and the quality of the generated textures.'}, 'zh': {'title': 'FlexPainter：灵活高效的纹理生成新方法', 'desc': 'FlexPainter是一种新型的纹理生成管道，利用共享的条件嵌入空间实现灵活的多模态引导，从而确保高质量和一致性的纹理图生成。该方法结合了图像扩散先验和3D感知模型，解决了传统方法在控制灵活性和提示模态方面的限制。通过构建共享的条件嵌入空间，FlexPainter能够在不同输入模态之间进行灵活聚合，提升生成效果。实验结果表明，FlexPainter在灵活性和生成质量上显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.04734', 'title': 'Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design', 'url': 'https://huggingface.co/papers/2506.04734', 'abstract': 'Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.', 'score': 10, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'ce810e0cc38b26e4', 'authors': ['Lin Sun', 'Weihong Lin', 'Jinzhu Wu', 'Yongfu Zhu', 'Xiaoqi Jian', 'Guangxiang Zhao', 'Change Jia', 'Linglin Zhang', 'Sai-er Hu', 'Yuhan Wu', 'Xiangzheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.04734.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': '🎢', 'ru': {'title': 'Нестабильность оценок: вызов для бенчмаркинга языковых моделей', 'desc': 'Исследование показывает, что результаты оценки моделей серии Deepseek-R1-Distill подвержены значительным колебаниям из-за различных факторов. Небольшие изменения в условиях оценки могут привести к существенным различиям в результатах. Аналогичные явления наблюдаются и в других моделях, основанных на Deepseek-R1-Distill, а также в модели QwQ-32B. Авторы призывают к созданию более строгой парадигмы оценки производительности моделей машинного обучения.'}, 'en': {'title': 'Ensuring Reliable Evaluations for Deep Learning Models', 'desc': 'The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results.'}, 'zh': {'title': '建立更严格的模型评估标准', 'desc': 'Deepseek-R1-Distill系列模型在数学、科学和编程等领域表现出色，受到开源社区的广泛采用。然而，我们的研究发现，这些模型的基准评估结果受到多种因素的显著波动影响。评估条件的细微差异可能导致结果的重大变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型中，因此我们呼吁建立更严格的模型性能评估范式。'}}}, {'id': 'https://huggingface.co/papers/2506.04209', 'title': 'Language-Image Alignment with Fixed Text Encoders', 'url': 'https://huggingface.co/papers/2506.04209', 'abstract': 'Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.', 'score': 10, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '921137445b3be92e', 'authors': ['Jingfeng Yang', 'Ziyang Wu', 'Yue Zhao', 'Yi Ma'], 'affiliations': ['The University of Hong Kong', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.04209.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#long_context', '#alignment', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Фиксированный языковой энкодер улучшает визуальное обучение', 'desc': 'Исследование LIFT предлагает новый подход к обучению визуальных представлений с использованием предобученных языковых моделей (LLM) в качестве фиксированного текстового энкодера. Этот метод превосходит совместное обучение текстовых и визуальных энкодеров, как в CLIP, особенно в задачах композиционного понимания и работы с длинными подписями. LIFT демонстрирует высокую эффективность и вычислительную эффективность, обучая только визуальный энкодер. Результаты исследования открывают новые перспективы использования текстовых эмбеддингов из LLM для улучшения визуального обучения.'}, 'en': {'title': 'LIFT: Efficient Language-Image Alignment with Fixed Text Encoders', 'desc': 'This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment.'}, 'zh': {'title': '简化训练，提升视觉理解的LIFT方法', 'desc': '本文提出了一种新的方法，称为LIFT（使用固定文本编码器的语言-图像对齐），旨在通过预训练的大型语言模型来指导视觉表示学习。与传统的联合训练方法（如CLIP）相比，LIFT只训练图像编码器，而使用固定的文本编码器，从而简化了训练过程。研究表明，LIFT在处理组合理解和长文本描述时，表现优于CLIP，并且在计算效率上也有显著提升。该研究为如何利用大型语言模型的文本嵌入来指导视觉学习提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2506.01011', 'title': 'Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack', 'url': 'https://huggingface.co/papers/2506.01011', 'abstract': 'A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.', 'score': 8, 'issue_id': 4158, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '5475ba18f16db6f7', 'authors': ['Siqi Hui', 'Yiren Song', 'Sanping Zhou', 'Ye Deng', 'Wenli Huang', 'Jinjun Wang'], 'affiliations': ['National University of Singapore', 'Ningbo University of Technology', 'Southwestern University of Finance and Economics', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01011.jpg', 'data': {'categories': ['#cv', '#security', '#video'], 'emoji': '🔐', 'ru': {'title': 'Защита авторегрессионных моделей генерации изображений с помощью лексических водяных знаков', 'desc': "Статья представляет новую технику водяных знаков для авторегрессионных моделей генерации изображений - Lexical Bias Watermarking (LBW). LBW встраивает водяные знаки непосредственно в карты токенов, смещая выбор токенов в сторону предопределенного 'зеленого списка' во время генерации. Этот подход обеспечивает бесшовную интеграцию с существующими авторегрессионными моделями и естественно расширяется до постфактумной вставки водяных знаков. Эксперименты показывают, что LBW достигает превосходной устойчивости водяных знаков, особенно в противостоянии атакам регенерации."}, 'en': {'title': 'Secure Your Images with Lexical Bias Watermarking!', 'desc': 'The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.'}, 'zh': {'title': '增强自回归模型安全性的水印技术', 'desc': '本文提出了一种新颖的水印技术，称为词汇偏置水印（Lexical Bias Watermarking），旨在增强自回归图像生成模型的安全性。该方法通过在生成过程中偏向选择预定义的绿色列表，将水印嵌入到令牌选择中，从而有效抵御再生攻击。与现有的扩散模型水印技术不同，LBW能够直接与自回归模型集成，并支持后期水印处理。实验结果表明，LBW在抵抗再生攻击方面表现出色，显著提高了水印的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.05348', 'title': 'FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction', 'url': 'https://huggingface.co/papers/2506.05348', 'abstract': 'A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin.', 'score': 5, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '41deb088324d4fce', 'authors': ['Yifan Wang', 'Peishan Yang', 'Zhen Xu', 'Jiaming Sun', 'Zhanhua Zhang', 'Yong Chen', 'Hujun Bao', 'Sida Peng', 'Xiaowei Zhou'], 'affiliations': ['Geely Automobile Research Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05348.jpg', 'data': {'categories': ['#3d'], 'emoji': '🔄', 'ru': {'title': 'Свободное время и пространство для гауссовых примитивов в динамических 3D-сценах', 'desc': 'FreeTimeGS - это новый метод 4D-представления для моделирования динамических 3D-сцен. Он позволяет гауссовым примитивам появляться в произвольное время и в произвольных местах, что улучшает качество рендеринга по сравнению с существующими методами. FreeTimeGS преодолевает ограничения предыдущих подходов, использующих деформационные поля для отображения канонических примитивов. Метод также включает функцию движения для каждого примитива, что уменьшает временную избыточность.'}, 'en': {'title': 'Revolutionizing Dynamic 3D Scene Modeling with FreeTimeGS', 'desc': 'This paper introduces FreeTimeGS, a new 4D representation that enhances the modeling of dynamic 3D scenes. By allowing Gaussian primitives to appear at any time and location, it provides greater flexibility compared to traditional methods that rely on fixed canonical spaces. The approach includes motion functions for each Gaussian primitive, enabling them to transition smoothly over time and reducing redundancy in the scene representation. Experimental results demonstrate that FreeTimeGS significantly improves rendering quality, outperforming existing techniques in handling complex motions.'}, 'zh': {'title': '动态3D场景建模的新突破：FreeTimeGS', 'desc': '本文提出了一种新颖的4D表示方法FreeTimeGS，旨在增强动态3D场景的建模能力。与传统的3D高斯原语不同，FreeTimeGS允许高斯原语在任意时间和位置出现，从而提高了渲染质量。该方法通过为每个高斯原语赋予运动函数，使其能够随时间移动到相邻区域，减少了时间冗余。实验结果表明，FreeTimeGS在多个数据集上的渲染质量显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.00830', 'title': 'SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.00830', 'abstract': 'SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.', 'score': 5, 'issue_id': 4157, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7a04cf6593bac4b3', 'authors': ['Zhengcong Fei', 'Hao Jiang', 'Di Qiu', 'Baoxuan Gu', 'Youqiang Zhang', 'Jiahua Wang', 'Jialin Bai', 'Debang Li', 'Mingyuan Fan', 'Guibin Chen', 'Yahui Zhou'], 'affiliations': ['Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00830.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#audio', '#diffusion', '#synthetic', '#video'], 'emoji': '🎭', 'ru': {'title': 'Революция в синтезе говорящих портретов: от аудио к реалистичному видео', 'desc': 'SkyReels-Audio - это унифицированная система для создания высококачественных видео с говорящими портретами на основе аудио. Она использует предобученные видео-диффузионные трансформеры и поддерживает генерацию видео бесконечной длины с разнообразными условиями. В системе применяется гибридная стратегия куррикулярного обучения для постепенного согласования аудио и движений лица. Для улучшения локальной согласованности лица вводятся специальные функции потерь и механизм аудио-управляемого безклассового наведения.'}, 'en': {'title': 'Revolutionizing Talking Portraits with SkyReels-Audio', 'desc': 'SkyReels-Audio is a novel framework that generates high-quality talking portrait videos by using pretrained video diffusion transformers. It allows for the creation and editing of videos based on audio, text, and images, making it versatile for various multimodal inputs. The framework employs a hybrid curriculum learning strategy to ensure that the audio aligns well with facial movements, enhancing the control over video sequences. Additionally, it introduces advanced loss mechanisms to improve facial coherence and uses a sliding-window denoising technique to maintain visual quality and consistency over time.'}, 'zh': {'title': 'SkyReels-Audio：音频驱动的高保真说话肖像生成', 'desc': 'SkyReels-Audio 是一个统一框架，利用预训练的视频扩散变换器生成高保真且连贯的音频条件下的说话肖像视频。该框架支持无限长度的生成和编辑，并通过多模态输入实现多样化和可控的条件设置。我们采用混合课程学习策略，逐步对齐音频与面部运动，从而实现对长视频序列的精细控制。通过引入面部掩膜损失和音频引导的无分类器指导机制，SkyReels-Audio 在复杂条件下展现出卓越的唇同步精度和身份一致性。'}}}, {'id': 'https://huggingface.co/papers/2505.20914', 'title': 'Geometry-Editable and Appearance-Preserving Object Compositon', 'url': 'https://huggingface.co/papers/2505.20914', 'abstract': 'General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.', 'score': 5, 'issue_id': 4156, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'f95a4c2b427959d4', 'authors': ['Jianman Lin', 'Haojie Li', 'Chunmei Qing', 'Zhijing Yang', 'Liang Lin', 'Tianshui Chen'], 'affiliations': ['Guangdong University of Technology', 'South China University of Technology', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20914.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Точное редактирование геометрии и сохранение деталей в композиции объектов', 'desc': 'Статья представляет новую модель DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) для композиции объектов в сцене. DGAD использует семантические эмбеддинги для управления геометрией объекта и механизм кросс-внимания для сохранения деталей внешнего вида. Модель интегрирует семантические эмбеддинги в предобученные диффузионные модели для гибкого манипулирования геометрией объекта. DGAD применяет плотный механизм кросс-внимания для извлечения и пространственного выравнивания признаков внешнего вида с соответствующими регионами.'}, 'en': {'title': 'Seamless Object Integration with Geometry and Appearance Preservation', 'desc': "The paper presents a new model called Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) for integrating objects into backgrounds while maintaining their detailed appearance. DGAD uses semantic embeddings to understand the desired geometric changes and a cross-attention mechanism to align fine-grained appearance features with these changes. This approach allows for precise editing of object geometry without losing the intricate details of the object's appearance. The model builds on existing diffusion techniques and shows improved performance in object composition tasks through extensive experiments."}, 'zh': {'title': '解耦几何与外观保留的物体组合新方法', 'desc': '一般物体组合（GOC）旨在将目标物体无缝地融入背景场景中，同时保持其细致的外观细节。现有方法通过语义嵌入与先进的扩散模型结合，实现几何可编辑的生成。然而，这些紧凑的嵌入仅编码高层语义信息，难以保留细致的外观细节。我们提出了一种解耦几何可编辑和外观保留的扩散模型（DGAD），通过语义嵌入捕捉几何变换，并利用交叉注意力机制对齐外观特征，从而实现精确的几何编辑和真实的外观保留。'}}}, {'id': 'https://huggingface.co/papers/2506.05333', 'title': 'Kinetics: Rethinking Test-Time Scaling Laws', 'url': 'https://huggingface.co/papers/2506.05333', 'abstract': 'Inference with small models is less efficient due to memory bottlenecks, leading to a new Kinetics Scaling Law emphasizing sparse attention for better test-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.', 'score': 4, 'issue_id': 4172, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '8462c5d73273bc69', 'authors': ['Ranajoy Sadhukhan', 'Zhuoming Chen', 'Haizhong Zheng', 'Yang Zhou', 'Emma Strubell', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05333.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': '🔬', 'ru': {'title': 'Разреженное внимание - ключ к эффективному масштабированию моделей', 'desc': 'Статья представляет новый закон масштабирования Kinetics, который учитывает как вычислительные затраты, так и затраты на доступ к памяти при инференсе моделей машинного обучения. Исследование показывает, что эффективность маленьких моделей часто переоценивается из-за узких мест в памяти. Авторы предлагают новую парадигму масштабирования, основанную на разреженном внимании (sparse attention), что позволяет снизить затраты на токен и генерировать более длинные последовательности. Эмпирические результаты демонстрируют преимущество моделей с разреженным вниманием над плотными аналогами в различных режимах затрат.'}, 'en': {'title': 'Unlocking Efficiency: Sparse Attention for Better Model Performance', 'desc': "This paper introduces the Kinetics Scaling Law, which highlights the importance of sparse attention in improving the efficiency of smaller machine learning models during inference. It argues that previous assessments of smaller models' effectiveness have underestimated the impact of memory access bottlenecks that arise during test-time strategies. The authors demonstrate that larger models, particularly those utilizing sparse attention, can significantly enhance performance while managing resource allocation more effectively. Their empirical results show that sparse attention models outperform dense models, leading to substantial gains in accuracy across various cost regimes."}, 'zh': {'title': '稀疏注意力：提升小模型推理效率的关键', 'desc': '本文探讨了小模型在推理时的效率问题，指出由于内存瓶颈，导致其性能被高估。我们提出了一种新的Kinetics Scaling Law，强调稀疏注意力机制在测试时的优势。通过对不同参数规模模型的分析，发现测试时的计算效率在超过一定阈值的模型上更为显著。实验结果表明，稀疏注意力模型在低成本和高成本环境下均优于密集模型，提升了问题解决的准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.04598', 'title': 'Scaling Laws for Robust Comparison of Open Foundation Language-Vision\n  Models and Datasets', 'url': 'https://huggingface.co/papers/2506.04598', 'abstract': "Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves 80.3% zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.", 'score': 4, 'issue_id': 4165, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '3add105c0b3c0782', 'authors': ['Marianna Nezhurina', 'Tomer Porian', 'Giovanni Pucceti', 'Tommie Kerssies', 'Romain Beaumont', 'Mehdi Cherti', 'Jenia Jitsev'], 'affiliations': ['Eindhoven University of Technology', 'Institute of Information Science and Technologies A. Faedo - CNR Pisa', 'Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)', 'LAION', 'Open-Ψ (Open-Sci) Collective'], 'pdf_title_img': 'assets/pdf/title_img/2506.04598.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#training', '#dataset', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Масштабируемые законы как ключ к сравнению мультимодальных моделей', 'desc': 'В статье представлены масштабируемые законы для моделей CLIP и MaMMUT, позволяющие сравнить их производительность и эффективность использования данных при различных масштабах и наборах данных. Исследователи показывают, как вывод законов масштабирования может использоваться для сравнения моделей и датасетов, помогая выбрать оптимальную процедуру предобучения. Результаты демонстрируют, что MaMMUT показывает более сильное улучшение с увеличением масштаба и лучшую эффективность использования данных по сравнению со стандартным CLIP. Авторы также предоставляют законы масштабирования для различных задач и наборов данных, подтверждая наблюдаемые тенденции.'}, 'en': {'title': 'Unlocking Model Potential: Scaling Laws for Better Comparisons', 'desc': "This paper explores scaling laws for two language-vision models, CLIP and MaMMUT, to evaluate their performance and efficiency as they are trained on larger datasets. By deriving these scaling laws, the authors provide a framework for comparing different pre-training methods, highlighting MaMMUT's superior performance and sample efficiency over CLIP. The study includes various downstream tasks and datasets, ensuring that the observed trends are consistent across different scenarios. The findings aim to guide future improvements in foundation models and datasets by offering a systematic approach to model comparison."}, 'zh': {'title': '模型与数据集比较的新方法', 'desc': '本文研究了CLIP和MaMMUT模型的缩放规律，以比较它们在不同规模和数据集上的性能和样本效率。通过对这两种重要的语言-视觉学习方法进行全面的缩放规律推导，揭示了MaMMUT在规模扩大时的性能提升和样本效率优于标准CLIP。我们还展示了在不同下游任务和开放数据集上，缩放规律的一致性趋势，确保了比较的有效性。最终，我们发布了所有预训练模型及其中间检查点，以支持后续研究和实验。'}}}, {'id': 'https://huggingface.co/papers/2506.04405', 'title': 'MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale', 'url': 'https://huggingface.co/papers/2506.04405', 'abstract': 'We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '2cf822e179634776', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Yishan Zhong', 'Yue Yu', 'Xiangru Tang', 'Hang Wu', 'May D. Wang', 'Peifeng Ruan', 'Donghan Yang', 'Tao Wang', 'Guanghua Xiao', 'Carl Yang', 'Yang Xie', 'Wenqi Shi'], 'affiliations': ['Emory University', 'Georgia Tech', 'UT Southwestern Medical Center', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04405.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#open_source', '#training', '#rl'], 'emoji': '🩺', 'ru': {'title': 'MedAgentGYM: Революция в обучении ИИ для медицинского кодирования', 'desc': 'MedAgentGYM - это новая среда обучения для улучшения навыков медицинского рассуждения у агентов на основе больших языковых моделей (LLM). Она включает более 72 тысяч задач из 129 категорий, основанных на реальных биомедицинских сценариях. Задачи представлены в виде исполняемых кодовых сред с подробными описаниями, интерактивной обратной связью и верифицируемыми аннотациями. Используя MedAgentGYM, модель Med-Copilot-7B достигла значительного улучшения производительности через тонкую настройку и обучение с подкреплением.'}, 'en': {'title': 'Empowering Medical Reasoning in LLMs with MedAgentGYM', 'desc': 'MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o.'}, 'zh': {'title': 'MedAgentGYM：提升医学推理能力的创新平台', 'desc': '我们介绍了MedAgentGYM，这是第一个公开可用的训练环境，旨在增强大型语言模型（LLM）代理的基于编码的医学推理能力。MedAgentGYM包含72,413个任务实例，涵盖129个类别，来源于真实的生物医学场景。每个任务都在可执行的编码环境中封装，提供详细的任务描述、互动反馈机制、可验证的真实注释和可扩展的训练轨迹生成。通过对30多种LLM的广泛基准测试，发现商业API模型与开源模型之间存在显著的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2506.04245', 'title': 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04245', 'abstract': 'As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8716d3ca53b1d58f', 'authors': ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim'], 'affiliations': ['Microsoft', 'National University of Singapore', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04245.jpg', 'data': {'categories': ['#synthetic', '#agents', '#benchmark', '#reasoning', '#dataset', '#transfer_learning', '#rl', '#leakage'], 'emoji': '🔐', 'ru': {'title': 'Разумное раскрытие информации: обучение ИИ-агентов контекстной целостности', 'desc': 'Статья посвящена проблеме контекстной целостности (CI) в эпоху автономных агентов, принимающих решения за пользователей. Авторы предлагают метод, использующий языковые модели (LLM) и обучение с подкреплением (RL) для обучения агентов рассуждать о контексте и принимать решения о раскрытии информации. Эксперименты на синтетическом наборе данных показали значительное снижение неуместного раскрытия информации при сохранении производительности задач. Улучшения переносятся на реальные бенчмарки CI, такие как PrivacyLens.'}, 'en': {'title': 'Enhancing Contextual Integrity in Autonomous Agents', 'desc': 'This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks.'}, 'zh': {'title': '确保上下文完整性，提升自主代理决策能力', 'desc': '在自主代理为用户做决策的时代，确保上下文完整性（CI）成为一个重要问题。本文提出，CI需要代理在执行任务时对其操作的上下文进行推理。我们首先让大型语言模型（LLMs）明确推理CI，以决定披露哪些信息。接着，我们开发了一个强化学习（RL）框架，进一步增强模型进行CI所需的推理能力，实验结果表明，该方法显著减少了不当信息披露，同时保持了任务性能。'}}}, {'id': 'https://huggingface.co/papers/2506.05282', 'title': 'Rectified Point Flow: Generic Point Cloud Pose Estimation', 'url': 'https://huggingface.co/papers/2506.05282', 'abstract': 'We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.', 'score': 3, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c7d3c7ca688358d9', 'authors': ['Tao Sun', 'Liyuan Zhu', 'Shengyu Huang', 'Shuran Song', 'Iro Armeni'], 'affiliations': ['NVIDIA Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05282.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Единый подход к регистрации облаков точек и сборке форм', 'desc': 'Представлен метод Rectified Point Flow, который объединяет регистрацию облаков точек и сборку многокомпонентных форм в единую условную генеративную задачу. Метод обучает непрерывное поточечное поле скоростей, которое перемещает зашумленные точки к целевым позициям. В отличие от предыдущих подходов, данный метод изначально учитывает симметрии при сборке без явной разметки. Вместе с самоконтролируемым энкодером, фокусирующимся на перекрывающихся точках, метод достигает нового уровня производительности на шести эталонных наборах данных.'}, 'en': {'title': 'Unified Learning for Point Cloud Registration and Shape Assembly', 'desc': 'This paper presents Rectified Point Flow, a novel approach that combines point cloud registration and multi-part shape assembly into a single generative framework. The method learns a continuous velocity field that aligns noisy point clouds to their target configurations, allowing for the recovery of part poses. Unlike previous methods that require manual symmetry handling, this approach automatically learns assembly symmetries without needing explicit labels. By utilizing a self-supervised encoder for overlapping points, the method achieves state-of-the-art results across multiple benchmarks, enhancing accuracy through joint training on varied datasets.'}, 'zh': {'title': '统一点云配准与形状组装的创新方法', 'desc': '我们提出了修正点流（Rectified Point Flow），这是一种统一的参数化方法，将成对点云配准和多部件形状组装视为一个单一的条件生成问题。该方法在没有姿态信息的情况下，学习一个连续的点位速度场，将噪声点移动到目标位置，并从中恢复部件姿态。与之前的工作不同，我们的方法能够在没有对称标签的情况下，自然地学习组装对称性。通过专注于重叠点的自监督编码器，我们的方法在六个基准测试中实现了新的最先进性能，促进了共享几何先验的学习，从而提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.05278', 'title': 'Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning', 'url': 'https://huggingface.co/papers/2506.05278', 'abstract': 'A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.', 'score': 3, 'issue_id': 4158, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '119fed47e7d43a96', 'authors': ['Nan Huo', 'Jinyang Li', 'Bowen Qin', 'Ge Qu', 'Xiaolong Li', 'Xiaodong Li', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05278.jpg', 'data': {'categories': ['#interpretability', '#rag', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Micro-Act: умное разрешение конфликтов знаний в RAG-системах', 'desc': 'Фреймворк Micro-Act решает проблему конфликтов знаний в системах генерации с дополнением из источников (RAG). Он адаптивно разбивает источники знаний на мелкие сравнения, представленные как последовательность действий. Это позволяет рассуждать за пределами поверхностного контекста. Эксперименты показали значительное повышение точности ответов на вопросы по сравнению с существующими методами.'}, 'en': {'title': 'Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy', 'desc': 'The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.'}, 'zh': {'title': 'Micro-Act：解决知识冲突的智能框架', 'desc': 'Micro-Act是一个新框架，旨在解决检索增强生成（RAG）中的知识冲突问题。它通过自适应地分解知识源，改善了问答（QA）的准确性。与现有方法不同，Micro-Act采用分层的行动空间，能够自动感知上下文的复杂性，并将知识源细分为一系列精细的比较步骤。这种方法在五个基准数据集上的实验中显示出显著的QA准确性提升，尤其在时间和语义类型的冲突中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.04956', 'title': 'FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.04956', 'abstract': 'Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT.', 'score': 3, 'issue_id': 4166, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e1a303f65a6d378c', 'authors': ['Huihan Wang', 'Zhiwen Yang', 'Hui Zhang', 'Dan Zhao', 'Bingzheng Wei', 'Yan Xu'], 'affiliations': ['ByteDance Inc., Beijing 100098, China', 'Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China', 'Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China', 'School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04956.jpg', 'data': {'categories': ['#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🏥', 'ru': {'title': 'FEAT: Эффективный трансформер для синтеза медицинских видео', 'desc': 'В статье представлен FEAT - новый трансформер для синтеза динамических медицинских видео. FEAT использует последовательные механизмы внимания для пространственных, временных и канальных измерений, что позволяет захватывать глобальные зависимости. Архитектура имеет линейную сложность вычислений и включает модуль остаточного управления значениями для точной адаптации к разным уровням шума. FEAT превосходит существующие методы по эффективности и масштабируемости на стандартных наборах данных.'}, 'en': {'title': 'FEAT: Revolutionizing Medical Video Synthesis with Efficient Attention', 'desc': "This paper introduces FEAT, a novel Transformer model designed to create high-quality dynamic medical videos by effectively managing spatial and temporal information. It overcomes limitations of existing models by implementing a unified attention mechanism that captures dependencies across spatial, temporal, and channel dimensions. FEAT also features a linear-complexity attention design, which reduces computational demands while maintaining performance. Additionally, a residual value guidance module enhances the model's ability to adapt to varying noise levels, leading to superior results on benchmark tasks with fewer parameters than previous state-of-the-art models."}, 'zh': {'title': '高效动态医疗视频合成的新突破', 'desc': '本论文提出了一种新的全维高效注意力变换器（FEAT），旨在解决动态医疗视频合成中的空间一致性和时间动态建模问题。FEAT通过三项创新来克服现有方法的局限性，包括统一的时空通道注意力机制、线性复杂度的注意力设计以及残差值引导模块，以适应不同的噪声水平。实验结果表明，FEAT-S在参数量仅为最先进模型Endora的23%的情况下，仍能实现相当或更优的性能。FEAT-L在多个数据集上超越了所有对比方法，展示了其卓越的有效性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.03643', 'title': 'Images are Worth Variable Length of Representations', 'url': 'https://huggingface.co/papers/2506.03643', 'abstract': 'Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a simple image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a dynamic vision encoder that produces a variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder.github.io/dove-encoder.', 'score': 3, 'issue_id': 4165, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '0581c037604119ee', 'authors': ['Lingjun Mao', 'Rodolfo Corona', 'Xin Liang', 'Wenhao Yan', 'Zineng Tang'], 'affiliations': ['University of California, Berkeley', 'University of California, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.03643.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#multimodal'], 'emoji': '🦅', 'ru': {'title': 'Адаптивное кодирование изображений: больше информации, меньше токенов', 'desc': 'DOVE - это динамический кодировщик изображений, который создает переменное количество визуальных токенов для реконструкции каждого изображения. В отличие от существующих энкодеров с фиксированной длиной последовательности, DOVE адаптируется к сложности изображения, используя больше токенов для визуально сложных сцен и меньше для простых. Результаты показывают, что DOVE значительно сокращает среднее количество токенов, сохраняя высокое качество реконструкции. Модель превосходит существующие методы токенизации на основе автоэнкодеров в задачах линейного пробинга и мультимодальных задачах, используя гораздо меньше токенов.'}, 'en': {'title': 'Dynamic Tokenization for Enhanced Image Understanding', 'desc': 'This paper introduces DOVE, a dynamic vision encoder that adapts the number of visual tokens based on the complexity of the image being processed. Unlike traditional methods that use a fixed number of tokens, DOVE generates a variable number of tokens, allowing it to capture more information from visually complex images. The results demonstrate that DOVE not only reduces the average number of tokens needed but also maintains high-quality image reconstruction. Additionally, DOVE incorporates query-conditioned tokenization to enhance semantic extraction by focusing on relevant image regions, outperforming existing autoencoder-based methods in various tasks.'}, 'zh': {'title': '动态视觉编码，提升信息提取效率', 'desc': '现有的视觉编码器通常将图像映射为固定长度的标记序列，但不同图像的信息量不同。我们提出了DOVE，一个动态视觉编码器，可以生成可变数量的视觉标记，以重建每个图像。DOVE在保持高重建质量的同时，显著减少了平均标记数量，并在多个任务中超越了现有的基于自编码器的标记化方法。通过查询条件标记化，DOVE能够更有效地提取与查询相关的语义特征。'}}}, {'id': 'https://huggingface.co/papers/2506.02751', 'title': 'RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS', 'url': 'https://huggingface.co/papers/2506.02751', 'abstract': 'RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.', 'score': 3, 'issue_id': 4161, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9e5cefa9df6461fd', 'authors': ['Chuanyu Fu', 'Yuqi Zhang', 'Kunbin Yao', 'Guanying Chen', 'Yuan Xiong', 'Chuan Huang', 'Shuguang Cui', 'Xiaochun Cao'], 'affiliations': ['FNii-Shenzhen', 'SSE, CUHKSZ', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02751.jpg', 'data': {'categories': ['#3d'], 'emoji': '🌟', 'ru': {'title': 'Устранение артефактов в 3D-сценах с помощью умного роста гауссианов', 'desc': 'RobustSplat - это новый метод в области 3D-моделирования, направленный на устранение артефактов, вызванных временными объектами в сценах. Он использует стратегию отложенного роста гауссианов, что позволяет сначала оптимизировать статическую структуру сцены. Кроме того, RobustSplat применяет каскадный подход к созданию маски временных объектов, начиная с низкого разрешения и постепенно увеличивая его. Эксперименты показали, что этот метод превосходит существующие решения в задаче робастного 3D-рендеринга.'}, 'en': {'title': 'Enhancing 3D Gaussian Splatting with Robust Techniques', 'desc': 'RobustSplat is a novel approach designed to improve 3D Gaussian Splatting (3DGS) by addressing artifacts caused by transient objects in rendered images. The method introduces a delayed Gaussian growth strategy that focuses on optimizing the static elements of a scene before dealing with transient disturbances, reducing the risk of overfitting. Additionally, it employs a scale-cascaded mask bootstrapping technique that starts with lower-resolution features for initial mask estimation, ensuring better semantic consistency before refining to high-resolution predictions. Through extensive testing, RobustSplat demonstrates superior performance compared to existing methods, showcasing its effectiveness in producing high-quality, artifact-free renderings.'}, 'zh': {'title': '增强3D渲染的鲁棒性', 'desc': 'RobustSplat 是一种针对 3D 高斯点云渲染中因瞬态物体引起的伪影问题的解决方案。该方法通过延迟高斯生长和尺度级联掩码自举来优化静态场景结构，减少对瞬态物体的过拟合。首先，延迟高斯生长策略确保在允许高斯分裂之前，先优化静态场景。其次，尺度级联掩码自举方法利用低分辨率特征相似性进行初步掩码估计，随后再进行高分辨率监督，以提高掩码预测的精确度。'}}}, {'id': 'https://huggingface.co/papers/2506.05313', 'title': 'MARBLE: Material Recomposition and Blending in CLIP-Space', 'url': 'https://huggingface.co/papers/2506.05313', 'abstract': 'MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/', 'score': 2, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e941d4fe8c8db7a2', 'authors': ['Ta-Ying Cheng', 'Prafull Sharma', 'Mark Boss', 'Varun Jampani'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.05313.jpg', 'data': {'categories': ['#cv', '#games', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Умное редактирование материалов в изображениях с помощью ИИ', 'desc': 'MARBLE - это метод для смешивания материалов и изменения их свойств в изображениях с использованием встраиваний в пространстве CLIP. Он позволяет контролировать предобученные модели text-to-image для редактирования материалов объектов на основе примеров. MARBLE находит блок в U-Net, отвечающий за атрибуцию материалов, и использует неглубокую нейронную сеть для предсказания направления изменения атрибутов. Метод обеспечивает параметрический контроль над такими свойствами материалов как шероховатость, металличность, прозрачность и свечение.'}, 'en': {'title': 'Blend and Recompose Materials with MARBLE!', 'desc': 'MARBLE is a novel method that enhances material editing in images by utilizing material embeddings in CLIP-space. It allows for the blending and recomposing of material properties in images through pre-trained text-to-image models. By identifying specific blocks in the denoising UNet that handle material attributes, MARBLE can manipulate fine-grained properties like roughness and transparency. The method also supports multiple edits in one pass, showcasing its efficiency and versatility in applications such as digital painting.'}, 'zh': {'title': 'MARBLE：智能材料编辑的新方法', 'desc': 'MARBLE是一种利用CLIP空间中的材料嵌入来控制预训练文本到图像模型的方法。它可以实现图像中材料属性的混合和重组，并对细粒度材料属性进行参数化控制。通过在去噪UNet中找到与材料归属相关的块，MARBLE改进了基于示例的材料编辑。该方法能够在一次前向传递中进行多次编辑，并适用于绘画。'}}}, {'id': 'https://huggingface.co/papers/2506.05046', 'title': 'FlowDirector: Training-Free Flow Steering for Precise Text-to-Video\n  Editing', 'url': 'https://huggingface.co/papers/2506.05046', 'abstract': 'FlowDirector, an inversion-free video editing framework, uses ODEs for spatiotemporal coherent editing and attention-guided masking for localized control, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, a novel inversion-free video editing framework. Our framework models the editing process as a direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present a guidance-enhanced editing strategy inspired by Classifier-Free Guidance, which leverages differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing a new paradigm for efficient and coherent video editing without inversion.', 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'a9c76743a2f7e8d6', 'authors': ['Guangzhao Li', 'Yanming Yang', 'Chenxi Song', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Central South University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05046.jpg', 'data': {'categories': ['#games', '#video', '#multimodal', '#benchmark', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Плавное и точное редактирование видео с помощью ОДУ и масок внимания', 'desc': 'FlowDirector - это новая структура для редактирования видео без инверсии. Она использует обыкновенные дифференциальные уравнения (ODE) для моделирования процесса редактирования как эволюции в пространстве данных, сохраняя временную согласованность и структурные детали. Механизм маскирования на основе внимания позволяет осуществлять локализованное и контролируемое редактирование. Стратегия редактирования с улучшенным наведением повышает семантическое соответствие инструкциям по редактированию.'}, 'en': {'title': 'FlowDirector: Revolutionizing Video Editing with ODEs and Attention', 'desc': "FlowDirector is a new video editing framework that avoids the common inversion-based techniques, which can cause problems like temporal inconsistencies. Instead, it uses Ordinary Differential Equations (ODEs) to guide video edits directly in the data space, ensuring smooth transitions while maintaining the video's structure and timing. The framework also incorporates an attention-guided masking system that allows for precise control over which parts of the video are edited, preserving the areas that should remain unchanged. Additionally, it employs a guidance-enhanced strategy to improve the alignment of edits with user instructions, achieving top performance in video editing tasks."}, 'zh': {'title': '无反演视频编辑的新范式', 'desc': 'FlowDirector是一种无反演的视频编辑框架，利用常微分方程（ODE）进行时空一致性编辑。该框架通过直接在数据空间中建模编辑过程，确保视频在其固有的时空流形上平滑过渡，从而保持时间一致性和结构细节。为了实现局部可控的编辑，FlowDirector引入了基于注意力的掩膜机制，调节ODE速度场，保护非目标区域的时空特性。此外，采用增强指导的编辑策略，进一步提高了与编辑指令的语义对齐，确保了编辑的完整性和结构一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.04559', 'title': 'Perceptual Decoupling for Scalable Multi-modal Reasoning via\n  Reward-Optimized Captioning', 'url': 'https://huggingface.co/papers/2506.04559', 'abstract': "A reasoning-aligned reinforcement learning strategy enhances visual representations in multi-modal large language models by optimizing captions for downstream reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment.", 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'cc35ebabca1ba808', 'authors': ['Yunhao Gou', 'Kai Chen', 'Zhili Liu', 'Lanqing Hong', 'Xin Jin', 'Zhenguo Li', 'James T. Kwok', 'Yu Zhang'], 'affiliations': ['Huawei Cloud', 'Huawei Noahs Ark Lab', 'Southern University of Science and Technology', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04559.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#multimodal', '#rl', '#rag', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Улучшение визуального восприятия ИИ через оптимизацию для задач рассуждения', 'desc': 'Статья представляет метод RACRO для улучшения мультимодальных больших языковых моделей. RACRO использует обучение с подкреплением для оптимизации генерации описаний изображений, ориентированных на последующие задачи рассуждения. Этот подход позволяет разделить восприятие и рассуждение, сохраняя при этом информативность визуальных представлений. Эксперименты показывают, что RACRO достигает наилучших результатов на мультимодальных тестах по математике и естественным наукам.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with RACRO', 'desc': "This paper introduces a new reinforcement learning strategy called Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) to improve how visual information is represented in multi-modal large language models (MLLMs). The approach focuses on generating captions from visual inputs that are both accurate and informative, which helps in enhancing reasoning tasks. By using a reward-based optimization method, RACRO aligns the visual extractor's output with the reasoning goals of the model. The results show that RACRO achieves top performance on various benchmarks while allowing for easier integration with advanced reasoning models without the need for expensive retraining."}, 'zh': {'title': '推理驱动的视觉表示优化策略', 'desc': '本文提出了一种名为RACRO的强化学习策略，旨在通过优化图像描述来增强多模态大语言模型的视觉表示。该方法通过将感知与推理解耦，将视觉输入转换为语言表示，并利用强大的文本推理模型进行处理。RACRO通过奖励优化，使得图像描述既忠实于图像，又能支持准确的推理任务，从而提升视觉对齐能力。实验结果表明，RACRO在多模态数学和科学基准测试中表现优异，且具有良好的可扩展性和适应性。'}}}, {'id': 'https://huggingface.co/papers/2506.02587', 'title': "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations", 'url': 'https://huggingface.co/papers/2506.02587', 'abstract': "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib.", 'score': 2, 'issue_id': 4160, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '8d1e0e49dea5dcad', 'authors': ['Weiduo Yuan', 'Jerry Li', 'Justin Yue', 'Divyank Shah', 'Konstantinos Karydis', 'Hang Qiu'], 'affiliations': ['University of California, Riverside', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.02587.jpg', 'data': {'categories': ['#robotics', '#cv', '#dataset', '#optimization', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'Революция в калибровке LiDAR-камеры с помощью вида сверху', 'desc': 'BEVCALIB - это модель, использующая функции вида сверху для точной калибровки LiDAR-камеры по необработанным данным. Модель извлекает и объединяет функции вида сверху как для камеры, так и для LiDAR в общее пространство признаков. BEVCALIB демонстрирует превосходную производительность в различных условиях шума по сравнению с существующими методами. Модель устанавливает новый стандарт в области калибровки LiDAR-камеры, значительно превосходя базовые показатели на наборах данных KITTI и NuScenes.'}, 'en': {'title': 'Revolutionizing LiDAR-Camera Calibration with BEV Features', 'desc': "The BEVCALIB model introduces a novel approach for calibrating LiDAR and camera systems using bird's-eye view (BEV) features extracted from raw data. This method addresses the limitations of traditional calibration techniques that struggle with dynamic transformations during vehicle or robot movement. By fusing separate BEV features from both LiDAR and camera into a shared feature space, BEVCALIB enhances the accuracy of multi-modal perception in autonomous systems. The model demonstrates significant performance improvements under various noise conditions, setting a new benchmark in the field with extensive evaluations on multiple datasets."}, 'zh': {'title': 'BEVCALIB：鸟瞰视图特征助力激光雷达与相机精确标定', 'desc': 'BEVCALIB模型利用鸟瞰视图特征进行激光雷达与相机的精确标定，能够从原始数据中提取信息。与传统方法相比，BEVCALIB在各种噪声条件下表现出色，特别是在自动驾驶和机器人系统中融合多模态感知时至关重要。该模型通过分别提取相机和激光雷达的鸟瞰视图特征，并将其融合到共享的特征空间中，显著提高了标定的准确性。通过引入新颖的特征选择器，BEVCALIB在减少内存消耗的同时，实现了高效的训练和更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23115', 'title': 'Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2505.23115', 'abstract': 'Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.', 'score': 2, 'issue_id': 4162, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '9780859f709be640', 'authors': ['Yunshen Wang', 'Yicheng Liu', 'Tianyuan Yuan', 'Yucheng Mao', 'Yingshi Liang', 'Xiuyu Yang', 'Honggang Zhang', 'Hang Zhao'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute for Interdisciplinary Information Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23115.jpg', 'data': {'categories': ['#diffusion', '#agents', '#3d', '#cv'], 'emoji': '🚗', 'ru': {'title': 'Диффузионные модели улучшают 3D-восприятие беспилотных автомобилей', 'desc': 'В этой работе авторы предлагают использовать генеративные диффузионные модели для предсказания трехмерных карт занятости в задаче автономного вождения. В отличие от дискриминативных методов, такой подход позволяет лучше справляться с шумными данными и неполными наблюдениями. Эксперименты показывают, что генеративные модели превосходят современные дискриминативные подходы, особенно в областях с плохой видимостью. Улучшенные предсказания значительно повышают качество планирования маршрута для беспилотных автомобилей.'}, 'en': {'title': 'Revolutionizing 3D Occupancy Prediction with Diffusion Models', 'desc': 'This paper addresses the challenge of predicting 3D occupancy grids from visual inputs for autonomous driving, particularly in the presence of noisy data and incomplete observations. The authors propose a novel approach by reframing the problem as a generative modeling task using diffusion models, which learn the data distribution and incorporate 3D scene priors. This method improves prediction consistency and robustness against noise, effectively managing the complexities of 3D spatial structures. Experimental results demonstrate that their diffusion-based models outperform traditional discriminative methods, leading to more accurate occupancy predictions that enhance downstream planning tasks in real-world driving scenarios.'}, 'zh': {'title': '生成模型提升3D占用预测的准确性', 'desc': '本研究将3D占用网格的预测视为生成建模任务，采用扩散模型来处理视觉输入。与传统的判别方法相比，扩散模型能够更好地应对噪声数据和不完整观测，同时有效捕捉3D场景的复杂结构。实验结果表明，基于扩散的生成模型在占用预测的准确性和一致性上优于现有的最先进判别方法，尤其在遮挡或低可见度区域表现更佳。该方法的改进预测显著提升了后续规划任务的效果，展示了其在自动驾驶实际应用中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.04996', 'title': 'PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment', 'url': 'https://huggingface.co/papers/2506.04996', 'abstract': 'PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications.', 'score': 1, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'f0a38c5292395f88', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.04996.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': '🏋️', 'ru': {'title': 'PATS: Умная временная выборка для оценки спортивного мастерства', 'desc': 'PATS (Proficiency-Aware Temporal Sampling) - это новый метод временной выборки для анализа видео спортивных навыков. Он обеспечивает захват полных паттернов движения, превосходя существующие методы в различных областях. PATS адаптивно сегментирует видео, чтобы каждая анализируемая часть содержала полное выполнение критических компонентов производительности. Оцененный на бенчмарке EgoExo4D с использованием SkillFormer, PATS превосходит современные показатели точности во всех конфигурациях просмотра.'}, 'en': {'title': 'Enhancing Athletic Skill Analysis with PATS', 'desc': 'PATS, or Proficiency-Aware Temporal Sampling, is a new method designed to improve the analysis of athletic skills in videos. It captures complete movement patterns by maintaining the temporal continuity necessary for evaluating performance. This method adaptively segments videos to ensure that each analyzed part includes the full execution of key skills, enhancing the accuracy of assessments. PATS has shown to outperform existing techniques in various sports and activities, making it a significant advancement in automated skill evaluation.'}, 'zh': {'title': 'PATS：提升运动技能分析的时间采样新方法', 'desc': 'PATS是一种新颖的时间采样方法，旨在提升运动技能的视频分析。它通过确保完整的运动模式被捕捉，超越了现有的采样方法。PATS能够自适应地分段视频，确保每个分析部分都包含关键表现组件的完整执行。经过评估，PATS在多个领域的准确性上均优于现有技术，显示出其在自动化技能评估中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.04462', 'title': 'Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation', 'url': 'https://huggingface.co/papers/2506.04462', 'abstract': 'Watermarking techniques in LLMs can degrade truthfulness, safety, and helpfulness, and alignment resampling is proposed to restore alignment while ensuring watermark detectability.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.', 'score': 1, 'issue_id': 4168, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '35a6c458e61b75e3', 'authors': ['Apurv Verma', 'NhatHai Phan', 'Shubhendu Trivedi'], 'affiliations': ['MIT', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04462.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#rlhf', '#inference'], 'emoji': '🔒', 'ru': {'title': 'Баланс между водяными знаками и выравниванием в LLM', 'desc': 'Статья исследует влияние методов водяных знаков на большие языковые модели (LLM), выявляя их негативное воздействие на правдивость, безопасность и полезность выходных данных. Авторы обнаружили два паттерна деградации: ослабление защиты и чрезмерное усиление защиты. Для решения этой проблемы предлагается метод Alignment Resampling (AR), использующий внешнюю модель вознаграждения для восстановления выравнивания. Экспериментальные результаты показывают, что AR успешно восстанавливает базовое выравнивание при сохранении сильной обнаруживаемости водяных знаков.'}, 'en': {'title': 'Balancing Watermarking and Model Alignment in LLMs', 'desc': 'This paper investigates how watermarking techniques in large language models (LLMs) can negatively affect their truthfulness, safety, and helpfulness. It identifies two main degradation patterns: guard attenuation, where increased helpfulness compromises safety, and guard amplification, where excessive caution limits helpfulness. To address these issues, the authors propose a method called Alignment Resampling (AR), which uses an external reward model to restore alignment during inference. The results show that AR can effectively recover alignment scores while ensuring that the watermarks remain detectable, highlighting the delicate balance between watermark strength and model performance.'}, 'zh': {'title': '水印与模型对齐的平衡', 'desc': '本文探讨了大语言模型（LLMs）中的水印技术对输出质量的影响，特别是对真实性、安全性和有用性的影响。研究分析了两种流行的水印方法——Gumbel和KGW，如何影响这四个对齐属性。实验结果显示，水印引起的代币分布变化导致了两种不同的降级模式：保护减弱和保护增强。为了解决这些问题，提出了一种名为对齐重采样（AR）的方法，通过外部奖励模型在推理时恢复对齐，同时保持水印的可检测性。'}}}, {'id': 'https://huggingface.co/papers/2506.03238', 'title': 'Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach', 'url': 'https://huggingface.co/papers/2506.03238', 'abstract': 'OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'f234199601bef528', 'authors': ['Ziheng Zhao', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Department of Radiology, Renmin Hospital of Wuhan University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03238.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#healthcare', '#cv'], 'emoji': '🔬', 'ru': {'title': 'ИИ-революция в интерпретации КТ-снимков', 'desc': 'Модель OminiAbnorm-CT предназначена для автоматизированной интерпретации КТ-изображений. Она превосходит существующие методы в локализации и описании аномалий в различных областях тела с использованием текстовых запросов и визуальных подсказок. Модель основана на всеобъемлющей иерархической системе классификации, разработанной совместно с опытными радиологами. OminiAbnorm-CT обучена на большом наборе данных КТ-изображений с тщательно размеченными аномалиями.'}, 'en': {'title': 'Revolutionizing CT Image Analysis with OminiAbnorm-CT', 'desc': 'OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology.'}, 'zh': {'title': 'OminiAbnorm-CT：CT图像异常自动解读的新突破', 'desc': 'OminiAbnorm-CT是一种用于自动解读CT图像的模型，能够在不同身体部位中定位和描述异常情况。该研究通过与资深放射科医生合作，提出了一个包含404种异常发现的层次分类系统。我们还贡献了一个包含超过14.5K CT图像的数据集，并为超过19K异常提供了详细的注释。通过大量实验，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02444', 'title': 'SViMo: Synchronized Diffusion for Video and Motion Generation in\n  Hand-object Interaction Scenarios', 'url': 'https://huggingface.co/papers/2506.02444', 'abstract': "A framework combining visual priors and dynamic constraints within a synchronized diffusion process generates HOI video and motion simultaneously, enhancing video-motion consistency and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at https://github.com/Droliven/SViMo\\_project.", 'score': 1, 'issue_id': 4168, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'a1ed02142b4e22ee', 'authors': ['Lingwei Dang', 'Ruizhi Shao', 'Hongwen Zhang', 'Wei Min', 'Yebin Liu', 'Qingyao Wu'], 'affiliations': ['Department of Automation, Tsinghua University', 'School of Artificial Intelligence, Beijing Normal University', 'School of Software Engineering, South China University of Technology', 'Shadow AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.02444.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#3d', '#games', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'Синхронная генерация видео и движения для реалистичного взаимодействия человека с объектами', 'desc': 'Предложена новая система для одновременной генерации видео и движения взаимодействия человека с объектами (HOI). Метод объединяет визуальные приоры и динамические ограничения в синхронизированном процессе диффузии. Используется тримодальная адаптивная модуляция для выравнивания признаков и 3D полное внимание для моделирования зависимостей. Система включает зависимое от зрения 3D диффузионное моделирование взаимодействий с обратной связью, что улучшает согласованность видео и движений.'}, 'en': {'title': 'Synchronized Diffusion for Realistic HOI Generation', 'desc': 'This paper presents a new framework for generating Hand-Object Interaction (HOI) videos and motions simultaneously, improving consistency and generalization. It combines visual priors and dynamic constraints using a synchronized diffusion process, which allows for the creation of realistic interactions without relying on predefined 3D models. The method employs tri-modal adaptive modulation and 3D full-attention to align features and model dependencies effectively. Experimental results show that this approach outperforms existing methods in producing high-quality, physically plausible HOI sequences, even in new scenarios.'}, 'zh': {'title': '同步扩散生成高保真手-物体交互视频', 'desc': '本文提出了一种新颖的框架，通过结合视觉先验和动态约束，在同步扩散过程中同时生成手-物体交互（HOI）视频和运动。这种方法克服了传统3D HOI运动生成依赖预定义模型和捕获数据的局限性，提升了生成的一致性和泛化能力。我们采用三模态自适应调制和3D全注意力机制来对齐特征，并引入视觉感知的3D交互扩散模型，直接从同步扩散输出生成3D交互序列。实验结果表明，该方法在生成高保真、动态合理的HOI序列方面优于现有技术，尤其在未见过的真实场景中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.00981', 'title': 'What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training', 'url': 'https://huggingface.co/papers/2506.00981', 'abstract': "Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition performance.  \t\t\t\t\tAI-generated summary \t\t\t\t How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.", 'score': 1, 'issue_id': 4165, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'e571a309d437ed8b', 'authors': ['Marianne de Heer Kloots', 'Hosein Mohebbi', 'Charlotte Pouw', 'Gaofei Shen', 'Willem Zuidema', 'Martijn Bentum'], 'affiliations': ['Centre for Language Studies, Radboud University, Netherlands', 'Cognitive Science and Artificial Intelligence, Tilburg University, The Netherlands', 'Institute for Logic, Language and Computation, University of Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.00981.jpg', 'data': {'categories': ['#transfer_learning', '#multilingual', '#audio', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Языково-специфичное предобучение улучшает представление речи', 'desc': 'Исследование показывает, что модели Wav2Vec2, предобученные исключительно на голландских данных, лучше кодируют лингвистические особенности голландского языка по сравнению с моделями, обученными на английском или многоязычных данных. Это преимущество выявляется с помощью методов кластеризации и классификации. Улучшение представления лингвистических особенностей также приводит к повышению производительности в задаче автоматического распознавания речи. Результаты подчеркивают важность использования языково-специфичных данных при предобучении моделей для конкретного языка.'}, 'en': {'title': 'Unlocking Dutch: The Power of Language-Specific Pre-Training', 'desc': 'This paper investigates how well self-supervised Wav2Vec2 models can learn Dutch language features when trained specifically on Dutch data. The study finds that pre-training exclusively on Dutch leads to better encoding of Dutch phonetic and lexical information compared to using English or multilingual data. The improvements are measured using clustering and classification probes, which show that the models capture language-specific characteristics more effectively. Additionally, these enhancements in linguistic representation correlate with better performance in Automatic Speech Recognition tasks.'}, 'zh': {'title': '专注荷兰语，提升语音识别表现', 'desc': '本研究探讨了自监督学习模型Wav2Vec2在编码荷兰语语言特征方面的表现。研究发现，当模型仅在荷兰语数据上进行预训练时，能够更准确地捕捉荷兰语的语音和词汇信息。与在英语或多语言数据上进行相似量的预训练相比，荷兰语特征的表示显著提高。该语言特定的优势通过聚类和分类探测器得到了验证，并且与自动语音识别的性能提升相一致。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (10)', '#agents (4)', '#agi', '#alignment (4)', '#architecture (7)', '#audio (2)', '#benchmark (21)', '#cv (12)', '#data (6)', '#dataset (15)', '#diffusion (9)', '#ethics (1)', '#games (5)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (5)', '#interpretability (2)', '#leakage (1)', '#long_context (5)', '#low_resource (2)', '#machine_translation', '#math (4)', '#multilingual (3)', '#multimodal (15)', '#open_source (11)', '#optimization (18)', '#plp', '#rag (2)', '#reasoning (11)', '#rl (5)', '#rlhf (1)', '#robotics (2)', '#science', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (3)', '#training (16)', '#transfer_learning (4)', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-07 12:46',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-07 12:46')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-07 12:46')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    