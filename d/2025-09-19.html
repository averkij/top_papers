
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. September 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-18.html">â¬…ï¸ <span id="prev-date">18.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-22.html">â¡ï¸ <span id="next-date">22.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 19', 'zh': '9æœˆ19æ—¥'};
        let feedDateNext = {'ru': '22.09', 'en': '09/22', 'zh': '9æœˆ22æ—¥'};
        let feedDatePrev = {'ru': '18.09', 'en': '09/18', 'zh': '9æœˆ18æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.15221', 'title': 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data', 'url': 'https://huggingface.co/papers/2509.15221', 'abstract': 'ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.', 'score': 67, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '6081b7f60504d5ef', 'authors': ['Zhaoyang Liu', 'JingJing Xie', 'Zichen Ding', 'Zehao Li', 'Bowen Yang', 'Zhenyu Wu', 'Xuehui Wang', 'Qiushi Sun', 'Shi Liu', 'Weiyun Wang', 'Shenglong Ye', 'Qingyun Li', 'Zeyue Tian', 'Gen Luo', 'Xiangyu Yue', 'Biqing Qi', 'Kai Chen', 'Bowen Zhou', 'Yu Qiao', 'Qifeng Chen', 'Wenhai Wang'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.15221.jpg', 'data': {'categories': ['#open_source', '#dataset', '#agents', '#cv'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ScaleCUA: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ScaleCUA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ScaleCUA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 6 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ 3 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ScaleCUA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Empowering Computer Use Agents with ScaleCUA', 'desc': 'ScaleCUA is a groundbreaking dataset and model designed for computer use agents (CUAs) that enhances their ability to operate graphical user interfaces (GUIs) autonomously. By integrating a large-scale dataset that covers six operating systems and three task domains, ScaleCUA significantly improves the performance of CUAs through data-driven scaling techniques. The model demonstrates impressive results, surpassing previous benchmarks and achieving state-of-the-art performance on various tasks. This work not only advances the capabilities of CUAs but also provides open-source resources to foster further research in the field.'}, 'zh': {'title': 'æ•°æ®é©±åŠ¨æ‰©å±•ï¼ŒåŠ©åŠ›è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„æœªæ¥', 'desc': 'ScaleCUAæ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸“ä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå¹³å°å’Œä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å…­ç§æ“ä½œç³»ç»Ÿå’Œä¸‰ä¸ªä»»åŠ¡é¢†åŸŸï¼Œé€šè¿‡å°†è‡ªåŠ¨åŒ–ä»£ç†ä¸äººç±»ä¸“å®¶ç»“åˆçš„é—­ç¯æµç¨‹æ„å»ºè€Œæˆã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼ŒScaleCUAèƒ½å¤Ÿåœ¨ä¸åŒå¹³å°ä¸Šæ— ç¼æ“ä½œï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—æå‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ•°æ®é©±åŠ¨æ‰©å±•åœ¨é€šç”¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15207', 'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning', 'url': 'https://huggingface.co/papers/2509.15207', 'abstract': 'FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.', 'score': 60, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': 'e1295c2f57ad673c', 'authors': ['Xuekai Zhu', 'Daixuan Cheng', 'Dinghuai Zhang', 'Hengli Li', 'Kaiyan Zhang', 'Che Jiang', 'Youbang Sun', 'Ermo Hua', 'Yuxin Zuo', 'Xingtai Lv', 'Qizheng Zhang', 'Lin Chen', 'Fanghao Shao', 'Bo Xue', 'Yunchong Song', 'Zhenjie Yang', 'Ganqu Cui', 'Ning Ding', 'Jianfeng Gao', 'Xiaodong Liu', 'Bowen Zhou', 'Hongyuan Mei', 'Zhouhan Lin'], 'affiliations': ['Microsoft Research', 'Peking University', 'Renmin University of China', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'Toyota Technological Institute at Chicago', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.15207.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#reasoning', '#training', '#math', '#rl'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'FlowRL: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlowRL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, FlowRL ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FlowRL.'}, 'en': {'title': 'FlowRL: Enhancing LLMs through Reward Distribution Matching', 'desc': 'FlowRL is a novel approach in reinforcement learning for large language models (LLMs) that focuses on matching the entire reward distribution rather than just maximizing rewards. By using flow balancing, it addresses the issue of over-optimization seen in traditional methods like PPO and GRPO, which often ignore less frequent but valid reasoning paths. This method transforms scalar rewards into a normalized target distribution and minimizes the reverse KL divergence, promoting diverse exploration and better reasoning. Experiments show that FlowRL significantly outperforms existing methods on math and code reasoning tasks, demonstrating the importance of reward distribution-matching for effective learning.'}, 'zh': {'title': 'FlowRLï¼šé€šè¿‡æµå¹³è¡¡å®ç°å¤šæ ·åŒ–æ¨ç†', 'desc': 'FlowRLæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡æµå¹³è¡¡åŒ¹é…å®Œæ•´çš„å¥–åŠ±åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•çº¯æœ€å¤§åŒ–å¥–åŠ±ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æœ€å¤§åŒ–æ–¹æ³•ï¼ˆå¦‚PPOå’ŒGRPOï¼‰å¾€å¾€è¿‡åº¦ä¼˜åŒ–ä¸»è¦çš„å¥–åŠ±ä¿¡å·ï¼Œå¿½è§†äº†è¾ƒå°‘å‡ºç°ä½†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œé™ä½äº†å¤šæ ·æ€§ã€‚FlowRLé€šè¿‡å¯å­¦ä¹ çš„åˆ†åŒºå‡½æ•°å°†æ ‡é‡å¥–åŠ±è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œå¹¶æœ€å°åŒ–ç­–ç•¥ä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„åå‘KLæ•£åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowRLåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†10.0%ï¼Œåœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¹Ÿæ›´ä¸ºä¼˜è¶Šï¼Œå¼ºè°ƒäº†åŒ¹é…å¥–åŠ±åˆ†å¸ƒåœ¨LLMå¼ºåŒ–å­¦ä¹ ä¸­å®ç°é«˜æ•ˆæ¢ç´¢å’Œå¤šæ ·åŒ–æ¨ç†çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.14760', 'title': 'Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration', 'url': 'https://huggingface.co/papers/2509.14760', 'abstract': "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.", 'score': 40, 'issue_id': 5978, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': 'e9e06e0e548bde84', 'authors': ['Haoran Zhang', 'Yafu Li', 'Xuyang Hu', 'Dongrui Liu', 'Zhilin Wang', 'Bo Li', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'University of Illinois at Urbana-Champaign', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.14760.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#benchmark', '#training', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Align3: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Align3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Test-Time Deliberation Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'specification alignment' - ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼, ÑÑ†ĞµĞ½Ğ°Ñ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpecBench, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 5 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², 103 ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ 1500 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Test-Time Deliberation Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼, Ğ° Align3 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."}, 'en': {'title': 'Aligning Language Models with Dynamic Specifications Efficiently', 'desc': 'The paper introduces Align3, a method that improves how large language models (LLMs) align with specific behavioral and safety requirements in various scenarios. It uses Test-Time Deliberation (TTD) to help models reflect on and adjust their responses according to dynamic specifications. The authors also present SpecBench, a benchmark designed to evaluate how well models adhere to these specifications across multiple scenarios and prompts. The findings demonstrate that Align3 not only enhances alignment but also balances safety and helpfulness with minimal computational cost.'}, 'zh': {'title': 'Align3ï¼šè½»é‡çº§çš„è§„èŒƒå¯¹é½æ–¹æ³•', 'desc': 'Align3æ˜¯ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶æ·±æ€ï¼ˆTest-Time Deliberationï¼‰æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„è§„èŒƒå¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…³æ³¨æ¨¡å‹åœ¨åŠ¨æ€ã€ç‰¹å®šåœºæ™¯ä¸‹éµå¾ªç”¨æˆ·æˆ–ç»„ç»‡å®šåˆ¶çš„è¡Œä¸ºå’Œå®‰å…¨è§„èŒƒçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å±‚æ¬¡åæ€å’Œä¿®æ­£ï¼ŒAlign3èƒ½å¤Ÿåœ¨è§„èŒƒè¾¹ç•Œä¸Šè¿›è¡Œæ¨ç†ï¼Œç¡®ä¿æ¨¡å‹çš„è¾“å‡ºç¬¦åˆé¢„æœŸã€‚æˆ‘ä»¬è¿˜æå‡ºäº†SpecBenchï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œç”¨äºæµ‹é‡è§„èŒƒå¯¹é½ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œè§„èŒƒï¼Œå¸®åŠ©è¯†åˆ«å¯¹é½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15194', 'title': 'Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation', 'url': 'https://huggingface.co/papers/2509.15194', 'abstract': "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.", 'score': 27, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '271ee6d47770b19f', 'authors': ['Yujun Zhou', 'Zhenwen Liang', 'Haolin Liu', 'Wenhao Yu', 'Kishan Panaganti', 'Linfeng Song', 'Dian Yu', 'Xiangliang Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'University of Notre Dame', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2509.15194.jpg', 'data': {'categories': ['#agi', '#rlhf', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğº', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ EVOL-RL Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. EVOL-RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EVOL-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Evolving Language Models with Label-Free Reinforcement Learning', 'desc': 'EVOL-RL is a novel reinforcement learning method designed to enhance large language models without relying on labeled data. It addresses the problem of entropy collapse, where models become less diverse and informative over time. By combining stability through majority voting with a novelty-aware reward system, EVOL-RL encourages exploration and variation in model responses. This approach not only improves generalization across different tasks but also significantly boosts performance metrics compared to existing methods.'}, 'zh': {'title': 'EVOL-RLï¼šæ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ çš„è¿›åŒ–ä¹‹è·¯', 'desc': 'EVOL-RLæ˜¯ä¸€ç§æ— æ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚å®ƒé€šè¿‡é˜²æ­¢ç†µå´©æºƒï¼Œä¿æŒç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒEVOL-RLç»“åˆäº†ç¨³å®šæ€§å’Œå˜åŒ–æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ²¡æœ‰å¤–éƒ¨æ ‡ç­¾çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEVOL-RLåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15185', 'title': 'Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation', 'url': 'https://huggingface.co/papers/2509.15185', 'abstract': 'Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.', 'score': 22, 'issue_id': 5976, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '3da8f7302a159d5f', 'authors': ['Xiaoyu Yue', 'Zidong Wang', 'Yuqing Wang', 'Wenlong Zhang', 'Xihui Liu', 'Wanli Ouyang', 'Lei Bai', 'Luping Zhou'], 'affiliations': ['Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'University of Hong Kong', 'University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2509.15185.jpg', 'data': {'categories': ['#optimization', '#training', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Self-guided Training for AutoRegressive models (ST-AR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¼ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ST-AR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Generation with Self-Guided Training', 'desc': "The paper introduces Self-guided Training for AutoRegressive models (ST-AR), a new framework aimed at improving image understanding and generation in autoregressive models. It identifies three main challenges in applying next-token prediction to visual data: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. By incorporating self-supervised objectives during training, ST-AR effectively addresses these challenges, enhancing the model's ability to learn high-level visual semantics. The results show significant improvements in image generation quality, with FID scores increasing by approximately 42% and 49% for different model versions without using pre-trained representations."}, 'zh': {'title': 'è‡ªæŒ‡å¯¼è®­ç»ƒæå‡å›¾åƒç”Ÿæˆä¸ç†è§£', 'desc': 'è‡ªæŒ‡å¯¼è®­ç»ƒï¼ˆST-ARï¼‰é€šè¿‡è‡ªç›‘ç£ç›®æ ‡è§£å†³äº†è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆä¸­çš„å…³é”®è§†è§‰è¯­ä¹‰æŒ‘æˆ˜ï¼Œä»è€Œæå‡äº†å›¾åƒç†è§£å’Œç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡çš„è§†è§‰è¡¨ç¤ºåœ¨å›¾åƒç”Ÿæˆä¸­è‡³å…³é‡è¦ï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†å°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼åº”ç”¨äºè§†è§‰é¢†åŸŸçš„æœºåˆ¶ï¼Œå¹¶è¯†åˆ«å‡ºå½±å“é«˜å±‚è§†è§‰è¯­ä¹‰å­¦ä¹ çš„ä¸‰ä¸ªå…³é”®ç‰¹æ€§ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªç›‘ç£ç›®æ ‡ï¼ŒST-ARæ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹çš„å›¾åƒç†è§£èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.13160', 'title': 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning', 'url': 'https://huggingface.co/papers/2509.13160', 'abstract': 'FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is a particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and time-sensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation -- closely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement a rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly.By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers a professional, high-difficulty testbed for complex financial search and reasoning.', 'score': 20, 'issue_id': 5975, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 16', 'zh': '9æœˆ16æ—¥'}, 'hash': 'a1faa8bf123c24e6', 'authors': ['Liang Hu', 'Jianpeng Jiao', 'Jiashuo Liu', 'Yanle Ren', 'Zhoufutu Wen', 'Kaiyuan Zhang', 'Xuanliang Zhang', 'Xiang Gao', 'Tianci He', 'Fei Hu', 'Yali Liao', 'Zaiyuan Wang', 'Chenghao Yang', 'Qianyu Yang', 'Mingren Yin', 'Zhiyuan Zeng', 'Ge Zhang', 'Xinyi Zhang', 'Xiying Zhao', 'Zhenwei Zhu', 'Hongseok Namkoong', 'Wenhao Huang', 'Yuwen Tang'], 'affiliations': ['ByteDance', 'Columbia Business School'], 'pdf_title_img': 'assets/pdf/title_img/2509.13160.jpg', 'data': {'categories': ['#science', '#open_source', '#agents', '#reasoning', '#benchmark'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'FinSearchComp: Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'FinSearchComp - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²: Ğ¿Ğ¾Ğ¸ÑĞº Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 635 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ñ€Ñ‹Ğ½ĞºĞ°Ğ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 70 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° FinSearchComp.'}, 'en': {'title': 'FinSearchComp: Benchmarking AI in Financial Search and Reasoning', 'desc': 'FinSearchComp is an innovative benchmark designed to assess the financial search and reasoning abilities of end-to-end AI agents. It includes realistic tasks that mimic the workflows of financial analysts, such as fetching time-sensitive data and conducting historical investigations. The benchmark is supported by professional annotations from financial experts, ensuring high-quality and relevant evaluation criteria. By testing various models on this benchmark, researchers can better understand the effectiveness of AI in handling complex financial queries and improve their performance through enhanced search capabilities.'}, 'zh': {'title': 'é‡‘èæœç´¢ä¸æ¨ç†çš„ä¸“ä¸šåŸºå‡†æµ‹è¯•', 'desc': 'FinSearchCompæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç«¯åˆ°ç«¯æ™ºèƒ½ä½“åœ¨é‡‘èæœç´¢å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼Œæ¨¡æ‹ŸçœŸå®çš„é‡‘èåˆ†æå¸ˆå·¥ä½œæµç¨‹ï¼Œç¡®ä¿ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯é æ€§ã€‚é€šè¿‡ä¸70ä½ä¸“ä¸šé‡‘èä¸“å®¶åˆä½œè¿›è¡Œæ ‡æ³¨ï¼ŒFinSearchCompæä¾›äº†635ä¸ªé—®é¢˜ï¼Œæ¶µç›–å…¨çƒåŠå¤§ä¸­åå¸‚åœºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç½‘ç»œæœç´¢å’Œé‡‘èæ’ä»¶çš„æ™ºèƒ½ä½“åœ¨FinSearchCompä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚é‡‘èæœç´¢å’Œæ¨ç†ä¸­çš„ä¸“ä¸šæ€§å’Œé«˜éš¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15212', 'title': 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation', 'url': 'https://huggingface.co/papers/2509.15212', 'abstract': 'RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.', 'score': 11, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '0f4ee15179f6aeef', 'authors': ['Yuming Jiang', 'Siteng Huang', 'Shengke Xue', 'Yaxi Zhao', 'Jun Cen', 'Sicong Leng', 'Kehan Li', 'Jiayan Guo', 'Kexiang Wang', 'Mingxiu Chen', 'Fan Wang', 'Deli Zhao', 'Xin Li'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.15212.jpg', 'data': {'categories': ['#robotics', '#optimization', '#training', '#cv', '#games', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RynnVLA-001: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'RynnVLA-001 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ActionVAE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Robotics with RynnVLA-001: A Vision-Language-Action Breakthrough!', 'desc': "RynnVLA-001 is a vision-language-action model designed to improve robotics tasks through a two-stage pretraining approach. The first stage involves Ego-Centric Video Generative Pretraining, where the model learns to predict future video frames based on initial frames and language instructions using a large dataset of manipulation videos. The second stage, Human-Centric Trajectory-Aware Modeling, enhances the model's ability to predict keypoint trajectories, linking visual predictions with action outcomes. Additionally, the introduction of ActionVAE helps to simplify the action representation by compressing action sequences into compact latent embeddings, leading to better performance on robotics tasks compared to existing models."}, 'zh': {'title': 'RynnVLA-001ï¼šæå‡æœºå™¨äººä»»åŠ¡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'RynnVLA-001æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•å’ŒActionVAEæ¥æå‡æœºå™¨äººä»»åŠ¡çš„è¡¨ç°ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆé¢„è®­ç»ƒï¼Œåˆ©ç”¨1200ä¸‡æ®µè‡ªæˆ‘ä¸­å¿ƒçš„æ“ä½œè§†é¢‘è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼Œä»¥æ ¹æ®åˆå§‹å¸§å’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹æœªæ¥å¸§ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ä»¥äººä¸ºä¸­å¿ƒçš„è½¨è¿¹æ„ŸçŸ¥å»ºæ¨¡ï¼Œé€šè¿‡è”åˆé¢„æµ‹æœªæ¥å…³é”®ç‚¹è½¨è¿¹ï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰å¸§é¢„æµ‹ä¸åŠ¨ä½œé¢„æµ‹ç»“åˆèµ·æ¥ã€‚æ­¤å¤–ï¼ŒActionVAEä½œä¸ºå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå‹ç¼©åŠ¨ä½œåºåˆ—ä¸ºç´§å‡‘çš„æ½œåœ¨åµŒå…¥ï¼Œé™ä½äº†VLAè¾“å‡ºç©ºé—´çš„å¤æ‚æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.14476', 'title': 'AToken: A Unified Tokenizer for Vision', 'url': 'https://huggingface.co/papers/2509.14476', 'abstract': 'AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.', 'score': 9, 'issue_id': 5975, 'pub_date': '2025-09-17', 'pub_date_card': {'ru': '17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 17', 'zh': '9æœˆ17æ—¥'}, 'hash': '4904c9c747d48b89', 'authors': ['Jiasen Lu', 'Liangchen Song', 'Mingze Xu', 'Byeongjoo Ahn', 'Yanjun Wang', 'Chen Chen', 'Afshin Dehghan', 'Yinfei Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.14476.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d', '#architecture', '#video', '#training', '#cv', '#benchmark', '#games'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'AToken - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ 4D-Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. AToken Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ“Ñ€Ğ°Ğ¼Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unified Visual Tokenization for Next-Gen AI', 'desc': "AToken is a novel visual tokenizer that integrates high-fidelity reconstruction and semantic understanding for images, videos, and 3D assets using a 4D transformer architecture. It encodes various visual inputs into a shared latent space, allowing it to handle multiple modalities simultaneously. The model employs an adversarial-free training approach, utilizing perceptual and Gram matrix losses to ensure stable and high-quality outputs. AToken's performance is demonstrated through impressive metrics across different tasks, paving the way for advanced multimodal AI applications."}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰æ ‡è®°ï¼Œé‡å»ºä¸ç†è§£çš„å®Œç¾ç»“åˆ', 'desc': 'ATokenæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ ‡è®°å™¨ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒã€è§†é¢‘å’Œ3Dèµ„äº§ä¸­å®ç°é«˜ä¿çœŸé‡å»ºå’Œè¯­ä¹‰ç†è§£ã€‚ä¸ç°æœ‰çš„ä¸“æ³¨äºå•ä¸€æ¨¡æ€çš„æ ‡è®°å™¨ä¸åŒï¼ŒATokenå°†å¤šæ ·çš„è§†è§‰è¾“å…¥ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„4Dæ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œç»Ÿä¸€äº†é‡å»ºå’Œç†è§£ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨çº¯å˜æ¢å™¨æ¶æ„å’Œ4Dæ—‹è½¬ä½ç½®åµŒå…¥ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„åˆ†è¾¨ç‡å’Œæ—¶é—´é•¿åº¦çš„è§†è§‰è¾“å…¥ã€‚é€šè¿‡æ— å¯¹æŠ—è®­ç»ƒç›®æ ‡ï¼Œç»“åˆæ„ŸçŸ¥æŸå¤±å’ŒGramçŸ©é˜µæŸå¤±ï¼ŒATokenåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†ä¸‹ä¸€ä»£å¤šæ¨¡æ€AIç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15130', 'title': 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance', 'url': 'https://huggingface.co/papers/2509.15130', 'abstract': "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", 'score': 6, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': 'f91495ad752f8344', 'authors': ['Chenxi Song', 'Yanming Yang', 'Tong Zhao', 'Ruibo Li', 'Chi Zhang'], 'affiliations': ['AGI Lab, School of Engineering, Westlake University, Hangzhou, China', 'The College of Computing and Data Science, Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.15130.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#inference', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'WorldForge: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'WorldForge - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. WorldForge Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'WorldForge: Training-Free Control for Photorealistic Video Synthesis', 'desc': 'WorldForge is a novel framework that enhances video diffusion models without the need for retraining. It introduces three key modules: Intra-Step Recursive Refinement for optimizing predictions during inference, Flow-Gated Latent Fusion for separating motion from appearance, and Dual-Path Self-Corrective Guidance to correct trajectory drift. These components work together to provide precise motion control and generate photorealistic content. The framework demonstrates significant improvements in realism and consistency, making it a valuable tool for controllable video synthesis.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„è§†é¢‘åˆæˆæ–°èŒƒå¼', 'desc': 'WorldForgeæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡é€’å½’ä¼˜åŒ–ã€æµé—¨æ§æ½œåœ¨èåˆå’ŒåŒè·¯å¾„è‡ªæˆ‘æ ¡æ­£æŒ‡å¯¼ï¼Œå¢å¼ºäº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶å’ŒçœŸå®æ„Ÿå†…å®¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¯æ§æ€§å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œé¿å…äº†é‡æ–°è®­ç»ƒå¸¦æ¥çš„çŸ¥è¯†é€€åŒ–å’Œé«˜è®¡ç®—æˆæœ¬ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é€’å½’ä¼˜åŒ–æœºåˆ¶ï¼ŒWorldForgeèƒ½å¤Ÿç²¾ç¡®åœ°æ³¨å…¥è¿åŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ„Ÿã€è½¨è¿¹ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.14638', 'title': 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks', 'url': 'https://huggingface.co/papers/2509.14638', 'abstract': "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.", 'score': 4, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '35e0066dc539ba98', 'authors': ['Mingsong Li', 'Lin Liu', 'Hongjun Wang', 'Haoxing Chen', 'Xijun Gu', 'Shizhan Liu', 'Dong Gong', 'Junbo Zhao', 'Zhenzhong Lan', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'The University of Hong Kong', 'University of New South Wales', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.14638.jpg', 'data': {'categories': ['#open_source', '#dataset', '#cv', '#benchmark', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MultiEdit: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MultiEdit - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 107 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 18 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ 38 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ. Ğ”Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MultiEdit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'MultiEdit: Elevating Image Editing with a Rich Dataset', 'desc': 'The paper introduces MultiEdit, a new dataset designed to enhance instruction-based image editing (IBIE) methods by providing over 107,000 high-quality image editing samples. It addresses the limitations of existing datasets, which often contain noisy image-caption pairs and lack diversity in editing tasks. MultiEdit includes six challenging editing tasks and a variety of editing types, from style transfer to complex semantic operations. By utilizing multi-modal large language models for dataset construction, the paper demonstrates that fine-tuning models with MultiEdit significantly improves their performance on complex editing tasks while maintaining their effectiveness on standard benchmarks.'}, 'zh': {'title': 'MultiEditï¼šæå‡å¤æ‚å›¾åƒç¼–è¾‘èƒ½åŠ›çš„å…¨æ–°æ•°æ®é›†', 'desc': 'MultiEditæ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡107Ké«˜è´¨é‡å›¾åƒç¼–è¾‘æ ·æœ¬çš„ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤æ‚ç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†6ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…å«18ç§éé£æ ¼è½¬ç§»ç¼–è¾‘ç±»å‹å’Œ38ç§é£æ ¼è½¬ç§»æ“ä½œã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®é›†ç”Ÿæˆç®¡é“ï¼Œä»¥ç”Ÿæˆè§†è§‰é€‚åº”çš„ç¼–è¾‘æŒ‡ä»¤å¹¶åˆ¶ä½œé«˜ä¿çœŸç¼–è¾‘å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MultiEditè®­ç»ƒé›†å¾®è°ƒåŸºç¡€å¼€æºæ¨¡å‹æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤æ‚ç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº†å…¶åœ¨æ ‡å‡†ç¼–è¾‘åŸºå‡†ä¸Šçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15178', 'title': 'Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding', 'url': 'https://huggingface.co/papers/2509.15178', 'abstract': "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.", 'score': 2, 'issue_id': 5976, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': 'f4b09bebd69f88b8', 'authors': ['Zaiquan Yang', 'Yuhao Liu', 'Gerhard Hancke', 'Rynson W. H. Lau'], 'affiliations': ['Department of Computer Science, City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.15178.jpg', 'data': {'categories': ['#video', '#games', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (STVG) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ·ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½ÑƒÑ (zero-shot) ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ (DSTH) Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ (TAS). DSTH Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ LRA Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…. TAS ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Video Grounding with Multimodal Language Models', 'desc': "This paper presents a zero-shot framework for spatio-temporal video grounding (STVG) using multimodal large language models (MLLMs). The authors identify that MLLMs can dynamically assign grounding tokens but often struggle with integrating all relevant cues from text queries. To address this, they introduce two innovative strategies: decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS), which enhance the model's reasoning capabilities. The proposed methods improve grounding accuracy by effectively separating and utilizing attributes and actions from queries while ensuring temporal consistency in predictions."}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨¡å‹åŠ©åŠ›æ—¶ç©ºè§†é¢‘ç²¾å‡†å®šä½', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶-shotæ¡†æ¶ï¼Œç”¨äºæ—¶ç©ºè§†é¢‘å®šä½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æŸ¥è¯¢æ—¶ï¼Œèƒ½å¤ŸåŠ¨æ€åˆ†é…ç‰¹å®šçš„æ ‡è®°æ¥è¿›è¡Œå®šä½ï¼Œä½†åœ¨æ•´åˆæ–‡æœ¬ä¸­çš„çº¿ç´¢æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†åˆ†è§£æ—¶ç©ºé«˜äº®å’Œæ—¶é—´å¢å¼ºç»„è£…ç­–ç•¥ï¼Œä»¥æé«˜å®šä½çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›åˆ›æ–°æ–¹æ³•ï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.15020', 'title': 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs', 'url': 'https://huggingface.co/papers/2509.15020', 'abstract': 'Tokenizing the space with the answer letter in multiple-choice question answering improves LLM accuracy and calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string "Answer:" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model\'s confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.', 'score': 2, 'issue_id': 5984, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '6e87639367d48430', 'authors': ['Mario Sanz-Guerrero', 'Minh Duc Bui', 'Katharina von der Wense'], 'affiliations': ['Johannes Gutenberg University Mainz, Germany', 'University of Colorado Boulder, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.15020.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#benchmark', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ±ÑƒĞºĞ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ°, ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ±Ñ‹, Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğµ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 11% Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Tokenization Matters: Boosting LLM Accuracy in MCQA!', 'desc': 'This paper investigates the impact of tokenization strategies on the performance of large language models (LLMs) in multiple-choice question answering (MCQA). It reveals that the way space is tokenized after the prompt can lead to significant accuracy differences, affecting model rankings by up to 11%. The authors propose a specific method of tokenizing the space along with the answer letter, which consistently improves both accuracy and model calibration. These findings emphasize the necessity for standardized evaluation practices to ensure the reliability of LLM comparisons.'}, 'zh': {'title': 'ä¼˜åŒ–åˆ†è¯æå‡LLMå‡†ç¡®æ€§ä¸æ ¡å‡†æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤šé€‰é¢˜é—®ç­”ä¸­ï¼Œå¦‚ä½•å¤„ç†å†’å·åç©ºæ ¼çš„åˆ†è¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡†ç¡®æ€§å’Œæ ¡å‡†çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé‡‡ç”¨ä¸åŒçš„åˆ†è¯æ–¹å¼å¯èƒ½å¯¼è‡´å‡†ç¡®ç‡å·®å¼‚é«˜è¾¾11%ï¼Œå¹¶ä¸”å¯èƒ½æ”¹å˜æ¨¡å‹æ’åï¼Œå½±å“LLMæ¯”è¾ƒçš„å¯é æ€§ã€‚æˆ‘ä»¬æ¨èå°†ç©ºæ ¼ä¸ç­”æ¡ˆå­—æ¯ä¸€èµ·åˆ†è¯ï¼Œè¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºä¸€è‡´ä¸”æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¹Ÿæ”¹å–„äº†æ¨¡å‹çš„æ ¡å‡†æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°è®¾è®¡çš„é‡è¦æ€§ï¼Œå¹¶å‘¼åå»ºç«‹æ ‡å‡†åŒ–å’Œé€æ˜çš„è¯„ä¼°åè®®ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå¯æ¯”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.14977', 'title': 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence', 'url': 'https://huggingface.co/papers/2509.14977', 'abstract': 'EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM.', 'score': 0, 'issue_id': 5980, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '36663a681181ac46', 'authors': ['Chaoyin She', 'Ruifang Lu', 'Lida Chen', 'Wei Wang', 'Qinghua Huang'], 'affiliations': ['Northwestern Polytechnical University', 'The First Affiliated Hospital of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2509.14977.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#games', '#cv', '#open_source', '#training', '#science', '#dataset'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'EchoVLM: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'EchoVLM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Experts. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞµĞ¼Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. EchoVLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ BLEU-1 Ğ¸ ROUGE-1. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ² ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Ultrasound Diagnosis with EchoVLM', 'desc': 'EchoVLM is a specialized vision-language model designed to enhance ultrasound report generation and diagnosis by utilizing a Mixture of Experts (MoE) architecture. This model is trained on diverse data from seven anatomical regions, allowing it to effectively handle multiple tasks such as report generation, diagnosis, and visual question-answering. The results show that EchoVLM significantly outperforms existing models, achieving notable improvements in BLEU-1 and ROUGE-1 scores for ultrasound report generation. This advancement indicates that EchoVLM can greatly improve diagnostic accuracy in ultrasound imaging, making it a promising tool for clinical applications.'}, 'zh': {'title': 'EchoVLMï¼šæå‡è¶…å£°è¯Šæ–­çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'EchoVLMæ˜¯ä¸€ç§ä¸“é—¨ä¸ºè¶…å£°åŒ»å­¦æˆåƒè®¾è®¡çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨æ¥è‡ªä¸ƒä¸ªè§£å‰–åŒºåŸŸçš„æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†è¶…å£°æŠ¥å‘Šç”Ÿæˆå’Œè¯Šæ–­çš„æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEchoVLMåœ¨è¶…å£°æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºQwen2-VLï¼ŒBLEU-1å’ŒROUGE-1å¾—åˆ†åˆ†åˆ«æé«˜äº†10.15å’Œ4.77åˆ†ã€‚è¿™è¡¨æ˜EchoVLMåœ¨æé«˜è¶…å£°æˆåƒè¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ä¸´åºŠåº”ç”¨æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06482', 'title': 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection', 'url': 'https://huggingface.co/papers/2509.06482', 'abstract': 'FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.', 'score': 0, 'issue_id': 5976, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '89822775d7f8c973', 'authors': ['Zhongxiang Xie', 'Shuangxi Miao', 'Yuhan Jiang', 'Zhewei Zhang', 'Jing Yao', 'Xuecao Li', 'Jianxi Huang', 'Pedram Ghamisi'], 'affiliations': ['Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China', 'College of Land Science and Technology, China Agricultural University, Beijing 100193, China', 'Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China', 'Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany', 'Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K.'], 'pdf_title_img': 'assets/pdf/title_img/2509.06482.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'FSG-Net: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…', 'desc': 'FSG-Net - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. FSG-Net Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ DAWIM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚, STSAM Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ LGFU Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FSG-Net Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Change Detection with FSG-Net: Bridging Gaps and Reducing False Alarms', 'desc': 'FSG-Net is a novel approach designed to improve change detection in high-resolution remote sensing images by addressing false alarms and semantic gaps. It utilizes a frequency-spatial synergistic method that includes a Discrepancy-Aware Wavelet Interaction Module to reduce misinterpretations caused by radiometric variations. The model further enhances feature representation through a Synergistic Temporal-Spatial Attention Module, which focuses on highlighting genuine changes. Finally, a Lightweight Gated Fusion Unit effectively integrates high-level semantic information with detailed features, achieving state-of-the-art performance on several benchmarks.'}, 'zh': {'title': 'FSG-Netï¼šç²¾å‡†å˜åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'FSG-Netæ˜¯ä¸€ç§æ–°é¢–çš„ç½‘ç»œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå˜åŒ–æ£€æµ‹ä¸­çš„å‡è­¦æŠ¥å’Œè¯­ä¹‰å·®è·é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é¢‘ç‡-ç©ºé—´ååŒçš„æ–¹æ³•ï¼Œé€šè¿‡å°æ³¢äº¤äº’æ¨¡å—å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåŒºåˆ†çœŸå®å˜åŒ–ä¸å¹²æ‰°å˜åŒ–ã€‚FSG-Neté¦–å…ˆåœ¨é¢‘ç‡åŸŸä¸­å¤„ç†ä¸åŒé¢‘ç‡æˆåˆ†ï¼Œä»¥å‡å°‘ä¼ªå˜åŒ–çš„å½±å“ï¼Œç„¶ååœ¨ç©ºé—´åŸŸä¸­å¢å¼ºçœŸå®å˜åŒ–åŒºåŸŸçš„æ˜¾è‘—æ€§ã€‚æœ€åï¼Œé€šè¿‡è½»é‡çº§é—¨æ§èåˆå•å…ƒï¼ŒFSG-Netå°†é«˜å±‚è¯­ä¹‰ä¸ä½å±‚ç»†èŠ‚æœ‰æ•ˆç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å˜åŒ–æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (8)', '#cv (7)', '#data (1)', '#dataset (3)', '#diffusion (1)', '#ethics', '#games (5)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (2)', '#open_source (4)', '#optimization (6)', '#plp', '#rag', '#reasoning (4)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (8)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-09-19 12:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-19 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-19 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    