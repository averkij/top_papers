
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. September 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 сентября</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-18.html">⬅️ <span id="prev-date">18.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-22.html">➡️ <span id="next-date">22.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 сентября', 'en': 'September 19', 'zh': '9月19日'};
        let feedDateNext = {'ru': '22.09', 'en': '09/22', 'zh': '9月22日'};
        let feedDatePrev = {'ru': '18.09', 'en': '09/18', 'zh': '9月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.15221', 'title': 'ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data', 'url': 'https://huggingface.co/papers/2509.15221', 'abstract': 'ScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.', 'score': 67, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '6081b7f60504d5ef', 'authors': ['Zhaoyang Liu', 'JingJing Xie', 'Zichen Ding', 'Zehao Li', 'Bowen Yang', 'Zhenyu Wu', 'Xuehui Wang', 'Qiushi Sun', 'Shi Liu', 'Weiyun Wang', 'Shenglong Ye', 'Qingyun Li', 'Zeyue Tian', 'Gen Luo', 'Xiangyu Yue', 'Biqing Qi', 'Kai Chen', 'Bowen Zhou', 'Yu Qiao', 'Qifeng Chen', 'Wenhai Wang'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.15221.jpg', 'data': {'categories': ['#open_source', '#dataset', '#agents', '#cv'], 'emoji': '🖥️', 'ru': {'title': 'ScaleCUA: Масштабирование агентов компьютерного использования на основе данных', 'desc': 'ScaleCUA представляет собой масштабный датасет и модель для агентов компьютерного использования. Модель достигает передовых результатов на различных платформах и задачах благодаря масштабированию, основанному на данных. ScaleCUA включает в себя большой набор данных, охватывающий 6 операционных систем и 3 области задач, созданный с помощью замкнутого цикла, объединяющего автоматизированных агентов и экспертов-людей. Обученная на этих расширенных данных, модель ScaleCUA демонстрирует значительные улучшения по сравнению с базовыми моделями и устанавливает новые рекорды производительности в различных тестах.'}, 'en': {'title': 'Empowering Computer Use Agents with ScaleCUA', 'desc': 'ScaleCUA is a groundbreaking dataset and model designed for computer use agents (CUAs) that enhances their ability to operate graphical user interfaces (GUIs) autonomously. By integrating a large-scale dataset that covers six operating systems and three task domains, ScaleCUA significantly improves the performance of CUAs through data-driven scaling techniques. The model demonstrates impressive results, surpassing previous benchmarks and achieving state-of-the-art performance on various tasks. This work not only advances the capabilities of CUAs but also provides open-source resources to foster further research in the field.'}, 'zh': {'title': '数据驱动扩展，助力计算机使用代理的未来', 'desc': 'ScaleCUA是一个大型数据集和模型，专为计算机使用代理（CUAs）设计，能够在多个平台和任务上实现最先进的性能。该数据集涵盖了六种操作系统和三个任务领域，通过将自动化代理与人类专家结合的闭环流程构建而成。经过大规模数据训练，ScaleCUA能够在不同平台上无缝操作，并在多个基准测试中取得显著提升。研究结果强调了数据驱动扩展在通用计算机使用代理中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2509.15207', 'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning', 'url': 'https://huggingface.co/papers/2509.15207', 'abstract': 'FlowRL enhances LLM reinforcement learning by matching the full reward distribution through flow balancing, improving diversity and performance over reward-maximizing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.', 'score': 60, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'e1295c2f57ad673c', 'authors': ['Xuekai Zhu', 'Daixuan Cheng', 'Dinghuai Zhang', 'Hengli Li', 'Kaiyan Zhang', 'Che Jiang', 'Youbang Sun', 'Ermo Hua', 'Yuxin Zuo', 'Xingtai Lv', 'Qizheng Zhang', 'Lin Chen', 'Fanghao Shao', 'Bo Xue', 'Yunchong Song', 'Zhenjie Yang', 'Ganqu Cui', 'Ning Ding', 'Jianfeng Gao', 'Xiaodong Liu', 'Bowen Zhou', 'Hongyuan Mei', 'Zhouhan Lin'], 'affiliations': ['Microsoft Research', 'Peking University', 'Renmin University of China', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'Toyota Technological Institute at Chicago', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.15207.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#reasoning', '#training', '#math', '#rl'], 'emoji': '🌊', 'ru': {'title': 'FlowRL: баланс потоков для разнообразного обучения языковых моделей', 'desc': 'Статья представляет FlowRL - новый метод обучения с подкреплением для больших языковых моделей. В отличие от традиционных подходов максимизации награды, FlowRL сопоставляет полное распределение наград через балансировку потоков. Это позволяет улучшить разнообразие и производительность модели по сравнению с методами максимизации награды. Эксперименты на задачах математических и кодовых рассуждений показывают значительное улучшение результатов с использованием FlowRL.'}, 'en': {'title': 'FlowRL: Enhancing LLMs through Reward Distribution Matching', 'desc': 'FlowRL is a novel approach in reinforcement learning for large language models (LLMs) that focuses on matching the entire reward distribution rather than just maximizing rewards. By using flow balancing, it addresses the issue of over-optimization seen in traditional methods like PPO and GRPO, which often ignore less frequent but valid reasoning paths. This method transforms scalar rewards into a normalized target distribution and minimizes the reverse KL divergence, promoting diverse exploration and better reasoning. Experiments show that FlowRL significantly outperforms existing methods on math and code reasoning tasks, demonstrating the importance of reward distribution-matching for effective learning.'}, 'zh': {'title': 'FlowRL：通过流平衡实现多样化推理', 'desc': 'FlowRL是一种增强大型语言模型（LLM）强化学习的方法，通过流平衡匹配完整的奖励分布，而不是单纯最大化奖励。传统的奖励最大化方法（如PPO和GRPO）往往过度优化主要的奖励信号，忽视了较少出现但有效的推理路径，从而降低了多样性。FlowRL通过可学习的分区函数将标量奖励转化为标准化的目标分布，并最小化策略与目标分布之间的反向KL散度。实验结果表明，FlowRL在数学基准测试中平均提高了10.0%，在代码推理任务中表现也更为优越，强调了匹配奖励分布在LLM强化学习中实现高效探索和多样化推理的重要性。'}}}, {'id': 'https://huggingface.co/papers/2509.14760', 'title': 'Reasoning over Boundaries: Enhancing Specification Alignment via\n  Test-time Delibration', 'url': 'https://huggingface.co/papers/2509.14760', 'abstract': "Align3, a lightweight method using Test-Time Deliberation, enhances specification alignment in large language models across diverse scenarios with minimal overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.", 'score': 40, 'issue_id': 5978, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'e9e06e0e548bde84', 'authors': ['Haoran Zhang', 'Yafu Li', 'Xuyang Hu', 'Dongrui Liu', 'Zhilin Wang', 'Bo Li', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'University of Illinois at Urbana-Champaign', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.14760.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#benchmark', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Align3: Точная настройка языковых моделей под пользовательские требования', 'desc': "Статья представляет метод Align3, который использует Test-Time Deliberation для улучшения соответствия спецификациям в больших языковых моделях (LLM). Авторы вводят понятие 'specification alignment' - способность LLM следовать динамическим, сценарно-специфичным спецификациям с точки зрения поведения и безопасности. Для оценки этой способности создан бенчмарк SpecBench, охватывающий 5 сценариев, 103 спецификации и 1500 промптов. Эксперименты показали, что Test-Time Deliberation повышает соответствие спецификациям, а Align3 улучшает баланс между безопасностью и полезностью с минимальными накладными расходами."}, 'en': {'title': 'Aligning Language Models with Dynamic Specifications Efficiently', 'desc': 'The paper introduces Align3, a method that improves how large language models (LLMs) align with specific behavioral and safety requirements in various scenarios. It uses Test-Time Deliberation (TTD) to help models reflect on and adjust their responses according to dynamic specifications. The authors also present SpecBench, a benchmark designed to evaluate how well models adhere to these specifications across multiple scenarios and prompts. The findings demonstrate that Align3 not only enhances alignment but also balances safety and helpfulness with minimal computational cost.'}, 'zh': {'title': 'Align3：轻量级的规范对齐方法', 'desc': 'Align3是一种轻量级的方法，利用测试时深思（Test-Time Deliberation）来增强大型语言模型在多种场景下的规范对齐能力。该方法关注模型在动态、特定场景下遵循用户或组织定制的行为和安全规范的能力。通过引入层次反思和修正，Align3能够在规范边界上进行推理，确保模型的输出符合预期。我们还提出了SpecBench，一个统一的基准，用于测量规范对齐，涵盖了多种场景和规范，帮助识别对齐差距。'}}}, {'id': 'https://huggingface.co/papers/2509.15194', 'title': 'Evolving Language Models without Labels: Majority Drives Selection,\n  Novelty Promotes Variation', 'url': 'https://huggingface.co/papers/2509.15194', 'abstract': "EVOL-RL, a label-free reinforcement learning method, enhances large language models by balancing stability and variation, preventing entropy collapse and improving generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.", 'score': 27, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '271ee6d47770b19f', 'authors': ['Yujun Zhou', 'Zhenwen Liang', 'Haolin Liu', 'Wenhao Yu', 'Kishan Panaganti', 'Linfeng Song', 'Dian Yu', 'Xiangliang Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'University of Notre Dame', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2509.15194.jpg', 'data': {'categories': ['#agi', '#rlhf', '#optimization', '#training', '#rl'], 'emoji': '🧬', 'ru': {'title': 'Эволюция языковых моделей без меток', 'desc': 'Метод EVOL-RL предлагает новый подход к обучению с подкреплением без использования меток для улучшения больших языковых моделей. Он сочетает стабильность и вариативность, предотвращая коллапс энтропии и улучшая обобщающую способность моделей. EVOL-RL использует голосование большинством для стабильности и награду за новизну для поддержания разнообразия. Эксперименты показывают, что EVOL-RL превосходит базовые методы на различных задачах и доменах.'}, 'en': {'title': 'Evolving Language Models with Label-Free Reinforcement Learning', 'desc': 'EVOL-RL is a novel reinforcement learning method designed to enhance large language models without relying on labeled data. It addresses the problem of entropy collapse, where models become less diverse and informative over time. By combining stability through majority voting with a novelty-aware reward system, EVOL-RL encourages exploration and variation in model responses. This approach not only improves generalization across different tasks but also significantly boosts performance metrics compared to existing methods.'}, 'zh': {'title': 'EVOL-RL：无标签强化学习的进化之路', 'desc': 'EVOL-RL是一种无标签的强化学习方法，旨在增强大型语言模型的稳定性和多样性。它通过防止熵崩溃，保持生成内容的多样性，从而提高模型的泛化能力。与传统方法不同，EVOL-RL结合了稳定性和变化性，确保模型在没有外部标签的情况下自我改进。实验结果表明，EVOL-RL在多个任务上表现优于现有的无标签强化学习基线。'}}}, {'id': 'https://huggingface.co/papers/2509.15185', 'title': 'Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation', 'url': 'https://huggingface.co/papers/2509.15185', 'abstract': 'Self-guided Training for AutoRegressive models (ST-AR) enhances image understanding and generation quality in autoregressive models by addressing key visual semantics challenges through self-supervised objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.', 'score': 22, 'issue_id': 5976, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '3da8f7302a159d5f', 'authors': ['Xiaoyu Yue', 'Zidong Wang', 'Yuqing Wang', 'Wenlong Zhang', 'Xihui Liu', 'Wanli Ouyang', 'Lei Bai', 'Luping Zhou'], 'affiliations': ['Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'University of Hong Kong', 'University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2509.15185.jpg', 'data': {'categories': ['#optimization', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Самообучение авторегрессионных моделей улучшает генерацию изображений', 'desc': 'Статья представляет новый метод обучения авторегрессионных моделей для улучшения понимания и генерации изображений - Self-guided Training for AutoRegressive models (ST-AR). Авторы выявили три ключевые проблемы, мешающие обучению высокоуровневой визуальной семантики в таких моделях. Предложенный подход решает эти проблемы с помощью самоконтролируемых целей обучения. ST-AR значительно улучшает качество генерации изображений без использования предобученных моделей представлений.'}, 'en': {'title': 'Enhancing Image Generation with Self-Guided Training', 'desc': "The paper introduces Self-guided Training for AutoRegressive models (ST-AR), a new framework aimed at improving image understanding and generation in autoregressive models. It identifies three main challenges in applying next-token prediction to visual data: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. By incorporating self-supervised objectives during training, ST-AR effectively addresses these challenges, enhancing the model's ability to learn high-level visual semantics. The results show significant improvements in image generation quality, with FID scores increasing by approximately 42% and 49% for different model versions without using pre-trained representations."}, 'zh': {'title': '自指导训练提升图像生成与理解', 'desc': '自指导训练（ST-AR）通过自监督目标解决了自回归模型在图像理解和生成中的关键视觉语义挑战，从而提升了图像理解和生成质量。研究表明，高质量的视觉表示在图像生成中至关重要，而自回归模型在图像理解方面存在局限性。本文首次系统性地探讨了将下一个标记预测范式应用于视觉领域的机制，并识别出影响高层视觉语义学习的三个关键特性。通过在训练过程中引入自监督目标，ST-AR显著提高了自回归模型的图像理解能力，并改善了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2509.13160', 'title': 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning', 'url': 'https://huggingface.co/papers/2509.13160', 'abstract': 'FinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Search has emerged as core infrastructure for LLM-based agents and is widely viewed as critical on the path toward more general intelligence. Finance is a particularly demanding proving ground: analysts routinely conduct complex, multi-step searches over time-sensitive, domain-specific data, making it ideal for assessing both search proficiency and knowledge-grounded reasoning. Yet no existing open financial datasets evaluate data searching capability of end-to-end agents, largely because constructing realistic, complicated tasks requires deep financial expertise and time-sensitive data is hard to evaluate. We present FinSearchComp, the first fully open-source agent benchmark for realistic, open-domain financial search and reasoning. FinSearchComp comprises three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation -- closely reproduce real-world financial analyst workflows. To ensure difficulty and reliability, we engage 70 professional financial experts for annotation and implement a rigorous multi-stage quality-assurance pipeline. The benchmark includes 635 questions spanning global and Greater China markets, and we evaluate 21 models (products) on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy. DouBao (web) leads on the Greater China subset. Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp, and the country origin of models and tools impact performance significantly.By aligning with realistic analyst tasks and providing end-to-end evaluation, FinSearchComp offers a professional, high-difficulty testbed for complex financial search and reasoning.', 'score': 20, 'issue_id': 5975, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'a1faa8bf123c24e6', 'authors': ['Liang Hu', 'Jianpeng Jiao', 'Jiashuo Liu', 'Yanle Ren', 'Zhoufutu Wen', 'Kaiyuan Zhang', 'Xuanliang Zhang', 'Xiang Gao', 'Tianci He', 'Fei Hu', 'Yali Liao', 'Zaiyuan Wang', 'Chenghao Yang', 'Qianyu Yang', 'Mingren Yin', 'Zhiyuan Zeng', 'Ge Zhang', 'Xinyi Zhang', 'Xiying Zhao', 'Zhenwei Zhu', 'Hongseok Namkoong', 'Wenhao Huang', 'Yuwen Tang'], 'affiliations': ['ByteDance', 'Columbia Business School'], 'pdf_title_img': 'assets/pdf/title_img/2509.13160.jpg', 'data': {'categories': ['#science', '#open_source', '#agents', '#reasoning', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinSearchComp: профессиональный тест для оценки финансового ИИ', 'desc': 'FinSearchComp - это открытый эталонный тест для оценки возможностей финансового поиска и рассуждений агентов на основе нейросетей. Он включает три задачи, имитирующие реальные рабочие процессы финансовых аналитиков: поиск актуальных данных, простой исторический поиск и сложное историческое исследование. Тест содержит 635 вопросов по глобальным и китайским рынкам, разработанных с участием 70 профессиональных финансовых экспертов. Эксперименты показали, что оснащение агентов веб-поиском и финансовыми плагинами значительно улучшает результаты на FinSearchComp.'}, 'en': {'title': 'FinSearchComp: Benchmarking AI in Financial Search and Reasoning', 'desc': 'FinSearchComp is an innovative benchmark designed to assess the financial search and reasoning abilities of end-to-end AI agents. It includes realistic tasks that mimic the workflows of financial analysts, such as fetching time-sensitive data and conducting historical investigations. The benchmark is supported by professional annotations from financial experts, ensuring high-quality and relevant evaluation criteria. By testing various models on this benchmark, researchers can better understand the effectiveness of AI in handling complex financial queries and improve their performance through enhanced search capabilities.'}, 'zh': {'title': '金融搜索与推理的专业基准测试', 'desc': 'FinSearchComp是一个开源基准，用于评估端到端智能体在金融搜索和推理方面的能力。该基准包含三个任务，模拟真实的金融分析师工作流程，确保任务的复杂性和可靠性。通过与70位专业金融专家合作进行标注，FinSearchComp提供了635个问题，涵盖全球及大中华市场。实验结果表明，结合网络搜索和金融插件的智能体在FinSearchComp上的表现显著提升，展示了其在复杂金融搜索和推理中的专业性和高难度。'}}}, {'id': 'https://huggingface.co/papers/2509.15212', 'title': 'RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation', 'url': 'https://huggingface.co/papers/2509.15212', 'abstract': 'RynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.', 'score': 11, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '0f4ee15179f6aeef', 'authors': ['Yuming Jiang', 'Siteng Huang', 'Shengke Xue', 'Yaxi Zhao', 'Jun Cen', 'Sicong Leng', 'Kehan Li', 'Jiayan Guo', 'Kexiang Wang', 'Mingxiu Chen', 'Fan Wang', 'Deli Zhao', 'Xin Li'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.15212.jpg', 'data': {'categories': ['#robotics', '#optimization', '#training', '#cv', '#games', '#rl'], 'emoji': '🤖', 'ru': {'title': 'RynnVLA-001: Передовая модель зрения-языка-действия для робототехники', 'desc': 'RynnVLA-001 - это модель зрения-языка-действия (VLA), разработанная для задач робототехники. Модель использует двухэтапный подход предобучения на большом масштабе видеоданных с человеческими демонстрациями. Первый этап включает генеративное предобучение на эгоцентрических видео, а второй этап добавляет предсказание траекторий ключевых точек. Дополнительно предложен вариационный автоэнкодер ActionVAE для улучшения представления действий.'}, 'en': {'title': 'Revolutionizing Robotics with RynnVLA-001: A Vision-Language-Action Breakthrough!', 'desc': "RynnVLA-001 is a vision-language-action model designed to improve robotics tasks through a two-stage pretraining approach. The first stage involves Ego-Centric Video Generative Pretraining, where the model learns to predict future video frames based on initial frames and language instructions using a large dataset of manipulation videos. The second stage, Human-Centric Trajectory-Aware Modeling, enhances the model's ability to predict keypoint trajectories, linking visual predictions with action outcomes. Additionally, the introduction of ActionVAE helps to simplify the action representation by compressing action sequences into compact latent embeddings, leading to better performance on robotics tasks compared to existing models."}, 'zh': {'title': 'RynnVLA-001：提升机器人任务的视觉-语言-动作模型', 'desc': 'RynnVLA-001是一种视觉-语言-动作模型，采用两阶段预训练方法和ActionVAE来提升机器人任务的表现。第一阶段是以自我为中心的视频生成预训练，利用1200万段自我中心的操作视频训练图像到视频模型，以根据初始帧和语言指令预测未来帧。第二阶段是以人为中心的轨迹感知建模，通过联合预测未来关键点轨迹，有效地将视觉帧预测与动作预测结合起来。此外，ActionVAE作为变分自编码器，压缩动作序列为紧凑的潜在嵌入，降低了VLA输出空间的复杂性。'}}}, {'id': 'https://huggingface.co/papers/2509.14476', 'title': 'AToken: A Unified Tokenizer for Vision', 'url': 'https://huggingface.co/papers/2509.14476', 'abstract': 'AToken, a unified visual tokenizer, achieves high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a 4D transformer architecture with adversarial-free training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.', 'score': 9, 'issue_id': 5975, 'pub_date': '2025-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '4904c9c747d48b89', 'authors': ['Jiasen Lu', 'Liangchen Song', 'Mingze Xu', 'Byeongjoo Ahn', 'Yanjun Wang', 'Chen Chen', 'Afshin Dehghan', 'Yinfei Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.14476.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d', '#architecture', '#video', '#training', '#cv', '#benchmark', '#games'], 'emoji': '🎭', 'ru': {'title': 'Единый токенизатор для всех визуальных данных', 'desc': 'AToken - это унифицированный визуальный токенизатор, способный обрабатывать изображения, видео и 3D-объекты. Он использует архитектуру трансформера с 4D-позиционным кодированием для создания общего латентного пространства. AToken обучается без использования генеративно-состязательных сетей, применяя перцептивные потери и потери матрицы Грама. Модель демонстрирует высокие результаты как в задачах реконструкции, так и в задачах семантического понимания для различных визуальных модальностей.'}, 'en': {'title': 'Unified Visual Tokenization for Next-Gen AI', 'desc': "AToken is a novel visual tokenizer that integrates high-fidelity reconstruction and semantic understanding for images, videos, and 3D assets using a 4D transformer architecture. It encodes various visual inputs into a shared latent space, allowing it to handle multiple modalities simultaneously. The model employs an adversarial-free training approach, utilizing perceptual and Gram matrix losses to ensure stable and high-quality outputs. AToken's performance is demonstrated through impressive metrics across different tasks, paving the way for advanced multimodal AI applications."}, 'zh': {'title': '统一视觉标记，重建与理解的完美结合', 'desc': 'AToken是一种统一的视觉标记器，能够在图像、视频和3D资产中实现高保真重建和语义理解。与现有的专注于单一模态的标记器不同，AToken将多样的视觉输入编码到一个共享的4D潜在空间中，从而统一了重建和理解任务。该方法采用纯变换器架构和4D旋转位置嵌入，能够处理任意分辨率和时间长度的视觉输入。通过无对抗训练目标，结合感知损失和Gram矩阵损失，AToken在多个基准测试中表现出色，推动了下一代多模态AI系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2509.15130', 'title': 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model\n  via Training-Free Guidance', 'url': 'https://huggingface.co/papers/2509.15130', 'abstract': "WorldForge, a training-free framework, enhances video diffusion models with precise motion control and photorealistic content generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", 'score': 6, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'f91495ad752f8344', 'authors': ['Chenxi Song', 'Yanming Yang', 'Tong Zhao', 'Ruibo Li', 'Chi Zhang'], 'affiliations': ['AGI Lab, School of Engineering, Westlake University, Hangzhou, China', 'The College of Computing and Data Science, Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.15130.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#inference', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'WorldForge: Точный контроль движения в видео-диффузионных моделях без переобучения', 'desc': 'WorldForge - это фреймворк для улучшения видео-диффузионных моделей без дополнительного обучения. Он обеспечивает точный контроль движения и фотореалистичную генерацию контента с помощью рекурсивного уточнения, слияния латентных пространств на основе оптического потока и двухпутевого самокорректирующего управления. WorldForge решает проблемы ограниченной управляемости и геометрической несогласованности существующих моделей. Этот подход позволяет эффективно использовать богатые латентные представления предобученных моделей для задач пространственного интеллекта.'}, 'en': {'title': 'WorldForge: Training-Free Control for Photorealistic Video Synthesis', 'desc': 'WorldForge is a novel framework that enhances video diffusion models without the need for retraining. It introduces three key modules: Intra-Step Recursive Refinement for optimizing predictions during inference, Flow-Gated Latent Fusion for separating motion from appearance, and Dual-Path Self-Corrective Guidance to correct trajectory drift. These components work together to provide precise motion control and generate photorealistic content. The framework demonstrates significant improvements in realism and consistency, making it a valuable tool for controllable video synthesis.'}, 'zh': {'title': '无训练的视频合成新范式', 'desc': 'WorldForge是一个无需训练的框架，通过递归优化、流门控潜在融合和双路径自我校正指导，增强了视频扩散模型的运动控制和真实感内容生成。该方法解决了现有视频扩散模型在可控性和几何一致性方面的不足，避免了重新训练带来的知识退化和高计算成本。通过在推理过程中引入递归优化机制，WorldForge能够精确地注入运动轨迹。实验结果表明，该方法在真实感、轨迹一致性和视觉保真度方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2509.14638', 'title': 'MultiEdit: Advancing Instruction-based Image Editing on Diverse and\n  Challenging Tasks', 'url': 'https://huggingface.co/papers/2509.14638', 'abstract': "MultiEdit, a comprehensive dataset with over 107K high-quality image editing samples, improves performance on sophisticated editing tasks using a novel pipeline with multi-modal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.", 'score': 4, 'issue_id': 5975, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '35e0066dc539ba98', 'authors': ['Mingsong Li', 'Lin Liu', 'Hongjun Wang', 'Haoxing Chen', 'Xijun Gu', 'Shizhan Liu', 'Dong Gong', 'Junbo Zhao', 'Zhenzhong Lan', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'The University of Hong Kong', 'University of New South Wales', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.14638.jpg', 'data': {'categories': ['#open_source', '#dataset', '#cv', '#benchmark', '#games'], 'emoji': '🖼️', 'ru': {'title': 'MultiEdit: революция в редактировании изображений с помощью ИИ', 'desc': 'Статья представляет MultiEdit - новый набор данных, содержащий более 107 тысяч высококачественных образцов редактирования изображений. Набор данных охватывает 6 сложных задач редактирования через 18 типов редактирования без переноса стиля и 38 операций переноса стиля. Для создания набора данных используется новый конвейер с двумя мультимодальными большими языковыми моделями для генерации инструкций по редактированию и создания отредактированных изображений. Эксперименты показывают, что дообучение моделей на MultiEdit значительно улучшает их производительность в сложных задачах редактирования.'}, 'en': {'title': 'MultiEdit: Elevating Image Editing with a Rich Dataset', 'desc': 'The paper introduces MultiEdit, a new dataset designed to enhance instruction-based image editing (IBIE) methods by providing over 107,000 high-quality image editing samples. It addresses the limitations of existing datasets, which often contain noisy image-caption pairs and lack diversity in editing tasks. MultiEdit includes six challenging editing tasks and a variety of editing types, from style transfer to complex semantic operations. By utilizing multi-modal large language models for dataset construction, the paper demonstrates that fine-tuning models with MultiEdit significantly improves their performance on complex editing tasks while maintaining their effectiveness on standard benchmarks.'}, 'zh': {'title': 'MultiEdit：提升复杂图像编辑能力的全新数据集', 'desc': 'MultiEdit是一个包含超过107K高质量图像编辑样本的综合数据集，旨在提升复杂编辑任务的性能。该数据集涵盖了6种具有挑战性的编辑任务，包含18种非风格转移编辑类型和38种风格转移操作。通过使用多模态大型语言模型（MLLMs），我们构建了一种新颖的数据集生成管道，以生成视觉适应的编辑指令并制作高保真编辑图像。实验结果表明，使用MultiEdit训练集微调基础开源模型显著提高了模型在复杂编辑任务上的表现，同时有效保留了其在标准编辑基准上的能力。'}}}, {'id': 'https://huggingface.co/papers/2509.15178', 'title': 'Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding', 'url': 'https://huggingface.co/papers/2509.15178', 'abstract': "A zero-shot framework using multimodal large language models for spatio-temporal video grounding employs decomposed spatio-temporal highlighting and temporal-augmented assembling strategies to improve grounding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.", 'score': 2, 'issue_id': 5976, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'f4b09bebd69f88b8', 'authors': ['Zaiquan Yang', 'Yuhao Liu', 'Gerhard Hancke', 'Rynson W. H. Lau'], 'affiliations': ['Department of Computer Science, City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.15178.jpg', 'data': {'categories': ['#video', '#games', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Улучшение пространственно-временной локализации в видео с помощью мультимодальных языковых моделей', 'desc': 'Статья представляет новый подход к задаче пространственно-временной локализации объектов в видео (STVG) с использованием мультимодальных больших языковых моделей (MLLM). Авторы предлагают безэталонную (zero-shot) систему, включающую стратегии декомпозированного пространственно-временного выделения (DSTH) и сборки с временным расширением (TAS). DSTH разделяет запрос на подзапросы по атрибутам и действиям, используя модуль LRA для обучения латентных переменных. TAS улучшает временную согласованность, собирая предсказания с использованием оригинальных и временно-расширенных кадров.'}, 'en': {'title': 'Enhancing Video Grounding with Multimodal Language Models', 'desc': "This paper presents a zero-shot framework for spatio-temporal video grounding (STVG) using multimodal large language models (MLLMs). The authors identify that MLLMs can dynamically assign grounding tokens but often struggle with integrating all relevant cues from text queries. To address this, they introduce two innovative strategies: decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS), which enhance the model's reasoning capabilities. The proposed methods improve grounding accuracy by effectively separating and utilizing attributes and actions from queries while ensuring temporal consistency in predictions."}, 'zh': {'title': '多模态模型助力时空视频精准定位', 'desc': '这篇论文提出了一种基于多模态大型语言模型的零-shot框架，用于时空视频定位。研究表明，多模态大型语言模型在处理文本查询时，能够动态分配特定的标记来进行定位，但在整合文本中的线索时常常表现不佳。为了解决这个问题，论文引入了分解时空高亮和时间增强组装策略，以提高定位的准确性。通过这些创新方法，研究展示了该框架在多个基准测试中超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2509.15020', 'title': 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question\n  Answering with LLMs', 'url': 'https://huggingface.co/papers/2509.15020', 'abstract': 'Tokenizing the space with the answer letter in multiple-choice question answering improves LLM accuracy and calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string "Answer:" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model\'s confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.', 'score': 2, 'issue_id': 5984, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '6e87639367d48430', 'authors': ['Mario Sanz-Guerrero', 'Minh Duc Bui', 'Katharina von der Wense'], 'affiliations': ['Johannes Gutenberg University Mainz, Germany', 'University of Colorado Boulder, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.15020.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#benchmark', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Маленькая токенизация - большая разница в оценке языковых моделей', 'desc': 'Исследование показывает, что токенизация пробела вместе с буквой ответа в задачах множественного выбора значительно улучшает точность и калибровку больших языковых моделей (LLM). Эта, казалось бы, незначительная деталь может привести к разнице в точности до 11% и изменить рейтинги моделей. Авторы рекомендуют использовать эту стратегию токенизации для более надежной оценки LLM. Исследование подчеркивает важность тщательного проектирования методов оценки и стандартизации протоколов для обеспечения сопоставимых результатов.'}, 'en': {'title': 'Tokenization Matters: Boosting LLM Accuracy in MCQA!', 'desc': 'This paper investigates the impact of tokenization strategies on the performance of large language models (LLMs) in multiple-choice question answering (MCQA). It reveals that the way space is tokenized after the prompt can lead to significant accuracy differences, affecting model rankings by up to 11%. The authors propose a specific method of tokenizing the space along with the answer letter, which consistently improves both accuracy and model calibration. These findings emphasize the necessity for standardized evaluation practices to ensure the reliability of LLM comparisons.'}, 'zh': {'title': '优化分词提升LLM准确性与校准性', 'desc': '本文探讨了在多选题问答中，如何处理冒号后空格的分词对大型语言模型（LLM）准确性和校准的影响。研究发现，采用不同的分词方式可能导致准确率差异高达11%，并且可能改变模型排名，影响LLM比较的可靠性。我们推荐将空格与答案字母一起分词，这种方法在性能上表现出一致且显著的提升，同时也改善了模型的校准性。我们的研究强调了评估设计的重要性，并呼吁建立标准化和透明的评估协议，以确保结果的可靠性和可比性。'}}}, {'id': 'https://huggingface.co/papers/2509.14977', 'title': 'EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence', 'url': 'https://huggingface.co/papers/2509.14977', 'abstract': 'EchoVLM, a vision-language model with a Mixture of Experts architecture, improves ultrasound report generation and diagnosis by leveraging data from multiple anatomical regions.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM.', 'score': 0, 'issue_id': 5980, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '36663a681181ac46', 'authors': ['Chaoyin She', 'Ruifang Lu', 'Lida Chen', 'Wei Wang', 'Qinghua Huang'], 'affiliations': ['Northwestern Polytechnical University', 'The First Affiliated Hospital of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2509.14977.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#games', '#cv', '#open_source', '#training', '#science', '#dataset'], 'emoji': '🔊', 'ru': {'title': 'EchoVLM: Умный помощник для ультразвуковой диагностики', 'desc': 'EchoVLM - это модель машинного обучения для анализа ультразвуковых изображений, использующая архитектуру Mixture of Experts. Она обучена на данных из семи анатомических областей и способна выполнять несколько задач, включая генерацию отчетов, диагностику и визуальный вопросно-ответный анализ. EchoVLM значительно превосходит существующие модели в задаче генерации ультразвуковых отчетов, улучшая показатели BLEU-1 и ROUGE-1. Эта модель имеет большой потенциал для повышения точности диагностики в ультразвуковой визуализации.'}, 'en': {'title': 'Revolutionizing Ultrasound Diagnosis with EchoVLM', 'desc': 'EchoVLM is a specialized vision-language model designed to enhance ultrasound report generation and diagnosis by utilizing a Mixture of Experts (MoE) architecture. This model is trained on diverse data from seven anatomical regions, allowing it to effectively handle multiple tasks such as report generation, diagnosis, and visual question-answering. The results show that EchoVLM significantly outperforms existing models, achieving notable improvements in BLEU-1 and ROUGE-1 scores for ultrasound report generation. This advancement indicates that EchoVLM can greatly improve diagnostic accuracy in ultrasound imaging, making it a promising tool for clinical applications.'}, 'zh': {'title': 'EchoVLM：提升超声诊断的智能助手', 'desc': 'EchoVLM是一种专门为超声医学成像设计的视觉-语言模型，采用混合专家架构。该模型通过利用来自七个解剖区域的数据，显著提高了超声报告生成和诊断的效率。实验结果显示，EchoVLM在超声报告生成任务中，相较于Qwen2-VL，BLEU-1和ROUGE-1得分分别提高了10.15和4.77分。这表明EchoVLM在提高超声成像诊断准确性方面具有重要潜力，为未来的临床应用提供了可行的技术解决方案。'}}}, {'id': 'https://huggingface.co/papers/2509.06482', 'title': 'FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution\n  Remote Sensing Change Detection', 'url': 'https://huggingface.co/papers/2509.06482', 'abstract': 'FSG-Net addresses false alarms and semantic gaps in change detection by using a frequency-spatial synergistic approach with wavelet interaction, attention mechanisms, and gated fusion.  \t\t\t\t\tAI-generated summary \t\t\t\t Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication.', 'score': 0, 'issue_id': 5976, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '89822775d7f8c973', 'authors': ['Zhongxiang Xie', 'Shuangxi Miao', 'Yuhan Jiang', 'Zhewei Zhang', 'Jing Yao', 'Xuecao Li', 'Jianxi Huang', 'Pedram Ghamisi'], 'affiliations': ['Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China', 'College of Land Science and Technology, China Agricultural University, Beijing 100193, China', 'Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China', 'Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany', 'Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K.'], 'pdf_title_img': 'assets/pdf/title_img/2509.06482.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': '🛰️', 'ru': {'title': 'FSG-Net: точное обнаружение изменений на спутниковых снимках', 'desc': 'FSG-Net - это новая нейросетевая архитектура для обнаружения изменений на спутниковых снимках высокого разрешения. Она использует вейвлет-преобразование и механизмы внимания для уменьшения ложных срабатываний и улучшения семантического анализа. FSG-Net включает в себя модули DAWIM для обработки частотных компонент, STSAM для усиления важных пространственных областей и LGFU для объединения признаков разного уровня. Эксперименты показали, что FSG-Net превосходит существующие методы на нескольких наборах данных по обнаружению изменений.'}, 'en': {'title': 'Enhancing Change Detection with FSG-Net: Bridging Gaps and Reducing False Alarms', 'desc': 'FSG-Net is a novel approach designed to improve change detection in high-resolution remote sensing images by addressing false alarms and semantic gaps. It utilizes a frequency-spatial synergistic method that includes a Discrepancy-Aware Wavelet Interaction Module to reduce misinterpretations caused by radiometric variations. The model further enhances feature representation through a Synergistic Temporal-Spatial Attention Module, which focuses on highlighting genuine changes. Finally, a Lightweight Gated Fusion Unit effectively integrates high-level semantic information with detailed features, achieving state-of-the-art performance on several benchmarks.'}, 'zh': {'title': 'FSG-Net：精准变化检测的新方法', 'desc': 'FSG-Net是一种新颖的网络模型，旨在解决高分辨率遥感图像变化检测中的假警报和语义差距问题。该模型采用频率-空间协同的方法，通过小波交互模块和注意力机制，有效区分真实变化与干扰变化。FSG-Net首先在频率域中处理不同频率成分，以减少伪变化的影响，然后在空间域中增强真实变化区域的显著性。最后，通过轻量级门控融合单元，FSG-Net将高层语义与低层细节有效结合，显著提高了变化检测的准确性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (8)', '#cv (7)', '#data (1)', '#dataset (3)', '#diffusion (1)', '#ethics', '#games (5)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (2)', '#open_source (4)', '#optimization (6)', '#plp', '#rag', '#reasoning (4)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (8)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-19 12:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-19 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-19 12:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    