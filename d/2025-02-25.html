
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. February 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-24.html">â¬…ï¸ <span id="prev-date">24.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-26.html">â¡ï¸ <span id="next-date">26.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'};
        let feedDateNext = {'ru': '26.02', 'en': '02/26', 'zh': '2æœˆ26æ—¥'};
        let feedDatePrev = {'ru': '24.02', 'en': '02/24', 'zh': '2æœˆ24æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.17129', 'title': 'Thus Spake Long-Context Large Language Model', 'url': 'https://huggingface.co/papers/2502.17129', 'abstract': 'Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.   Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.', 'score': 46, 'issue_id': 2387, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '8b44dbb5e39d9b38', 'authors': ['Xiaoran Liu', 'Ruixiao Li', 'Mianqiu Huang', 'Zhigeng Liu', 'Yuerong Song', 'Qipeng Guo', 'Siyang He', 'Qiqi Wang', 'Linlin Li', 'Qun Liu', 'Yaqian Zhou', 'Xuanjing Huang', 'Xipeng Qiu'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2502.17129.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#survey', '#training', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€ĞµĞ¼Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ LLM Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞ²Ğ¾Ñ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ 10 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking the Power of Long Context in Language Models', 'desc': "This paper discusses the importance of long context in Natural Language Processing (NLP) and its impact on Large Language Models (LLMs). It highlights the advancements in extending context length to millions of tokens, which enhances the models' capabilities. The authors explore the challenges faced by LLMs in balancing the need for longer context with their inherent limitations. Additionally, the paper provides a comprehensive overview of the lifecycle of long-context LLMs, covering aspects such as architecture, infrastructure, training, and evaluation, while also posing ten critical questions for future research."}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç«äº‰åŠ›', 'desc': 'é•¿ä¸Šä¸‹æ–‡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦ä¸»é¢˜ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡è¿½æ±‚é•¿ä¸Šä¸‹æ–‡é¢ä¸´è®¸å¤šæŒ‘æˆ˜ï¼Œä½†å®ƒä»ç„¶æ˜¯LLMsçš„æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ã€‚è¿‘å¹´æ¥ï¼ŒLLMsçš„ä¸Šä¸‹æ–‡é•¿åº¦å·²çªç ´åˆ°æ•°ç™¾ä¸‡ä¸ªæ ‡è®°ï¼Œç ”ç©¶ä¹Ÿä»é•¿åº¦å»¶å±•æ‰©å±•åˆ°æ¶æ„ã€åŸºç¡€è®¾æ–½ã€è®­ç»ƒå’Œè¯„ä¼°æŠ€æœ¯çš„å…¨é¢å…³æ³¨ã€‚æœ¬æ–‡å°†ä»å››ä¸ªè§’åº¦å±•ç¤ºé•¿ä¸Šä¸‹æ–‡LLMsçš„ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶æå‡ºå½“å‰é¢ä¸´çš„åä¸ªæœªè§£é—®é¢˜ï¼Œä»¥æœŸä¸ºé•¿ä¸Šä¸‹æ–‡LLMsçš„ç ”ç©¶æä¾›ç³»ç»Ÿæ€§çš„ä»‹ç»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17258', 'title': 'VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing', 'url': 'https://huggingface.co/papers/2502.17258', 'abstract': "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/", 'score': 40, 'issue_id': 2389, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'aaaaf48d4594432f', 'authors': ['Xiangpeng Yang', 'Linchao Zhu', 'Hehe Fan', 'Yi Yang'], 'affiliations': ['ReLER Lab, AAII, University of Technology Sydney', 'ReLER Lab, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17258.jpg', 'data': {'categories': ['#diffusion', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'VideoGrain - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoGrain Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Fine-Grained Control for Enhanced Video Editing with VideoGrain', 'desc': 'This paper introduces VideoGrain, a novel approach to enhance video generation and editing using diffusion models. It tackles the challenges of multi-grained video editing by improving the control over different levels of video content, such as class, instance, and part modifications. The method employs advanced attention mechanisms to ensure that local prompts effectively target specific regions while minimizing irrelevant interactions. Experimental results show that VideoGrain outperforms existing methods, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'VideoGrainï¼šç²¾ç»†æ§åˆ¶è§†é¢‘ç¼–è¾‘çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVideoGrainçš„é›¶-shotæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šç²’åº¦è§†é¢‘ç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è°ƒèŠ‚æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¯¹è§†é¢‘å†…å®¹çš„ç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬å¢å¼ºäº†æ–‡æœ¬åˆ°åŒºåŸŸçš„æ§åˆ¶ï¼Œç¡®ä¿æ¯ä¸ªå±€éƒ¨æç¤ºçš„æ³¨æ„åŠ›é›†ä¸­åœ¨ç›¸åº”çš„ç©ºé—´åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘ä¸æ— å…³åŒºåŸŸçš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æé«˜åŒºåŸŸå†…çš„æ„è¯†å’Œå‡å°‘åŒºåŸŸé—´çš„å¹²æ‰°ï¼Œæ”¹å–„äº†ç‰¹å¾åˆ†ç¦»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17157', 'title': 'DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks', 'url': 'https://huggingface.co/papers/2502.17157', 'abstract': 'Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.', 'score': 36, 'issue_id': 2387, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'f8790b30bb553397', 'authors': ['Canyu Zhao', 'Mingyu Liu', 'Huanyi Zheng', 'Muzhi Zhu', 'Zhiyue Zhao', 'Hao Chen', 'Tong He', 'Chunhua Shen'], 'affiliations': ['Shanghai AI Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17157.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#diffusion', '#cv', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DICEPTION - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. DICEPTION ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. DICEPTION Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'DICEPTION: Efficient Generalist Perception with Minimal Data', 'desc': "This paper presents DICEPTION, a versatile perception model designed to perform multiple tasks efficiently while minimizing the need for extensive computational resources and training data. By leveraging pre-trained text-to-image diffusion models, DICEPTION achieves competitive performance on various perception tasks using only a fraction of the data required by traditional models. The innovative use of color encoding for output representation enhances the model's effectiveness in both entity and semantic segmentation. Overall, DICEPTION demonstrates that it is possible to create a powerful generalist model that requires minimal fine-tuning and can adapt quickly to new tasks."}, 'zh': {'title': 'DICEPTIONï¼šé«˜æ•ˆçš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹', 'desc': 'æœ¬æ–‡çš„ä¸»è¦ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªé€šç”¨çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å¤„ç†å¤šç§ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨äº†åœ¨æ•°åäº¿å¼ å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒDICEPTIONåœ¨å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨é€‚åº”å…¶ä»–ä»»åŠ¡æ—¶ï¼Œä»…éœ€å¯¹50å¼ å›¾åƒå’Œ1%çš„å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§å’Œæ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15814', 'title': 'Slamming: Training a Speech Language Model on One GPU in a Day', 'url': 'https://huggingface.co/papers/2502.15814', 'abstract': 'We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .', 'score': 32, 'issue_id': 2388, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 19', 'zh': '2æœˆ19æ—¥'}, 'hash': '735714e237272170', 'authors': ['Gallil Maimon', 'Avishai Elmakies', 'Yossi Adi'], 'affiliations': ['The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2502.15814.jpg', 'data': {'categories': ['#audio', '#synthetic', '#data', '#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Slam - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ GPU Ğ·Ğ° 24 Ñ‡Ğ°ÑĞ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… SLM Ğ¿Ñ€Ğ¸ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ´ĞµĞ»Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ SLM Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Slam: Fast and Efficient Speech Language Model Training', 'desc': 'This paper presents Slam, a method for efficiently training high-quality Speech Language Models (SLMs) using a single academic GPU within 24 hours. The authors analyze various factors such as model initialization, architecture, and synthetic training data to optimize the training process. Their empirical results show that Slam not only achieves competitive performance compared to leading SLMs but also does so at a significantly lower computational cost. This research aims to make SLM training more accessible and demonstrates that it can exceed expected performance based on scaling laws.'}, 'zh': {'title': 'Slamï¼šé«˜æ•ˆè®­ç»ƒè¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºSlamçš„è®­ç»ƒé«˜è´¨é‡è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥åœ¨24å°æ—¶å†…ä½¿ç”¨å•ä¸ªå­¦æœ¯GPUå®Œæˆã€‚é€šè¿‡å¯¹æ¨¡å‹åˆå§‹åŒ–ã€æ¶æ„ã€åˆæˆè®­ç»ƒæ•°æ®å’Œåå¥½ä¼˜åŒ–ç­‰æ–¹é¢çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†è¿™ä¸€è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œè¿™ç§è®­ç»ƒæ–¹æ¡ˆåœ¨è®¡ç®—èµ„æºå¢åŠ æ—¶ä¹Ÿèƒ½è‰¯å¥½æ‰©å±•ï¼Œä¸”åœ¨è®¡ç®—æˆæœ¬ä¸Šè¿œä½äºé¢†å…ˆçš„SLMã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›è§è§£èƒ½ä½¿SLMçš„è®­ç»ƒå’Œç ”ç©¶å˜å¾—æ›´åŠ å¯åŠã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16584', 'title': 'Audio-FLAN: A Preliminary Release', 'url': 'https://huggingface.co/papers/2502.16584', 'abstract': 'Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.', 'score': 22, 'issue_id': 2388, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': '67b5d61b3df1a4bc', 'authors': ['Liumeng Xue', 'Ziya Zhou', 'Jiahao Pan', 'Zixuan Li', 'Shuai Fan', 'Yinghao Ma', 'Sitong Cheng', 'Dongchao Yang', 'Haohan Guo', 'Yujia Xiao', 'Xinsheng Wang', 'Zixuan Shen', 'Chuanbo Zhu', 'Xinshen Zhang', 'Tianchi Liu', 'Ruibin Yuan', 'Zeyue Tian', 'Haohe Liu', 'Emmanouil Benetos', 'Ge Zhang', 'Yike Guo', 'Wei Xue'], 'affiliations': ['Beihang University', 'Inner Mongolia University', 'National University of Singapore', 'Queen Mary University of London', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.16584.jpg', 'data': {'categories': ['#audio', '#dataset'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Audio-FLAN - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 80 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€ĞµÑ‡Ğ¸, Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ğ·Ğ²ÑƒĞºĞ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Audio-FLAN ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° HuggingFace Ğ¸ GitHub Ğ¸ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒÑÑ.'}, 'en': {'title': 'Unifying Audio Understanding and Generation with Audio-FLAN', 'desc': 'This paper presents Audio-FLAN, a large-scale dataset designed for instruction tuning in audio tasks. It addresses the challenge of integrating audio understanding and generation into unified audio-language models. By providing over 100 million instances across 80 diverse tasks, Audio-FLAN enables models to perform both comprehension and generation tasks in a zero-shot manner. The dataset aims to enhance the capabilities of large language models in handling various audio domains effectively.'}, 'zh': {'title': 'éŸ³é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€ä¹‹è·¯', 'desc': 'æœ€è¿‘éŸ³é¢‘æ ‡è®°æŠ€æœ¯çš„è¿›æ­¥æ˜¾è‘—æå‡äº†éŸ³é¢‘èƒ½åŠ›ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•´åˆã€‚ç„¶è€Œï¼ŒéŸ³é¢‘ç†è§£å’Œç”Ÿæˆé€šå¸¸è¢«è§†ä¸ºä¸åŒçš„ä»»åŠ¡ï¼Œè¿™é˜»ç¢äº†çœŸæ­£ç»Ÿä¸€çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚å°½ç®¡æŒ‡ä»¤è°ƒä¼˜åœ¨æ–‡æœ¬å’Œè§†è§‰é¢†åŸŸçš„æ³›åŒ–å’Œé›¶æ ·æœ¬å­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨éŸ³é¢‘é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Audio-FLANï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–80ä¸ªå¤šæ ·åŒ–ä»»åŠ¡çš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1äº¿ä¸ªå®ä¾‹ï¼Œä¸ºç»Ÿä¸€çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17435', 'title': 'GCC: Generative Color Constancy via Diffusing a Color Checker', 'url': 'https://huggingface.co/papers/2502.17435', 'abstract': "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. GCC demonstrates superior robustness in cross-camera scenarios, achieving state-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in bi-directional evaluations. These results highlight our method's stability and generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile solution for real-world applications.", 'score': 18, 'issue_id': 2390, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '896a996c71c167eb', 'authors': ['Chen-Wei Chang', 'Cheng-De Fan', 'Chia-Che Chang', 'Yi-Chen Lo', 'Yu-Chee Tseng', 'Jiun-Long Huang', 'Yu-Lun Liu'], 'affiliations': ['MediaTek Inc.', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17435.jpg', 'data': {'categories': ['#inference', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ĞºĞ°Ğ¼ĞµÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GCC Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. GCC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑˆĞºĞ°Ğ», Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ›Ğ°Ğ¿Ğ»Ğ°ÑĞ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑˆĞºĞ°Ğ»Ñ‹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ° Ğº Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. GCC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ Ğ² Ğ½Ğ°Ğ¸Ñ…ÑƒĞ´ÑˆĞ¸Ñ… 25% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ².'}, 'en': {'title': 'GCC: Robust Color Constancy Across Cameras', 'desc': 'This paper introduces GCC, a novel method for achieving color constancy in images taken with different camera sensors. It utilizes diffusion models to inpaint color checkers, which helps in estimating the illumination of the scene. Key innovations include a single-step inference process for inpainting, a technique to maintain the structure of checkers while adapting colors based on illumination, and a data augmentation strategy to improve the handling of color checker annotations. The results show that GCC outperforms existing methods in challenging cross-camera scenarios, demonstrating its robustness and ability to generalize without needing specific training for each camera.'}, 'zh': {'title': 'GCCï¼šè·¨ç›¸æœºåœºæ™¯ä¸­çš„é¢œè‰²æ’å¸¸æ€§æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢œè‰²æ’å¸¸æ€§æ–¹æ³•GCCï¼Œæ—¨åœ¨è§£å†³ä¸åŒç›¸æœºä¼ æ„Ÿå™¨ä¹‹é—´çš„æ³›åŒ–é—®é¢˜ã€‚GCCåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†é¢œè‰²æ£‹ç›˜æ ¼å¡«å……åˆ°å›¾åƒä¸­ï¼Œä»¥ä¼°è®¡å…‰ç…§æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å•æ­¥ç¡®å®šæ€§æ¨ç†ã€æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯å’ŒåŸºäºæ©ç çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç¡®ä¿äº†åœ¨ä¸åŒå…‰ç…§ä¸‹çš„é¢œè‰²é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCCåœ¨è·¨ç›¸æœºåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¼˜è¶Šçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16614', 'title': 'CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2502.16614', 'abstract': 'The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.', 'score': 18, 'issue_id': 2386, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': 'c70e868ad4c1b726', 'authors': ['Alexander Zhang', 'Marcus Dong', 'Jiaheng Liu', 'Wei Zhang', 'Yejie Wang', 'Jian Yang', 'Ge Zhang', 'Tianyu Liu', 'Zhongyuan Peng', 'Yingshui Tan', 'Yuanxing Zhang', 'Zhexu Wang', 'Weixun Wang', 'Yancheng He', 'Ken Deng', 'Wangchunshu Zhou', 'Wenhao Huang', 'Zhaoxiang Zhang'], 'affiliations': ['Alibaba', 'BUAA', 'BUPT', 'CASIA', 'Kuaishou', 'M-A-P', 'NJU', 'OPPO'], 'pdf_title_img': 'assets/pdf/title_img/2502.16614.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'CodeCriticBench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeCriticBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. CodeCriticBench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ LLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°.'}, 'en': {'title': 'Enhancing Code Critique with CodeCriticBench', 'desc': 'This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°LLMsçš„ä»£ç æ‰¹è¯„èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ‰¹è¯„èƒ½åŠ›å¯¹äºæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¯ä»¥æä¾›å¿…è¦çš„å»ºè®®ï¼Œå¦‚è¯¦ç»†åˆ†æå’Œå»ºè®¾æ€§åé¦ˆã€‚ä¸ºäº†è¯„ä¼°LLMsçš„æ‰¹è¯„èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šä¸ªæ‰¹è¯„åŸºå‡†ï¼Œä½†ç°æœ‰åŸºå‡†å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚å¯¹ä»£ç ä»»åŠ¡çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ä»£ç æ‰¹è¯„åŸºå‡†ï¼Œç§°ä¸ºCodeCriticBenchï¼Œæ¶µç›–äº†ä»£ç ç”Ÿæˆå’Œä»£ç é—®ç­”ä¸¤ç§ä¸»æµä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ç»†è‡´çš„è¯„ä¼°æ ‡å‡†ã€‚é€šè¿‡å¯¹ç°æœ‰LLMsçš„å¹¿æ³›å®éªŒç»“æœï¼Œæˆ‘ä»¬éªŒè¯äº†CodeCriticBenchçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17407', 'title': 'Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2502.17407', 'abstract': 'Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.', 'score': 14, 'issue_id': 2388, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'f77ddcdca8181036', 'authors': ['Guijin Son', 'Jiwoo Hong', 'Hyunwoo Ko', 'James Thorne'], 'affiliations': ['KAIST AI', 'OneLineAI', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17407.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#long_context', '#multilingual', '#benchmark', '#math'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¯Ğœ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MCLM - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ° 55 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Qwen2.5-1.5B Math Ğ¸ MR1-1.5B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ 'Ğ´ÑƒĞ¼Ğ°ÑÑ‰Ğ¸Ñ…' Ğ¯Ğœ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ."}, 'en': {'title': 'Exploring Test-Time Scaling for Multilingual LLMs', 'desc': 'This paper investigates the effectiveness of test-time scaling methods for multilingual large language models (LLMs) using a new benchmark called MCLM, which includes math problems in 55 languages. The authors evaluate three scaling techniques: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF) on two LLMs, Qwen2.5-1.5B Math and MR1-1.5B. Results indicate that ORM with Qwen2.5-1.5B Math achieves the highest score, while BF shows limited improvement across languages compared to English. The findings suggest that traditional scaling methods may perform similarly to advanced LLMs when inference resources are equal, and that test-time scaling may not generalize well to multilingual tasks.'}, 'zh': {'title': 'å¤šè¯­è¨€æ•°å­¦åŸºå‡†ä¸æµ‹è¯•æ—¶é—´æ‰©å±•çš„æ¢ç´¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMCLMçš„å¤šè¯­è¨€æ•°å­¦åŸºå‡†ï¼Œæ¶µç›–55ç§è¯­è¨€çš„ç«äº‰çº§é—®é¢˜ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸‰ç§æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼šç»“æœå¥–åŠ±å»ºæ¨¡ï¼ˆORMï¼‰ã€è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼ˆORMï¼‰å’Œé¢„ç®—å¼ºåˆ¶ï¼ˆBFï¼‰ï¼Œå¹¶åœ¨ä¸¤ä¸ªå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Qwen2.5-1.5B Mathä¸ORMç»“åˆæ—¶ï¼Œåœ¨MCLMä¸Šå¾—åˆ†ä¸º35.8ï¼Œè€ŒMR1-1.5Båœ¨BFä¸‹å¾—åˆ†ä¸º35.2ã€‚å°½ç®¡â€œæ€è€ƒå‹å¤§è¯­è¨€æ¨¡å‹â€å—åˆ°å…³æ³¨ï¼Œä½†æˆ‘ä»¬å‘ç°å…¶æ€§èƒ½ä¸ä¼ ç»Ÿçš„æ‰©å±•æ–¹æ³•ç›¸å½“ï¼Œä¸”æµ‹è¯•æ—¶é—´æ‰©å±•åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šçš„æ•ˆæœä¸å¦‚é¢„æœŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16894', 'title': 'Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment', 'url': 'https://huggingface.co/papers/2502.16894', 'abstract': "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose Great LoRA Mixture-of-Expert (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.", 'score': 13, 'issue_id': 2387, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '020c0f54f92a9238', 'authors': ['Chenghao Fan', 'Zhenyi Lu', 'Sichen Liu', 'Xiaoye Qu', 'Wei Wei', 'Chengfeng Gu', 'Yu Cheng'], 'affiliations': ['School of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2502.16894.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning'], 'emoji': 'ğŸ', 'ru': {'title': 'GOAT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸', 'desc': 'Ğ­Ñ‚a ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GOAT (Great LoRA Mixture-of-Expert) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (LoRA) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. GOAT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SVD-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ LoRA Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 25 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GOAT Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Boosting LoRA with GOAT: A New Path to Efficiency in LLMs', 'desc': 'This paper introduces Great LoRA Mixture-of-Expert (GOAT), a new framework designed to enhance the performance of Low-Rank Adaptation (LoRA) for Large Language Models (LLMs). GOAT addresses the limitations of existing methods by integrating adaptive priors through a singular value decomposition (SVD)-structured Mixture-of-Experts (MoE) architecture. It also aligns the optimization process with that of fully fine-tuned MoE models by introducing a theoretical scaling factor. The results show that GOAT significantly improves efficiency and performance across various tasks, effectively bridging the gap between LoRA and Full Fine-Tuning.'}, 'zh': {'title': 'æå‡LoRAæ€§èƒ½çš„å…¨æ–°æ¡†æ¶ï¼šGOAT', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGreat LoRA Mixture-of-Expertï¼ˆGOATï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ€§èƒ½ã€‚GOATé€šè¿‡è‡ªé€‚åº”æ•´åˆç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ï¼Œä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ç»“æ„çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¥ä¼˜åŒ–LoRAã€‚è¯¥æ¡†æ¶è¿˜é€šè¿‡æ¨å¯¼ç†è®ºç¼©æ”¾å› å­ï¼Œä½¿ä¼˜åŒ–è¿‡ç¨‹ä¸å®Œå…¨å¾®è°ƒï¼ˆFull FTï¼‰çš„MoEå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGOATåœ¨25ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç¼©å°äº†ä¸å®Œå…¨å¾®è°ƒçš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16033', 'title': 'Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models', 'url': 'https://huggingface.co/papers/2502.16033', 'abstract': "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.", 'score': 12, 'issue_id': 2386, 'pub_date': '2025-02-22', 'pub_date_card': {'ru': '22 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 22', 'zh': '2æœˆ22æ—¥'}, 'hash': '3e5fc69b8713e252', 'authors': ['Qianqi Yan', 'Yue Fan', 'Hongquan Li', 'Shan Jiang', 'Yang Zhao', 'Xinze Guan', 'Ching-Chen Kuo', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Cruz', 'eBay'], 'pdf_title_img': 'assets/pdf/title_img/2502.16033.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMIR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. MMIR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 534 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ ÑˆĞµÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² MLLM.'}, 'en': {'title': 'Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content', 'desc': 'This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ç°å®ä¸–ç•Œçš„ä¸ä¸€è‡´æ€§', 'desc': 'ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦åœ¨ä¸€è‡´çš„è§†è§‰-æ–‡æœ¬è¾“å…¥ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬èƒ½å¦å¤„ç†ç°å®ä¸–ç•Œä¸­å¸ƒå±€ä¸°å¯Œå†…å®¹çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§æ¨ç†ï¼ˆMMIRï¼‰åŸºå‡†ï¼Œä»¥è¯„ä¼°MLLMsåœ¨æ£€æµ‹å’Œæ¨ç†è¯­ä¹‰ä¸åŒ¹é…æ–¹é¢çš„èƒ½åŠ›ã€‚MMIRåŒ…å«534ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæ¶µç›–äº”ä¸ªæ¨ç†å¯†é›†å‹ç±»åˆ«çš„åˆæˆé”™è¯¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·å¤‡ä¸“é—¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨å¤„ç†ä¸ä¸€è‡´æ€§æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œè€Œå¼€æºæ¨¡å‹åˆ™ç‰¹åˆ«å®¹æ˜“å—åˆ°ä¸ä¸€è‡´æ€§é”™è¯¯çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15894', 'title': 'RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2502.15894', 'abstract': 'Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality 2times extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables 3times extrapolation by minimal fine-tuning without long videos. Project page and codes: https://riflex-video.github.io/{https://riflex-video.github.io/.}', 'score': 9, 'issue_id': 2388, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '3c5243aaae60e8eb', 'authors': ['Min Zhao', 'Guande He', 'Yixiao Chen', 'Hongzhou Zhu', 'Chongxuan Li', 'Jun Zhu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods', 'Pazhou Laboratory (Huangpu)', 'ShengShu', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.15894.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'RIFLEx: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RIFLEx Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ¾Ğ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ… Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ, Ğ²Ğ»Ğ¸ÑÑÑ‰ÑƒÑ Ğ½Ğ° ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ. RIFLEx ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€ĞµÑ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ - Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹.'}, 'en': {'title': 'RIFLEx: Enhancing Video Length Extrapolation with Frequency Insights', 'desc': 'This paper addresses the challenge of generating longer videos while maintaining temporal coherence. The authors analyze how frequency components in positional embeddings affect video extrapolation and identify a key frequency that influences this behavior. They introduce RIFLEx, a simple yet effective method that reduces this intrinsic frequency to minimize repetition and ensure consistent motion. RIFLEx achieves impressive results, allowing for 2x extrapolation without additional training and improving quality for 3x extrapolation with minimal fine-tuning.'}, 'zh': {'title': 'RIFLExï¼šé«˜æ•ˆè§†é¢‘å¤–æ¨çš„æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„é•¿è¾¾ä¸€åˆ†é’Ÿçš„è§†é¢‘ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ›´é•¿çš„è§†é¢‘å¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œç°æœ‰çš„é•¿åº¦å¤–æ¨æ–¹æ³•å¾€å¾€å¯¼è‡´æ—¶é—´é‡å¤æˆ–è¿åŠ¨å‡é€Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†ä½ç½®åµŒå…¥ä¸­é¢‘ç‡æˆåˆ†çš„ä½œç”¨ï¼Œå¹¶è¯†åˆ«å‡ºä¸€ç§ä¸»è¦å½±å“å¤–æ¨è¡Œä¸ºçš„å†…åœ¨é¢‘ç‡ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†RIFLExï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡é™ä½å†…åœ¨é¢‘ç‡æ¥æŠ‘åˆ¶é‡å¤ï¼ŒåŒæ—¶ä¿æŒè¿åŠ¨ä¸€è‡´æ€§ï¼Œæ— éœ€ä»»ä½•é¢å¤–ä¿®æ”¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17110', 'title': 'Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2502.17110', 'abstract': 'The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks.', 'score': 8, 'issue_id': 2387, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '1900233d7723f824', 'authors': ['Junyang Wang', 'Haiyang Xu', 'Xi Zhang', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Jitao Sang'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17110.jpg', 'data': {'categories': ['#video', '#agents'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'Mobile-Agent-V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 30% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Mobile Automation with Video Guidance', 'desc': "The paper presents Mobile-Agent-V, a novel framework designed to enhance task management on mobile devices through automation. It addresses the limitations of existing AI frameworks that lack sufficient operational knowledge by utilizing video guidance to provide rich, actionable insights. The framework employs a sliding window strategy along with a video agent and deep-reflection agent to ensure that the system's actions are in line with user instructions. Experimental results demonstrate that Mobile-Agent-V significantly improves performance by 30% over traditional methods, making it a more efficient solution for mobile automation."}, 'zh': {'title': 'ç§»åŠ¨è‡ªåŠ¨åŒ–çš„æ–°çªç ´ï¼šMobile-Agent-V', 'desc': 'éšç€ç§»åŠ¨è®¾å¤‡ä½¿ç”¨çš„å¿«é€Ÿå¢é•¿ï¼Œä»»åŠ¡ç®¡ç†çš„è‡ªåŠ¨åŒ–éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ã€‚è®¸å¤šåŸºäºäººå·¥æ™ºèƒ½çš„æ¡†æ¶ç”±äºç¼ºä¹è¶³å¤Ÿçš„æ“ä½œçŸ¥è¯†è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„Mobile-Agent-Væ¡†æ¶åˆ©ç”¨è§†é¢‘æŒ‡å¯¼æä¾›ä¸°å¯Œä¸”ç»æµçš„æ“ä½œçŸ¥è¯†ï¼Œä»è€Œæ”¹å–„ç§»åŠ¨è‡ªåŠ¨åŒ–ã€‚é€šè¿‡é›†æˆæ»‘åŠ¨çª—å£ç­–ç•¥å’Œè§†é¢‘ä»£ç†ï¼ŒMobile-Agent-Vèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ å’Œæ‰§è¡Œä»»åŠ¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½æ¯”ç°æœ‰æ¡†æ¶æé«˜äº†30%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16707', 'title': 'Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation', 'url': 'https://huggingface.co/papers/2502.16707', 'abstract': 'Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs\' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a "reflection" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.', 'score': 7, 'issue_id': 2389, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': 'e739378d26e8e7ab', 'authors': ['Yunhai Feng', 'Jiaming Han', 'Zhuoran Yang', 'Xiangyu Yue', 'Sergey Levine', 'Jianlan Luo'], 'affiliations': ['Cornell University', 'The Chinese University of Hong Kong', 'University of California, Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16707.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#robotics', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ VLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ VLM Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ VLM Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."}, 'en': {'title': 'Enhancing VLMs for Better Robotic Manipulation through Reflection', 'desc': "This paper addresses the challenges of robotic manipulation over long time frames by enhancing vision-language models (VLMs) with improved physical reasoning. The authors propose a test-time computation framework that incorporates a 'reflection' mechanism, allowing the VLM to predict future states and refine its actions based on these predictions. This iterative process helps the model to better handle complex tasks by addressing potential errors before they occur. Experimental results show that this approach significantly outperforms existing VLMs and other advanced techniques like Monte Carlo Tree Search (MCTS)."}, 'zh': {'title': 'æå‡æœºå™¨äººæ“ä½œçš„ç‰©ç†æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šé˜¶æ®µæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç§â€œåæ€â€æœºåˆ¶ï¼Œè¿­ä»£åœ°æ”¹è¿›é¢„è®­ç»ƒçš„VLMï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹æƒ³è±¡æœªæ¥çš„ä¸–ç•ŒçŠ¶æ€ï¼Œå¹¶æ ¹æ®è¿™äº›é¢„æµ‹æŒ‡å¯¼åŠ¨ä½œé€‰æ‹©ã€‚é€šè¿‡åæ€æ½œåœ¨çš„æ¬¡ä¼˜å†³ç­–ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¤šç§æœ€å…ˆè¿›çš„å•†ä¸šVLMå’Œå…¶ä»–åè®­ç»ƒæ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16922', 'title': 'Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties', 'url': 'https://huggingface.co/papers/2502.16922', 'abstract': 'Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.', 'score': 7, 'issue_id': 2387, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '8e0389428d77685b', 'authors': ['Zhenglin Wang', 'Jialong Wu', 'Pengfei LI', 'Yong Jiang', 'Deyu Zhou'], 'affiliations': ['School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.16922.jpg', 'data': {'categories': ['#science', '#reasoning', '#multilingual', '#benchmark'], 'emoji': 'â³', 'ru': {'title': 'CTM: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Chinese Time Reasoning (CTM) Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. CTM Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ CTM, Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM.'}, 'en': {'title': 'Enhancing Temporal Reasoning with Chinese Time Benchmark', 'desc': 'This paper presents the Chinese Time Reasoning (CTM) benchmark, which aims to enhance the evaluation of Large Language Models (LLMs) in the area of temporal reasoning. Unlike existing benchmarks that are rule-based and limited in scope, CTM focuses on the rich context of Chinese dynastic history, allowing for a deeper assessment of temporal relationships. It emphasizes the importance of cross-entity relationships and pairwise temporal alignment, ensuring that the reasoning is both contextualized and culturally relevant. The results from extensive experiments indicate significant challenges for LLMs in this domain, suggesting areas for future research and improvement.'}, 'zh': {'title': 'ä¸­æ–‡æ—¶é—´æ¨ç†ï¼šæå‡æœºå™¨å­¦ä¹ çš„æ—¶é—´ç†è§£èƒ½åŠ›', 'desc': 'æ—¶é—´æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„åŸºç¡€ï¼Œå¯¹è®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†ä¸»è¦ä¾èµ–äºåŸºäºè§„åˆ™çš„æ„å»ºï¼Œç¼ºä¹ä¸Šä¸‹æ–‡æ·±åº¦ï¼Œå¹¶ä¸”æ¶‰åŠçš„æ—¶é—´å®ä½“èŒƒå›´æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸­æ–‡æ—¶é—´æ¨ç†ï¼ˆCTMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­å›½å†å²æ—¶é—´æ¨ç†æ–¹é¢çš„åŸºå‡†ã€‚CTMå¼ºè°ƒè·¨å®ä½“å…³ç³»ã€æˆå¯¹æ—¶é—´å¯¹é½ä»¥åŠä¸Šä¸‹æ–‡åŒ–å’Œæ–‡åŒ–åŸºç¡€çš„æ¨ç†ï¼Œæä¾›äº†å…¨é¢çš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17055', 'title': 'Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam', 'url': 'https://huggingface.co/papers/2502.17055', 'abstract': 'This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical l_2-norm statistics; and (3) inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to 2 perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git.', 'score': 6, 'issue_id': 2394, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '7ba1b35f1ba59325', 'authors': ['Tianjin Huang', 'Haotian Hu', 'Zhenyu Zhang', 'Gaojie Jin', 'Xiang Li', 'Li Shen', 'Tianlong Chen', 'Lu Liu', 'Qingsong Wen', 'Zhangyang Wang', 'Shiwei Liu'], 'affiliations': ['Department of Computer Science, The University of North Carolina at Chapel Hill', 'Department of Computer Science, University of Exeter', 'Department of Computer Science, University of Reading', 'Department of Electrical and Computer Engineering, University of Texas at Austin', 'Department of Mathematics and Computer Science, Eindhoven University of Technology', 'Mathematical Institute, University of Oxford', 'School of Cyber Science and Technology, Sun Yat-sen University', 'School of the Gifted Young, University of Science and Technology of China', 'Squirrel Ai Learning'], 'pdf_title_img': 'assets/pdf/title_img/2502.17055.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ¾Ñ€Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Stable-SPAM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Stable-SPAM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ² 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Adam Ğ¸ SPAM.'}, 'en': {'title': 'Stable Training with 4-Bit Precision: Introducing Stable-SPAM', 'desc': 'This paper evaluates various optimizers for training machine learning models with 4-bit precision, highlighting the challenges posed by low-bit training, such as sensitivity to learning rates and unstable gradient norms. The authors introduce Stable-SPAM, an improved version of the SPAM optimizer, which incorporates advanced techniques for gradient normalization and clipping to enhance stability. Stable-SPAM adaptively adjusts clipping thresholds based on historical gradient data and normalizes gradients using their historical l2-norm statistics, while also maintaining momentum reset to prevent gradient spikes. Experimental results demonstrate that Stable-SPAM not only stabilizes gradient norms but also outperforms traditional optimizers like Adam, achieving better performance with fewer training steps.'}, 'zh': {'title': 'ç¨³å®šçš„4ä½è®­ç»ƒä¼˜åŒ–å™¨ï¼šStable-SPAM', 'desc': 'æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å‡ ç§æœ€è¿‘æå‡ºçš„4ä½è®­ç»ƒä¼˜åŒ–å™¨ï¼Œå‘ç°ä½ä½ç²¾åº¦å¯¹å­¦ä¹ ç‡çš„æ•æ„Ÿæ€§å¢å¼ºï¼Œå¸¸å¯¼è‡´æ¢¯åº¦èŒƒæ•°ä¸ç¨³å®šï¼Œä»è€Œåœ¨è¾ƒé«˜å­¦ä¹ ç‡ä¸‹å‡ºç°å‘æ•£ã€‚SPAMæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–å™¨ï¼Œå…·æœ‰åŠ¨é‡é‡ç½®å’Œå°–å³°æ„ŸçŸ¥æ¢¯åº¦è£å‰ªï¼Œè™½ç„¶åœ¨ä¸åŒä½æ•°ä¸‹è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨ç¨³å®šæ¢¯åº¦èŒƒæ•°æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´å­¦ä¹ ç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Stable-SPAMï¼Œç»“åˆäº†å¢å¼ºçš„æ¢¯åº¦å½’ä¸€åŒ–å’Œè£å‰ªæŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¨³å®š4ä½LLMè®­ç»ƒä¸­çš„æ¢¯åº¦èŒƒæ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒStable-SPAMåœ¨æ€§èƒ½ä¸Šä¼˜äºAdamå’ŒSPAMï¼Œå°¤å…¶æ˜¯åœ¨4ä½LLaMA-1Bæ¨¡å‹è®­ç»ƒä¸­ï¼Œè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.14132', 'title': 'Can Community Notes Replace Professional Fact-Checkers?', 'url': 'https://huggingface.co/papers/2502.14132', 'abstract': 'Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.', 'score': 5, 'issue_id': 2393, 'pub_date': '2025-02-19', 'pub_date_card': {'ru': '19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 19', 'zh': '2æœˆ19æ—¥'}, 'hash': 'b46d43e5e12c38f4', 'authors': ['Nadav Borenstein', 'Greta Warren', 'Desmond Elliott', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2502.14132.jpg', 'data': {'categories': ['#multimodal', '#science', '#ethics', '#data', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤Ğ°ĞºÑ‚Ñ‡ĞµĞºĞ¸Ğ½Ğ³ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Twitter/X, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑÑ‹Ğ»Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ Ğ¿ÑÑ‚Ğ¸ Ñ€Ğ°Ğ· Ñ‡Ğ°Ñ‰Ğµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ½ĞµĞµ. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶ĞµĞ½ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Community Moderation Thrives on Fact-Checking Support', 'desc': 'This paper investigates the relationship between community moderation and professional fact-checking in combating misinformation on social media platforms like Twitter/X and Meta. Using language models, the authors analyze community notes to identify their attributes, including topics and cited sources. The findings indicate that community notes frequently reference fact-checking sources, particularly when addressing broader misinformation narratives. Ultimately, the study highlights the importance of integrating professional fact-checking into community-driven moderation efforts to enhance their effectiveness.'}, 'zh': {'title': 'ç¤¾åŒºç®¡ç†ä¾èµ–äºä¸“ä¸šäº‹å®æ ¸æŸ¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ç¤¾äº¤åª’ä½“ä¸Šå¯¹æŠ—è™šå‡ä¿¡æ¯çš„ä¸¤ç§ç­–ç•¥ï¼šä¸“ä¸šæœºæ„çš„äº‹å®æ ¸æŸ¥å’Œç”¨æˆ·çš„ç¤¾åŒºç®¡ç†ã€‚ç ”ç©¶å‘ç°ï¼Œç¤¾åŒºç¬”è®°åœ¨å¼•ç”¨äº‹å®æ ¸æŸ¥æ¥æºæ—¶ï¼Œé¢‘ç‡æ¯”ä¹‹å‰æŠ¥å‘Šçš„é«˜å‡ºäº”å€ï¼Œå°¤å…¶æ˜¯åœ¨ä¸æ›´å¹¿æ³›çš„è™šå‡å™äº‹ç›¸å…³çš„å¸–å­ä¸­ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒæˆåŠŸçš„ç¤¾åŒºç®¡ç†åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºä¸“ä¸šçš„äº‹å®æ ¸æŸ¥ã€‚æ”¿ç­–å˜åŒ–æ˜¾ç¤ºï¼Œç¤¾äº¤åª’ä½“å¹³å°æ­£åœ¨é€æ¸è½¬å‘ä¾èµ–ç”¨æˆ·ç”Ÿæˆçš„å†…å®¹ï¼Œä½†äº‹å®æ ¸æŸ¥ä»ç„¶è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17414', 'title': 'X-Dancer: Expressive Music to Human Dance Video Generation', 'url': 'https://huggingface.co/papers/2502.17414', 'abstract': 'We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.', 'score': 5, 'issue_id': 2389, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': 'ef6013182ae4065e', 'authors': ['Zeyuan Chen', 'Hongyi Xu', 'Guoxian Song', 'You Xie', 'Chenxu Zhang', 'Xin Chen', 'Chao Wang', 'Di Chang', 'Linjie Luo'], 'affiliations': ['ByteDance', 'UC San Diego', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.17414.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#video', '#multimodal'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'Ğ¢Ğ°Ğ½Ñ†ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ğ˜Ğ˜ Ğ¾Ğ¶Ğ¸Ğ²Ğ»ÑĞµÑ‚ Ñ„Ğ¾Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ', 'desc': 'X-Dancer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚Ğ°Ğ½Ñ†ĞµĞ² Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾Ğ· Ñ‚ĞµĞ»Ğ°, Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ÑƒĞº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. X-Dancer Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ 2D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ°Ğ½Ñ†Ğ°, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ½ÑĞ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±Ğ¸Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ X-Dancer ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Transforming Static Images into Dynamic Dance Videos with Music', 'desc': 'X-Dancer is a new system that creates realistic dance videos from just one image, using music as a guide. It combines a transformer model that generates dance poses with a diffusion model that turns these poses into video frames. This approach allows it to produce a wide variety of 2D dance movements that match the rhythm of the music, overcoming limitations of traditional 3D methods. The results show that X-Dancer excels in creating diverse and expressive dance videos, outperforming existing techniques.'}, 'zh': {'title': 'X-Dancerï¼šä»é™æ€å›¾åƒç”ŸæˆéŸ³ä¹é©±åŠ¨çš„èˆè¹ˆè§†é¢‘', 'desc': 'X-Danceræ˜¯ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œå¯ä»¥ä»å•ä¸€é™æ€å›¾åƒç”Ÿæˆå¤šæ ·åŒ–ä¸”é•¿æ—¶é—´çš„é€¼çœŸäººç±»èˆè¹ˆè§†é¢‘ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å˜æ¢å™¨-æ‰©æ•£æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªå›å½’å˜æ¢å™¨æ¨¡å‹åˆæˆä¸éŸ³ä¹åŒæ­¥çš„2Dèº«ä½“ã€å¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿çš„æ‰©å±•åºåˆ—ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸”çœŸå®çš„èˆè¹ˆè§†é¢‘å¸§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ç”Ÿæˆ3Däººç±»è¿åŠ¨ä¸åŒï¼ŒX-Danceré€šè¿‡å»ºæ¨¡å¹¿æ³›çš„2Dèˆè¹ˆåŠ¨ä½œï¼Œæ•æ‰ä¸éŸ³ä¹èŠ‚æ‹çš„ç»†å¾®å¯¹é½ï¼Œè§£å†³äº†æ•°æ®é™åˆ¶å¹¶å¢å¼ºäº†å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Danceråœ¨å¤šæ ·æ€§ã€è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15987', 'title': 'Forecasting Open-Weight AI Model Growth on Hugging Face', 'url': 'https://huggingface.co/papers/2502.15987', 'abstract': "As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage.", 'score': 5, 'issue_id': 2388, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '4e2786a48c6c49ab', 'authors': ['Kushal Raj Bhandari', 'Pin-Yu Chen', 'Jianxi Gao'], 'affiliations': ['Department of Computer Science, Network Science and Technology Center, Rensselaer Polytechnic Institute, Troy, NY, USA', 'IBM Research, Yorktown Heights, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.15987.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ framework Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ open-weight Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ„ĞµÑ€Ğµ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ’Ğ°Ğ½Ğ³Ğ° Ğ¸ Ğ´Ñ€., Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²ĞµÑ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ adoption open-weight Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Tracking AI Model Influence: A Citation-Style Approach', 'desc': 'This paper presents a framework to analyze the influence of open-weight AI models over time, similar to how citations are tracked in scientific research. It introduces three parameters: immediacy, longevity, and relative fitness, to measure the impact of these models based on the number of fine-tuned versions created from them. The study finds that this citation-style method can effectively illustrate the varying adoption patterns of open-weight models, highlighting both typical trends and unique outliers. This approach helps predict which models are likely to lead innovation in the AI field.'}, 'zh': {'title': 'é¢„æµ‹å¼€æ”¾æƒé‡æ¨¡å‹çš„å½±å“åŠ›æ¼”å˜', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹å¼€æ”¾æƒé‡æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿä¸­çš„å½±å“åŠ›æ¼”å˜ã€‚æˆ‘ä»¬å€Ÿé‰´äº†ç§‘å­¦æ–‡çŒ®ä¸­çš„å¼•ç”¨åŠ¨æ€ï¼Œä½¿ç”¨ä¸‰ä¸ªå…³é”®å‚æ•°ï¼šå³æ—¶æ€§ã€æŒä¹…æ€§å’Œç›¸å¯¹é€‚åº”æ€§ï¼Œæ¥è·Ÿè¸ªå¼€æ”¾æƒé‡æ¨¡å‹çš„å¾®è°ƒæ¨¡å‹æ•°é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§å¼•ç”¨é£æ ¼çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¼€æ”¾æƒé‡æ¨¡å‹é‡‡ç”¨çš„å¤šæ ·åŒ–è½¨è¿¹ã€‚å¤§å¤šæ•°æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œè€Œå¼‚å¸¸å€¼åˆ™æ˜¾ç¤ºå‡ºç‹¬ç‰¹çš„æ¨¡å¼æˆ–ä½¿ç”¨çš„çªç„¶å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16701', 'title': 'Beyond Release: Access Considerations for Generative AI Systems', 'url': 'https://huggingface.co/papers/2502.16701', 'abstract': 'Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.', 'score': 5, 'issue_id': 2386, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 23', 'zh': '2æœˆ23æ—¥'}, 'hash': 'db3e8ec873d10ddb', 'authors': ['Irene Solaiman', 'Rishi Bommasani', 'Dan Hendrycks', 'Ariel Herbert-Voss', 'Yacine Jernite', 'Aviya Skowron', 'Andrew Trask'], 'affiliations': ['Center for AI Safety', 'EleutherAI', 'Hugging Face', 'OpenMined', 'RunSybil', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16701.jpg', 'data': {'categories': ['#ethics', '#open_source'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ˜Ğ˜: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ€ĞµĞ»Ğ¸Ğ·', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ñ€ĞµĞ»Ğ¸Ğ·Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¾ÑÑĞ¼: Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑƒĞ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Understanding Access: Key to Responsible AI Release', 'desc': 'This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access.'}, 'zh': {'title': 'è§£æ„ç”Ÿæˆæ€§AIçš„è®¿é—®ä¸é£é™©ç®¡ç†', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å‘å¸ƒå†³ç­–å¯¹ç³»ç»Ÿç»„ä»¶å¯ç”¨æ€§çš„å½±å“ã€‚ä½œè€…æŒ‡å‡ºï¼Œå‘å¸ƒå¹¶ä¸èƒ½è§£å†³ç”¨æˆ·å’Œåˆ©ç›Šç›¸å…³è€…ä¸ç³»ç»Ÿäº’åŠ¨çš„æ‰€æœ‰é—®é¢˜ï¼Œè®¿é—®ç³»ç»Ÿç»„ä»¶çš„æ–¹å¼ä¹Ÿä¼šå½±å“æ½œåœ¨çš„é£é™©å’Œæ”¶ç›Šã€‚è®ºæ–‡å°†è®¿é—®åˆ†ä¸ºä¸‰ä¸ªæ–¹é¢ï¼šèµ„æºã€æŠ€æœ¯å¯ç”¨æ€§å’Œæ•ˆç”¨ï¼Œå¹¶åœ¨æ¯ä¸ªç±»åˆ«ä¸­æ˜ç¡®äº†ä¸åŒå˜é‡çš„æƒè¡¡ã€‚é€šè¿‡æ¯”è¾ƒå››ç§é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹çš„å¯è®¿é—®æ€§ï¼Œä½œè€…å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è®¿é—®å˜é‡æ¥è¯„ä¼°å’Œç®¡ç†é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15425', 'title': 'TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.15425', 'abstract': 'Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems.TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems.', 'score': 4, 'issue_id': 2394, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '8c7cfe64bc678e90', 'authors': ['Giuseppe Paolo', 'Abdelhakim Benechehab', 'Hamza Cherkaoui', 'Albert Thomas', 'BalÃ¡zs KÃ©gl'], 'affiliations': ['Noahs Ark Lab, Huawei Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.15425.jpg', 'data': {'categories': ['#optimization', '#rl', '#games', '#architecture', '#agents', '#benchmark', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TAME Agent Framework (TAG) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. TAG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ LevelEnv, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑˆĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ»Ğ°Ğ±ÑƒÑ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TAG, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ RL-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ….'}, 'en': {'title': 'Unlocking Scalable Intelligence with Decentralized Hierarchical Learning', 'desc': 'The paper presents the TAME Agent Framework (TAG), which allows for the creation of decentralized hierarchical multi-agent systems. Unlike traditional hierarchical reinforcement learning methods that are limited to two levels or require centralized training, TAG supports hierarchies of any depth. It introduces the LevelEnv concept, which treats each level of the hierarchy as an environment for the agents above it, facilitating better information flow and integration of various agent types. The results indicate that TAG improves learning speed and performance compared to standard multi-agent reinforcement learning approaches.'}, 'zh': {'title': 'å»ä¸­å¿ƒåŒ–çš„å±‚æ¬¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºTAME Agent Framework (TAG)ï¼Œç”¨äºæ„å»ºå®Œå…¨å»ä¸­å¿ƒåŒ–çš„å±‚æ¬¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚TAGé€šè¿‡å¼•å…¥LevelEnvæ¦‚å¿µï¼Œä½¿å¾—å±‚æ¬¡ç»“æ„å¯ä»¥è¾¾åˆ°ä»»æ„æ·±åº¦ï¼Œæ¯ä¸ªå±‚æ¬¡éƒ½è¢«æŠ½è±¡ä¸ºä¸Šå±‚æ™ºèƒ½ä½“çš„ç¯å¢ƒã€‚è¿™ç§æ–¹æ³•æ ‡å‡†åŒ–äº†å±‚æ¬¡ä¹‹é—´çš„ä¿¡æ¯æµï¼ŒåŒæ—¶ä¿æŒäº†æ¾è€¦åˆï¼Œå…è®¸ä¸åŒç±»å‹çš„æ™ºèƒ½ä½“æ— ç¼é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå»ä¸­å¿ƒåŒ–çš„å±‚æ¬¡ç»„ç»‡æé«˜äº†å­¦ä¹ é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼ŒTAGåœ¨å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å±•ç°å‡ºè‰¯å¥½çš„å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15122', 'title': 'MONSTER: Monash Scalable Time Series Evaluation Repository', 'url': 'https://huggingface.co/papers/2502.15122', 'abstract': 'We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data.', 'score': 2, 'issue_id': 2389, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '0befd2d9ae63b48e', 'authors': ['Angus Dempster', 'Navid Mohammadi Foumani', 'Chang Wei Tan', 'Lynn Miller', 'Amish Mishra', 'Mahsa Salehi', 'Charlotte Pelletier', 'Daniel F. Schmidt', 'Geoffrey I. Webb'], 'affiliations': ['Monash University, Melbourne, Australia', 'Universite Bretagne Sud, IRISA, Vannes, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.15122.jpg', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': 'ğŸ¦–', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²', 'desc': 'MONSTER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ²ĞµÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ UCR Ğ¸ UEA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. MONSTER Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ğ¾ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ°Ñ….'}, 'en': {'title': 'Unlocking Potential with Large Datasets in Time Series Classification', 'desc': 'This paper presents MONSTER, a new repository designed for evaluating time series classification using large datasets. Current benchmarks, like UCR and UEA, consist of small datasets that limit the types of models that can be effectively tested. By providing larger datasets, MONSTER aims to encourage the development of models that can handle scalability and learn from more extensive data. The authors believe that this will lead to significant advancements in the field of time series classification.'}, 'zh': {'title': 'å¼•å…¥å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¨åŠ¨æ—¶é—´åºåˆ—åˆ†ç±»è¿›æ­¥', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†MONSTERâ€”â€”è’™çº³å£«å¯æ‰©å±•æ—¶é—´åºåˆ—è¯„ä¼°åº“ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤§é‡æ—¶é—´åºåˆ—åˆ†ç±»æ•°æ®é›†çš„é›†åˆã€‚ç°æœ‰çš„æ—¶é—´åºåˆ—åˆ†ç±»åŸºå‡†ï¼ˆå¦‚UCRå’ŒUEAï¼‰ä¸­çš„æ•°æ®é›†è¾ƒå°ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¤šæ ·æ€§ã€‚MONSTERæ—¨åœ¨é€šè¿‡å¼•å…¥æ›´å¤§çš„æ•°æ®é›†æ¥ä¸°å¯Œè¿™ä¸€é¢†åŸŸï¼Œä¿ƒè¿›æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶çš„æœ‰æ•ˆå­¦ä¹ ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œé¢å¯¹æ›´å¤§æ•°æ®é‡çš„ç†è®ºå’Œå®è·µæŒ‘æˆ˜ï¼Œå°†ä¸ºæ—¶é—´åºåˆ—åˆ†ç±»é¢†åŸŸå¸¦æ¥æ–°çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17237', 'title': 'MegaLoc: One Retrieval to Place Them All', 'url': 'https://huggingface.co/papers/2502.17237', 'abstract': 'Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc', 'score': 0, 'issue_id': 2397, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '7eada0ecd3a7e714', 'authors': ['Gabriele Berton', 'Carlo Masone'], 'affiliations': ['Polytechnic of Turin'], 'pdf_title_img': 'assets/pdf/title_img/2502.17237.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'MegaLoc: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'MegaLoc - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. MegaLoc Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚, Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ²Ğ¾Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'MegaLoc: One Model to Rule Multiple Vision Tasks', 'desc': 'This paper presents MegaLoc, a versatile retrieval model designed to improve performance across various computer vision tasks such as Visual Place Recognition, Landmark Retrieval, and Visual Localization. Unlike existing models that are tailored for specific tasks, MegaLoc integrates multiple methods and training techniques to adapt to different requirements and out-of-distribution data. The authors demonstrate that MegaLoc achieves state-of-the-art results on numerous Visual Place Recognition datasets and excels in Landmark Retrieval, while also setting a new benchmark for Visual Localization on the LaMAR datasets. This approach highlights the importance of flexibility in model design for effective image retrieval in diverse applications.'}, 'zh': {'title': 'MegaLocï¼šå¤šä»»åŠ¡å›¾åƒæ£€ç´¢çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMegaLocçš„æ£€ç´¢æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„å›¾åƒæ£€ç´¢é—®é¢˜ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šå¸¸åªé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œéš¾ä»¥é€‚åº”ä¸åŒçš„éœ€æ±‚æˆ–åˆ†å¸ƒå¤–æ•°æ®ã€‚MegaLocç»“åˆäº†å¤šç§ç°æœ‰æ–¹æ³•ã€è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®é›†ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMegaLocåœ¨å¤šä¸ªè§†è§‰ä½ç½®è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶åœ¨å¸¸è§çš„åœ°æ ‡æ£€ç´¢æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.13074', 'title': 'The snake in the Brownian sphere', 'url': 'https://huggingface.co/papers/2502.13074', 'abstract': "The Brownian sphere is a random metric space, homeomorphic to the two-dimensional sphere, which arises as the universal scaling limit of many types of random planar maps. The direct construction of the Brownian sphere is via a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection. The CVS bijection maps labeled trees to planar maps, and the continuous version maps Aldous' continuum random tree with Brownian labels (the Brownian snake) to the Brownian sphere. In this work, we describe the inverse of the continuous CVS bijection, by constructing the Brownian snake as a measurable function of the Brownian sphere. Special care is needed to work with the orientation of the Brownian sphere.", 'score': 0, 'issue_id': 2392, 'pub_date': '2025-02-18', 'pub_date_card': {'ru': '18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 18', 'zh': '2æœˆ18æ—¥'}, 'hash': '1723a9c12f61fd5f', 'authors': ['Omer Angel', 'Emmanuel Jacob', 'Brett Kolesnik', 'GrÃ©gory Miermont'], 'affiliations': ['Institut Universitaire de France', 'University of British Columbia, Department of Mathematics', 'University of Warwick, Department of Statistics', 'Ã‰cole Normale SupÃ©rieure de Lyon, UnitÃ© de MathÃ©matiques Pures et AppliquÃ©es'], 'pdf_title_img': 'assets/pdf/title_img/2502.13074.jpg', 'data': {'categories': ['#math'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹ Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ±Ğ¸ĞµĞºÑ†Ğ¸Ğ¸ ĞšĞ¾Ñ€Ğ¸-Ğ’Ğ¾ĞºĞµĞ»ĞµĞ½Ğ°-Ğ¨ĞµÑ„Ñ„ĞµÑ€Ğ° (CVS) Ğ´Ğ»Ñ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹. Ğ‘Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ°Ñ ÑÑ„ĞµÑ€Ğ° - ÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ³Ğ¾Ğ¼ĞµĞ¾Ğ¼Ğ¾Ñ€Ñ„Ğ½Ğ¾Ğµ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ĞµĞµ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ» Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚. ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ±Ğ¸ĞµĞºÑ†Ğ¸Ğ¸ CVS Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾ ĞĞ»Ğ´Ğ¾ÑĞ° Ñ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ (Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ°Ñ Ğ·Ğ¼ĞµÑ) Ğ½Ğ° Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºÑƒÑ ÑÑ„ĞµÑ€Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒÑÑ‚ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºÑƒÑ Ğ·Ğ¼ĞµÑ ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ğ¼ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹, ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ñ€Ğ¾ÑƒĞ½Ğ¾Ğ²ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹.'}, 'en': {'title': 'Connecting Random Trees to the Brownian Sphere', 'desc': 'The paper introduces the Brownian sphere, a random metric space that resembles a two-dimensional sphere and serves as a scaling limit for various random planar maps. It details the continuous version of the Cori--Vauquelin--Schaeffer (CVS) bijection, which connects labeled trees to planar maps. The authors construct the Brownian snake, a continuous object derived from the Brownian sphere, as a measurable function, emphasizing the importance of orientation in this context. This work provides a deeper understanding of the relationship between random trees and planar maps through the lens of stochastic processes.'}, 'zh': {'title': 'å¸ƒæœ—çƒï¼šéšæœºå¹³é¢å›¾çš„æé™', 'desc': 'å¸ƒæœ—çƒæ˜¯ä¸€ä¸ªéšæœºåº¦é‡ç©ºé—´ï¼Œä¸äºŒç»´çƒé¢åŒèƒšï¼Œæ˜¯è®¸å¤šéšæœºå¹³é¢å›¾çš„æ™®éç¼©æ”¾æé™ã€‚å¸ƒæœ—çƒçš„ç›´æ¥æ„é€ æ˜¯é€šè¿‡Cori-Vauquelin-Schaeffer (CVS)åŒå°„çš„è¿ç»­ç±»æ¯”å®ç°çš„ã€‚CVSåŒå°„å°†æ ‡è®°æ ‘æ˜ å°„åˆ°å¹³é¢å›¾ï¼Œè€Œè¿ç»­ç‰ˆæœ¬åˆ™å°†Aldousçš„è¿ç»­éšæœºæ ‘ä¸å¸ƒæœ—æ ‡ç­¾ï¼ˆå¸ƒæœ—è›‡ï¼‰æ˜ å°„åˆ°å¸ƒæœ—çƒã€‚æœ¬æ–‡æè¿°äº†è¿ç»­CVSåŒå°„çš„é€†è¿‡ç¨‹ï¼Œé€šè¿‡å°†å¸ƒæœ—è›‡æ„é€ ä¸ºå¸ƒæœ—çƒçš„å¯æµ‹å‡½æ•°æ¥å®ç°ï¼Œç‰¹åˆ«æ³¨æ„å¸ƒæœ—çƒçš„æ–¹å‘æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.15167', 'title': 'M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment', 'url': 'https://huggingface.co/papers/2502.15167', 'abstract': 'The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.', 'score': 0, 'issue_id': 2392, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 21', 'zh': '2æœˆ21æ—¥'}, 'hash': '457cffd47ddda6bb', 'authors': ['Chuan Cui', 'Kejiang Chen', 'Zhihua Wei', 'Wen Shen', 'Weiming Zhang', 'Nenghai Yu'], 'affiliations': ['Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation', 'Hefei High-Dimensional Data Technology Co.,Ltd.', 'School and Technology, Tongji University, Shanghai, of Computer Science China'], 'pdf_title_img': 'assets/pdf/title_img/2502.15167.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#benchmark', '#interpretability', '#agi'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ˜Ğ˜-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ - M3-AGIQA. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. M3-AGIQA ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¸ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'M3-AGIQA: A New Standard for Evaluating AI-Generated Images', 'desc': 'This paper presents M3-AGIQA, a new framework designed to evaluate the quality of AI-generated images by considering various factors like perceptual quality and authenticity. It utilizes Multimodal Large Language Models (MLLMs) to analyze both text and images, enhancing the evaluation process through Low-Rank Adaptation (LoRA) fine-tuning. The framework features a multi-round evaluation system that generates intermediate descriptions to provide detailed insights into the quality of the images. The results show that M3-AGIQA outperforms existing methods and is highly adaptable across different datasets, making it a robust tool for assessing AGI quality.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°AGIè´¨é‡çš„M3-AGIQAæ¡†æ¶', 'desc': 'éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒï¼ˆAGIï¼‰æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å…¶è´¨é‡é¢ä¸´ç€å¤šç»´åº¦çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥è´¨é‡ã€æç¤ºå¯¹åº”æ€§å’ŒçœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M3-AGIQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„AGIè´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œå…·æœ‰å¤šæ¨¡æ€ã€å¤šè½®æ¬¡å’Œå¤šæ–¹é¢çš„ç‰¹ç‚¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºæ–‡æœ¬å’Œå›¾åƒçš„è”åˆç¼–ç å™¨ï¼Œå¹¶é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒå°†åœ¨çº¿MLLMsçš„é«˜çº§æè¿°èƒ½åŠ›æç‚¼åˆ°æœ¬åœ°æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç»“æ„åŒ–çš„å¤šè½®è¯„ä¼°æœºåˆ¶ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´å›¾åƒæè¿°æ¥æ·±å…¥æ´å¯Ÿè´¨é‡ã€å¯¹åº”æ€§å’ŒçœŸå®æ€§çš„å„ä¸ªæ–¹é¢ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (3)', '#agi (1)', '#alignment', '#architecture (5)', '#audio (2)', '#benchmark (10)', '#cv (3)', '#data (2)', '#dataset (7)', '#diffusion (4)', '#ethics (2)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (6)', '#open_source (4)', '#optimization (9)', '#plp', '#rag', '#reasoning (5)', '#rl (1)', '#rlhf', '#robotics (1)', '#science (3)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (8)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-02-25 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-25 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-25 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    