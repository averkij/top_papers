
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. July 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 июля</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-09.html">⬅️ <span id="prev-date">09.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-11.html">➡️ <span id="next-date">11.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'};
        let feedDateNext = {'ru': '11.07', 'en': '07/11', 'zh': '7月11日'};
        let feedDatePrev = {'ru': '09.07', 'en': '07/09', 'zh': '7月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.06448', 'title': 'Perception-Aware Policy Optimization for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2507.06448', 'abstract': 'Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.', 'score': 14, 'issue_id': 4739, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '371bd96113b91da3', 'authors': ['Zhenhailong Wang', 'Xuehang Guo', 'Sofia Stoica', 'Haiyang Xu', 'Hongru Wang', 'Hyeonjeong Ha', 'Xiusi Chen', 'Yangyi Chen', 'Ming Yan', 'Fei Huang', 'Heng Ji'], 'affiliations': ['Alibaba Group', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.06448.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#rl', '#rlhf', '#optimization'], 'emoji': '👁️', 'ru': {'title': 'Улучшение восприятия в мультимодальном обучении с подкреплением', 'desc': 'Статья представляет метод Perception-Aware Policy Optimization (PAPO) для улучшения обучения с подкреплением в задачах мультимодального рассуждения. PAPO интегрирует неявную функцию потерь восприятия в целевую функцию GRPO, что позволяет модели одновременно учиться воспринимать и рассуждать. Этот подход значительно улучшает общую производительность на различных мультимодальных бенчмарках, особенно в задачах с высокой зависимостью от зрения. Авторы также выявили и решили проблему взлома функции потерь с помощью двойной энтропийной функции потерь.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Perception-Aware Learning', 'desc': 'Perception-Aware Policy Optimization (PAPO) is a novel approach that enhances reinforcement learning by integrating implicit perception loss to improve visual reasoning in multimodal tasks. It addresses the limitations of existing methods that primarily focus on textual data, leading to errors in visual input perception. By introducing a KL divergence term to the GRPO objective, PAPO significantly boosts performance on multimodal benchmarks, especially in vision-dependent tasks. This method not only reduces perception errors but also establishes a new framework for visually grounded reasoning in reinforcement learning.'}, 'zh': {'title': '提升视觉推理的感知优化策略', 'desc': '感知意识策略优化（PAPO）通过整合隐式感知损失，增强了强化学习在多模态推理中的可验证奖励。该方法改善了视觉感知和推理能力，特别是在处理视觉输入时显著减少了错误。PAPO是对GRPO的有效扩展，能够在没有额外数据或外部奖励模型的情况下，利用内部监督信号进行学习。实验结果显示，PAPO在多模态基准测试中提高了4.4%的性能，尤其在视觉依赖性高的任务中，提升接近8.0%。'}}}, {'id': 'https://huggingface.co/papers/2507.06457', 'title': 'A Systematic Analysis of Hybrid Linear Attention', 'url': 'https://huggingface.co/papers/2507.06457', 'abstract': 'Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers face quadratic complexity and memory issues with long sequences, prompting the adoption of linear attention mechanisms using fixed-size hidden states. However, linear models often suffer from limited recall performance, leading to hybrid architectures that combine linear and full attention layers. Despite extensive hybrid architecture research, the choice of linear attention component has not been deeply explored. We systematically evaluate various linear attention models across generations - vector recurrences to advanced gating mechanisms - both standalone and hybridized. To enable this comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six linear attention variants across five hybridization ratios. Benchmarking on standard language modeling and recall tasks reveals that superior standalone linear models do not necessarily excel in hybrids. While language modeling remains stable across linear-to-full attention ratios, recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. Our study highlights selective gating, hierarchical recurrence, and controlled forgetting as critical for effective hybrid models. We recommend architectures such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall efficiently. Our models are open-sourced at https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.', 'score': 7, 'issue_id': 4739, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '31144b92e85957ae', 'authors': ['Dustin Wang', 'Rui-Jie Zhu', 'Steven Abreu', 'Yong Shan', 'Taylor Kergan', 'Yuqi Pan', 'Yuhong Chou', 'Zheng Li', 'Ge Zhang', 'Wenhao Huang', 'Jason Eshraghian'], 'affiliations': ['ByteDance Seed', 'CASIA', 'M-A-P', 'PolyU', 'UC Santa Cruz', 'University of Groningen'], 'pdf_title_img': 'assets/pdf/title_img/2507.06457.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#benchmark', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Гибридные архитектуры внимания: оптимизация производительности и эффективности', 'desc': 'Исследование оценивает различные модели линейного внимания и их интеграцию с полным вниманием в трансформерах. Ключевыми механизмами для улучшения производительности запоминания определены селективное гейтирование и иерархическая рекуррентность. Авторы обучили и открыли исходный код 72 моделей с различными параметрами и вариантами линейного внимания. Результаты показывают, что лучшие автономные линейные модели не обязательно превосходят в гибридных архитектурах.'}, 'en': {'title': 'Enhancing Transformer Recall with Hybrid Linear Attention Models', 'desc': 'This research investigates different linear attention models and their combination with full attention in Transformers to improve recall performance. It identifies important mechanisms like selective gating and hierarchical recurrence that enhance the effectiveness of these hybrid models. The study systematically evaluates 72 models, revealing that the best standalone linear models do not always perform well in hybrid settings. The findings suggest optimal architectures and ratios for combining linear and full attention to achieve better recall in language tasks.'}, 'zh': {'title': '优化变换器的混合注意力模型', 'desc': '本研究评估了各种线性注意力模型及其与全注意力在变换器中的结合，识别出选择性门控和层次递归等关键机制，以提高回忆性能。变换器在处理长序列时面临二次复杂性和内存问题，因此采用了固定大小的隐藏状态的线性注意力机制。然而，线性模型通常在回忆性能上存在局限，导致出现结合线性和全注意力层的混合架构。我们的研究表明，选择性门控、层次递归和控制遗忘是有效混合模型的关键因素，并推荐在3:1到6:1的线性与全注意力比例下的架构。'}}}, {'id': 'https://huggingface.co/papers/2507.06920', 'title': 'Rethinking Verification for LLM Code Generation: From Generation to\n  Testing', 'url': 'https://huggingface.co/papers/2507.06920', 'abstract': 'A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.', 'score': 6, 'issue_id': 4738, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '93b419a1fb85f819', 'authors': ['Zihan Ma', 'Taolin Zhang', 'Maosong Cao', 'Wenwei Zhang', 'Minnan Luo', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['MOE KLINNS Lab, Xian Jiaotong University, China', 'School of Computer Science and Technology, Xian Jiaotong University, China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.06920.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#dataset', '#training', '#games'], 'emoji': '🧪', 'ru': {'title': 'Человек и ИИ объединяются для создания надежных тестов кода', 'desc': 'Статья представляет новый метод SAGA для генерации тестовых случаев, объединяющий человеческий опыт и возможности больших языковых моделей. Авторы вводят многомерные метрики для оценки полноты набора тестов и создают специальный бенчмарк TCGBench. Эксперименты показывают, что SAGA достигает 90.62% обнаружения ошибок и 32.58% точности верификатора на TCGBench. Метод улучшает надежность оценки генерации кода большими языковыми моделями.'}, 'en': {'title': 'Enhancing Code Reliability with Human-LLM Collaboration', 'desc': 'This paper presents a method called SAGA that combines human expertise with large language models (LLMs) to improve the generation of test cases for code evaluation. The authors highlight that existing benchmarks often miss subtle errors due to their limited and similar test cases, which can lead to misleading performance metrics. By introducing multi-dimensional metrics and a new test-case generation benchmark (TCGBench), they rigorously assess the thoroughness of test suites. The results show that SAGA significantly enhances detection rates and verifier accuracy, indicating its potential to improve the reliability of LLMs in code generation tasks.'}, 'zh': {'title': '人机协作提升代码测试生成的可靠性', 'desc': '本文提出了一种人类与大型语言模型（LLM）协作的方法，以增强代码生成中的测试用例生成，提升代码评估基准的可靠性和检测率。研究发现，现有的评估套件通常只包含有限的同质测试用例，导致一些细微错误未被发现，从而影响了性能评估的准确性。为了解决这些问题，本文提出了多维度指标来量化测试套件的全面性，并引入了SAGA方法，结合人类编程专家与LLM的推理能力，显著提高了生成测试用例的覆盖率和质量。实验结果表明，SAGA在TCGBench上的检测率达到90.62%，验证器准确率为32.58%，显示了该方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.07095', 'title': 'Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data', 'url': 'https://huggingface.co/papers/2507.07095', 'abstract': 'A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes.', 'score': 5, 'issue_id': 4738, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'd8bd0a82b6576b80', 'authors': ['Ke Fan', 'Shunlin Lu', 'Minyue Dai', 'Runyi Yu', 'Lixing Xiao', 'Zhiyang Dou', 'Junting Dong', 'Lizhuang Ma', 'Jingbo Wang'], 'affiliations': ['CUHK, Shenzhen', 'East China Normal University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07095.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#benchmark', '#dataset', '#robotics', '#games'], 'emoji': '🤖', 'ru': {'title': 'Революция в генерации движений: от текста к реальности', 'desc': 'Исследователи представили новый набор данных MotionMillion и систему оценки MotionMillion-Eval для улучшения генерации движений по текстовому описанию с нулевым обучением. MotionMillion содержит более 2 миллионов высококачественных последовательностей движений, что делает его крупнейшим набором данных о движениях человека на сегодняшний день. Авторы также разработали масштабируемую архитектуру модели с 7 миллиардами параметров для решения этой задачи. Результаты демонстрируют сильную генерализацию на движения вне обучающей выборки и сложные составные движения, что является значительным шагом к генерации движений человека с нулевым обучением.'}, 'en': {'title': 'Revolutionizing Zero-Shot Motion Generation with MotionMillion', 'desc': 'This paper presents a new dataset and evaluation framework aimed at enhancing zero-shot text-to-motion generation. The authors introduce MotionMillion, the largest dataset of human motion sequences, which includes over 2 million high-quality motions. They also propose MotionMillion-Eval, a comprehensive benchmark for assessing the performance of zero-shot motion generation models. By utilizing a scalable model architecture with 7 billion parameters, the study demonstrates improved generalization capabilities for generating complex human motions from textual descriptions.'}, 'zh': {'title': '推动零-shot人类动作生成的新纪元', 'desc': '本文提出了一种新的数据集和评估框架，以改善零-shot文本到动作生成的能力。我们开发了MotionMillion，这是迄今为止最大的高质量人类动作数据集，包含超过2000小时和200万条动作序列。通过引入MotionMillion-Eval，我们建立了一个全面的基准来评估零-shot动作生成的效果。我们的模型在7B参数的可扩展架构下表现出色，能够有效地生成复杂的动作序列，标志着零-shot人类动作生成的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2507.06804', 'title': 'Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving', 'url': 'https://huggingface.co/papers/2507.06804', 'abstract': "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .", 'score': 5, 'issue_id': 4738, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '4c82f29e08bec1a6', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.06804.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#dataset', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Разделяй и властвуй: новый подход к автоматическому доказательству теорем', 'desc': 'Статья представляет новый подход к автоматическому доказательству теорем, разделяющий процессы рассуждения и формального доказательства. Авторы используют две отдельные модели: мощный Reasoner для генерации стратегических подцелей и эффективный Prover для их строгой верификации. Этот метод позволяет преодолеть ограничения существующих систем, которые часто отдают предпочтение поверхностным тактикам в ущерб глубоким рассуждениям. Новый подход успешно решил 5 сложных задач IMO после 2000 года, демонстрируя значительный прогресс в автоматизированном математическом рассуждении.'}, 'en': {'title': 'Decoupling Reasoning and Proving for Enhanced Theorem Proving', 'desc': 'This paper presents a new framework for Automated Theorem Proving (ATP) that separates the processes of reasoning and proving to enhance performance on complex mathematical problems. Current models struggle with formal proving due to their reliance on shallow tactics, which limits deep reasoning capabilities. The proposed solution involves using two specialized models: a Reasoner that generates strategic subgoals and a Prover that verifies these goals rigorously. By decoupling these functions, the framework achieves notable success on challenging IMO problems, marking a significant advancement in automated reasoning.'}, 'zh': {'title': '解耦推理与证明，提升自动定理证明性能', 'desc': '本文提出了一种新颖的框架，将推理与证明解耦，以提高自动定理证明（ATP）的性能。当前的最先进证明器将推理与证明紧密结合，导致深度推理受到抑制，而更倾向于浅层的策略。我们的方法使用两个独立的模型：一个强大的通用推理器生成多样的子目标引理，另一个高效的证明器对其进行严格验证。通过这种模块化设计，我们成功解决了五个2000年后国际数学奥林匹克（IMO）问题，展示了在极具挑战性的数学问题上实现自动推理的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2506.24044', 'title': 'A Survey on Vision-Language-Action Models for Autonomous Driving', 'url': 'https://huggingface.co/papers/2506.24044', 'abstract': "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at https://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.", 'score': 5, 'issue_id': 4738, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'ae24de54097b310f', 'authors': ['Sicong Jiang', 'Zilin Huang', 'Kangan Qian', 'Ziang Luo', 'Tianze Zhu', 'Yang Zhong', 'Yihong Tang', 'Menglin Kong', 'Yunlong Wang', 'Siwen Jiao', 'Hao Ye', 'Zihao Sheng', 'Xin Zhao', 'Tuopu Wen', 'Zheng Fu', 'Sikai Chen', 'Kun Jiang', 'Diange Yang', 'Seongjin Choi', 'Lijun Sun'], 'affiliations': ['McGill University, Canada', 'State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University, China', 'Tsinghua University, China', 'University of Minnesota-Twin Cities, USA', 'University of Wisconsin-Madison, USA', 'Xiaomi Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2506.24044.jpg', 'data': {'categories': ['#reasoning', '#agents', '#architecture', '#benchmark', '#survey', '#dataset', '#multimodal', '#alignment', '#interpretability'], 'emoji': '🚗', 'ru': {'title': 'VLA: Новый горизонт для интерпретируемых и социально-ориентированных беспилотных автомобилей', 'desc': 'Это обзор парадигм Vision-Language-Action (VLA) и их адаптации для автономного вождения. В статье рассматриваются архитектурные компоненты, эволюция моделей и наборы данных для VLA в контексте беспилотных автомобилей. Авторы анализируют более 20 репрезентативных моделей и обсуждают прогресс VLA в области автономного вождения. Также освещаются открытые проблемы, такие как надежность, эффективность в реальном времени и формальная верификация.'}, 'en': {'title': 'Driving the Future: Integrating Vision, Language, and Action in Autonomous Vehicles', 'desc': "This paper surveys the integration of Vision-Language-Action (VLA) paradigms in the context of autonomous driving. It discusses how multimodal large language models can enhance vehicles' abilities to understand visual inputs and natural language commands for decision-making. The authors analyze the evolution of VLA models, compare various architectures, and consolidate datasets relevant to this field. They also identify key challenges such as robustness and real-time efficiency, providing a roadmap for future research in VLA for autonomous driving."}, 'zh': {'title': '视觉-语言-行动：自动驾驶的未来之路', 'desc': '这篇调查论文全面概述了视觉-语言-行动（VLA）范式及其在自动驾驶中的应用，详细介绍了架构组件、模型演变、数据集和未来挑战。随着多模态大语言模型（MLLM）的快速发展，VLA范式将视觉感知、自然语言理解和控制整合在一个策略中。研究人员正在积极将这些方法适应于车辆领域，以实现能够理解高层指令、推理复杂交通场景并自主决策的自动驾驶汽车。论文还总结了现有数据集和基准，强调了共同测量驾驶安全性、准确性和解释质量的协议，并指出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2507.07017', 'title': 'First Return, Entropy-Eliciting Explore', 'url': 'https://huggingface.co/papers/2507.07017', 'abstract': "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.", 'score': 4, 'issue_id': 4740, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '93dc431bd34cdb14', 'authors': ['Tianyu Zheng', 'Tianshun Xing', 'Qingshui Gu', 'Taoran Liang', 'Xingwei Qu', 'Xin Zhou', 'Yizhi Li', 'Zhoufutu Wen', 'Chenghua Lin', 'Wenhao Huang', 'Qian Liu', 'Ge Zhang', 'Zejun Ma'], 'affiliations': ['ByteDance', 'M-A-P', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2507.07017.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'FR3E: Структурированное исследование для улучшения рассуждений ИИ', 'desc': 'FR3E - это новый метод для улучшения способностей рассуждения больших языковых моделей (LLM). Он использует структурированное исследование, выполняя целевые прогоны в точках с высокой неопределенностью. Это приводит к более стабильному обучению и точным ответам LLM. Эмпирические результаты на математических тестах показывают, что FR3E улучшает стабильность обучения, генерирует более длинные и связные ответы, а также увеличивает долю полностью правильных рассуждений.'}, 'en': {'title': 'Structured Exploration for Enhanced LLM Reasoning', 'desc': "FR3E is a framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by focusing on high-uncertainty decision points during their training. It utilizes targeted rollouts to provide structured exploration, which helps in generating semantically grounded feedback without needing extensive supervision. This approach leads to more stable training processes and improves the accuracy of the model's responses. Empirical tests demonstrate that FR3E results in longer, more coherent outputs and a higher rate of correct reasoning paths."}, 'zh': {'title': 'FR3E：提升LLM推理的结构化探索', 'desc': 'FR3E是一种增强大型语言模型（LLM）推理能力的方法。它通过在高不确定性决策点进行有针对性的探索，提供结构化的反馈，从而实现更稳定的训练和更准确的响应。该方法不依赖于密集的监督，而是通过构建语义上扎实的中间反馈来指导模型。实验结果表明，FR3E在数学推理基准测试中表现出更长、更连贯的响应，并提高了完全正确轨迹的比例。'}}}, {'id': 'https://huggingface.co/papers/2507.05687', 'title': 'AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs', 'url': 'https://huggingface.co/papers/2507.05687', 'abstract': 'Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton.', 'score': 4, 'issue_id': 4738, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '1e138b69433f5481', 'authors': ['Shangzhan Li', 'Zefan Wang', 'Ye He', 'Yuxuan Li', 'Qi Shi', 'Jianling Li', 'Yonggang Hu', 'Wanxiang Che', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Harbin Institute of Technology', 'OpenBMB', 'Tianjin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05687.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'AutoTriton: Революция в оптимизации ядер глубокого обучения с помощью RL', 'desc': 'AutoTriton - это первая модель, основанная на обучении с подкреплением (RL), для программирования на Triton. Она использует управляемую тонкую настройку (SFT) для приобретения экспертных знаний в Triton и применяет алгоритм Group Relative Policy Optimization (GRPO) для дальнейшего улучшения навыков программирования. Эксперименты показывают, что 8B-модель AutoTriton достигает производительности, сравнимой с ведущими большими моделями. Это исследование демонстрирует потенциал RL для автоматической генерации высокопроизводительных ядер, что важно для создания более эффективных систем искусственного интеллекта.'}, 'en': {'title': 'AutoTriton: Revolutionizing GPU Programming with Reinforcement Learning', 'desc': 'This paper presents AutoTriton, a novel model designed to enhance Triton programming for GPU optimization using reinforcement learning (RL). It addresses the challenges developers face in manually tuning parameters for performance by automating the process through supervised fine-tuning and RL techniques. The model employs a unique reward system that combines rule-based and execution-based rewards to improve kernel generation. Experimental results show that AutoTriton achieves performance on par with leading models, highlighting its potential to streamline the development of high-performance kernels essential for AI systems.'}, 'zh': {'title': '自动化Triton编程，提升深度学习性能！', 'desc': '本论文介绍了AutoTriton，这是一个基于强化学习的模型，旨在优化Triton编程。通过监督微调和群体相对策略优化算法，AutoTriton能够自动调整关键参数，从而提高GPU编程的性能。实验结果表明，AutoTriton的表现与主流大型模型相当，展示了强化学习在自动生成高性能内核方面的潜力。该研究为构建更高效的人工智能系统奠定了重要基础。'}}}, {'id': 'https://huggingface.co/papers/2507.06260', 'title': "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework", 'url': 'https://huggingface.co/papers/2507.06260', 'abstract': "Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework. Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. Based on this evaluation, we find that Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.", 'score': 0, 'issue_id': 4738, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'e5d9ffe28d4a0466', 'authors': ['Satyapriya Krishna', 'Ninareh Mehrabi', 'Abhinav Mohanty', 'Matteo Memelli', 'Vincent Ponzo', 'Payal Motwani', 'Rahul Gupta'], 'affiliations': ['Amazon Nova Responsible AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.06260.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#multimodal', '#alignment', '#security'], 'emoji': '🔬', 'ru': {'title': 'Nova Premier: мощная и безопасная мультимодальная модель ИИ от Amazon', 'desc': 'Amazon представила Nova Premier - мультимодальную фундаментальную модель, способную обрабатывать текст, изображения и видео с контекстным окном в миллион токенов. Модель прошла комплексную оценку безопасности по методологии Frontier Model Safety Framework в трех критических областях: ХБРЯ, кибероперации и автоматизированные ИИ-исследования. На основе проведенных тестов, включавших автоматизированные бенчмарки и экспертный анализ, модель была признана безопасной для публичного релиза. Amazon планирует продолжать совершенствовать процессы оценки и снижения рисков для передовых моделей ИИ.'}, 'en': {'title': 'Nova Premier: A Safe Multimodal Model for Complex Analysis', 'desc': "Nova Premier is a powerful multimodal foundation model developed by Amazon that can understand and analyze text, images, and videos all at once. It has a large context window of one million tokens, allowing it to handle extensive inputs like long documents and videos in a single prompt. The paper presents a thorough evaluation of Nova Premier's safety using the Frontier Model Safety Framework, focusing on high-risk areas such as CBRN and Offensive Cyber Operations. The findings indicate that Nova Premier meets safety standards for public use, and the team plans to continuously improve safety measures as new challenges arise."}, 'zh': {'title': 'Nova Premier：安全的多模态基础模型', 'desc': 'Nova Premier是亚马逊最强大的多模态基础模型，能够处理文本、图像和视频，具有一百万个标记的上下文窗口。这使得它能够在单个提示中分析大型代码库、400页文档和90分钟视频。我们首次全面评估了Nova Premier在前沿模型安全框架下的关键风险特征，重点关注化学、生物、放射性和核（CBRN）、进攻性网络操作和自动化人工智能研发等高风险领域。评估结果表明，Nova Premier符合2025年巴黎人工智能安全峰会的承诺，适合公开发布，并将继续增强安全评估和风险缓解流程。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment (2)', '#architecture (3)', '#audio', '#benchmark (6)', '#cv (1)', '#data', '#dataset (5)', '#diffusion', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (2)', '#optimization (4)', '#plp', '#rag', '#reasoning (4)', '#rl (4)', '#rlhf (1)', '#robotics (1)', '#science', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-10 04:24',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-10 04:24')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-10 04:24')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    