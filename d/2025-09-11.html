
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. September 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 сентября</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-10.html">⬅️ <span id="prev-date">10.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-12.html">➡️ <span id="next-date">12.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'};
        let feedDateNext = {'ru': '12.09', 'en': '09/12', 'zh': '9月12日'};
        let feedDatePrev = {'ru': '10.09', 'en': '09/10', 'zh': '9月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.08827', 'title': 'A Survey of Reinforcement Learning for Large Reasoning Models', 'url': 'https://huggingface.co/papers/2509.08827', 'abstract': 'Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs', 'score': 91, 'issue_id': 5829, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'ea01091be58aef4b', 'authors': ['Kaiyan Zhang', 'Yuxin Zuo', 'Bingxiang He', 'Youbang Sun', 'Runze Liu', 'Che Jiang', 'Yuchen Fan', 'Kai Tian', 'Guoli Jia', 'Pengfei Li', 'Yu Fu', 'Xingtai Lv', 'Yuchen Zhang', 'Sihang Zeng', 'Shang Qu', 'Haozhan Li', 'Shijie Wang', 'Yuru Wang', 'Xinwei Long', 'Fangfu Liu', 'Xiang Xu', 'Jiaze Ma', 'Xuekai Zhu', 'Ermo Hua', 'Yihao Liu', 'Zonglin Li', 'Huayu Chen', 'Xiaoye Qu', 'Yafu Li', 'Weize Chen', 'Zhenzhao Yuan', 'Junqi Gao', 'Dong Li', 'Zhiyuan Ma', 'Ganqu Cui', 'Zhiyuan Liu', 'Biqing Qi', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Harbin Institute of Technology', 'Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University College London', 'University of Science and Technology of China', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.08827.jpg', 'data': {'categories': ['#training', '#rl', '#rlhf', '#survey', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением: ключ к улучшению рассуждений языковых моделей', 'desc': 'Эта статья представляет обзор последних достижений в области обучения с подкреплением (RL) для улучшения способностей больших языковых моделей (LLM) к рассуждению. Авторы рассматривают успехи RL в решении сложных логических задач, таких как математика и программирование, что привело к появлению языковых моделей с улучшенными навыками рассуждения (LRM). В работе обсуждаются проблемы масштабирования RL для LRM, включая вычислительные ресурсы, алгоритмы, данные для обучения и инфраструктуру. Статья анализирует текущее состояние области и перспективы развития RL для создания моделей с более широкими возможностями рассуждения.'}, 'en': {'title': 'Scaling Reinforcement Learning for Advanced Reasoning in Language Models', 'desc': 'This paper reviews the integration of Reinforcement Learning (RL) with Large Language Models (LLMs) to improve their reasoning capabilities. It highlights the success of RL in enhancing LLMs for complex tasks like mathematics and coding, positioning RL as a key method for evolving LLMs into more advanced reasoning models (LRMs). The authors discuss the challenges of scaling RL, including the need for better computational resources, algorithm design, and training data. They aim to identify future research directions to further develop RL applications in reasoning tasks, especially in the context of achieving Artificial SuperIntelligence (ASI).'}, 'zh': {'title': '强化学习助力大型语言模型推理能力提升', 'desc': '本论文调查了强化学习（RL）在大型语言模型（LLM）推理任务中的最新进展。强化学习在提升LLM能力方面取得了显著成功，尤其是在解决复杂的逻辑任务如数学和编程方面。随着该领域的快速发展，RL在大型语言模型的扩展面临着计算资源、算法设计、训练数据和基础设施等基础性挑战。我们希望通过这项综述促进未来在更广泛推理模型上应用强化学习的研究。'}}}, {'id': 'https://huggingface.co/papers/2509.08826', 'title': 'RewardDance: Reward Scaling in Visual Generation', 'url': 'https://huggingface.co/papers/2509.08826', 'abstract': 'RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model\'s probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.', 'score': 49, 'issue_id': 5829, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '773c62b4801465a6', 'authors': ['Jie Wu', 'Yu Gao', 'Zilyu Ye', 'Ming Li', 'Liang Li', 'Hanzhong Guo', 'Jie Liu', 'Zeyue Xue', 'Xiaoxia Hou', 'Wei Liu', 'Yan Zeng', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.08826.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#rlhf', '#alignment', '#multimodal'], 'emoji': '💃', 'ru': {'title': 'RewardDance: Танцуя с вознаграждениями в масштабируемом обучении генеративных моделей', 'desc': "RewardDance - это масштабируемая система моделирования вознаграждений, совместимая с архитектурами VLM. Она позволяет эффективно масштабировать модели вознаграждений и решает проблемы эксплуатации вознаграждений в генеративных моделях. RewardDance переформулирует оценку вознаграждения как вероятность модели предсказать токен 'да', что позволяет масштабировать как саму модель, так и контекст. Эксперименты показывают, что RewardDance превосходит современные методы в задачах генерации изображений и видео, сохраняя высокую вариативность вознаграждений при обучении с подкреплением."}, 'en': {'title': 'RewardDance: Scaling Reward Models for Better AI Generation', 'desc': 'RewardDance is a new framework designed to improve reward modeling in visual generation tasks by aligning with Vision-Language Models (VLMs). It addresses the limitations of existing reward models, which often struggle with architectural constraints and misalignment with next-token predictions. The framework introduces a novel generative reward paradigm that reformulates reward scoring, allowing for effective scaling of reward models up to 26 billion parameters. Importantly, RewardDance mitigates the issue of reward hacking, ensuring that models produce diverse and high-quality outputs during reinforcement learning fine-tuning.'}, 'zh': {'title': 'RewardDance：解决奖励黑客的可扩展奖励建模框架', 'desc': 'RewardDance是一个可扩展的奖励建模框架，旨在与视觉语言模型（VLM）架构对齐，从而有效地扩展奖励模型（RM）并解决生成模型中的奖励黑客问题。现有的奖励模型在视觉生成中的扩展性受到架构和输入模态的限制，而流行的Bradley-Terry损失与VLM的下一个标记预测机制不匹配，阻碍了有效扩展。通过将奖励分数重新定义为模型预测“是”标记的概率，RewardDance使奖励目标与VLM架构内在对齐，从而在模型和上下文两个维度上实现扩展。实验表明，RewardDance在文本到图像、文本到视频和图像到视频生成方面显著超越了现有的最先进方法，并有效解决了奖励黑客问题。'}}}, {'id': 'https://huggingface.co/papers/2509.07996', 'title': '3D and 4D World Modeling: A Survey', 'url': 'https://huggingface.co/papers/2509.07996', 'abstract': "This survey provides a comprehensive review of 3D and 4D world modeling and generation, establishing definitions, taxonomy, datasets, and evaluation metrics, and discussing applications and challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey", 'score': 37, 'issue_id': 5830, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '1cdd6619184c7e1e', 'authors': ['Lingdong Kong', 'Wesley Yang', 'Jianbiao Mei', 'Youquan Liu', 'Ao Liang', 'Dekai Zhu', 'Dongyue Lu', 'Wei Yin', 'Xiaotao Hu', 'Mingkai Jia', 'Junyuan Deng', 'Kaiwen Zhang', 'Yang Wu', 'Tianyi Yan', 'Shenyuan Gao', 'Song Wang', 'Linfeng Li', 'Liang Pan', 'Yong Liu', 'Jianke Zhu', 'Wei Tsang Ooi', 'Steven C. H. Hoi', 'Ziwei Liu'], 'affiliations': ['CNRS@CREATE, Singapore', 'HKUST', 'Horizon Robotics', 'HyperGAI', 'Nanjing University of Science and Technology', 'Nanyang Technological University, Singapore', 'National University of Singapore', 'Shanghai AI Laboratory', 'Technical University of Munich', 'Tsinghua University', 'University of Macau', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07996.jpg', 'data': {'categories': ['#benchmark', '#survey', '#3d', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Комплексный обзор моделирования 3D и 4D миров: от определений до применений', 'desc': 'Этот обзор представляет собой всесторонний анализ моделирования и генерации 3D и 4D миров. В нем устанавливаются определения, таксономия, наборы данных и метрики оценки для этой области. Авторы вводят структурированную классификацию, охватывающую подходы на основе видео (VideoGen), занятости пространства (OccGen) и LiDAR (LiDARGen). Обсуждаются практические применения, открытые проблемы и перспективные направления исследований в сфере моделирования трехмерных и четырехмерных миров.'}, 'en': {'title': 'Unifying 3D and 4D World Modeling for AI Understanding', 'desc': 'This paper reviews the field of 3D and 4D world modeling and generation, focusing on how AI can understand and predict environments. It highlights the importance of using native 3D and 4D data types, like RGB-D images and LiDAR point clouds, which are often overlooked in favor of 2D methods. The authors propose a clear taxonomy for world models, categorizing them into video-based, occupancy-based, and LiDAR-based approaches. Additionally, the survey outlines datasets, evaluation metrics, and discusses applications and challenges in the field, aiming to unify and advance research in 3D and 4D modeling.'}, 'zh': {'title': '3D与4D世界建模的全面综述', 'desc': '这篇综述文章全面回顾了3D和4D世界建模与生成的研究，建立了相关的定义、分类、数据集和评估指标，并讨论了应用和挑战。文章指出，尽管以往的研究主要集中在2D图像和视频数据的生成方法上，但对3D和4D表示（如RGB-D图像、占用网格和LiDAR点云）的研究正在快速增长。为了填补文献中缺乏标准化定义和分类的空白，本文首次系统性地总结了3D和4D世界建模与生成的相关工作。最后，文章还讨论了实际应用、识别开放挑战，并强调了未来的研究方向，旨在为该领域的进步提供一个连贯的基础参考。'}}}, {'id': 'https://huggingface.co/papers/2509.08755', 'title': 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.08755', 'abstract': 'AgentGym-RL is a modular RL framework for training LLM agents in diverse environments without supervised fine-tuning, featuring ScalingInter-RL for balanced exploration-exploitation.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.', 'score': 19, 'issue_id': 5830, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'ed8314c7d1c6d19b', 'authors': ['Zhiheng Xi', 'Jixuan Huang', 'Chenyang Liao', 'Baodai Huang', 'Honglin Guo', 'Jiaqi Liu', 'Rui Zheng', 'Junjie Ye', 'Jiazheng Zhang', 'Wenxiang Chen', 'Wei He', 'Yiwen Ding', 'Guanyu Li', 'Zehui Chen', 'Zhengyin Du', 'Xuesong Yao', 'Yufei Xu', 'Jiecao Chen', 'Tao Gui', 'Zuxuan Wu', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Seed', 'Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.08755.jpg', 'data': {'categories': ['#optimization', '#open_source', '#rl', '#training', '#agents', '#games'], 'emoji': '🤖', 'ru': {'title': 'AgentGym-RL: Универсальная платформа для обучения автономных LLM-агентов', 'desc': 'AgentGym-RL - это новая модульная платформа для обучения агентов на основе больших языковых моделей (LLM) с помощью обучения с подкреплением (RL) без использования обучения с учителем. Она включает в себя разнообразные реалистичные среды и поддерживает основные алгоритмы RL. Предложенный подход ScalingInter-RL обеспечивает баланс между исследованием и использованием опыта в процессе обучения. Эксперименты показывают, что обученные агенты соответствуют или превосходят коммерческие модели на 27 задачах в различных средах.'}, 'en': {'title': 'Empowering LLM Agents with AgentGym-RL: Explore, Learn, Succeed!', 'desc': 'AgentGym-RL is a new modular reinforcement learning (RL) framework designed to train large language model (LLM) agents in various environments without the need for supervised fine-tuning. It allows agents to learn through exploration and interaction, similar to human cognitive development, enabling them to make intelligent decisions in complex tasks. The framework includes a unique training method called ScalingInter-RL, which balances exploration and exploitation to optimize learning stability. Extensive experiments show that agents trained with this framework perform competitively against commercial models across multiple tasks, and the framework will be open-sourced to support further research in intelligent agent development.'}, 'zh': {'title': 'AgentGym-RL：无监督强化学习的智能代理训练框架', 'desc': 'AgentGym-RL是一个模块化的强化学习框架，旨在训练大型语言模型（LLM）代理在多样化环境中进行决策，而无需监督微调。该框架采用了ScalingInter-RL方法，以平衡探索与利用，确保代理在学习过程中能够有效地获取知识和技能。通过限制早期的交互次数，框架强调利用，随后逐步增加探索的范围，从而鼓励多样化的问题解决策略。实验结果表明，AgentGym-RL框架及其训练方法在27个任务中表现优于商业模型，展示了其稳定性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.06784', 'title': 'P3-SAM: Native 3D Part Segmentation', 'url': 'https://huggingface.co/papers/2509.06784', 'abstract': 'P3-SAM, a native 3D point-promptable part segmentation model, achieves precise and robust segmentation of complex 3D objects using a feature extractor, multiple segmentation heads, and an IoU predictor.  \t\t\t\t\tAI-generated summary \t\t\t\t Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon.', 'score': 13, 'issue_id': 5830, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'c461d5e9a3042a90', 'authors': ['Changfeng Ma', 'Yang Li', 'Xinhao Yan', 'Jiachen Xu', 'Yunhan Yang', 'Chunshi Wang', 'Zibo Zhao', 'Yanwen Guo', 'Zhuo Chen', 'Chunchao Guo'], 'affiliations': ['HKU', 'NJU', 'ShanghaiTech', 'Tencent Hunyuan', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2509.06784.jpg', 'data': {'categories': ['#optimization', '#3d', '#dataset', '#training', '#games'], 'emoji': '🧩', 'ru': {'title': 'Автоматическая сегментация сложных 3D-объектов с помощью глубокого обучения', 'desc': 'P3-SAM - это модель сегментации частей 3D-объектов, использующая экстрактор признаков, несколько сегментационных головок и предиктор IoU. Модель обучена на наборе данных из 3,7 миллионов моделей с размеченными сегментами. P3-SAM позволяет проводить точную и надежную сегментацию сложных 3D-объектов, превосходя существующие методы. Модель также предлагает алгоритм для автоматического выбора и объединения масок для сегментации экземпляров частей.'}, 'en': {'title': 'Automating 3D Part Segmentation with P3-SAM', 'desc': 'P3-SAM is a novel model designed for segmenting 3D objects into their individual parts using point prompts. It utilizes a feature extractor and multiple segmentation heads, along with an Intersection over Union (IoU) predictor, to enhance segmentation accuracy and robustness. The model aims to automate the segmentation process, addressing limitations of existing methods that struggle with complex shapes. Trained on a large dataset of 3.7 million models, P3-SAM demonstrates state-of-the-art performance in part instance segmentation.'}, 'zh': {'title': 'P3-SAM：实现3D物体的自动化精确分割', 'desc': 'P3-SAM是一种原生的3D点提示部件分割模型，能够精确且稳健地对复杂3D物体进行分割。该模型采用特征提取器、多重分割头和IoU预测器，旨在实现3D物体的自动化分割。与现有方法相比，P3-SAM在处理复杂物体时表现出更强的鲁棒性，并支持用户进行交互式分割。我们在一个包含近370万个模型的新数据集上训练了该模型，实验结果表明其在分割精度和鲁棒性方面达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2509.05209', 'title': 'Hunyuan-MT Technical Report', 'url': 'https://huggingface.co/papers/2509.05209', 'abstract': 'Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are multilingual translation models that outperform existing models, especially in translating between Mandarin and minority languages, through a combination of pre-training, supervised fine-tuning, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic.', 'score': 9, 'issue_id': 5830, 'pub_date': '2025-09-05', 'pub_date_card': {'ru': '5 сентября', 'en': 'September 5', 'zh': '9月5日'}, 'hash': '1bc53ae7f9d89dff', 'authors': ['Mao Zheng', 'Zheng Li', 'Bingxin Qu', 'Mingyang Song', 'Yang Du', 'Mingrui Sun', 'Di Wang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2509.05209.jpg', 'data': {'categories': ['#machine_translation', '#open_source', '#low_resource', '#multilingual', '#rl', '#training'], 'emoji': '🌐', 'ru': {'title': 'Прорыв в многоязычном машинном переводе', 'desc': 'Модели Hunyuan-MT-7B и Hunyuan-MT-Chimera-7B - это многоязычные модели машинного перевода, превосходящие существующие аналоги. Они особенно эффективны при переводе между китайским и языками меньшинств. Модели были обучены с использованием предварительного обучения, контролируемой тонкой настройки и обучения с подкреплением. В задаче WMT2025 по общему машинному переводу эти модели показали лучшие результаты для 30 из 31 языковых пар.'}, 'en': {'title': 'Revolutionizing Multilingual Translation with Hunyuan Models', 'desc': 'Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are advanced multilingual translation models designed to excel in translating between Mandarin and various minority languages. They utilize a comprehensive training approach that includes pre-training, supervised fine-tuning, and reinforcement learning to enhance translation accuracy. The Chimera model innovatively combines multiple outputs from the Hunyuan-MT-7B to improve performance beyond traditional models. Experimental results show that these models achieve state-of-the-art performance in multilingual translation tasks, particularly excelling in low-resource language pairs.'}, 'zh': {'title': '超越传统的多语言翻译新模型', 'desc': 'Hunyuan-MT-7B和Hunyuan-MT-Chimera-7B是多语言翻译模型，特别擅长于普通话与少数民族语言之间的翻译。这些模型通过预训练、监督微调和强化学习的结合，显著提升了翻译性能。Hunyuan-MT-Chimera-7B采用慢思维模式，整合了多种输出，超越了传统的链式思维模型。经过全面实验，我们的模型在WMT2025共享任务中表现出色，在31个语言对中排名第一，展示了其在多样语言环境中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2509.08358', 'title': "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs", 'url': 'https://huggingface.co/papers/2509.08358', 'abstract': 'Models fine-tuned on synthetic toxic data generated by LLMs perform worse than those trained on human data due to a lexical diversity gap in the synthetic content.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.', 'score': 5, 'issue_id': 5838, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'e5ca2a20313423bf', 'authors': ['Sergey Pletenev', 'Daniil Moskovskiy', 'Alexander Panchenko'], 'affiliations': ['AIRI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2509.08358.jpg', 'data': {'categories': ['#ethics', '#training', '#data', '#healthcare', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Человеческие данные превосходят синтетические в обучении моделей детоксификации текста', 'desc': 'Исследование показало, что модели машинного обучения, обученные на синтетических токсичных данных, сгенерированных языковыми моделями (LLM), работают хуже, чем те, которые обучены на человеческих данных. Основная причина заключается в недостаточном лексическом разнообразии синтетического контента. LLM генерируют токсичный контент, используя ограниченный и повторяющийся словарь оскорблений, который не отражает нюансы и разнообразие человеческой токсичности. Это исследование подчеркивает ограничения современных LLM в области детоксификации текста и важность разнообразных, аннотированных человеком данных для создания надежных систем детоксификации.'}, 'en': {'title': 'Synthetic Toxic Data: A Step Back in Detoxification Performance', 'desc': 'This paper investigates the effectiveness of using synthetic toxic data generated by Large Language Models (LLMs) for training detoxification models. The authors found that models fine-tuned on this synthetic data performed significantly worse than those trained on human-generated data, with performance drops of up to 30%. The main issue identified is a lexical diversity gap, where LLMs produce a limited range of toxic language, lacking the complexity found in human-generated content. This study underscores the necessity of using diverse, human-annotated data to create more effective detoxification systems.'}, 'zh': {'title': '合成数据无法替代人类数据的去毒化训练', 'desc': '本研究探讨了使用大型语言模型（LLMs）生成的合成有毒数据来训练文本去毒化模型的可能性。实验结果表明，基于合成数据微调的模型性能明显低于基于人类数据训练的模型，性能下降可达30%。造成这一现象的原因是合成数据在词汇多样性上存在显著差距，LLMs生成的有毒内容使用了有限且重复的侮辱性词汇，无法捕捉人类毒性表达的细微差别。研究结果强调了在去毒化系统构建中，依然需要多样化的人类标注数据的重要性。'}}}, {'id': 'https://huggingface.co/papers/2509.06870', 'title': 'The Majority is not always right: RL training for solution aggregation', 'url': 'https://huggingface.co/papers/2509.06870', 'abstract': 'A reinforcement learning approach to aggregating multiple solutions for large language models improves performance on reasoning tasks by learning to synthesize correct answers from candidate solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.', 'score': 5, 'issue_id': 5841, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'c594673839c4eb24', 'authors': ['Wenting Zhao', 'Pranjal Aggarwal', 'Swarnadeep Saha', 'Asli Celikyilmaz', 'Jason Weston', 'Ilia Kulikov'], 'affiliations': ['CMU', 'FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2509.06870.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Умная агрегация решений ИИ через обучение с подкреплением', 'desc': 'Статья представляет новый подход к агрегации решений больших языковых моделей с помощью обучения с подкреплением. Метод AggLM обучается синтезировать правильные ответы из набора кандидатов-решений, превосходя базовые методы агрегации. Ключевой особенностью является баланс простых и сложных примеров при обучении. Подход эффективно обобщается на решения от различных моделей, даже более сильных, чем в обучающих данных.'}, 'en': {'title': 'Learn to Aggregate: Enhancing Reasoning in LLMs with Reinforcement Learning', 'desc': 'This paper presents a novel reinforcement learning method called AggLM for aggregating multiple solutions generated by large language models (LLMs) to enhance their performance on reasoning tasks. Instead of relying on traditional methods like majority voting, AggLM learns to synthesize the best answer from a set of candidate solutions by training an aggregator model using reinforcement learning with verifiable rewards. The approach emphasizes a balanced training strategy that includes both easy and challenging examples, enabling the model to identify correct minority answers as well as majority answers. Experimental results show that AggLM outperforms existing aggregation methods and effectively generalizes to solutions from various models, while also being more efficient in token usage.'}, 'zh': {'title': '强化学习提升语言模型推理能力', 'desc': '本文提出了一种强化学习方法，用于聚合多个解决方案以提高大型语言模型在推理任务上的表现。我们训练了一个聚合模型，能够从候选解决方案中审查、调和并合成最终的正确答案。与传统的简单多数投票或奖励模型排名方法相比，我们的方法通过学习聚合作为一种明确的推理技能，取得了更好的效果。实验结果表明，AggLM在多个基准测试中超越了强大的基于规则和奖励模型的基线，并且在处理来自不同模型的解决方案时表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.08088', 'title': 'EnvX: Agentize Everything with Agentic AI', 'url': 'https://huggingface.co/papers/2509.08088', 'abstract': "EnvX leverages Agentic AI to transform GitHub repositories into intelligent agents capable of natural language interaction and collaboration, automating the entire process of understanding, initializing, and operationalizing repository functionality.  \t\t\t\t\tAI-generated summary \t\t\t\t The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.", 'score': 2, 'issue_id': 5830, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '0afcc93155a2cf60', 'authors': ['Linyao Chen', 'Zimian Peng', 'Yingxuan Yang', 'Yikun Wang', 'Wenzheng Tom Tang', 'Hiroki H. Kobayashi', 'Weinan Zhang'], 'affiliations': ['EnvX Team', 'Fudan University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Tokyo', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.08088.jpg', 'data': {'categories': ['#agi', '#open_source', '#benchmark', '#multimodal', '#agents'], 'emoji': '🤖', 'ru': {'title': 'EnvX: Превращаем GitHub-репозитории в интеллектуальных агентов', 'desc': 'EnvX - это фреймворк, использующий агентный искусственный интеллект для превращения GitHub-репозиториев в интеллектуальных автономных агентов. Он автоматизирует процесс понимания, инициализации и операционализации функциональности репозиториев через трехфазный процесс. EnvX позволяет агентам взаимодействовать на естественном языке и сотрудничать друг с другом. Оценка на GitTaskBench показала, что EnvX превосходит существующие фреймворки по показателям выполнения и прохождения задач.'}, 'en': {'title': 'Transforming Repositories into Intelligent Agents for Seamless Collaboration', 'desc': 'EnvX is a framework that transforms GitHub repositories into intelligent agents using Agentic AI, allowing for natural language interaction and collaboration. It automates the understanding, initialization, and operationalization of repository functionality, addressing the challenges developers face with manual and error-prone processes. The framework operates in three phases: initializing the environment, enabling autonomous task performance, and facilitating collaboration between agents. By leveraging large language models and structured tool integration, EnvX significantly improves the efficiency of software reuse and collaboration in open-source projects.'}, 'zh': {'title': '将代码库转变为智能代理的革命性框架', 'desc': 'EnvX是一个利用Agentic AI的框架，旨在将GitHub代码库转变为智能代理，能够进行自然语言交互和协作。它通过三个阶段的过程实现这一目标：首先是环境初始化，设置必要的依赖和数据；其次是人类对齐的自动化，使得特定代码库的代理能够自主执行实际任务；最后是代理间协议，允许多个代理进行协作。EnvX不仅自动生成代码，还自动化理解、初始化和操作代码库功能的整个过程，显著提高了软件重用的效率。'}}}, {'id': 'https://huggingface.co/papers/2509.07054', 'title': 'Statistical Methods in Generative AI', 'url': 'https://huggingface.co/papers/2509.07054', 'abstract': 'Statistical methods are reviewed for improving the reliability, quality, and efficiency of generative AI techniques, highlighting their applications and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.   In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.', 'score': 2, 'issue_id': 5848, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'e4fb628edc34b0e7', 'authors': ['Edgar Dobriban'], 'affiliations': ['Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.07054.jpg', 'data': {'categories': ['#ethics', '#training', '#survey', '#data', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'Статистика на страже надежности генеративного ИИ', 'desc': 'Статья рассматривает статистические методы для улучшения надежности, качества и эффективности генеративных методов искусственного интеллекта. Авторы объясняют, как статистические подходы могут помочь в оценке и улучшении генеративных моделей, которые по умолчанию не гарантируют корректность или безопасность результатов. Обсуждаются существующие работы в этой области, описываются применяемые статистические техники и их приложения к генеративному ИИ. Также рассматриваются ограничения методов и потенциальные направления будущих исследований.'}, 'en': {'title': 'Enhancing Generative AI with Statistical Reliability', 'desc': 'This paper reviews statistical methods that can enhance the reliability, quality, and efficiency of generative AI techniques. Generative AI, while promising, often lacks guarantees regarding correctness and fairness due to its reliance on probabilistic models. The authors highlight how statistical approaches can address these issues and improve AI evaluation processes. Additionally, they discuss the limitations of current methods and suggest future research directions to further advance the field.'}, 'zh': {'title': '提升生成式人工智能的可靠性与效率', 'desc': '本文回顾了统计方法在提高生成式人工智能技术的可靠性、质量和效率方面的应用与局限性。生成式人工智能是一项新兴技术，具有变革多个领域的潜力，但其基于概率模型的采样方法缺乏正确性、安全性和公平性等保证。统计方法为提高生成式人工智能的可靠性提供了有希望的解决方案，同时也能改善人工智能评估的质量和效率。我们讨论了现有工作的概述，解释了所用的统计技术及其在生成式人工智能中的应用，并探讨了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2509.08494', 'title': 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants', 'url': 'https://huggingface.co/papers/2509.08494', 'abstract': 'A benchmark evaluates human agency in AI assistants using large language models, finding varying support across systems and dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.', 'score': 0, 'issue_id': 5830, 'pub_date': '2025-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'f9f3a3cdf8cd2c8d', 'authors': ['Benjamin Sturgeon', 'Daniel Samuelson', 'Jacob Haimes', 'Jacy Reese Anthis'], 'affiliations': ['AI Safety Cape Town', 'Apart Research', 'Sentience Institute', 'Stanford University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.08494.jpg', 'data': {'categories': ['#ethics', '#alignment', '#benchmark', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Оценка человеческой субъектности в эпоху ИИ-ассистентов', 'desc': 'Статья представляет новый бенчмарк HumanAgencyBench (HAB) для оценки поддержки человеческой субъектности в ИИ-ассистентах на основе больших языковых моделей (LLM). HAB измеряет шесть аспектов субъектности, включая способность задавать уточняющие вопросы и избегать манипуляций ценностями. Исследование показало низкую и среднюю поддержку субъектности в современных LLM-ассистентах, с существенными различиями между разработчиками и измерениями. Авторы призывают к более надежным целям безопасности и согласованности ИИ.'}, 'en': {'title': 'Empowering Human Agency in AI: A New Benchmark Approach', 'desc': 'This paper introduces a benchmark called HumanAgencyBench (HAB) to evaluate how well AI assistants support human agency across various dimensions. It combines philosophical and scientific theories of agency with AI evaluation methods, specifically using large language models (LLMs) to assess AI responses to user queries. The study finds that current LLM-based assistants provide low-to-moderate support for human agency, with significant differences among systems and dimensions. The authors suggest that improving agency support should not solely rely on enhancing LLM capabilities but also focus on establishing better safety and alignment standards.'}, 'zh': {'title': '提升AI助手中的人类代理权', 'desc': '本论文探讨了人工智能助手中人类代理权的评估，使用大型语言模型（LLMs）进行模拟和验证用户查询。我们提出了HumanAgencyBench（HAB），这是一个可扩展的基准，涵盖六个维度的人类代理权，旨在评估AI助手在不同场景下的表现。研究发现，当前基于LLM的助手在人类代理权支持方面表现低至中等，并且在不同系统开发者和维度之间存在显著差异。我们建议在AI系统的安全性和对齐目标上进行更强有力的改进，以更好地支持人类代理权。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (1)', '#alignment (2)', '#architecture', '#audio', '#benchmark (5)', '#cv', '#data (2)', '#dataset (2)', '#diffusion', '#ethics (3)', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (2)', '#open_source (3)', '#optimization (4)', '#plp', '#rag', '#reasoning (2)', '#rl (5)', '#rlhf (3)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (3)', '#synthetic (1)', '#training (8)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-11 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-11 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-11 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    