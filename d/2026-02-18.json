{
    "date": {
        "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 18",
        "zh": "2æœˆ18æ—¥"
    },
    "time_utc": "2026-02-18 10:37",
    "weekday": 2,
    "issue_id": 1110,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.15763",
            "title": "GLM-5: from Vibe Coding to Agentic Engineering",
            "url": "https://huggingface.co/papers/2602.15763",
            "abstract": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
            "score": 19,
            "issue_id": 1104,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "fda04435921a43b6",
            "authors": [
                "GLM-5 Team",
                ":",
                "Aohan Zeng",
                "Xin Lv",
                "Zhenyu Hou",
                "Zhengxiao Du",
                "Qinkai Zheng",
                "Bin Chen",
                "Da Yin",
                "Chendi Ge",
                "Chengxing Xie",
                "Cunxiang Wang",
                "Gengzheng Pan",
                "Hao Zeng",
                "Haoke Zhang",
                "Haoran Wang",
                "Huilong Chen",
                "Jiajie Zhang",
                "Jian Jiao",
                "Jiaqi Guo",
                "Jingsen Wang",
                "Jingzhao Du",
                "Jinzhu Wu",
                "Kedong Wang",
                "Lei Li",
                "Lin Fan",
                "Lucen Zhong",
                "Mingdao Liu",
                "Mingming Zhao",
                "Pengfan Du",
                "Qian Dong",
                "Rui Lu",
                "Shuang-Li",
                "Shulin Cao",
                "Song Liu",
                "Ting Jiang",
                "Xiaodong Chen",
                "Xiaohan Zhang",
                "Xuancheng Huang",
                "Xuezhen Dong",
                "Yabo Xu",
                "Yao Wei",
                "Yifan An",
                "Yilin Niu",
                "Yitong Zhu",
                "Yuanhao Wen",
                "Yukuo Cen",
                "Yushi Bai",
                "Zhongpei Qiao",
                "Zihan Wang",
                "Zikang Wang",
                "Zilin Zhu",
                "Ziqiang Liu",
                "Zixuan Li",
                "Bojie Wang",
                "Bosi Wen",
                "Can Huang",
                "Changpeng Cai",
                "Chao Yu",
                "Chen Li",
                "Chen Li",
                "Chenghua Huang",
                "Chengwei Hu",
                "Chenhui Zhang",
                "Chenzheng Zhu",
                "Congfeng Yin",
                "Daoyan Lin",
                "Dayong Yang",
                "Di Wang",
                "Ding Ai",
                "Erle Zhu",
                "Fangzhou Yi",
                "Feiyu Chen",
                "Guohong Wen",
                "Hailong Sun",
                "Haisha Zhao",
                "Haiyi Hu",
                "Hanchen Zhang",
                "Hanrui Liu",
                "Hanyu Zhang",
                "Hao Peng",
                "Hao Tai",
                "Haobo Zhang",
                "He Liu",
                "Hongwei Wang",
                "Hongxi Yan",
                "Hongyu Ge",
                "Huan Liu",
                "Huan Liu",
                "Huanpeng Chu",
                "Jia'ni Zhao",
                "Jiachen Wang",
                "Jiajing Zhao",
                "Jiamin Ren",
                "Jiapeng Wang",
                "Jiaxin Zhang",
                "Jiayi Gui",
                "Jiayue Zhao",
                "Jijie Li",
                "Jing An",
                "Jing Li",
                "Jingwei Yuan",
                "Jinhua Du",
                "Jinxin Liu",
                "Junkai Zhi",
                "Junwen Duan",
                "Kaiyue Zhou",
                "Kangjian Wei",
                "Ke Wang",
                "Keyun Luo",
                "Laiqiang Zhang",
                "Leigang Sha",
                "Liang Xu",
                "Lindong Wu",
                "Lintao Ding",
                "Lu Chen",
                "Minghao Li",
                "Nianyi Lin",
                "Pan Ta",
                "Qiang Zou",
                "Rongjun Song",
                "Ruiqi Yang",
                "Shangqing Tu",
                "Shangtong Yang",
                "Shaoxiang Wu",
                "Shengyan Zhang",
                "Shijie Li",
                "Shuang Li",
                "Shuyi Fan",
                "Wei Qin",
                "Wei Tian",
                "Weining Zhang",
                "Wenbo Yu",
                "Wenjie Liang",
                "Xiang Kuang",
                "Xiangmeng Cheng",
                "Xiangyang Li",
                "Xiaoquan Yan",
                "Xiaowei Hu",
                "Xiaoying Ling",
                "Xing Fan",
                "Xingye Xia",
                "Xinyuan Zhang",
                "Xinze Zhang",
                "Xirui Pan",
                "Xunkai Zhang",
                "Yandong Wu",
                "Yanfu Li",
                "Yidong Wang",
                "Yifan Zhu",
                "Yijun Tan",
                "Yilin Zhou",
                "Yiming Pan",
                "Ying Zhang",
                "Yinpei Su",
                "Yipeng Geng",
                "Yipeng Geng",
                "Yong Yan",
                "Yonglin Tan",
                "Yuean Bi",
                "Yuhan Shen",
                "Yuhao Yang",
                "Yujiang Li",
                "Yunan Liu",
                "Yunqing Wang",
                "Yuntao Li",
                "Yurong Wu",
                "Yutao Zhang",
                "Yuxi Duan",
                "Yuxuan Zhang",
                "Zezhen Liu",
                "Zhengtao Jiang",
                "Zhenhe Yan",
                "Zheyu Zhang",
                "Zhixiang Wei",
                "Zhuo Chen",
                "Zhuoer Feng",
                "Zijun Yao",
                "Ziwei Chai",
                "Ziyuan Wang",
                "Zuzhou Zhang",
                "Bin Xu",
                "Minlie Huang",
                "Hongning Wang",
                "Juanzi Li",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15763.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#agents",
                    "#training",
                    "#alignment",
                    "#optimization",
                    "#architecture",
                    "#plp",
                    "#open_source",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "GLM-5 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ DSA Ğ´Ğ»Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞµÑ‘ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼. GLM-5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² end-to-end Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "GLM-5: Revolutionizing Software Engineering with Advanced AI",
                    "desc": "GLM-5 is a cutting-edge foundation model that enhances the capabilities of machine learning in software engineering. It utilizes Dynamic Sparse Attention (DSA) to lower the costs associated with training and inference while preserving the ability to handle long contexts. The model also incorporates asynchronous reinforcement learning to improve alignment and efficiency, allowing it to learn better from complex interactions. As a result, GLM-5 sets new benchmarks in real-world coding tasks, outperforming previous models in software engineering applications."
                },
                "zh": {
                    "title": "GLM-5ï¼šæå‡ç¼–ç èƒ½åŠ›ä¸æˆæœ¬æ•ˆç›Šçš„åŸºç¡€æ¨¡å‹",
                    "desc": "GLM-5æ˜¯ä¸€ç§æ–°ä¸€ä»£åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥åŠ¨æ€ç»“æ„è°ƒæ•´ï¼ˆDSAï¼‰æ¥é™ä½è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒé•¿ä¸Šä¸‹æ–‡çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½ï¼Œæ˜¾è‘—æé«˜äº†åæœŸè®­ç»ƒçš„æ•ˆç‡ï¼Œä½¿ç”Ÿæˆä¸è®­ç»ƒè§£è€¦ã€‚GLM-5è¿˜æå‡ºäº†æ–°å‹çš„å¼‚æ­¥ä»£ç†å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä»å¤æ‚çš„é•¿æ—¶é—´äº¤äº’ä¸­å­¦ä¹ ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒGLM-5åœ¨ä¸»è¦å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å®é™…ç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14299",
            "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
            "url": "https://huggingface.co/papers/2602.14299",
            "abstract": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
            "score": 15,
            "issue_id": 1104,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "8484c159d88b35e7",
            "authors": [
                "Ming Li",
                "Xirui Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14299.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ±ĞµĞ· ÑĞ¾Ñ†Ğ¸ÑƒĞ¼Ğ°: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ˜Ğ˜-Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğº ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑÑƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ, Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ğ½Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dynamic Diversity in AI Agent Societies",
                    "desc": "This paper explores how large language model agents behave in networked environments, focusing on their ability to maintain individual diversity without forming a collective influence. The authors introduce a framework to quantitatively analyze the dynamics of these AI agent societies, measuring factors like semantic stabilization and lexical turnover. Their findings indicate that while the overall language used by agents stabilizes, individual agents continue to evolve independently, showing little adaptation to each other. This lack of mutual influence prevents the formation of stable social structures, suggesting that simply increasing the number of agents does not lead to effective socialization."
                },
                "zh": {
                    "title": "åŠ¨æ€ç¨³å®šæ€§ä¸ä¸ªä½“å¤šæ ·æ€§çš„å¹³è¡¡",
                    "desc": "åœ¨ç½‘ç»œç¯å¢ƒä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¡¨ç°å‡ºåŠ¨æ€ç¨³å®šæ€§ï¼Œä½†å¹¶æœªå®ç°çœŸæ­£çš„ç¤¾ä¼šè¶‹åŒã€‚è¿™äº›ä»£ç†ä¿æŒä¸ªä½“å¤šæ ·æ€§ï¼Œç¼ºä¹é›†ä½“å½±å“ç»“æ„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å…¨çƒè¯­ä¹‰å¹³å‡å€¼è¿…é€Ÿç¨³å®šï¼Œä¸ªä½“ä»£ç†å´ä¿æŒé«˜åº¦å¤šæ ·æ€§å’ŒæŒç»­çš„è¯æ±‡å˜åŠ¨ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»£ç†ä¹‹é—´çš„ç›¸äº’å½±å“å¾®å¼±ï¼Œæ— æ³•å½¢æˆç¨³å®šçš„é›†ä½“å½±å“é”šç‚¹ï¼Œæç¤ºæˆ‘ä»¬åœ¨è®¾è®¡ä¸‹ä¸€ä»£AIä»£ç†ç¤¾ä¼šæ—¶éœ€è€ƒè™‘è¿™äº›å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15112",
            "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
            "url": "https://huggingface.co/papers/2602.15112",
            "abstract": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
            "score": 9,
            "issue_id": 1104,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "9e29c2baa76e59ee",
            "authors": [
                "Aniketh Garikaparthi",
                "Manasi Patwardhan",
                "Arman Cohan"
            ],
            "affiliations": [
                "TCS Research",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15112.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#agents",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…",
                    "desc": "ResearchGym Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ ICML, ICLR Ğ¸ ACL, ÑĞ¾Ğ·Ğ´Ğ°Ğ² 39 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPT-5 Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ: Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿ÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ» Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ»Ğ¸ÑˆÑŒ 26.5% Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ‚Ğ¾, Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ… frontier Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ state-of-the-art Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Bridging the Capability-Reliability Gap in AI Research",
                    "desc": "ResearchGym is a new benchmark environment designed to assess AI agents on complex research tasks. It uses real datasets and evaluation methods from notable machine learning conferences but omits the original methods proposed in the papers. The study reveals that even advanced agents like GPT-5 show a significant gap between their capabilities and reliability, succeeding in only a small fraction of tasks. This highlights persistent challenges in AI, such as managing resources and coordinating experiments, despite occasional high performance."
                },
                "zh": {
                    "title": "è¯„ä¼°AIä»£ç†çš„èƒ½åŠ›ä¸å¯é æ€§å·®è·",
                    "desc": "ResearchGymæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç«¯åˆ°ç«¯ç ”ç©¶ä»»åŠ¡ä¸­çš„åŸºå‡†ç¯å¢ƒã€‚å°½ç®¡æŸäº›ä»£ç†å¶å°”èƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨èƒ½åŠ›å’Œå¯é æ€§ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚é€šè¿‡å¯¹äº”ç¯‡ä¼šè®®è®ºæ–‡çš„ä»»åŠ¡è¿›è¡Œå®¹å™¨åŒ–å¤„ç†ï¼Œä»£ç†éœ€è¦æå‡ºæ–°å‡è®¾å¹¶è¿›è¡Œå®éªŒï¼Œä»¥è¶…è¶Šäººç±»åŸºçº¿ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»£ç†åœ¨15æ¬¡è¯„ä¼°ä¸­ä»…åœ¨1æ¬¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œä¸”å¹³å‡ä»…å®Œæˆ26.5%çš„å­ä»»åŠ¡ï¼Œæ­ç¤ºäº†é•¿æœŸå¤±è´¥æ¨¡å¼çš„æ™®éæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12279",
            "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
            "url": "https://huggingface.co/papers/2602.12279",
            "abstract": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
            "score": 7,
            "issue_id": 1109,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "c3adc0263e504a86",
            "authors": [
                "Leon Liangyu Chen",
                "Haoyu Ma",
                "Zhipeng Fan",
                "Ziqi Huang",
                "Animesh Sinha",
                "Xiaoliang Dai",
                "Jialiang Wang",
                "Zecheng He",
                "Jianwei Yang",
                "Chunyuan Li",
                "Junzhe Sun",
                "Chu Wang",
                "Serena Yeung-Levy",
                "Felix Juefei-Xu"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "Nanyang Technological University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12279.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#training",
                    "#agents",
                    "#inference"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞœÑ‹ÑĞ»ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "UniT â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹: Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°."
                },
                "en": {
                    "title": "Iterative Reasoning for Unified Multimodal Mastery",
                    "desc": "The UniT framework introduces a method for unified multimodal models to enhance their reasoning and output refinement through iterative processes. It allows these models to break down complex tasks into smaller parts, verify their results, and make necessary adjustments over multiple rounds of reasoning. By employing test-time scaling, UniT improves the performance of these models during inference, making them more efficient and effective. The findings suggest that training on shorter reasoning paths can lead to better performance on longer tasks, and that sequential reasoning is more efficient than parallel approaches."
                },
                "zh": {
                    "title": "UniTï¼šå¤šæ¨¡æ€æ¨ç†ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "UniTæ¡†æ¶ä½¿å¾—ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é“¾å¼æ€ç»´çš„æµ‹è¯•æ—¶é—´æ‰©å±•è¿›è¡Œè¿­ä»£æ¨ç†å’Œç²¾ç‚¼ï¼Œä»è€Œæå‡ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸åœ¨å•æ¬¡æ¨ç†ä¸­å·¥ä½œï¼Œç¼ºä¹å¯¹è¾“å‡ºçš„è¿­ä»£ä¿®æ­£èƒ½åŠ›ï¼Œè€ŒUniTåˆ™å…è®¸æ¨¡å‹åœ¨å¤šä¸ªå›åˆä¸­è¿›è¡Œæ¨ç†ã€éªŒè¯å’Œç²¾ç‚¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡çŸ­æœŸæ¨ç†è½¨è¿¹è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶æ¨å¹¿åˆ°æ›´é•¿çš„æ¨ç†é“¾ï¼Œä¸”é¡ºåºé“¾å¼æ€ç»´æ¨ç†æ¯”å¹¶è¡Œé‡‡æ ·æ›´å…·å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ç”Ÿæˆå’Œç¼–è¾‘è½¨è¿¹çš„è®­ç»ƒï¼ŒUniTåœ¨å¤„ç†åˆ†å¸ƒå¤–è§†è§‰æ¨ç†æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœï¼Œè¯æ˜äº†å¤šæ¨¡æ€æµ‹è¯•æ—¶é—´æ‰©å±•åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15200",
            "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
            "url": "https://huggingface.co/papers/2602.15200",
            "abstract": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
            "score": 5,
            "issue_id": 1108,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "5c4df6c0dba2d549",
            "authors": [
                "Denis Makhov",
                "Dmitriy Shopkhoev",
                "Magauiya Zhussip",
                "Ammar Ali",
                "Baher Mohammad",
                "Stamatios Lefkimmiatis"
            ],
            "affiliations": [
                "Fundamental Research Center MWS AI",
                "ITMO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15200.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "ĞÑ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "COMPOT â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Transformer Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑÑ‚Ğ° Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑƒÑ‡ĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾ĞµĞ² ÑĞµÑ‚Ğ¸, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ COMPOT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Effortless Compression for Transformers with COMPOT",
                    "desc": "COMPOT is a novel framework designed for compressing Transformer models without the need for additional training. It utilizes sparse dictionary learning with orthogonal dictionaries, allowing for efficient updates that improve the balance between model quality and compression. Unlike traditional methods that rely on singular value decomposition, COMPOT avoids iterative optimization by using closed-form updates for both the dictionary and coefficients. Additionally, it introduces a dynamic allocation strategy to adaptively manage compression across different layers, ensuring optimal performance across various tasks and architectures."
                },
                "zh": {
                    "title": "COMPOTï¼šæ— è®­ç»ƒçš„Transformerå‹ç¼©æ–°æ–¹æ³•",
                    "desc": "COMPOTæ˜¯ä¸€ç§æ— è®­ç»ƒçš„å‹ç¼©æ¡†æ¶ï¼Œä¸“ä¸ºTransformeræ¨¡å‹è®¾è®¡ã€‚å®ƒåˆ©ç”¨ç¨€ç–å­—å…¸å­¦ä¹ å’Œæ­£äº¤å­—å…¸ï¼Œé€šè¿‡é—­å¼æ›´æ–°å®ç°æ›´å¥½çš„è´¨é‡ä¸å‹ç¼©ä¹‹é—´çš„å¹³è¡¡ã€‚ä¸ä¼ ç»Ÿçš„ä½ç§©æ–¹æ³•ç›¸æ¯”ï¼ŒCOMPOTèƒ½å¤Ÿåœ¨ä¸æŸå¤±å‡†ç¡®åº¦çš„æƒ…å†µä¸‹ï¼Œçµæ´»åœ°å¤„ç†ä¸åŒå±‚çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡åŠ¨æ€åˆ†é…å‹ç¼©ç‡ï¼ŒCOMPOTåœ¨å¤šç§æ¶æ„å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¼˜äºç°æœ‰åŸºçº¿çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14486",
            "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
            "url": "https://huggingface.co/papers/2602.14486",
            "abstract": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.",
            "score": 5,
            "issue_id": 1105,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "c2473f1b45e95196",
            "authors": [
                "Fabian GrÃ¶ger",
                "Shuo Wen",
                "Maria BrbiÄ‡"
            ],
            "affiliations": [
                "EPFL",
                "HSLU",
                "University of Basel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14486.jpg",
            "data": {
                "categories": [
                    "#interpretability"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞÑ‚ ĞŸĞ»Ğ°Ñ‚Ğ¾Ğ½Ğ° Ğº ĞÑ€Ğ¸ÑÑ‚Ğ¾Ñ‚ĞµĞ»Ñ: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ° Ğ¾ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğº ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ ÑĞµÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ·Ğ°Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€Ğ°Ğ¼ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»ÑĞ±ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğµ Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ…, Ğ° Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Local Neighborhoods Over Global Models in Neural Representations",
                    "desc": "This paper investigates how neural network representations relate to each other, focusing on local neighborhood relationships rather than global patterns. The authors find that traditional metrics for measuring similarity can be misleading due to the scale of the networks, as larger models tend to show inflated similarity scores. To address this issue, they introduce a new calibration method that provides more accurate similarity measurements. Their findings lead to the conclusion that while global similarities may diminish after calibration, local neighborhood relationships remain consistent across different types of data."
                },
                "zh": {
                    "title": "ç¥ç»ç½‘ç»œè¡¨ç¤ºè¶‹å‘äºå…±äº«å±€éƒ¨é‚»åŸŸå…³ç³»",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºå¦‚ä½•è¶‹å‘äºå…±äº«çš„å±€éƒ¨é‚»åŸŸå…³ç³»ï¼Œè€Œä¸æ˜¯å…¨å±€ç»Ÿè®¡æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è¡¨ç¤ºç›¸ä¼¼æ€§åº¦é‡å—åˆ°ç½‘ç»œè§„æ¨¡çš„å½±å“ï¼Œæ¨¡å‹çš„æ·±åº¦æˆ–å®½åº¦å¢åŠ ä¼šç³»ç»Ÿæ€§åœ°æé«˜ç›¸ä¼¼æ€§åˆ†æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºç½®æ¢çš„é›¶æ ¡å‡†æ¡†æ¶ï¼Œå¯ä»¥å°†ä»»ä½•è¡¨ç¤ºç›¸ä¼¼æ€§åº¦é‡è½¬åŒ–ä¸ºå…·æœ‰ç»Ÿè®¡ä¿è¯çš„æ ¡å‡†åˆ†æ•°ã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œä½œè€…é‡æ–°å®¡è§†äº†æŸæ‹‰å›¾è¡¨ç¤ºå‡è¯´ï¼Œå‘ç°ç»è¿‡æ ¡å‡†åï¼Œå…¨çƒè°±åº¦é‡æŠ¥å‘Šçš„æ˜æ˜¾æ”¶æ•›ç°è±¡å¤§éƒ¨åˆ†æ¶ˆå¤±ï¼Œè€Œå±€éƒ¨é‚»åŸŸç›¸ä¼¼æ€§åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ä»ç„¶ä¿æŒæ˜¾è‘—ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15322",
            "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
            "url": "https://huggingface.co/papers/2602.15322",
            "abstract": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
            "score": 3,
            "issue_id": 1104,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "de1c836aa6754a7d",
            "authors": [
                "Taejong Joo",
                "Wenhan Xia",
                "Cheolmin Kim",
                "Ming Zhang",
                "Eugene Ie"
            ],
            "affiliations": [
                "Google",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15322.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ RMSProp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Magma, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞºĞ°Ğº Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Random Masking Revolutionizes LLM Optimization",
                    "desc": "This paper presents a novel approach to optimizing large language models (LLMs) by using random parameter update masking. The authors demonstrate that this method, particularly a masked variant of the RMSProp optimizer, can outperform traditional adaptive optimizers. They introduce a new technique called Momentum-aligned gradient masking (Magma), which enhances the effectiveness of the random masking by aligning it with momentum gradients. The results show significant improvements in model performance, with Magma achieving lower perplexity compared to state-of-the-art optimizers while maintaining low computational costs."
                },
                "zh": {
                    "title": "éšæœºæ©è”½ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§éšæœºå‚æ•°æ›´æ–°æ©è”½çš„æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡éšæœºæ©è”½å‚æ•°æ›´æ–°ï¼Œç ”ç©¶è¡¨æ˜è¿™ç§æ–¹æ³•åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿå¼•å…¥ä¸æ›²ç‡ç›¸å…³çš„å‡ ä½•æ­£åˆ™åŒ–ï¼Œä»è€Œå¹³æ»‘ä¼˜åŒ–è½¨è¿¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åä¸ºMagmaçš„åŠ¨é‡å¯¹é½æ¢¯åº¦æ©è”½æ–¹æ³•ï¼Œå®ƒé€šè¿‡åŠ¨é‡ä¸æ¢¯åº¦çš„å¯¹é½æ¥è°ƒèŠ‚æ©è”½æ›´æ–°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMagmaåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†å›°æƒ‘åº¦ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09653",
            "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
            "url": "https://huggingface.co/papers/2602.09653",
            "abstract": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
            "score": 3,
            "issue_id": 1105,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "4bb3daa41b5258b3",
            "authors": [
                "Shiwei Lyu",
                "Xidong Wang",
                "Lei Liu",
                "Hao Zhu",
                "Chaohe Zhang",
                "Jian Wang",
                "Jinjie Gu",
                "Benyou Wang",
                "Yue Shen"
            ],
            "affiliations": [
                "Ant Group",
                "Peking University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09653.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#healthcare",
                    "#rlhf",
                    "#alignment",
                    "#reasoning",
                    "#science",
                    "#small_models"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞšĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ€Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HealthRubrics Ñ 7034 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ†Ğ¸ÑÑ‚Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² HealthPrinciples â€” 119 Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ 30B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€ĞµÑÑƒÑ€ÑĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Aligning AI with Clinician Preferences for Better Medical Outcomes",
                    "desc": "This paper presents a two-stage framework designed to align large language models (LLMs) with the preferences of clinicians for better medical reasoning. The first stage involves creating HealthRubrics, a dataset of physician-verified examples where clinicians refine LLM-generated outputs to meet medical standards. The second stage distills these rubrics into HealthPrinciples, a set of reusable clinical guidelines that facilitate scalable supervision. The proposed framework allows for both offline alignment and real-time self-revision, resulting in a more efficient model that outperforms larger counterparts in clinical tasks."
                },
                "zh": {
                    "title": "æå‡åŒ»ç–—æ¨ç†çš„æ™ºèƒ½å¯¹é½æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä¸´åºŠåŒ»ç”Ÿçš„åå¥½å¯¹é½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†HealthRubricsæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«7034ä¸ªç»è¿‡åŒ»ç”ŸéªŒè¯çš„åå¥½ç¤ºä¾‹ï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿå®Œå–„LLMç”Ÿæˆçš„è¯„åˆ†æ ‡å‡†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è¿™äº›è¯„åˆ†æ ‡å‡†æç‚¼ä¸ºHealthPrinciplesï¼Œå½¢æˆ119ä¸ªå¯å¹¿æ³›é‡ç”¨çš„ä¸´åºŠåŸåˆ™ï¼Œä»¥ä¾¿äºåœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹è¿›è¡Œå¤§è§„æ¨¡ç›‘ç£ã€‚é€šè¿‡è¿™ç§æ¡†æ¶è®­ç»ƒçš„30Bå‚æ•°æ¨¡å‹åœ¨HealthBench-Hardä¸Šå–å¾—äº†33.4%çš„æˆç»©ï¼Œè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„æ¨¡å‹ï¼Œå»ºç«‹äº†ä¸´åºŠå¯¹é½çš„èµ„æºé«˜æ•ˆåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15772",
            "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
            "url": "https://huggingface.co/papers/2602.15772",
            "abstract": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.",
            "score": 2,
            "issue_id": 1105,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "c39b631c83ff418c",
            "authors": [
                "Sen Ye",
                "Mengde Xu",
                "Shuyang Gu",
                "Di He",
                "Liwei Wang",
                "Han Hu"
            ],
            "affiliations": [
                "Center for Data Science, Peking University",
                "Center for Machine Learning Research, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15772.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Reason-Reflect-Refine (R3), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ²Ñ‘Ğ½Ğ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ-Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ', Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Generation and Understanding in Multimodal Models",
                    "desc": "The Reason-Reflect-Refine (R3) framework tackles the challenge in multimodal models where improving generation often reduces understanding. It identifies the conflict between these two capabilities as a key issue. By transforming the generation process into a multi-step approach that includes reasoning and reflection, the framework enhances both aspects simultaneously. This leads to better generation outcomes while also improving the model's understanding, paving the way for more advanced multimodal systems."
                },
                "zh": {
                    "title": "ç”Ÿæˆä¸ç†è§£çš„å®Œç¾å¹³è¡¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†Reason-Reflect-Refineï¼ˆR3ï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨¡å‹ä¸­ç”Ÿæˆèƒ½åŠ›ä¸ç†è§£èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æ½œåœ¨å†²çªæ˜¯å¯¼è‡´è¿™ä¸€é—®é¢˜çš„ä¸»è¦åŸå› ã€‚R3æ¡†æ¶å°†å•æ­¥ç”Ÿæˆä»»åŠ¡é‡æ–°æ„å»ºä¸ºä¸€ä¸ªå¤šæ­¥è¿‡ç¨‹ï¼Œå…·ä½“ä¸ºâ€œç”Ÿæˆ-ç†è§£-å†ç”Ÿæˆâ€ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜ç¡®åˆ©ç”¨æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æˆåŠŸç¼“è§£äº†ä¼˜åŒ–å›°å¢ƒï¼Œæå‡äº†ç”Ÿæˆæ•ˆæœå’Œç†è§£èƒ½åŠ›ï¼Œä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15620",
            "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
            "url": "https://huggingface.co/papers/2602.15620",
            "abstract": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.",
            "score": 2,
            "issue_id": 1104,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "9ce7a6bfe1a7f741",
            "authors": [
                "Shiqi Liu",
                "Zeyu He",
                "Guojian Zhan",
                "Letian Tao",
                "Zhilong Zheng",
                "Jiang Wu",
                "Yinuo Wang",
                "Yang Guan",
                "Kehua Sheng",
                "Bo Zhang",
                "Keqiang Li",
                "Jingliang Duan",
                "Shengbo Eben Li"
            ],
            "affiliations": [
                "DiDi Voyager Labs, DiDi Autonomous Driving",
                "School of Vehicle and Mobility & College of AI, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15620.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ°Ñ…Ğ¾Ğ´Ğ¸ Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ğ¹: ĞºĞ°Ğº Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ fine-tuning Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ reinforcement learning Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 0,01%), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ STAPO (Spurious-Token-Aware Policy Optimization), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ STAPO Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 7,13% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Masking Spurious Tokens for Stable RL Fine-Tuning",
                    "desc": "This paper addresses the issue of training instability in reinforcement learning (RL) fine-tuning of large language models caused by spurious tokens. Spurious tokens are a small fraction of tokens that, when present in correct responses, lead to disproportionately large gradient updates without contributing to reasoning quality. The authors introduce a new method called Spurious-Token-Aware Policy Optimization (STAPO), which selectively masks these problematic updates and adjusts the loss calculation to focus on valid tokens. The results show that STAPO improves reasoning performance and stability across multiple benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "è™šå‡æ ‡è®°ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„éšæ‚£ä¸è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶å‘ç°ï¼Œè™šå‡æ ‡è®°æ˜¯å¯¼è‡´å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸ç¨³å®šçš„åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°å±è”½æœ‰é—®é¢˜çš„æ¢¯åº¦æ›´æ–°æ¥æé«˜æ¨ç†æ€§èƒ½ã€‚å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„å¾®è°ƒæ–¹æ³•ä¾èµ–äºå¯å‘å¼æŠ€æœ¯ï¼Œå¯¼è‡´è®­ç»ƒåæœŸæ€§èƒ½å´©æºƒã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ ‡è®°çº§ç­–ç•¥æ¢¯åº¦çš„å¤§å°ä¸æ ‡è®°æ¦‚ç‡å’Œå±€éƒ¨ç­–ç•¥ç†µå‘ˆè´Ÿç›¸å…³ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†è™šå‡æ ‡è®°æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆSTAPOï¼‰ï¼Œé€šè¿‡å±è”½ä¸è‰¯æ›´æ–°å¹¶å¯¹æœ‰æ•ˆæ ‡è®°é‡æ–°å½’ä¸€åŒ–æŸå¤±ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15278",
            "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
            "url": "https://huggingface.co/papers/2602.15278",
            "abstract": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.  \t\t\t\t\tAI-generated summary \t\t\t\t The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
            "score": 2,
            "issue_id": 1104,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "3406ed25de76ed9b",
            "authors": [
                "Manuel Cherep",
                "Pranav M R",
                "Pattie Maes",
                "Nikhil Singh"
            ],
            "affiliations": [
                "BITS Pilani, Goa, India",
                "Department of Computer Science, Dartmouth College, Hanover, USA",
                "Media Lab, Massachusetts Institute of Technology, Cambridge, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15278.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² VLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unveiling Visual Preferences in AI Decision-Making",
                    "desc": "This paper investigates how visual-language models (VLMs) make decisions based on images by using controlled tasks that alter the input images. The authors propose a framework that treats the decision-making process of VLMs as a latent visual utility, which can be inferred from the choices made between modified images. They introduce methods for optimizing visual prompts, applying systematic edits to images to see how these changes affect the model's selection probabilities. The findings reveal significant shifts in decision-making based on these edits, highlighting potential vulnerabilities and safety issues in AI systems that rely on visual inputs."
                },
                "zh": {
                    "title": "æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å†³ç­–åå¥½ä¸å®‰å…¨éšæ‚£",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒé€‰æ‹©ä»»åŠ¡ä¸­çš„å†³ç­–åå¥½ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°æ‰°åŠ¨è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›æ¨¡å‹çš„è§†è§‰è„†å¼±æ€§å’Œå®‰å…¨éšæ‚£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å›¾åƒé€‰æ‹©ä»»åŠ¡æ¥åˆ†æVLMçš„å†³ç­–å‡½æ•°ï¼Œå¹¶ä¼˜åŒ–è§†è§‰æç¤ºä»¥æé«˜é€‰æ‹©æ¦‚ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡ä¼˜åŒ–çš„å›¾åƒç¼–è¾‘æ˜¾è‘—æ”¹å˜äº†æ¨¡å‹çš„é€‰æ‹©æ¦‚ç‡ï¼Œä»è€Œä¸ºå›¾åƒåŸºç¡€çš„AIä»£ç†æä¾›äº†æ›´æœ‰æ•ˆçš„å®¡è®¡å’Œæ²»ç†æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12978",
            "title": "Learning Native Continuation for Action Chunking Flow Policies",
            "url": "https://huggingface.co/papers/2602.12978",
            "abstract": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.",
            "score": 2,
            "issue_id": 1105,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "3e47cf82f5e2685d",
            "authors": [
                "Yufeng Liu",
                "Hang Yu",
                "Juntu Zhao",
                "Bocheng Li",
                "Di Zhang",
                "Mingzhu Li",
                "Wenxuan Wu",
                "Yingdong Hu",
                "Junyuan Xie",
                "Junliang Guo",
                "Dequan Wang",
                "Yang Gao"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Spirit AI",
                "Tongji University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12978.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Legato â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision Language Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ (action chunking) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ñ€Ñ‹Ğ²ĞºĞ°Ğ¼ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸. Legato Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° ÑĞ¼ĞµÑÑŒÑ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Legato Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Legato: Smoother Actions for Real-Time VLA Models",
                    "desc": "Legato enhances Vision Language Action (VLA) models by implementing training-time continuation methods that create smoother action trajectories. It addresses the problem of discontinuities at chunk boundaries that can occur during real-time execution. By using a mixture of known actions and noise during training, Legato helps the model learn to produce consistent and smooth actions. The method also incorporates randomized schedules to adapt to varying delays, resulting in improved performance in task completion and reduced multimodal switching."
                },
                "zh": {
                    "title": "Legatoï¼šæå‡è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„å¹³æ»‘æ€§ä¸æ•ˆç‡",
                    "desc": "Legatoæ˜¯ä¸€ç§æ”¹è¿›çš„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡åŸºäºè§†è§‰è¯­è¨€çš„åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è¿ç»­æ€§æ–¹æ³•ï¼ŒLegatoç¡®ä¿äº†åŠ¨ä½œæ‰§è¡Œçš„å¹³æ»‘æ€§ï¼Œå‡å°‘äº†å¤šæ¨¡æ€åˆ‡æ¢çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å·²çŸ¥åŠ¨ä½œä¸å™ªå£°æ··åˆï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ¥è§¦éƒ¨åˆ†åŠ¨ä½œä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLegatoåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è½¨è¿¹çš„å¹³æ»‘åº¦å’Œä»»åŠ¡å®Œæˆæ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15327",
            "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
            "url": "https://huggingface.co/papers/2602.15327",
            "abstract": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",
            "score": 1,
            "issue_id": 1105,
            "pub_date": "2026-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "c8e142702e665020",
            "authors": [
                "Hanlin Zhang",
                "Jikai Jin",
                "Vasilis Syrgkanis",
                "Sham Kakade"
            ],
            "affiliations": [
                "Harvard University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15327.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#optimization",
                    "#science"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ñ Ğ¸Ğ· Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ 5000 Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ 2000 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ³Ğ»Ğ°Ğ¶ĞµĞ½Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ñ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Proteus 2k Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ² Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»Ğ¸ÑˆÑŒ 20% Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Predicting Model Performance: Stability and Scaling Insights",
                    "desc": "This paper focuses on understanding how well foundation models perform based on their training resources, specifically using quantile regression to predict performance boundaries. It analyzes a large dataset to estimate how accuracy changes with different amounts of pre-training compute, providing insights into the stability of these predictions over time. The study finds that while most tasks show stable performance boundaries, math reasoning tasks are improving rapidly. Additionally, the authors present a new algorithm that efficiently estimates performance frontiers, contributing to a dataset called Proteus 2k for evaluating model performance."
                },
                "zh": {
                    "title": "åŸºç¡€æ¨¡å‹èƒ½åŠ›è¾¹ç•Œä¸æ€§èƒ½é¢„æµ‹çš„ç¨³å®šæ€§åˆ†æ",
                    "desc": "æœ¬è®ºæ–‡é€šè¿‡é‡åŒ–å›å½’åˆ†æå¤§è§„æ¨¡è§‚å¯Ÿæ•°æ®ï¼Œä¼°è®¡åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œå’Œæ€§èƒ½é¢„æµ‹ï¼Œå¹¶è¯„ä¼°å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ—¶é—´ç¨³å®šæ€§ã€‚ç ”ç©¶è€…ä½¿ç”¨äº†5000ä¸ªè§‚å¯Ÿæ•°æ®å’Œ2000ä¸ªæ–°é‡‡æ ·æ•°æ®ï¼Œåˆ©ç”¨å¹³æ»‘çš„é‡åŒ–å›å½’æ–¹æ³•ï¼Œåˆ†æäº†é¢„è®­ç»ƒè®¡ç®—é¢„ç®—ä¸ä¸‹æ¸¸å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚ç»“æœæ˜¾ç¤ºï¼Œé™¤äº†æ•°å­¦æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›è¾¹ç•Œéšç€æ—¶é—´ä¸æ–­æå‡å¤–ï¼Œå…¶ä»–ä»»åŠ¡çš„è¾¹ç•Œå¤§å¤šä¿æŒç¨³å®šã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨çº¦20%çš„è¯„ä¼°é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œæ¢å¤æ¥è¿‘å®Œæ•´çš„æ•°æ®è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07854",
            "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
            "url": "https://huggingface.co/papers/2602.07854",
            "abstract": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.  \t\t\t\t\tAI-generated summary \t\t\t\t Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.",
            "score": 1,
            "issue_id": 1104,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "9e1ba6fe1b318876",
            "authors": [
                "Chendong Xiang",
                "Jiajun Liu",
                "Jintao Zhang",
                "Xiao Yang",
                "Zhengwei Fang",
                "Shizun Wang",
                "Zijun Wang",
                "Yingtian Zou",
                "Hang Su",
                "Jun Zhu"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07854.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "ViewRope â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ° Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ»ÑƒÑ‡ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞºÑ€Ğ°Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ñƒ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞµÑ‰ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¹. ViewRope Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ»ÑƒÑ‡ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ViewBench Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¼Ñ‹ĞºĞ°Ğ½Ğ¸Ñ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°."
                },
                "en": {
                    "title": "Enhancing Consistency in Predictive Models with Geometry-Aware Encoding",
                    "desc": "ViewRope is a new method that improves how predictive world models maintain consistency over time by using camera-ray directions in video transformer attention layers. This approach addresses the problem of spatial persistence, where models often lose track of stable scene structures when revisiting locations. By focusing on relative ray geometry instead of just pixel positions, ViewRope helps the model better understand 3D content across different time frames. Additionally, it introduces a technique called Geometry-Aware Frame-Sparse Attention to enhance efficiency while keeping memory consistent."
                },
                "zh": {
                    "title": "ViewRopeï¼šæå‡é¢„æµ‹æ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§",
                    "desc": "ViewRopeæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡å°†ç›¸æœºå…‰çº¿æ–¹å‘æ³¨å…¥è§†é¢‘å˜æ¢å™¨çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œå¢å¼ºäº†é¢„æµ‹ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ã€‚å½“å‰çš„ç³»ç»Ÿåœ¨ç©ºé—´æŒä¹…æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œæ— æ³•åœ¨é•¿æ—¶é—´è½¨è¿¹ä¸­ä¿æŒç¨³å®šçš„åœºæ™¯ç»“æ„ã€‚æˆ‘ä»¬å‘ç°è¿™ç§å‡ ä½•æ¼‚ç§»æºäºå¯¹å±å¹•ç©ºé—´ä½ç½®åµŒå…¥çš„ä¾èµ–ï¼Œè¿™ä¸3Dä¸€è‡´æ€§æ‰€éœ€çš„æŠ•å½±å‡ ä½•ç›¸å†²çªã€‚ViewRopeé€šè¿‡ç›¸å¯¹å…‰çº¿å‡ ä½•å‚æ•°åŒ–æ³¨æ„åŠ›ï¼Œæä¾›äº†ä¸€ä¸ªæ¨¡å‹æœ¬åœ°çš„å½’çº³åç½®ï¼Œä»è€Œåœ¨æ—¶é—´é—´éš”ä¸­æ£€ç´¢3Dä¸€è‡´çš„å†…å®¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11389",
            "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
            "url": "https://huggingface.co/papers/2602.11389",
            "abstract": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.",
            "score": 0,
            "issue_id": 1109,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4d11ce1a6dd4b7cd",
            "authors": [
                "Heejeong Nam",
                "Quentin Le Lidec",
                "Lucas Maes",
                "Yann LeCun",
                "Randall Balestriero"
            ],
            "affiliations": [
                "Brown University",
                "Mila",
                "New York University",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11389.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#multimodal",
                    "#open_source",
                    "#robotics"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "C-JEPA â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (masked joint embedding prediction) Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸Ğ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…, Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 20% Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1% Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… patch-based Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "C-JEPA: Enhancing Object-Centric Understanding for Better Reasoning and Control",
                    "desc": "C-JEPA is a novel approach that enhances masked joint embedding prediction by focusing on object-centric representations. It introduces object-level masking, which requires the model to infer the state of one object based on the states of other objects, promoting a deeper understanding of interactions. This method helps prevent shortcut solutions and encourages essential reasoning about interactions, leading to significant improvements in tasks like visual question answering and agent control. The results show that C-JEPA not only boosts performance but also reduces the amount of data needed for effective planning, demonstrating its efficiency and effectiveness in relational understanding."
                },
                "zh": {
                    "title": "C-JEPAï¼šæå‡å¯¹è±¡ä¸­å¿ƒæ¨ç†çš„å¼ºå¤§å·¥å…·",
                    "desc": "C-JEPAæ˜¯ä¸€ç§æ‰©å±•äº†æ©è”½è”åˆåµŒå…¥é¢„æµ‹çš„å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ–¹æ³•ã€‚å®ƒé€šè¿‡å¯¹è±¡çº§æ©è”½æ¥å¢å¼ºå¯¹å…³ç³»çš„ç†è§£ï¼Œä»è€Œæé«˜æ¨ç†å’Œæ§åˆ¶ä»»åŠ¡çš„æ•ˆæœã€‚C-JEPAèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œåäº‹å®æ¨ç†ï¼Œå¹¶é˜²æ­¢æ·å¾„è§£å†³æ–¹æ¡ˆçš„å‡ºç°ï¼Œä½¿å¾—äº¤äº’æ¨ç†å˜å¾—è‡³å…³é‡è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC-JEPAåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºçº¦20%çš„ç»å¯¹æå‡ï¼Œå¹¶åœ¨ä»£ç†æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†è§„åˆ’æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-17.html",
    "link_next": "2026-02-19.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "17.02",
        "en": "02/17",
        "zh": "2æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.02",
        "en": "02/19",
        "zh": "2æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 0
    }
}