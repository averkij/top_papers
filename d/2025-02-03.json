{
    "date": {
        "ru": "3 февраля",
        "en": "February 3",
        "zh": "2月3日"
    },
    "time_utc": "2025-02-03 08:14",
    "weekday": 0,
    "issue_id": 1999,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.19393",
            "title": "s1: Simple test-time scaling",
            "url": "https://huggingface.co/papers/2501.19393",
            "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.",
            "score": 19,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "8fcf84a9effc288f",
            "authors": [
                "Niklas Muennighoff",
                "Zitong Yang",
                "Weijia Shi",
                "Xiang Lisa Li",
                "Li Fei-Fei",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Percy Liang",
                "Emmanuel Candès",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Contextual AI",
                "Stanford University",
                "University of Washington, Seattle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19393.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Простое масштабирование для улучшения рассуждений языковых моделей",
                    "desc": "Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода."
                },
                "en": {
                    "title": "Enhancing Language Models with Test-Time Scaling",
                    "desc": "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."
                },
                "zh": {
                    "title": "测试时间扩展：提升语言模型性能的新方法",
                    "desc": "本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19324",
            "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2501.19324",
            "abstract": "",
            "score": 4,
            "issue_id": 1995,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "ce2d414eedfb7a1e",
            "authors": [
                "Baohao Liao",
                "Yuhui Xu",
                "Hanze Dong",
                "Junnan Li",
                "Christof Monz",
                "Silvio Savarese",
                "Doyen Sahoo",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Language Technology Lab, University of Amsterdam",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19324.jpg",
            "data": {
                "categories": [],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый шаг в обучении больших языковых моделей",
                    "desc": "В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента."
                },
                "en": {
                    "title": "Hybrid Networks: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "优化数据处理，提升机器学习性能",
                    "desc": "这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18837",
            "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
            "url": "https://huggingface.co/papers/2501.18837",
            "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.",
            "score": 1,
            "issue_id": 1996,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "62d14973b1140e58",
            "authors": [
                "Mrinank Sharma",
                "Meg Tong",
                "Jesse Mu",
                "Jerry Wei",
                "Jorrit Kruthoff",
                "Scott Goodfriend",
                "Euan Ong",
                "Alwin Peng",
                "Raj Agarwal",
                "Cem Anil",
                "Amanda Askell",
                "Nathan Bailey",
                "Joe Benton",
                "Emma Bluemke",
                "Samuel R. Bowman",
                "Eric Christiansen",
                "Hoagy Cunningham",
                "Andy Dau",
                "Anjali Gopal",
                "Rob Gilson",
                "Logan Graham",
                "Logan Howard",
                "Nimit Kalra",
                "Taesung Lee",
                "Kevin Lin",
                "Peter Lofgren",
                "Francesco Mosconi",
                "Clare O'Hara",
                "Catherine Olsson",
                "Linda Petrini",
                "Samir Rajani",
                "Nikhil Saxena",
                "Alex Silverstein",
                "Tanya Singh",
                "Theodore Sumers",
                "Leonard Tang",
                "Kevin K. Troy",
                "Constantin Weisser",
                "Ruiqi Zhong",
                "Giulio Zhou",
                "Jan Leike",
                "Jared Kaplan",
                "Ethan Perez"
            ],
            "affiliations": [
                "Safeguards Research Team, Anthropic"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18837.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#security",
                    "#inference"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Конституционные Классификаторы: надежная защита языковых моделей",
                    "desc": "Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости."
                },
                "en": {
                    "title": "Defending LLMs with Constitutional Classifiers",
                    "desc": "This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications."
                },
                "zh": {
                    "title": "宪法分类器：保护大型语言模型的安全",
                    "desc": "大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18841",
            "title": "Trading Inference-Time Compute for Adversarial Robustness",
            "url": "https://huggingface.co/papers/2501.18841",
            "abstract": "We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.",
            "score": 1,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "f1e75e6b24f3e044",
            "authors": [
                "Wojciech Zaremba",
                "Evgenia Nitishinskaya",
                "Boaz Barak",
                "Stephanie Lin",
                "Sam Toyer",
                "Yaodong Yu",
                "Rachel Dias",
                "Eric Wallace",
                "Kai Xiao",
                "Johannes Heidecke",
                "Amelia Glaese"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.18841.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Больше вычислений - выше защита: повышение устойчивости ИИ к атакам",
                    "desc": "Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам."
                },
                "en": {
                    "title": "Boosting Robustness: More Compute, Less Vulnerability",
                    "desc": "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."
                },
                "zh": {
                    "title": "增加推理计算，提升模型鲁棒性",
                    "desc": "本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2404.07097",
            "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
            "url": "https://huggingface.co/papers/2404.07097",
            "abstract": "This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.",
            "score": 0,
            "issue_id": 1999,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a526ae197fe3a8c7",
            "authors": [
                "Yoni Kasten",
                "Wuyue Lu",
                "Haggai Maron"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Simon Fraser University",
                "Technion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2404.07097.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективная 3D реконструкция из видео с помощью глубокого обучения",
                    "desc": "Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction from Casual Videos",
                    "desc": "This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time."
                },
                "zh": {
                    "title": "高效重建3D结构，TracksTo4D引领新潮流",
                    "desc": "本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04983",
            "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
            "url": "https://huggingface.co/papers/2411.04983",
            "abstract": "The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.",
            "score": 0,
            "issue_id": 1999,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "e72081596b626524",
            "authors": [
                "Gaoyue Zhou",
                "Hengkai Pan",
                "Yann LeCun",
                "Lerrel Pinto"
            ],
            "affiliations": [
                "Courant Institute, New York University",
                "Meta-FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.04983.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DINO-WM: универсальная модель мира для планирования поведения без реконструкции",
                    "desc": "Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами."
                },
                "en": {
                    "title": "DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning",
                    "desc": "This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities."
                },
                "zh": {
                    "title": "DINO-WM：无任务依赖的世界模型",
                    "desc": "本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。"
                }
            }
        }
    ],
    "link_prev": "2025-01-31.html",
    "link_next": "2025-02-04.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1月31日"
    },
    "short_date_next": {
        "ru": "04.02",
        "en": "02/04",
        "zh": "2月4日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了如何确保大语言模型（LLMs）在安全关键应用中的安全性。作者提出了GuardReasoner，一种新的保护机制，通过引导保护模型学会推理来提高性能。研究团队创建了一个包含127K样本和460K详细推理步骤的数据集，并引入了推理SFT和难样本DPO来增强推理能力。实验结果显示，GuardReasoner在13个基准测试中表现优异，超越了其他模型。",
        "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
        "pinyin": "这篇文章讨论了如何确保大语言模型（LLMs）在安全关键应用中的安全性。\nZhè piān wénzhāng tǎolùn le rúhé quèbǎo dà yǔyán móxíng (LLMs) zài ānquán guǎnjiàn yìngyòng zhōng de ānquánxìng.\n\n作者提出了GuardReasoner，一种新的保护机制，通过引导保护模型学会推理来提高性能。\nZuòzhě tíchū le GuardReasoner, yīzhǒng xīn de bǎohù jīzhì, tōngguò yǐndǎo bǎohù móxíng xuéhuì tuīlǐ lái tígāo xìngnéng.\n\n研究团队创建了一个包含127K样本和460K详细推理步骤的数据集，并引入了推理SFT和难样本DPO来增强推理能力。\nYánjiū tuánduì chuàngjiàn le yīgè bāohán 127K yàngběn hé 460K xiángxì tuīlǐ bùzhòu de shùjùjí, bìng yǐnrù le tuīlǐ SFT hé nán yàngběn DPO lái zēngqiáng tuīlǐ nénglì.\n\n实验结果显示，GuardReasoner在13个基准测试中表现优异，超越了其他模型。\nShíyàn jiéguǒ xiǎnshì, GuardReasoner zài 13 gè jīzhǔn cèshì zhōng biǎoxiàn yōuyì, chāoyuè le qítā móxíng.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"确保\", \"pinyin\": \"què bǎo\", \"trans\": \"ensure\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"large language model\"},\n    {\"word\": \"安全\", \"pinyin\": \"ān quán\", \"trans\": \"security\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎn jiàn\", \"trans\": \"critical\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"GuardReasoner\", \"pinyin\": \"N/A\", \"trans\": \"GuardReasoner\"},\n    {\"word\": \"保护\", \"pinyin\": \"bǎo hù\", \"trans\": \"protect\"},\n    {\"word\": \"机制\", \"pinyin\": \"jī zhì\", \"trans\": \"mechanism\"},\n    {\"word\": \"引导\", \"pinyin\": \"yǐn dǎo\", \"trans\": \"guide\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reason\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"团队\", \"pinyin\": \"tuán duì\", \"trans\": \"team\"},\n    {\"word\": \"创建\", \"pinyin\": \"chuàng jiàn\", \"trans\": \"create\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"样本\", \"pinyin\": \"yàng běn\", \"trans\": \"sample\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiáng xì\", \"trans\": \"detailed\"},\n    {\"word\": \"步骤\", \"pinyin\": \"bù zhòu\", \"trans\": \"step\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐn rù\", \"trans\": \"introduce\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎn shì\", \"trans\": \"show\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cè shì\", \"trans\": \"test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses how to ensure the safety of large language models (LLMs) in security-critical applications. The authors propose GuardReasoner, a new protective mechanism that enhances performance by guiding the protective model to learn reasoning. The research team created a dataset containing 127K samples and 460K detailed reasoning steps and introduced reasoning SFT and hard sample DPO to enhance reasoning capabilities. Experimental results show that GuardReasoner performs excellently on 13 benchmark tests, outperforming other models.",
        "update_ts": "2025-02-02 12:36"
    }
}