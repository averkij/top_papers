{
    "date": {
        "ru": "3 февраля",
        "en": "February 3",
        "zh": "2月3日"
    },
    "time_utc": "2025-02-03 21:09",
    "weekday": 0,
    "issue_id": 2012,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.19393",
            "title": "s1: Simple test-time scaling",
            "url": "https://huggingface.co/papers/2501.19393",
            "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.",
            "score": 48,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "8fcf84a9effc288f",
            "authors": [
                "Niklas Muennighoff",
                "Zitong Yang",
                "Weijia Shi",
                "Xiang Lisa Li",
                "Li Fei-Fei",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Percy Liang",
                "Emmanuel Candès",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Contextual AI",
                "Stanford University",
                "University of Washington, Seattle"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19393.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Простое масштабирование для улучшения рассуждений языковых моделей",
                    "desc": "Статья представляет новый подход к языковому моделированию, называемый тестовым масштабированием. Авторы разработали модель s1, основанную на Qwen2.5-32B-Instruct, и метод бюджетного форсирования для контроля вычислений во время тестирования. Модель обучена на специально отобранном наборе данных s1K из 1000 вопросов с рассуждениями. Результаты показывают, что s1 превосходит модель o1-preview от OpenAI на математических задачах, демонстрируя эффективность предложенного подхода."
                },
                "en": {
                    "title": "Enhancing Language Models with Test-Time Scaling",
                    "desc": "This paper introduces a method called test-time scaling for enhancing language model performance during evaluation. The authors create a dataset of 1,000 questions with reasoning traces to train their model, focusing on difficulty, diversity, and quality. They implement a technique called budget forcing, which manipulates the model's response time to encourage deeper reasoning and correct errors. After fine-tuning their model, they demonstrate significant improvements in solving math competition questions compared to previous models, showcasing the effectiveness of their approach."
                },
                "zh": {
                    "title": "测试时间扩展：提升语言模型性能的新方法",
                    "desc": "本文介绍了一种新的语言建模方法——测试时间扩展，旨在通过增加测试时间计算来提高模型性能。研究者们创建了一个包含1000个问题及其推理过程的小数据集s1K，并通过难度、多样性和质量三个标准进行验证。为了控制测试时间计算，提出了预算强制的方法，通过强制终止模型的思考过程或在模型生成时多次添加“等待”来延长思考时间，从而促使模型检查答案。经过监督微调后，模型s1在数学竞赛问题上超越了OpenAI的o1模型，表现提升达27%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19324",
            "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2501.19324",
            "abstract": "",
            "score": 25,
            "issue_id": 1995,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "ce2d414eedfb7a1e",
            "authors": [
                "Baohao Liao",
                "Yuhui Xu",
                "Hanze Dong",
                "Junnan Li",
                "Christof Monz",
                "Silvio Savarese",
                "Doyen Sahoo",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Language Technology Lab, University of Amsterdam",
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19324.jpg",
            "data": {
                "categories": [],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый шаг в обучении больших языковых моделей",
                    "desc": "В статье рассматривается новый подход к обучению LLM, который позволяет моделям лучше понимать контекст и улучшать качество генерации текста. Исследователи предложили метод, который использует более сложные архитектуры нейронных сетей для обработки больших объемов данных. Эксперименты показали, что предложенный метод значительно повышает точность и скорость работы моделей. Это открывает новые возможности для применения AI в различных областях, таких как обработка естественного языка и генерация контента."
                },
                "en": {
                    "title": "Hybrid Networks: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "优化数据处理，提升机器学习性能",
                    "desc": "这篇论文探讨了一种新的机器学习算法，旨在提高模型的准确性和效率。作者提出了一种创新的方法，通过优化数据预处理和特征选择来增强学习效果。实验结果表明，该算法在多个数据集上表现优于现有技术。最终，研究表明，改进的数据处理流程对机器学习模型的性能至关重要。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18119",
            "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models",
            "url": "https://huggingface.co/papers/2501.18119",
            "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",
            "score": 12,
            "issue_id": 2002,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "d751c8a690173842",
            "authors": [
                "Qika Lin",
                "Tianzhe Zhao",
                "Kai He",
                "Zhen Peng",
                "Fangzhi Xu",
                "Ling Huang",
                "Jingying Ma",
                "Mengling Feng"
            ],
            "affiliations": [
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18119.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#graphs",
                    "#transfer_learning",
                    "#training",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная интеграция графов знаний и языковых моделей через квантованные коды",
                    "desc": "Статья представляет двухэтапный подход к интеграции графов знаний с большими языковыми моделями. Авторы предлагают метод самоконтролируемого квантованного представления (SSQR) для сжатия структурных и семантических знаний графа в дискретные коды. Эти коды затем используются для создания инструкций для обучения языковых моделей. Эксперименты показывают превосходство SSQR над существующими методами и улучшение производительности моделей LLaMA2 и LLaMA3.1 на задачах, связанных с графами знаний."
                },
                "en": {
                    "title": "Seamless Integration of Knowledge Graphs and Language Models",
                    "desc": "This paper addresses the challenge of integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) by proposing a two-stage framework. The framework utilizes a self-supervised quantized representation (SSQR) method to convert KG structural and semantic information into discrete codes that resemble language tokens. By treating these codes as features for LLMs, the approach allows for a more efficient and effective integration of KGs with LLMs. Experimental results show that SSQR outperforms traditional methods, enabling better performance in tasks like KG link prediction and triple classification with significantly fewer tokens."
                },
                "zh": {
                    "title": "无缝整合知识图谱与大型语言模型",
                    "desc": "本论文探讨了知识图谱（KG）结构与自然语言之间的差距，提出了一种两阶段框架，以实现KG与大型语言模型（LLM）的有效整合。首先，提出了一种自监督量化表示（SSQR）方法，将KG的结构和语义知识压缩为离散代码（即令牌），使其与语言句子的格式对齐。接着，设计了KG指令跟随数据，将这些学习到的代码视为特征，直接输入到LLM中，从而实现无缝整合。实验结果表明，SSQR在无监督量化方法中表现优越，生成的代码更具可区分性，且经过微调的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中表现出色，仅使用每个实体16个令牌，而不是传统方法中的数千个。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19339",
            "title": "PixelWorld: Towards Perceiving Everything as Pixels",
            "url": "https://huggingface.co/papers/2501.19339",
            "abstract": "Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.",
            "score": 8,
            "issue_id": 2007,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "3e10b792328f7a4b",
            "authors": [
                "Zhiheng Lyu",
                "Xueguang Ma",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Department of Computer Science, University of Waterloo",
                "Vector Institute, Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19339.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единый пиксельный взгляд на мир: новая парадигма для ИИ",
                    "desc": "В статье предлагается новый подход к обработке различных модальностей данных (текст, изображения, код и т.д.) в виде пикселей, названный PEAP (Perceive Everything as Pixels). Авторы представляют набор данных PixelWorld для оценки эффективности моделей в этом unified подходе. Результаты показывают, что PEAP превосходит базовые модели на мультимодальных данных, но выявляет снижение производительности в задачах рассуждения и кодирования. Исследование демонстрирует потенциал и ограничения восприятия пиксельных данных современными языковыми моделями."
                },
                "en": {
                    "title": "Unifying Perception: Everything as Pixels",
                    "desc": "This paper introduces a new approach called 'Perceive Everything as Pixels' (PEAP), which aims to unify various input modalities like text, images, and diagrams into a single pixel-based format. The authors present PixelWorld, a novel evaluation suite designed to assess the performance of existing models when using this unified pixel input. Their experiments reveal that PEAP outperforms traditional token-based methods in multimodal datasets, although it highlights a decline in reasoning and coding abilities across models when using pixel inputs. The study concludes that while current models excel in pixel perception, there is still significant potential for enhancing their overall perceptual capabilities."
                },
                "zh": {
                    "title": "统一感知：将一切视为像素",
                    "desc": "本论文提出了一种新的统一感知框架，称为“将一切视为像素”（PEAP），旨在将文本、表格、代码、图表和图像等多种输入形式统一为像素输入。我们引入了PixelWorld评估套件，以在像素空间中评估现有模型的性能。研究发现，PEAP在多模态数据集上优于基于标记的输入，显示出统一输入在消歧义方面的优势。同时，处理像素输入时，所有模型的推理和编码能力显著下降，表明需要增强基础模型的感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14677",
            "title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
            "url": "https://huggingface.co/papers/2501.14677",
            "abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.",
            "score": 6,
            "issue_id": 2010,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 января",
                "en": "January 24",
                "zh": "1月24日"
            },
            "hash": "a9968478421ddc33",
            "authors": [
                "Peiqing Yang",
                "Shangchen Zhou",
                "Jixin Zhao",
                "Qingyi Tao",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "MatAnyone: Точное выделение объектов на видео с помощью памяти и адаптивного обучения",
                    "desc": "MatAnyone - это новый подход к выделению объектов на видео без вспомогательных данных. Он использует модуль памяти для адаптивного объединения информации из предыдущих кадров, обеспечивая стабильность основных областей и сохраняя детали на границах объектов. Авторы также создали новый большой набор данных для обучения и разработали стратегию, использующую данные сегментации для повышения стабильности. MatAnyone превосходит существующие методы в различных сценариях реального мира."
                },
                "en": {
                    "title": "MatAnyone: Robust Video Matting with Memory Propagation",
                    "desc": "The paper introduces MatAnyone, a new framework for video matting that does not require auxiliary inputs. It utilizes a memory-based approach with a memory propagation module that adapts memory from previous frames to maintain semantic consistency and detail. The authors also present a larger and more diverse dataset for training, along with a novel strategy that uses extensive segmentation data to enhance matting stability. Overall, MatAnyone achieves superior performance in complex video environments compared to existing methods."
                },
                "zh": {
                    "title": "MatAnyone：视频抠图的新突破",
                    "desc": "本论文提出了一种名为MatAnyone的视频抠图框架，旨在解决复杂背景下的抠图问题。该方法基于记忆传播模块，通过区域自适应记忆融合，动态整合前一帧的记忆信息，从而确保核心区域的语义稳定性。为了提高训练的鲁棒性，研究团队构建了一个更大、更高质量且多样化的视频抠图数据集，并采用了一种新颖的训练策略，充分利用大规模分割数据。最终，MatAnyone在多种真实场景中展现出优越的抠图效果，超越了现有的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.19399",
            "title": "Scalable-Softmax Is Superior for Attention",
            "url": "https://huggingface.co/papers/2501.19399",
            "abstract": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.",
            "score": 6,
            "issue_id": 2009,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "12ed1cad789702aa",
            "authors": [
                "Ken M. Nakanishi"
            ],
            "affiliations": [
                "Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.19399.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "SSMax: улучшение внимания трансформеров для длинных текстов",
                    "desc": "Статья представляет новую функцию Scalable-Softmax (SSMax) для улучшения работы трансформеров с длинными контекстами. SSMax решает проблему уплощения распределения внимания при увеличении размера входного вектора. Эксперименты показывают, что модели с SSMax быстрее обучаются и лучше работают с длинными текстами. SSMax позволяет модели фокусироваться на ключевой информации даже в длинных контекстах."
                },
                "en": {
                    "title": "Enhancing Attention with Scalable-Softmax for Better Context Handling",
                    "desc": "This paper addresses a limitation in Transformer-based language models where the Softmax function causes attention scores to flatten as the input size increases. This flattening reduces the model's ability to focus on important information, especially in longer contexts. The authors propose a new method called Scalable-Softmax (SSMax) that replaces the traditional Softmax function, allowing for better attention distribution and improved performance in long contexts. Experimental results show that SSMax enhances loss reduction during pretraining and enables better retrieval of key information, even for models that have already begun pretraining."
                },
                "zh": {
                    "title": "可扩展Softmax：提升Transformer模型的注意力能力",
                    "desc": "本文提出了一种新的方法，称为可扩展Softmax（SSMax），旨在解决Transformer模型在处理长上下文时的注意力分布扁平化问题。传统的Softmax函数在输入向量增大时，最大元素趋近于零，导致模型无法有效地优先考虑关键信息。SSMax可以无缝集成到现有的Transformer架构中，实验结果表明，使用SSMax的模型在语言建模中不仅在预训练期间实现了更快的损失减少，还显著提高了在长上下文中的性能。通过分析注意力分数，SSMax使模型能够在长上下文中更好地关注关键信息，提升了模型的长度泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.04983",
            "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
            "url": "https://huggingface.co/papers/2411.04983",
            "abstract": "The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.",
            "score": 6,
            "issue_id": 1999,
            "pub_date": "2025-11-07",
            "pub_date_card": {
                "ru": "7 ноября",
                "en": "November 7",
                "zh": "11月7日"
            },
            "hash": "e72081596b626524",
            "authors": [
                "Gaoyue Zhou",
                "Hengkai Pan",
                "Yann LeCun",
                "Lerrel Pinto"
            ],
            "affiliations": [
                "Courant Institute, New York University",
                "Meta-FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.04983.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DINO-WM: универсальная модель мира для планирования поведения без реконструкции",
                    "desc": "Статья представляет новый метод моделирования визуальной динамики без реконструкции визуального мира - DINO World Model (DINO-WM). DINO-WM использует пространственные признаки патчей, предварительно обученные с помощью DINOv2, что позволяет ему учиться на офлайн-траекториях поведения, предсказывая будущие признаки патчей. Этот подход позволяет DINO-WM достигать наблюдаемых целей путем оптимизации последовательности действий, облегчая планирование поведения, независимое от задачи. Эксперименты показывают, что DINO-WM может генерировать поведенческие решения с нуля во время тестирования, демонстрируя сильные способности к обобщению по сравнению с предыдущими современными методами."
                },
                "en": {
                    "title": "DINO-WM: Predicting the Future with Passive Data for Task-Agnostic Planning",
                    "desc": "This paper introduces DINO World Model (DINO-WM), a novel approach for creating predictive models that can reason and plan across various tasks using only passive data. DINO-WM is designed to learn from offline trajectories without needing to reconstruct the visual environment, focusing instead on predicting future visual features. The model is capable of optimizing behavior at test time and supports task-agnostic reasoning, making it versatile for different applications. Experimental results show that DINO-WM can effectively generate solutions in diverse scenarios without relying on expert demonstrations or pre-learned models, showcasing its strong generalization abilities."
                },
                "zh": {
                    "title": "DINO-WM：无任务依赖的世界模型",
                    "desc": "本文提出了一种新的世界模型DINO-WM，旨在通过被动数据进行推理和规划。DINO-WM具有三个关键特性：可以在离线收集的轨迹上进行训练，支持测试时行为优化，并促进任务无关的推理。该模型利用DINOv2预训练的空间补丁特征，通过预测未来的补丁特征来学习，从而实现观察目标的行为规划。实验结果表明，DINO-WM在多种任务中表现出色，能够在没有专家示范和奖励建模的情况下生成零-shot行为解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18837",
            "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
            "url": "https://huggingface.co/papers/2501.18837",
            "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.",
            "score": 4,
            "issue_id": 1996,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "62d14973b1140e58",
            "authors": [
                "Mrinank Sharma",
                "Meg Tong",
                "Jesse Mu",
                "Jerry Wei",
                "Jorrit Kruthoff",
                "Scott Goodfriend",
                "Euan Ong",
                "Alwin Peng",
                "Raj Agarwal",
                "Cem Anil",
                "Amanda Askell",
                "Nathan Bailey",
                "Joe Benton",
                "Emma Bluemke",
                "Samuel R. Bowman",
                "Eric Christiansen",
                "Hoagy Cunningham",
                "Andy Dau",
                "Anjali Gopal",
                "Rob Gilson",
                "Logan Graham",
                "Logan Howard",
                "Nimit Kalra",
                "Taesung Lee",
                "Kevin Lin",
                "Peter Lofgren",
                "Francesco Mosconi",
                "Clare O'Hara",
                "Catherine Olsson",
                "Linda Petrini",
                "Samir Rajani",
                "Nikhil Saxena",
                "Alex Silverstein",
                "Tanya Singh",
                "Theodore Sumers",
                "Leonard Tang",
                "Kevin K. Troy",
                "Constantin Weisser",
                "Ruiqi Zhong",
                "Giulio Zhou",
                "Jan Leike",
                "Jared Kaplan",
                "Ethan Perez"
            ],
            "affiliations": [
                "Safeguards Research Team, Anthropic"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18837.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#security",
                    "#inference"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Конституционные Классификаторы: надежная защита языковых моделей",
                    "desc": "Исследование представляет концепцию Конституционных Классификаторов - защитных механизмов для больших языковых моделей (LLM), обученных на синтетических данных с использованием правил, определяющих допустимый контент. Эти классификаторы эффективно противостоят универсальным методам обхода защиты, не позволяя извлекать вредоносную информацию из защищенных моделей. Эксперименты показали устойчивость классификаторов к различным атакам и их практическую применимость с минимальным влиянием на производительность. Исследование демонстрирует возможность эффективной защиты LLM от универсальных методов обхода при сохранении практической применимости."
                },
                "en": {
                    "title": "Defending LLMs with Constitutional Classifiers",
                    "desc": "This paper addresses the vulnerability of large language models (LLMs) to universal jailbreaks, which are strategies that allow users to bypass safety measures. The authors propose a solution called Constitutional Classifiers, which are trained on synthetic data generated by LLMs using natural language rules that define what content is allowed or restricted. Through extensive testing, these classifiers showed strong resistance to jailbreak attempts, effectively protecting the model without significantly impacting its performance. The findings suggest that it is possible to defend against such attacks while still ensuring the model can be used effectively in real-world applications."
                },
                "zh": {
                    "title": "宪法分类器：保护大型语言模型的安全",
                    "desc": "大型语言模型（LLMs）容易受到普遍越狱攻击，这种攻击可以绕过模型的安全措施，允许用户进行有害操作。为此，我们提出了宪法分类器，这是一种基于合成数据训练的安全措施，合成数据是通过自然语言规则（即宪法）提示LLMs生成的，规定了允许和限制的内容。在超过3000小时的红队测试中，没有红队成员找到能够从早期分类器保护的LLM中提取信息的普遍越狱方法。我们的研究表明，在保持实际部署可行性的同时，防御普遍越狱攻击是可行的。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18841",
            "title": "Trading Inference-Time Compute for Adversarial Robustness",
            "url": "https://huggingface.co/papers/2501.18841",
            "abstract": "We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many cases (with important exceptions), the fraction of model samples where the attack succeeds tends to zero as the amount of test-time compute grows. We perform no adversarial training for the tasks we study, and we increase inference-time compute by simply allowing the models to spend more compute on reasoning, independently of the form of attack. Our results suggest that inference-time compute has the potential to improve adversarial robustness for Large Language Models. We also explore new attacks directed at reasoning models, as well as settings where inference-time compute does not improve reliability, and speculate on the reasons for these as well as ways to address them.",
            "score": 3,
            "issue_id": 1994,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "f1e75e6b24f3e044",
            "authors": [
                "Wojciech Zaremba",
                "Evgenia Nitishinskaya",
                "Boaz Barak",
                "Stephanie Lin",
                "Sam Toyer",
                "Yaodong Yu",
                "Rachel Dias",
                "Eric Wallace",
                "Kai Xiao",
                "Johannes Heidecke",
                "Amelia Glaese"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.18841.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Больше вычислений - выше защита: повышение устойчивости ИИ к атакам",
                    "desc": "Исследование посвящено влиянию увеличения вычислительных ресурсов во время вывода на устойчивость моделей рассуждений к состязательным атакам. Эксперименты показали, что увеличение вычислений при выводе улучшает робастность моделей к различным атакам. В большинстве случаев доля успешных атак стремится к нулю при росте вычислительных ресурсов. Результаты указывают на потенциал увеличения вычислений при выводе для повышения устойчивости больших языковых моделей к состязательным атакам."
                },
                "en": {
                    "title": "Boosting Robustness: More Compute, Less Vulnerability",
                    "desc": "This paper investigates how increasing the amount of compute used during inference can enhance the robustness of reasoning models, specifically OpenAI's o1-preview and o1-mini, against adversarial attacks. The authors find that as the compute increases, the success rate of these attacks generally decreases, indicating improved model resilience. Notably, this improvement occurs without any adversarial training, simply by allowing the models to utilize more resources for reasoning tasks. The study also examines new types of attacks and scenarios where increased compute does not lead to better reliability, providing insights into potential solutions."
                },
                "zh": {
                    "title": "增加推理计算，提升模型鲁棒性",
                    "desc": "本研究探讨了在推理模型中增加推理时间计算对其抵御对抗攻击的影响。我们发现，随着推理时间计算的增加，模型的鲁棒性得到了提升。大多数情况下，攻击成功的模型样本比例随着测试时间计算的增加而趋近于零。我们的结果表明，推理时间计算有潜力提高大型语言模型的对抗鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18052",
            "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
            "url": "https://huggingface.co/papers/2501.18052",
            "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.",
            "score": 2,
            "issue_id": 2011,
            "pub_date": "2025-01-29",
            "pub_date_card": {
                "ru": "29 января",
                "en": "January 29",
                "zh": "1月29日"
            },
            "hash": "d94056a77d806ada",
            "authors": [
                "Bartosz Cywiński",
                "Kamil Deja"
            ],
            "affiliations": [
                "IDEAS NCBR",
                "Warsaw University of Technology, Poland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18052.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#interpretability",
                    "#security",
                    "#architecture",
                    "#ethics",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧹",
                "ru": {
                    "title": "Чистка нейросетей: SAeUron удаляет нежелательные концепции из диффузионных моделей",
                    "desc": "SAeUron - это новый метод удаления нежелательных концепций в диффузионных моделях для генерации изображений по тексту. Он использует разреженные автоэнкодеры (SAE) для выделения интерпретируемых признаков, соответствующих конкретным концепциям. Метод позволяет точно вмешиваться в активации модели для блокировки целевого контента при сохранении общей производительности. SAeUron показывает лучшие результаты по сравнению с другими методами и может удалять несколько концепций одновременно."
                },
                "en": {
                    "title": "SAeUron: Safeguarding Diffusion Models with Sparse Autoencoders",
                    "desc": "This paper presents SAeUron, a new method designed to improve the safety of text-to-image diffusion models by removing unwanted concepts. It utilizes sparse autoencoders (SAEs) to learn and identify specific features from the model's activations, allowing for targeted interventions. The method enables precise control over the model's outputs while maintaining its overall performance. Evaluation shows that SAeUron outperforms existing techniques in unlearning tasks and effectively reduces the risk of generating harmful content."
                },
                "zh": {
                    "title": "SAeUron：去除不良内容的新方法",
                    "desc": "扩散模型虽然强大，但可能会生成有害或不良内容，带来伦理和安全问题。最近的机器遗忘方法提供了潜在的解决方案，但通常缺乏透明性，难以理解对基础模型的更改。我们提出了一种新方法SAeUron，利用稀疏自编码器（SAE）学习的特征来去除文本到图像扩散模型中的不必要概念。通过在多个去噪时间步的激活上无监督训练SAE，我们捕捉到与特定概念对应的稀疏和可解释特征，从而实现精确干预。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18965",
            "title": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training",
            "url": "https://huggingface.co/papers/2501.18965",
            "abstract": "We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound due to the absence of logarithmic terms. Further, we show that this surprisingly close match between optimization theory and practice can be exploited for learning-rate tuning: we achieve noticeable improvements for training 124M and 210M Llama-type models by (i) extending the schedule for continued training with optimal learning-rate, and (ii) transferring the optimal learning-rate across schedules.",
            "score": 2,
            "issue_id": 2007,
            "pub_date": "2025-01-31",
            "pub_date_card": {
                "ru": "31 января",
                "en": "January 31",
                "zh": "1月31日"
            },
            "hash": "a136293a2241150e",
            "authors": [
                "Fabian Schaipp",
                "Alexander Hägele",
                "Adrien Taylor",
                "Umut Simsekli",
                "Francis Bach"
            ],
            "affiliations": [
                "EPFL, Lausanne, Switzerland",
                "Inria, Departement dInformatique de lEcole Normale Superieure, PSL Research University, Paris, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18965.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#math",
                    "#training",
                    "#optimization"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Оптимизация графиков обучения для больших языковых моделей",
                    "desc": "В статье исследуются графики изменения скорости обучения для больших моделей машинного обучения. Авторы обнаружили неожиданное сходство этих графиков с теоретическими границами из теории невыпуклой оптимизации. Они предлагают новый метод настройки скорости обучения, основанный на этом наблюдении. Применение метода позволило улучшить результаты обучения языковых моделей типа Llama размером 124M и 210M параметров."
                },
                "en": {
                    "title": "Optimizing Learning Rates: Bridging Theory and Practice",
                    "desc": "This paper explores the relationship between learning-rate schedules in large model training and concepts from non-smooth convex optimization theory. It establishes a performance bound for a constant learning-rate schedule with a linear cooldown, highlighting the practical advantages of this approach. The authors demonstrate that the alignment between theoretical optimization and practical training can be leveraged for better learning-rate tuning. By optimizing the learning-rate schedule, they achieve significant improvements in training large Llama-type models."
                },
                "zh": {
                    "title": "优化学习率调度，提升大模型训练效果",
                    "desc": "本文探讨了大模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间的相似性。我们提供了一个线性冷却的常数调度的界限，特别是冷却的实际好处在于没有对数项的影响。进一步地，我们展示了优化理论与实践之间的紧密联系可以用于学习率调优：通过延长调度以继续训练并使用最佳学习率，我们在训练124M和210M的Llama类型模型时取得了显著的改进。最后，我们还展示了在不同调度之间转移最佳学习率的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18753",
            "title": "INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation",
            "url": "https://huggingface.co/papers/2501.18753",
            "abstract": "Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.",
            "score": 2,
            "issue_id": 2002,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "000663cf445862a1",
            "authors": [
                "Jian Hu",
                "Zixu Cheng",
                "Shaogang Gong"
            ],
            "affiliations": [
                "Queen Mary University of London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18753.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#healthcare",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Адаптивная сегментация изображений с помощью интеллектуального отбора промптов",
                    "desc": "Статья представляет новый метод сегментации изображений под названием INT (Instance-specific Negative Mining for Task-Generic Promptable Segmentation). Этот подход направлен на улучшение генерации промптов, специфичных для конкретных экземпляров изображений, путем адаптивного уменьшения влияния нерелевантных знаний и усиления наиболее вероятных. INT состоит из двух компонентов: генерации промптов, специфичных для экземпляров, и генерации семантической маски. Метод был проверен на шести наборах данных, включая камуфлированные объекты и медицинские изображения, демонстрируя эффективность, надежность и масштабируемость."
                },
                "en": {
                    "title": "Enhancing Image Segmentation with Smart Prompting",
                    "desc": "This paper presents a new method called Instance-specific Negative Mining for Task-Generic Promptable Segmentation (INT) to improve image segmentation using a single task description. The method addresses the challenge of Vision-Language Models (VLMs) struggling to generalize to certain image instances, which can lead to poor segmentation results. INT works by selectively reducing the impact of irrelevant information while enhancing the use of relevant prior knowledge through a process called negative mining. The effectiveness of INT is validated across six diverse datasets, showing its ability to produce accurate and robust segmentation results."
                },
                "zh": {
                    "title": "实例特定负采样优化图像分割",
                    "desc": "这篇论文提出了一种新的方法，称为实例特定负采样（INT），用于任务通用的可提示图像分割。INT的核心思想是自适应地减少无关的先验知识的影响，同时增加最有可能的先验知识的使用，以优化实例特定提示的生成。该方法包括两个主要部分：实例特定提示生成和语义掩码生成，确保每个图像实例的分割与提示的语义相匹配。通过在六个数据集上的验证，INT展示了其有效性、鲁棒性和可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18804",
            "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
            "url": "https://huggingface.co/papers/2501.18804",
            "abstract": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.",
            "score": 1,
            "issue_id": 2010,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "32db517ad974401b",
            "authors": [
                "Vitor Guizilini",
                "Muhammad Zubair Irshad",
                "Dian Chen",
                "Greg Shakhnarovich",
                "Rares Ambrus"
            ],
            "affiliations": [
                "Toyota Research Institute (TRI)",
                "Toyota Technological Institute at Chicago (TTIC)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18804.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#3d",
                    "#diffusion",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Генерация 3D сцен из разных ракурсов с помощью диффузионной модели",
                    "desc": "Статья представляет MVGD - архитектуру на основе диффузии для генерации изображений и карт глубины с новых ракурсов. Метод использует райкарты для обогащения визуальных признаков пространственной информацией и направления генерации. Ключевой аспект - многозадачная генерация изображений и карт глубины с использованием обучаемых эмбеддингов задач. Модель обучена на более чем 60 миллионах мультиракурсных примеров и показывает современные результаты в задачах синтеза новых ракурсов и оценки глубины."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Reconstruction with Direct Pixel-Level Generation",
                    "desc": "This paper presents MVGD, a new diffusion-based model for generating images and depth maps from multiple input views in 3D scene reconstruction. Unlike traditional methods that rely on intermediate 3D representations, MVGD directly produces pixel-level outputs, enhancing visual features with spatial information through raymap conditioning. The model employs multi-task learning, using task embeddings to effectively guide the generation process for both images and depth maps. Trained on a vast dataset of over 60 million samples, MVGD achieves state-of-the-art performance in novel view synthesis and depth estimation tasks."
                },
                "zh": {
                    "title": "MVGD：从多视角直接生成图像与深度图的创新方法",
                    "desc": "本文提出了一种名为MVGD的扩散基础架构，能够直接从多个视角生成图像和深度图。该方法通过光线图条件化，增强了视觉特征并引导生成过程。我们采用多任务生成技术，同时生成图像和深度图，并使用可学习的任务嵌入来优化扩散过程。经过在超过6000万多视角样本上的训练，我们在多个基准测试中取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.18128",
            "title": "Unraveling the Capabilities of Language Models in News Summarization",
            "url": "https://huggingface.co/papers/2501.18128",
            "abstract": "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.",
            "score": 1,
            "issue_id": 2000,
            "pub_date": "2025-01-30",
            "pub_date_card": {
                "ru": "30 января",
                "en": "January 30",
                "zh": "1月30日"
            },
            "hash": "1c3f3a16953a5a59",
            "authors": [
                "Abdurrahman Odabaşı",
                "Göksel Biricik"
            ],
            "affiliations": [
                "Department of Computer Engineering, Turkish-German University, 34820, Istanbul, Turkiye",
                "Department of Computer Engineering, Yıldız Technical University, 34220, Istanbul, Turkiye"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.18128.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multilingual",
                    "#small_models",
                    "#benchmark",
                    "#transfer_learning"
                ],
                "emoji": "📰",
                "ru": {
                    "title": "Маленькие модели бросают вызов гигантам в суммаризации новостей",
                    "desc": "В этой работе проводится комплексное сравнение 20 современных языковых моделей, с акцентом на меньшие модели, для задачи суммаризации новостей. Исследование охватывает обучение в режимах zero-shot и few-shot на трех различных наборах данных, используя автоматические метрики, человеческую оценку и LLM в качестве судьи. Интересно, что включение демонстрационных примеров в режиме few-shot не улучшило производительность моделей, а в некоторых случаях даже ухудшило качество генерируемых сводок. Результаты показали превосходство GPT-3.5-Turbo и GPT-4, но также выявили перспективные открытые модели, такие как Qwen1.5-7B и SOLAR-10.7B-Instruct-v1.0."
                },
                "en": {
                    "title": "Benchmarking News Summarization: Small Models Can Compete!",
                    "desc": "This paper benchmarks 20 recent language models specifically for the task of news summarization, emphasizing smaller models. It evaluates their performance in zero-shot and few-shot learning scenarios across three different datasets with varying writing styles. The study reveals that providing demonstration examples in few-shot settings often does not improve, and can even degrade, the quality of summaries due to the inadequacy of reference summaries. Notably, while larger models like GPT-3.5-Turbo and GPT-4 excel, several smaller models also show competitive performance, suggesting they could be viable alternatives for summarization tasks."
                },
                "zh": {
                    "title": "小模型在新闻摘要中的潜力与挑战",
                    "desc": "本研究对20种最新的语言模型进行了全面的基准测试，重点关注较小的模型在新闻摘要任务中的表现。我们系统地测试了这些模型在不同风格的新闻文章摘要中的能力和有效性，使用了三种不同的数据集。研究发现，在少量示例学习的设置中，提供示例并未提升模型的性能，反而在某些情况下导致生成摘要的质量下降。这主要是由于参考摘要的质量较差，影响了模型的表现，同时我们的研究结果显示，GPT-3.5-Turbo和GPT-4在性能上表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2404.07097",
            "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
            "url": "https://huggingface.co/papers/2404.07097",
            "abstract": "This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time.",
            "score": 1,
            "issue_id": 1999,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a526ae197fe3a8c7",
            "authors": [
                "Yoni Kasten",
                "Wuyue Lu",
                "Haggai Maron"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Simon Fraser University",
                "Technion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2404.07097.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективная 3D реконструкция из видео с помощью глубокого обучения",
                    "desc": "Статья представляет TracksTo4D - новый подход к реконструкции 3D структур из видео с динамическим содержанием. Метод использует нейронную сеть, обученную без учителя на 2D треках точек, извлеченных из обычных видео. TracksTo4D позволяет восстанавливать облако точек и положение камеры за один проход сети, значительно сокращая время вычислений по сравнению с существующими методами. Архитектура сети учитывает симметрии входных данных и предполагает низкоранговое представление паттернов движения."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction from Casual Videos",
                    "desc": "This paper introduces TracksTo4D, a novel machine learning approach for reconstructing 3D structures from casual videos. Unlike traditional methods that require extensive optimization or are limited to specific video types, TracksTo4D processes 2D point tracks directly in a single, efficient feed-forward pass. The architecture is designed to leverage the symmetries in point track data and utilizes low-rank approximations to represent movement patterns effectively. Trained unsupervised on a dataset of casual videos, TracksTo4D achieves high accuracy in reconstructing temporal point clouds and camera positions while significantly reducing processing time."
                },
                "zh": {
                    "title": "高效重建3D结构，TracksTo4D引领新潮流",
                    "desc": "本文解决了从动态内容视频重建3D结构的长期挑战。现有方法无法处理普通相机录制的随意视频，或需要较长的优化时间。我们提出了一种基于学习的方法TracksTo4D，通过单次高效的前馈传递，从随意视频中推断3D结构和相机位置。该方法直接处理2D点轨迹，并设计了专门的架构，能够在无监督的情况下进行训练，显著提高了重建效率。"
                }
            }
        }
    ],
    "link_prev": "2025-01-31.html",
    "link_next": "2025-02-04.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "31.01",
        "en": "01/31",
        "zh": "1月31日"
    },
    "short_date_next": {
        "ru": "04.02",
        "en": "02/04",
        "zh": "2月4日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的语言建模方法，称为测试时缩放。OpenAI的o1模型展示了这种能力，但没有公开其方法，导致许多复制努力。作者通过整理一个小数据集s1K和开发预算强制方法，寻找最简单的途径实现测试时缩放和强大的推理性能。他们的模型s1在数学竞赛问题上超越了o1-preview，并且通过预算强制进一步提升了性能。模型、数据和代码都是开源的。",
        "title": "s1: Simple test-time scaling",
        "pinyin": "这篇文章介绍了一种新的语言建模方法，称为测试时缩放。OpenAI的o1模型展示了这种能力，但没有公开其方法，导致许多复制努力。作者通过整理一个小数据集s1K和开发预算强制方法，寻找最简单的途径实现测试时缩放和强大的推理性能。他们的模型s1在数学竞赛问题上超越了o1-preview，并且通过预算强制进一步提升了性能。模型、数据和代码都是开源的。\n\nZhè piān wénzhāng jièshào le yīzhǒng xīn de yǔyán jiànmó fāngfǎ, chēngwéi cèshì shí suōfàng. OpenAI de o1 móxíng zhǎnshì le zhè zhǒng nénglì, dàn méiyǒu gōngkāi qí fāngfǎ, dǎozhì xǔduō fùzhì nǔlì. Zuòzhě tōngguò zhěnglǐ yīgè xiǎo shùjùjí s1K hé kāifā yùsuàn qiángzhì fāngfǎ, xúnzhǎo zuì jiǎndān de tújìng shíxiàn cèshì shí suōfàng hé qiángdà de tuīlǐ xìngnéng. Tāmen de móxíng s1 zài shùxué jìngsài wèntí shàng chāoyuè le o1-preview, bìngqiě tōngguò yùsuàn qiángzhì jìnfā tīshēng le xìngnéng. Móxíng, shùjù hé dàimǎ dōu shì kāiyuán de.",
        "vocab": "[\n{'word': '建模', 'pinyin': 'jiàn mó', 'trans': 'modeling'},\n{'word': '称为', 'pinyin': 'chēng wéi', 'trans': 'called'},\n{'word': '缩放', 'pinyin': 'suō fàng', 'trans': 'scaling'},\n{'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'capability'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': '复制', 'pinyin': 'fù zhì', 'trans': 'replicate'},\n{'word': '努力', 'pinyin': 'nǔ lì', 'trans': 'efforts'},\n{'word': '整理', 'pinyin': 'zhěng lǐ', 'trans': 'organize'},\n{'word': '预算', 'pinyin': 'yù suàn', 'trans': 'budget'},\n{'word': '强制', 'pinyin': 'qiáng zhì', 'trans': 'enforcement'},\n{'word': '途径', 'pinyin': 'tú jìng', 'trans': 'means'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '超越', 'pinyin': 'chāo yuè', 'trans': 'surpass'},\n{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'}\n]",
        "trans": "This article introduces a new language modeling method called test-time scaling. The o1 model from OpenAI demonstrated this capability, but its method was not made public, leading to many attempts to replicate it. The authors, by curating a small dataset s1K and developing a budget enforcement method, sought the simplest way to achieve test-time scaling and powerful reasoning performance. Their model, s1, outperformed o1-preview on mathematical competition problems and further enhanced performance through budget enforcement. The model, data, and code are all open-sourced.",
        "update_ts": "2025-02-03 09:11"
    }
}