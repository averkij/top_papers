{
    "date": {
        "ru": "6 Ğ¸ÑĞ½Ñ",
        "en": "June 6",
        "zh": "6æœˆ6æ—¥"
    },
    "time_utc": "2025-06-06 02:40",
    "weekday": 4,
    "issue_id": 4155,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.04308",
            "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
            "url": "https://huggingface.co/papers/2506.04308",
            "abstract": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.",
            "score": 7,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "ef5abd087929ed17",
            "authors": [
                "Enshen Zhou",
                "Jingkun An",
                "Cheng Chi",
                "Yi Han",
                "Shanyu Rong",
                "Chi Zhang",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Lu Sheng",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Academy of Artificial Intelligence",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04308.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#robotics",
                    "#reasoning",
                    "#dataset",
                    "#3d",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RoboRefer: ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "RoboRefer - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RefSpatial Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RefSpatial-Bench Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RoboRefer Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Gemini-2.5-Pro, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments",
                    "desc": "The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks."
                },
                "zh": {
                    "title": "RoboReferï¼šæå‡æœºå™¨äººç©ºé—´ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "ç©ºé—´æŒ‡å‘æ˜¯å…·èº«æœºå™¨äººä¸ä¸‰ç»´ç‰©ç†ä¸–ç•Œäº’åŠ¨çš„åŸºæœ¬èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾ˆå¼ºå¤§ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å¤æ‚çš„ä¸‰ç»´åœºæ™¯å’ŒåŠ¨æ€æ¨ç†æŒ‡ç¤ºä½ç½®æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RoboReferï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ä¸‰ç»´æ„ŸçŸ¥èƒ½åŠ›çš„VLMï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é›†æˆäº†ä¸“é—¨çš„æ·±åº¦ç¼–ç å™¨ï¼Œå®ç°äº†ç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚æ­¤å¤–ï¼ŒRoboReferé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ¨è¿›äº†å¤šæ­¥éª¤ç©ºé—´æ¨ç†ï¼Œé‡‡ç”¨é’ˆå¯¹ç©ºé—´æŒ‡å‘ä»»åŠ¡çš„åº¦é‡æ•æ„Ÿè¿‡ç¨‹å¥–åŠ±å‡½æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04209",
            "title": "Language-Image Alignment with Fixed Text Encoders",
            "url": "https://huggingface.co/papers/2506.04209",
            "abstract": "Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.",
            "score": 4,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "921137445b3be92e",
            "authors": [
                "Jingfeng Yang",
                "Ziyang Wu",
                "Yue Zhao",
                "Yi Ma"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04209.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#alignment",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¤Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LIFT Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², ĞºĞ°Ğº Ğ² CLIP, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. LIFT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "LIFT: Efficient Language-Image Alignment with Fixed Text Encoders",
                    "desc": "This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment."
                },
                "zh": {
                    "title": "ç®€åŒ–è®­ç»ƒï¼Œæå‡è§†è§‰ç†è§£çš„LIFTæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºLIFTï¼ˆä½¿ç”¨å›ºå®šæ–‡æœ¬ç¼–ç å™¨çš„è¯­è¨€-å›¾åƒå¯¹é½ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„è”åˆè®­ç»ƒæ–¹æ³•ï¼ˆå¦‚CLIPï¼‰ç›¸æ¯”ï¼ŒLIFTåªè®­ç»ƒå›¾åƒç¼–ç å™¨ï¼Œè€Œä½¿ç”¨å›ºå®šçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLIFTåœ¨å¤„ç†ç»„åˆç†è§£å’Œé•¿æ–‡æœ¬æè¿°æ—¶ï¼Œè¡¨ç°ä¼˜äºCLIPï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥æ¥æŒ‡å¯¼è§†è§‰å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23656",
            "title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models",
            "url": "https://huggingface.co/papers/2505.23656",
            "abstract": "Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.",
            "score": 4,
            "issue_id": 4155,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "aa7bbc7378df2b3e",
            "authors": [
                "Xiangdong Zhang",
                "Jiaqi Liao",
                "Shaofeng Zhang",
                "Fanqing Meng",
                "Xiangpeng Wan",
                "Junchi Yan",
                "Yu Cheng"
            ],
            "affiliations": [
                "Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University",
                "NetMind.AI",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23656.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: VideoREPA ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… T2V",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ VideoREPA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ text-to-video (T2V). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T2V Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Token Relation Distillation (TRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T2V. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoREPA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° CogVideoX."
                },
                "en": {
                    "title": "Bridging the Physics Gap in Text-to-Video Generation",
                    "desc": "This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation."
                },
                "zh": {
                    "title": "VideoREPAï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç‰©ç†ç†è§£èƒ½åŠ›",
                    "desc": "æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—é«˜ä¿çœŸå’ŒçœŸå®çš„è§†é¢‘åˆæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„T2Væ¨¡å‹åœ¨ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„å†…å®¹æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹ç‰©ç†çš„ç†è§£èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºVideoREPAï¼Œé€šè¿‡å¯¹é½ä»¤ç‰Œçº§å…³ç³»ï¼Œå°†è§†é¢‘ç†è§£åŸºç¡€æ¨¡å‹ä¸­çš„ç‰©ç†ç†è§£èƒ½åŠ›æç‚¼åˆ°T2Væ¨¡å‹ä¸­ï¼Œä»è€Œç¼©å°ç‰©ç†ç†è§£çš„å·®è·ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVideoREPAæ˜¾è‘—å¢å¼ºäº†åŸºçº¿æ–¹æ³•CogVideoXçš„ç‰©ç†å¸¸è¯†ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ç›´è§‚ç‰©ç†ä¸€è‡´çš„è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05344",
            "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
            "url": "https://huggingface.co/papers/2506.05344",
            "abstract": "Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.",
            "score": 3,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "0175b3788ebacf29",
            "authors": [
                "Jiahui Wang",
                "Zuyan Liu",
                "Yongming Rao",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Tencent Hunyuan Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05344.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ (Ğ¼ĞµĞ½ĞµĞµ 5%) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SparseMM Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ MLLM Ğ² 1,38 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 52% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ SparseMM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ°."
                },
                "en": {
                    "title": "Optimizing Visual Understanding in MLLMs with Sparse Attention",
                    "desc": "This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç¨€ç–è§†è§‰å¤´åŠ é€Ÿå¤šæ¨¡æ€æ¨¡å‹æ¨ç†",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¢åŠ è§†è§‰èƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†MLLMså¦‚ä½•å¤„ç†è§†è§‰è¾“å…¥ï¼Œåˆ†æäº†å®ƒä»¬çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç¨€ç–ç°è±¡ï¼šåœ¨LLMsä¸­ï¼Œåªæœ‰å°‘é‡ï¼ˆå¤§çº¦5%ä»¥ä¸‹ï¼‰çš„æ³¨æ„åŠ›å¤´ç§¯æå‚ä¸è§†è§‰ç†è§£ï¼Œè¿™äº›è¢«ç§°ä¸ºè§†è§‰å¤´ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†SparseMMï¼Œä¸€ç§KV-Cacheä¼˜åŒ–ç­–ç•¥ï¼Œæ ¹æ®è§†è§‰å¾—åˆ†ä¸ºLLMsä¸­çš„å¤´åˆ†é…ä¸å¯¹ç§°çš„è®¡ç®—é¢„ç®—ï¼Œä»è€ŒåŠ é€ŸMLLMsçš„æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05328",
            "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs",
            "url": "https://huggingface.co/papers/2506.05328",
            "abstract": "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.",
            "score": 3,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "774ccf3fd01aa4ef",
            "authors": [
                "Lidong Lu",
                "Guo Chen",
                "Zhiqi Li",
                "Yicheng Liu",
                "Tong Lu"
            ],
            "affiliations": [
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05328.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#long_context",
                    "#training",
                    "#video",
                    "#rl"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CG-AV-Counting Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AV-Reasoner, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ´Ğ½Ğ°ĞºĞ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Video Counting with CG-AV-Counting and AV-Reasoner",
                    "desc": "This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘è®¡æ•°èƒ½åŠ›çš„æ–°åŸºå‡†ä¸æ¨¡å‹",
                    "desc": "å°½ç®¡è§†é¢‘ç†è§£å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰çš„å¤šæ¨¡æ€å­¦ä¹ æ¨¡å‹åœ¨è®¡æ•°ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å—é™äºçŸ­è§†é¢‘ã€å°é—­å¼æŸ¥è¯¢ã€ç¼ºä¹çº¿ç´¢æ³¨é‡Šå’Œå¤šæ¨¡æ€è¦†ç›–ä¸è¶³ã€‚æœ¬æ–‡ä»‹ç»äº†CG-AV-Countingï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨æ³¨é‡Šçš„çº¿ç´¢åŸºç¡€è®¡æ•°åŸºå‡†ï¼ŒåŒ…å«1,027ä¸ªå¤šæ¨¡æ€é—®é¢˜å’Œ5,845ä¸ªæ³¨é‡Šçº¿ç´¢ï¼Œè¦†ç›–497ä¸ªé•¿è§†é¢‘ã€‚æˆ‘ä»¬æå‡ºçš„AV-Reasoneræ¨¡å‹é€šè¿‡GRPOå’Œè¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»ç›¸å…³ä»»åŠ¡ä¸­æ¨å¹¿è®¡æ•°èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05176",
            "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
            "url": "https://huggingface.co/papers/2506.05176",
            "abstract": "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.",
            "score": 3,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "90ebf52dd91334c2",
            "authors": [
                "Yanzhao Zhang",
                "Mingxin Li",
                "Dingkun Long",
                "Xin Zhang",
                "Huan Lin",
                "Baosong Yang",
                "Pengjun Xie",
                "An Yang",
                "Dayiheng Liu",
                "Junyang Lin",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Alibaba Group",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05176.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#multilingual",
                    "#open_source",
                    "#training",
                    "#small_models",
                    "#low_resource"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Qwen3 Embedding: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3 Embedding, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ¼ GTE-Qwen. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3 Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Qwen3 Embedding Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² (0.6B, 4B, 8B) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Qwen3 Embedding Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ MTEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Empowering Multilingual Text Understanding with Qwen3 Embeddings",
                    "desc": "The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research."
                },
                "zh": {
                    "title": "Qwen3åµŒå…¥ç³»åˆ—ï¼šå¤šè¯­è¨€æ–‡æœ¬å¤„ç†çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Qwen3åµŒå…¥ç³»åˆ—ï¼Œè¿™æ˜¯åœ¨æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºèƒ½åŠ›ä¸Šç›¸è¾ƒäºGTE-Qwenç³»åˆ—çš„é‡å¤§è¿›å±•ã€‚è¯¥ç³»åˆ—åŸºäºQwen3åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å¤šè¯­è¨€æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆå¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒå’Œé«˜è´¨é‡æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒã€‚é€šè¿‡æœ‰æ•ˆçš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼ŒQwen3åµŒå…¥ç³»åˆ—ç¡®ä¿äº†æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œæä¾›äº†å¤šç§æ¨¡å‹è§„æ¨¡ä»¥æ»¡è¶³ä¸åŒçš„éƒ¨ç½²åœºæ™¯ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒQwen3åµŒå…¥ç³»åˆ—åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå°¤å…¶åœ¨å¤šè¯­è¨€è¯„ä¼°åŸºå‡†MTEBä¸Šè¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05240",
            "title": "Aligning Latent Spaces with Flow Priors",
            "url": "https://huggingface.co/papers/2506.05240",
            "abstract": "This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.",
            "score": 2,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "c776325b1f9f6966",
            "authors": [
                "Yizhuo Li",
                "Yuying Ge",
                "Yixiao Ge",
                "Ying Shan",
                "Ping Luo"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05240.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#cv",
                    "#math",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞµ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ImageNet Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Latent Spaces with Flow-Based Models",
                    "desc": "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."
                },
                "zh": {
                    "title": "æ½œåœ¨ç©ºé—´å¯¹é½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œå°†å¯å­¦ä¹ çš„æ½œåœ¨ç©ºé—´å¯¹é½åˆ°ä»»æ„ç›®æ ‡åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåœ¨ç›®æ ‡ç‰¹å¾ä¸Šé¢„è®­ç»ƒæµæ¨¡å‹ï¼Œä»¥æ•æ‰æ½œåœ¨åˆ†å¸ƒã€‚ç„¶åï¼Œè¿™ä¸ªå›ºå®šçš„æµæ¨¡å‹é€šè¿‡å¯¹é½æŸå¤±æ¥è§„èŒƒåŒ–æ½œåœ¨ç©ºé—´ï¼Œé‡æ–°æ„é€ æµåŒ¹é…ç›®æ ‡ï¼Œå°†æ½œåœ¨å˜é‡è§†ä¸ºä¼˜åŒ–ç›®æ ‡ã€‚æˆ‘ä»¬æ­£å¼è¯æ˜ï¼Œæœ€å°åŒ–è¿™ä¸ªå¯¹é½æŸå¤±å»ºç«‹äº†ä¸€ä¸ªè®¡ç®—ä¸Šå¯å¤„ç†çš„æ›¿ä»£ç›®æ ‡ï¼Œç”¨äºæœ€å¤§åŒ–æ½œåœ¨å˜é‡åœ¨ç›®æ ‡åˆ†å¸ƒä¸‹çš„å˜åˆ†ä¸‹ç•Œçš„å¯¹æ•°ä¼¼ç„¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04405",
            "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale",
            "url": "https://huggingface.co/papers/2506.04405",
            "abstract": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.",
            "score": 2,
            "issue_id": 4155,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "2cf822e179634776",
            "authors": [
                "Ran Xu",
                "Yuchen Zhuang",
                "Yishan Zhong",
                "Yue Yu",
                "Xiangru Tang",
                "Hang Wu",
                "May D. Wang",
                "Peifeng Ruan",
                "Donghan Yang",
                "Tao Wang",
                "Guanghua Xiao",
                "Carl Yang",
                "Yang Xie",
                "Wenqi Shi"
            ],
            "affiliations": [
                "Emory University",
                "Georgia Tech",
                "UT Southwestern Medical Center",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04405.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "MedAgentGYM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "MedAgentGYM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 72 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 129 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MedAgentGYM, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Med-Copilot-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Medical Reasoning in LLMs with MedAgentGYM",
                    "desc": "MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o."
                },
                "zh": {
                    "title": "MedAgentGYMï¼šæå‡åŒ»å­¦æ¨ç†èƒ½åŠ›çš„åˆ›æ–°å¹³å°",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†MedAgentGYMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„è®­ç»ƒç¯å¢ƒï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„åŸºäºç¼–ç çš„åŒ»å­¦æ¨ç†èƒ½åŠ›ã€‚MedAgentGYMåŒ…å«72,413ä¸ªä»»åŠ¡å®ä¾‹ï¼Œæ¶µç›–129ä¸ªç±»åˆ«ï¼Œæ¥æºäºçœŸå®çš„ç”Ÿç‰©åŒ»å­¦åœºæ™¯ã€‚æ¯ä¸ªä»»åŠ¡éƒ½åœ¨å¯æ‰§è¡Œçš„ç¼–ç ç¯å¢ƒä¸­å°è£…ï¼Œæä¾›è¯¦ç»†çš„ä»»åŠ¡æè¿°ã€äº’åŠ¨åé¦ˆæœºåˆ¶ã€å¯éªŒè¯çš„çœŸå®æ³¨é‡Šå’Œå¯æ‰©å±•çš„è®­ç»ƒè½¨è¿¹ç”Ÿæˆã€‚é€šè¿‡å¯¹30å¤šç§LLMçš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å•†ä¸šAPIæ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04734",
            "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design",
            "url": "https://huggingface.co/papers/2506.04734",
            "abstract": "Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.",
            "score": 1,
            "issue_id": 4155,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "ce810e0cc38b26e4",
            "authors": [
                "Lin Sun",
                "Weihong Lin",
                "Jinzhu Wu",
                "Yongfu Zhu",
                "Xiaoqi Jian",
                "Guangxiang Zhao",
                "Change Jia",
                "Linglin Zhang",
                "Sai-er Hu",
                "Yuhan Wu",
                "Xiangzheng Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.04734.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ¢",
                "ru": {
                    "title": "ĞĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµÑ€Ğ¸Ğ¸ Deepseek-R1-Distill Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼ Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Deepseek-R1-Distill, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ QwQ-32B. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Ensuring Reliable Evaluations for Deep Learning Models",
                    "desc": "The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results."
                },
                "zh": {
                    "title": "å»ºç«‹æ›´ä¸¥æ ¼çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†",
                    "desc": "Deepseek-R1-Distillç³»åˆ—æ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå—åˆ°å¼€æºç¤¾åŒºçš„å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹çš„åŸºå‡†è¯„ä¼°ç»“æœå—åˆ°å¤šç§å› ç´ çš„æ˜¾è‘—æ³¢åŠ¨å½±å“ã€‚è¯„ä¼°æ¡ä»¶çš„ç»†å¾®å·®å¼‚å¯èƒ½å¯¼è‡´ç»“æœçš„é‡å¤§å˜åŒ–ã€‚ç±»ä¼¼ç°è±¡ä¹Ÿå‡ºç°åœ¨åŸºäºDeepseek-R1-Distillç³»åˆ—å¾®è°ƒçš„å…¶ä»–å¼€æºæ¨ç†æ¨¡å‹ä¸­ï¼Œå› æ­¤æˆ‘ä»¬å‘¼åå»ºç«‹æ›´ä¸¥æ ¼çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03238",
            "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach",
            "url": "https://huggingface.co/papers/2506.03238",
            "abstract": "OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.",
            "score": 1,
            "issue_id": 4155,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "f234199601bef528",
            "authors": [
                "Ziheng Zhao",
                "Lisong Dai",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "Department of Radiology, Renmin Hospital of Wuhan University",
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03238.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#healthcare",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ OminiAbnorm-CT Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‚ĞµĞ»Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ²ÑĞµĞ¾Ğ±ÑŠĞµĞ¼Ğ»ÑÑ‰ĞµĞ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. OminiAbnorm-CT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing CT Image Analysis with OminiAbnorm-CT",
                    "desc": "OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology."
                },
                "zh": {
                    "title": "OminiAbnorm-CTï¼šCTå›¾åƒå¼‚å¸¸è‡ªåŠ¨è§£è¯»çš„æ–°çªç ´",
                    "desc": "OminiAbnorm-CTæ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨è§£è¯»CTå›¾åƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­å®šä½å’Œæè¿°å¼‚å¸¸æƒ…å†µã€‚è¯¥ç ”ç©¶é€šè¿‡ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿåˆä½œï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«404ç§å¼‚å¸¸å‘ç°çš„å±‚æ¬¡åˆ†ç±»ç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡14.5K CTå›¾åƒçš„æ•°æ®é›†ï¼Œå¹¶ä¸ºè¶…è¿‡19Kå¼‚å¸¸æä¾›äº†è¯¦ç»†çš„æ³¨é‡Šã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒOminiAbnorm-CTåœ¨æ‰€æœ‰ä»»åŠ¡å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04245",
            "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.04245",
            "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.",
            "score": 1,
            "issue_id": 4155,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "8716d3ca53b1d58f",
            "authors": [
                "Guangchen Lan",
                "Huseyin A. Inan",
                "Sahar Abdelnabi",
                "Janardhan Kulkarni",
                "Lukas Wutschitz",
                "Reza Shokri",
                "Christopher G. Brinton",
                "Robert Sim"
            ],
            "affiliations": [
                "Microsoft",
                "National University of Singapore",
                "Purdue University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04245.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#transfer_learning",
                    "#rl",
                    "#leakage"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (CI) Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ CI, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº PrivacyLens."
                },
                "en": {
                    "title": "Enhancing Contextual Integrity in Autonomous Agents",
                    "desc": "This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks."
                },
                "zh": {
                    "title": "ç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œæå‡è‡ªä¸»ä»£ç†å†³ç­–èƒ½åŠ›",
                    "desc": "åœ¨è‡ªä¸»ä»£ç†ä¸ºç”¨æˆ·åšå†³ç­–çš„æ—¶ä»£ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆCIï¼‰æˆä¸ºä¸€ä¸ªé‡è¦é—®é¢˜ã€‚æœ¬æ–‡æå‡ºï¼ŒCIéœ€è¦ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¯¹å…¶æ“ä½œçš„ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬é¦–å…ˆè®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜ç¡®æ¨ç†CIï¼Œä»¥å†³å®šæŠ«éœ²å“ªäº›ä¿¡æ¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹è¿›è¡ŒCIæ‰€éœ€çš„æ¨ç†èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸å½“ä¿¡æ¯æŠ«éœ²ï¼ŒåŒæ—¶ä¿æŒäº†ä»»åŠ¡æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-05.html",
    "link_next": "2025-06-09.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "05.06",
        "en": "06/05",
        "zh": "6æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "09.06",
        "en": "06/09",
        "zh": "6æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 1,
        "#cv": 3,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬å¼€æºäº†ä¸¤ä¸ªå¼ºå¤§çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ŒMiMo-VL-7B-SFTå’ŒMiMo-VL-7B-RLï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚MiMo-VL-7B-RLåœ¨35ä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†Qwen2.5-VL-7Bï¼Œå¹¶åœ¨OlympiadBenchä¸Šå¾—åˆ†59.4ï¼Œè¶…è¿‡äº†å‚æ•°é‡é«˜è¾¾78Bçš„æ¨¡å‹ã€‚åœ¨GUIåº”ç”¨ä¸­ï¼Œå®ƒåœ¨OSWorld-Gä¸Šå¾—åˆ†56.1ï¼Œç”šè‡³è¶…è¿‡äº†ä¸“é—¨æ¨¡å‹UI-TARSã€‚æˆ‘ä»¬çš„è®­ç»ƒç»“åˆäº†å››é˜¶æ®µé¢„è®­ç»ƒå’Œæ··åˆåœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªåŒ…å«50å¤šä¸ªä»»åŠ¡çš„å…¨é¢è¯„ä¼°å¥—ä»¶ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œæ¨åŠ¨é¢†åŸŸå‘å±•ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å’Œå®Œæ•´è¯„ä¼°å¥—ä»¶å¯åœ¨https://github.com/XiaomiMiMo/MiMo-VLè·å–ã€‚",
        "title": "MiMo-VL Technical Report",
        "pinyin": "WÇ’men kÄiyuÃ¡nle liÇng gÃ¨ qiÃ¡ngdÃ  de shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng, MiMo-VL-7B-SFT hÃ© MiMo-VL-7B-RL, tÄmen zÃ i shÃ¬juÃ© lÇjiÄ› hÃ© duÅ mÃ³shÃ¬ tuÄ«lÇ fÄngmiÃ n biÇoxiÃ n chÅ«sÃ¨. MiMo-VL-7B-RL zÃ i 35 gÃ¨ rÃ¨nwÃ¹ zhÅng chÄoyuÃ¨le Qwen2.5-VL-7B, bÃ¬ng zÃ i OlympiadBench shÃ ng dÃ©fÄ“n 59.4, chÄoguÃ²le cÄnshÃ¹ liÃ ng gÄodÃ¡ 78B de mÃ³xÃ­ng. ZÃ i GUI yÃ¬ngyÃ²ng zhÅng, tÄ zÃ i OSWorld-G shÃ ng dÃ©fÄ“n 56.1, shÃ¨nzhÃ¬ chÄoguÃ²le zhuÄnmÃ©n mÃ³xÃ­ng UI-TARS. WÇ’men de xÃ¹nliÃ n jiÃ©hÃ©le sÃ¬ jiÄ“duÃ n yÃ¹xÃ¹nliÃ n hÃ© hÃ¹nhÃ© zÃ i cÃ¨lÃ¼Ã¨ qiÃ¡ngxuÃ©xuÃ©. WÇ’men hÃ¡i tÃ­gÅngle yÄ«gÃ¨ bÄohÃ¡n 50 duÅ gÃ¨ rÃ¨nwÃ¹ de quÃ¡nmiÃ n pÃ­nggÇ” tÃ ojiÃ n, yÇ cÃ¹jÃ¬n kÄ› chÃ³ngfÃ¹xÃ¬ng hÃ© tuÄ«dÃ²ng lÇngyÃ¹ fÄzhÇn. MÃ³xÃ­ng jiÇnchÃ¡ diÇn hÃ© wÃ¡nzhÄ›ng pÃ­nggÇ” tÃ ojiÃ n kÄ› zÃ i https://github.com/XiaomiMiMo/MiMo-VL huÃ²qÇ”.",
        "vocab": "[\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open source\"},\n    {\"word\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"è¡¨ç°å‡ºè‰²\", \"pinyin\": \"biÇo xiÃ n chÅ« sÃ¨\", \"trans\": \"perform excellently\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å‚æ•°é‡\", \"pinyin\": \"cÄn shÇ” liÃ ng\", \"trans\": \"parameter quantity\"},\n    {\"word\": \"GUI\", \"pinyin\": \"GUI\", \"trans\": \"Graphical User Interface\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"application\"},\n    {\"word\": \"ä¸“é—¨\", \"pinyin\": \"zhuÄn mÃ©n\", \"trans\": \"specialized\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"å¼ºåŒ–å­¦ä¹ \", \"pinyin\": \"qiÃ¡ng huÃ  xuÃ© xÃ­\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"å¥—ä»¶\", \"pinyin\": \"tÃ o jiÃ n\", \"trans\": \"suite\"},\n    {\"word\": \"å¯é‡å¤æ€§\", \"pinyin\": \"kÄ› chÃ³ng fÃ¹ xÃ¬ng\", \"trans\": \"reproducibility\"},\n    {\"word\": \"æ¨åŠ¨\", \"pinyin\": \"tuÄ« dÃ²ng\", \"trans\": \"promote\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"æ£€æŸ¥ç‚¹\", \"pinyin\": \"jiÇn chÃ¡ diÇn\", \"trans\": \"checkpoint\"}\n]",
        "trans": "We have open-sourced two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 tasks and scores 59.4 on the OlympiadBench, surpassing models with up to 78B parameters. In GUI applications, it scores 56.1 on OSWorld-G, even outperforming the specialized model UI-TARS. Our training combines four-stage pretraining and hybrid on-policy reinforcement learning. We also provide a comprehensive evaluation suite with over 50 tasks to promote reproducibility and advance the field. Model checkpoints and the complete evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
        "update_ts": "2025-06-05 09:13"
    }
}