{
    "date": {
        "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 30",
        "zh": "4æœˆ30æ—¥"
    },
    "time_utc": "2025-04-30 04:13",
    "weekday": 2,
    "issue_id": 3504,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.20734",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "url": "https://huggingface.co/papers/2504.20734",
            "abstract": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.",
            "score": 23,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "53427aff6a4d7ed5",
            "authors": [
                "Woongyeong Yeo",
                "Kangsan Kim",
                "Soyeong Jeong",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20734.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "UniversalRAG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "UniversalRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² RAG, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ğ¼Ğ¸, UniversalRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 8 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniversalRAG Ğ½Ğ°Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval",
                    "desc": "The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€çŸ¥è¯†æ•´åˆçš„å…¨æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGï¼Œæ—¨åœ¨ä»å¤šç§å¼‚æ„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•é€šå¸¸ä»…é™äºæ–‡æœ¬æ•°æ®ï¼Œè€ŒUniversalRAGèƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œè§†é¢‘ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¨¡æ€æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“ï¼Œä»è€Œè¿›è¡Œé’ˆå¯¹æ€§çš„æ£€ç´¢ã€‚æ­¤å¤–ï¼ŒUniversalRAGè¿˜å°†æ¯ç§æ¨¡æ€ç»„ç»‡ä¸ºå¤šä¸ªç²’åº¦çº§åˆ«ï¼Œä»¥ä¾¿æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´è¿›è¡Œç²¾ç»†åŒ–æ£€ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20595",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "url": "https://huggingface.co/papers/2504.20595",
            "abstract": "We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.",
            "score": 15,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "244cff2e64afeaa0",
            "authors": [
                "Rulin Shao",
                "Rui Qiao",
                "Varsha Kishore",
                "Niklas Muennighoff",
                "Xi Victoria Lin",
                "Daniela Rus",
                "Bryan Kian Hsiang Low",
                "Sewon Min",
                "Wen-tau Yih",
                "Pang Wei Koh",
                "Luke Zettlemoyer"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "FAIR at Meta",
                "Massachusetts Institute of Technology",
                "National University of Singapore",
                "Singapore-MIT Alliance for Research and Technology",
                "Stanford University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20595.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#rag",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReasonIR-8B: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "ReasonIR-8B - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BRIGHT. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ RAG, ReasonIR-8B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° MMLU Ğ¸ GPQA Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ baseline Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with ReasonIR-8B",
                    "desc": "ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications."
                },
                "zh": {
                    "title": "æ¨ç†ä»»åŠ¡çš„ä¸“å±æ£€ç´¢å™¨ï¼šReasonIR-8B",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ReasonIR-8Bï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºä¸€èˆ¬æ¨ç†ä»»åŠ¡è®­ç»ƒçš„æ£€ç´¢å™¨ã€‚ç°æœ‰çš„æ£€ç´¢å™¨åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç°æœ‰çš„è®­ç»ƒæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ä¸æ–‡æ¡£ç›´æ¥ç›¸å…³çš„çŸ­å°äº‹å®æŸ¥è¯¢ä¸Šã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§å’Œç›¸å…³æ€§çš„æŸ¥è¯¢ï¼Œä»¥åŠä¸€ä¸ªçœ‹ä¼¼ç›¸å…³ä½†å®é™…ä¸Šæ— ç”¨çš„å›°éš¾è´Ÿæ ·æœ¬ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®å’Œç°æœ‰å…¬å…±æ•°æ®çš„æ··åˆä¸Šè¿›è¡Œè®­ç»ƒï¼ŒReasonIR-8Båœ¨BRIGHTåŸºå‡†ä¸Šè¾¾åˆ°äº†29.9çš„nDCG@10ï¼ˆä¸ä½¿ç”¨é‡æ’åºå™¨ï¼‰å’Œ36.9çš„nDCG@10ï¼ˆä½¿ç”¨é‡æ’åºå™¨ï¼‰ï¼Œå¹¶åœ¨RAGä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†MMLUå’ŒGPQAçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20571",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "url": "https://huggingface.co/papers/2504.20571",
            "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "score": 12,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "5392cdfe5ab1de59",
            "authors": [
                "Yiping Wang",
                "Qing Yang",
                "Zhiyuan Zeng",
                "Liliang Ren",
                "Lucas Liu",
                "Baolin Peng",
                "Hao Cheng",
                "Xuehai He",
                "Kuan Wang",
                "Jianfeng Gao",
                "Weizhu Chen",
                "Shuohang Wang",
                "Simon Shaolei Du",
                "Yelong Shen"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Microsoft",
                "University of California, Santa Cruz",
                "University of Southern California",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20571.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#math",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ - Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑĞºĞ°Ñ‡Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-1.5B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ¶Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° 'Ğ³Ñ€Ğ¾ĞºĞ¸Ğ½Ğ³Ğ°'."
                },
                "en": {
                    "title": "Boosting Math Skills in LLMs with 1-Shot RLVR",
                    "desc": "This paper presents a method called 1-shot Reinforcement Learning with Verifiable Reward (RLVR) that significantly enhances the mathematical reasoning abilities of large language models (LLMs). By using just one training example, the authors demonstrate a remarkable increase in performance on the MATH500 benchmark, achieving a score of 73.6% compared to 36.0% before. The study also reveals that this approach leads to improvements across various models and algorithms, highlighting the importance of exploration in training. Additionally, the authors introduce the concept of post-saturation generalization, where performance continues to improve even after training accuracy levels off, suggesting new avenues for research in RLVR."
                },
                "zh": {
                    "title": "ä¸€ä¾‹å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡å±•ç¤ºäº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆ1-shot RLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å°†RLVRåº”ç”¨äºåŸºç¡€æ¨¡å‹Qwen2.5-Math-1.5Bï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªå•ä¸€ç¤ºä¾‹å¯ä»¥å°†æ¨¡å‹åœ¨MATH500ä¸Šçš„è¡¨ç°ä»36.0%æå‡è‡³73.6%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è§‚å¯Ÿåˆ°åœ¨1-shot RLVRè¿‡ç¨‹ä¸­å‡ºç°äº†ä¸€äº›æœ‰è¶£ç°è±¡ï¼Œå¦‚è·¨é¢†åŸŸæ³›åŒ–å’Œè‡ªæˆ‘åæ€é¢‘ç‡å¢åŠ ã€‚æˆ‘ä»¬éªŒè¯äº†1-shot RLVRçš„æœ‰æ•ˆæ€§ä¸»è¦æºäºç­–ç•¥æ¢¯åº¦æŸå¤±ï¼Œå¹¶å¼ºè°ƒäº†ä¿ƒè¿›æ¢ç´¢åœ¨è®­ç»ƒä¸­çš„å…³é”®ä½œç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20157",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "url": "https://huggingface.co/papers/2504.20157",
            "abstract": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.",
            "score": 7,
            "issue_id": 3504,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "18f16590c380c078",
            "authors": [
                "Zae Myung Kim",
                "Chanwoo Park",
                "Vipul Raheja",
                "Dongyeop Kang"
            ],
            "affiliations": [
                "Grammarly",
                "MIT",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20157.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rl",
                    "#open_source",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Meta Policy Optimization (MPO) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Reward Adjustment for Robust LLM Alignment",
                    "desc": "This paper presents Meta Policy Optimization (MPO), a new framework designed to improve reward-based alignment methods for large language models (LLMs). MPO tackles two main issues: the risk of reward hacking and the need for complex prompt engineering. By using a meta-reward model, MPO dynamically adjusts the reward prompts during training, ensuring that the reward signal remains effective and less exploitable. The results show that MPO achieves comparable or superior performance to traditional methods while simplifying the alignment process across various tasks."
                },
                "zh": {
                    "title": "å…ƒç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå…ƒç­–ç•¥ä¼˜åŒ–ï¼ˆMPOï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºäºå¥–åŠ±çš„å¯¹é½æ–¹æ³•ä¸­é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå¥–åŠ±é»‘å®¢å’Œè„†å¼±çš„æç¤ºå·¥ç¨‹ã€‚MPOé€šè¿‡å¼•å…¥ä¸€ä¸ªåŠ¨æ€è°ƒæ•´å¥–åŠ±æ¨¡å‹æç¤ºçš„å…ƒå¥–åŠ±æ¨¡å‹ï¼Œæ¥æé«˜å¯¹é½çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ç¯å¢ƒå˜åŒ–ï¼ŒæŒç»­ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå‡å°‘æ‰‹åŠ¨è®¾è®¡å¥–åŠ±æç¤ºçš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPOåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸éœ€è¦ä¸“é—¨çš„å¥–åŠ±è®¾è®¡ï¼Œå…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16046",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "url": "https://huggingface.co/papers/2504.16046",
            "abstract": "The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of \"copyright takedown\" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.",
            "score": 6,
            "issue_id": 3504,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "3a2630d279485a85",
            "authors": [
                "Jingyu Zhang",
                "Jiacan Yu",
                "Marc Marone",
                "Benjamin Van Durme",
                "Daniel Khashabi"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16046.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#ethics",
                    "#leakage"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "BloomScrub: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BloomScrub Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ° Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹. BloomScrub Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ‘Ğ»ÑƒĞ¼Ğ°) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ° Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "BloomScrub: Smart Copyright Protection for Language Models",
                    "desc": "This paper addresses the issue of copyright infringement by large language models (LLMs) that may unintentionally generate content similar to copyrighted material. It introduces BloomScrub, a novel method that detects and rewrites long quotes from copyrighted sources during the model's inference phase. By using Bloom filters for efficient quote detection, BloomScrub can effectively screen large datasets while maintaining the model's utility. The approach not only reduces the risk of copyright infringement but also allows for flexible enforcement levels through adaptive abstention when necessary."
                },
                "zh": {
                    "title": "BloomScrubï¼šæœ‰æ•ˆçš„ç‰ˆæƒä¿æŠ¤æ–¹æ³•",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ¥è§¦åˆ°ç‰ˆæƒææ–™æ‰€å¸¦æ¥çš„ç‰ˆæƒä¾µæƒé£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†\"ç‰ˆæƒæ’¤é”€\"æ–¹æ³•ï¼Œæ—¨åœ¨é˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸ç‰ˆæƒå†…å®¹ç›¸ä¼¼çš„æ–‡æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºBloomScrubçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶æ£€æµ‹å¼•ç”¨å¹¶è¿›è¡Œé‡å†™ï¼Œæ¥æœ‰æ•ˆåœ°å‡å°‘ç‰ˆæƒä¾µæƒé£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBloomScrubåœ¨é™ä½ä¾µæƒé£é™©çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å®ç”¨æ€§ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ‰§è¡Œä¸¥æ ¼æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20998",
            "title": "YoChameleon: Personalized Vision and Language Generation",
            "url": "https://huggingface.co/papers/2504.20998",
            "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting.",
            "score": 4,
            "issue_id": 3502,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "21fed074912b3e2f",
            "authors": [
                "Thao Nguyen",
                "Krishna Kumar Singh",
                "Jing Shi",
                "Trung Bui",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20998.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Yo'Chameleon - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ soft-prompt tuning Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3-5 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Yo'Chameleon Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ Ğ¸ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'soft-positive' Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Personalizing Multimodal Models with Yo'Chameleon",
                    "desc": "This paper presents Yo'Chameleon, a novel approach to personalize large multimodal models like GPT-4 for specific user concepts. By using 3-5 images of a subject, Yo'Chameleon applies soft-prompt tuning to incorporate personalized information, enabling the model to answer questions and generate contextually relevant images. The training involves a self-prompting optimization mechanism that ensures balanced performance across different modalities, as well as a 'soft-positive' image generation technique to improve image quality in few-shot scenarios. This work addresses the gap in adapting personalization methods for image generation within multimodal frameworks."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„åˆ›æ–°æ¢ç´¢",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Geminiã€Chameleonï¼‰å·²ç»å‘å±•æˆä¸ºå¼ºå¤§çš„å·¥å…·ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸‡ç”¨æˆ·ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶æ˜¯é€šç”¨çš„ï¼Œç¼ºä¹å¯¹ç‰¹å®šç”¨æˆ·æ¦‚å¿µçš„ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»äº†Yo'Chameleonï¼Œè¿™æ˜¯é¦–æ¬¡ç ”ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ªæ€§åŒ–çš„æ–¹æ³•ã€‚Yo'Chameleoné€šè¿‡è½¯æç¤ºè°ƒä¼˜ï¼Œå°†ç‰¹å®šä¸»é¢˜çš„ä¿¡æ¯åµŒå…¥æ¨¡å‹ï¼Œä»¥å›ç­”å…³äºè¯¥ä¸»é¢˜çš„é—®é¢˜å¹¶åœ¨æ–°ç¯å¢ƒä¸­é‡å»ºå›¾åƒçš„åƒç´ çº§ç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20995",
            "title": "TesserAct: Learning 4D Embodied World Models",
            "url": "https://huggingface.co/papers/2504.20995",
            "abstract": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.",
            "score": 4,
            "issue_id": 3503,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "6339aba982c02561",
            "authors": [
                "Haoyu Zhen",
                "Qiao Sun",
                "Hongxin Zhang",
                "Junyan Li",
                "Siyuan Zhou",
                "Yilun Du",
                "Chuang Gan"
            ],
            "affiliations": [
                "HKUST",
                "Harvard University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.20995.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#video",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "4D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… 4D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ 4D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ RGB-DN (RGB, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸), Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ 2D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RGB-DN Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ 4D-ÑÑ†ĞµĞ½Ñƒ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Prediction for Embodied Agents",
                    "desc": "This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments."
                },
                "zh": {
                    "title": "å­¦ä¹ å››ç»´ä¸–ç•Œæ¨¡å‹ï¼Œæå‡å…·èº«æ™ºèƒ½çš„é¢„æµ‹èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ æ–°é¢–çš„å››ç»´å…·èº«ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä¸‰ç»´åœºæ™¯åœ¨å…·èº«æ™ºèƒ½ä½“åŠ¨ä½œä¸‹çš„åŠ¨æ€æ¼”å˜ï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒRGB-DNï¼ˆRGBã€æ·±åº¦å’Œæ³•çº¿ï¼‰è§†é¢‘æ¥å­¦ä¹ å››ç»´ä¸–ç•Œæ¨¡å‹ï¼Œè¿™ç§æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„äºŒç»´æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†è¯¦ç»†çš„å½¢çŠ¶ã€é…ç½®å’Œæ—¶é—´å˜åŒ–çº³å…¥é¢„æµ‹ä¸­ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ç°æˆæ¨¡å‹æ‰©å±•ç°æœ‰çš„æœºå™¨äººæ“ä½œè§†é¢‘æ•°æ®é›†ï¼ŒåŠ å…¥æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªæ ‡æ³¨æ•°æ®é›†ä¸Šå¾®è°ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œè”åˆé¢„æµ‹æ¯ä¸€å¸§çš„RGB-DNï¼Œæœ€ç»ˆå°†ç”Ÿæˆçš„RGBã€æ·±åº¦å’Œæ³•çº¿è§†é¢‘ç›´æ¥è½¬æ¢ä¸ºé«˜è´¨é‡çš„å››ç»´åœºæ™¯ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-29.html",
    "link_next": "2025-05-01.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "29.04",
        "en": "04/29",
        "zh": "4æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "01.05",
        "en": "05/01",
        "zh": "5æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆéæ‹‰ä¸å­—æ¯çš„ç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RepTextï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„æ¨¡å‹ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚RepText é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶é¢å¤–æ•´åˆäº†è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€è¦è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡ä½¿ç”¨æ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œæ‰©æ•£æŸå¤±ï¼Œä»¥åŠåœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨å™ªå£°å­—å½¢æ½œåœ¨åˆå§‹åŒ–å’ŒåŒºåŸŸæ©ç ï¼ŒRepText æ˜¾è‘—æé«˜äº†æ¸²æŸ“çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepText åœ¨å¼€æºæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä¸å°é—­æºçš„å¤šè¯­è¨€æ¨¡å‹ç›¸åª²ç¾ã€‚æ–‡ç« æœ€åè¿˜è®¨è®ºäº†RepTextçš„å±€é™æ€§ã€‚",
        "title": "RepText: Rendering Visual Text via Replicating",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆéæ‹‰ä¸å­—æ¯çš„ç²¾ç¡®å’Œçµæ´»çš„æ’ç‰ˆå…ƒç´ æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RepTextï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå‡†ç¡®æ¸²æŸ“å¤šè¯­è¨€è§†è§‰æ–‡æœ¬çš„æ¨¡å‹ï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£è¿™äº›æ–‡æœ¬ã€‚RepText é‡‡ç”¨äº†ControlNetçš„è®¾ç½®ï¼Œå¹¶é¢å¤–æ•´åˆäº†è¯­è¨€æ— å…³çš„å­—å½¢å’Œæ¸²æŸ“æ–‡æœ¬çš„ä½ç½®ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€è¦è‡ªå®šä¹‰æ–‡æœ¬å†…å®¹ã€å­—ä½“å’Œä½ç½®ã€‚é€šè¿‡ä½¿ç”¨æ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œæ‰©æ•£æŸå¤±ï¼Œä»¥åŠåœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨å™ªå£°å­—å½¢æ½œåœ¨åˆå§‹åŒ–å’ŒåŒºåŸŸæ©ç ï¼ŒRepText æ˜¾è‘—æé«˜äº†æ¸²æŸ“çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepText åœ¨å¼€æºæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä¸å°é—­æºçš„å¤šè¯­è¨€æ¨¡å‹ç›¸åª²ç¾ã€‚æ–‡ç« æœ€åè¿˜è®¨è®ºäº†RepTextçš„å±€é™æ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÄng qiÃ¡n wÃ©n bÄ›n dÃ o tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng de jÃº xiÃ n xÃ¬ng, tÃ¨ biÃ© shÃ¬ zÃ i shÄ“ng chÃ©ng fÄ“i lÄ dÄ«ng zÃ¬ mÇ” de jÄ«ng quÃ¨ hÃ© lÃ­ng huÃ³ de pÃ¡i bÇn yuÃ¡n sÇ” fÄng miÃ n. wÃ¨i le jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le RepText, zhÃ¨ shÃ¬ yÄ« zhÇ’ng nÃ©ng gÃ²u zhÇ”n quÃ¨ xuÃ n rÃ¡n duÅ yÇ” yÃ¡n shÃ¬ jÃ¹ wÃ©n bÄ›n de mÃ³ xÃ­ng, Ã©r wÃº xÅ« zhÄ“n zhÃ¨ng lÇ jiÄ› zhÃ¨ xiÄ“ wÃ©n bÄ›n. RepText cuÅ yÃ²ng le ControlNet de shÃ¨ zhÃ¬, bÃ¬ng Ã© wÃ i zhÄ›ng hÃ© le yÇ” yÃ¡n wÃº guÄn de zÃ¬ xÃ­ng hÃ© xuÃ n rÃ¡n wÃ©n bÄ›n de wÃ¨i zhÃ¬, shÇ yÃ²ng hÃ¹ nÃ©ng gÃ²u yÄ«n yÃ²ng zhÃ¨n dÄ«ng wÃ©n bÄ›n nÃ¨i rÃ³ng, zÃ¬ tÇ hÃ© wÃ¨i zhÃ¬. tÅng guÃ² shÇ yÃ²ng wÃ©n bÄ›n gÇn juÃ© sÇ”n shÄ« hÃ© kuÃ² sÃ n sÇ”n shÄ«, yÇ jiÄ shÇ yÃ²ng tuÄ« lÇ jiÄ“ duÃ n cuÃ² yÃ²ng zÃ o shÄ“ng zÃ¬ xÃ­ng qiÃ¡n zÃ i chÅ« shÇ huÃ  hÃ© qÅ« yÃ¹ mÃ³ zhÃ o, RepText xiÇn zhÃ¹ tÃ­ gÄo le xuÃ n rÃ¡n de zhÇ”n quÃ¨ xÃ¬ng hÃ© wÄ›n dÃ¬ng xÃ¬ng. shÃ­ yÃ n jiÃ© guÇ’ biÇo mÃ­ng, RepText zÃ i kÄi yuÃ¡n fÄng fÇ zhÅng biÇo xiÃ n chÅ« sÃ¨, bÃ¬ng qiÄ› yÇ” fÄ“ng bÃ¬ yuÃ¡n de duÅ yÇ” yÃ¡n mÃ³ xÃ­ng xiÄng jÃ¬ mÇ. wÃ©n zhÄng zuÃ¬ hÃ²u hÃ¡i tÇo lÃ¹n le RepText de jÃº xiÃ n xÃ¬ng.\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÄng qiÃ¡n wÃ©n bÄ›n dÃ o tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng de jÃº xiÃ n xÃ¬ng, tÃ¨ biÃ© shÃ¬ zÃ i shÄ“ng chÃ©ng fÄ“i lÄ dÄ«ng zÃ¬ mÇ” de jÄ«ng quÃ¨ hÃ© lÃ­ng huÃ³ de pÃ¡i bÇn yuÃ¡n sÇ” fÄng miÃ n. wÃ¨i le jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le RepText, zhÃ¨ shÃ¬ yÄ« zhÇ’ng nÃ©ng gÃ²u zhÇ”n quÃ¨ xuÃ n rÃ¡n duÅ yÇ” yÃ¡n shÃ¬ jÃ¹ wÃ©n bÄ›n de mÃ³ xÃ­ng, Ã©r wÃº xÅ« zhÄ“n zhÃ¨ng lÇ jiÄ› zhÃ¨ xiÄ“ wÃ©n bÄ›n. RepText cuÅ yÃ²ng le ControlNet de shÃ¨ zhÃ¬, bÃ¬ng Ã© wÃ i zhÄ›ng hÃ© le yÇ” yÃ¡n wÃº guÄn de zÃ¬ xÃ­ng hÃ© xuÃ n rÃ¡n wÃ©n bÄ›n de wÃ¨i zhÃ¬, shÇ yÃ²ng hÃ¹ nÃ©ng gÃ²u yÄ«n yÃ²ng zhÃ¨n dÄ«ng wÃ©n bÄ›n nÃ¨i rÃ³ng, zÃ¬ tÇ hÃ© wÃ¨i zhÃ¬. tÅng guÃ² shÇ yÃ²ng wÃ©n bÄ›n gÇn juÃ© sÇ”n shÄ« hÃ© kuÃ² sÃ n sÇ”n shÄ«, yÇ jiÄ shÇ yÃ²ng tuÄ« lÇ jiÄ“ duÃ n cuÃ² yÃ²ng zÃ o shÄ“ng zÃ¬ xÃ­ng qiÃ¡n zÃ i chÅ« shÇ huÃ  hÃ© qÅ« yÃ¹ mÃ³ zhÃ o, RepText xiÇn zhÃ¹ tÃ­ gÄo le xuÃ n rÃ¡n de zhÇ”n quÃ¨ xÃ¬ng hÃ© wÄ›n dÃ¬ng xÃ¬ng. shÃ­ yÃ n jiÃ© guÇ’ biÇo mÃ­ng, RepText zÃ i kÄi yuÃ¡n fÄng fÇ zhÅng biÇo xiÃ n chÅ« sÃ¨, bÃ¬ng qiÄ› yÇ” fÄ“ng bÃ¬ yuÃ¡n de duÅ yÇ” yÃ¡n mÃ³ xÃ­ng xiÄng jÃ¬ mÇ. wÃ©n zhÄng zuÃ¬ hÃ²u hÃ¡i tÇo lÃ¹n le RepText de jÃº xiÃ n xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å½“å‰\", \"pinyin\": \"dÄng qiÃ¡n\", \"trans\": \"current\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å±€é™æ€§\", \"pinyin\": \"jÃº xiÃ n xÃ¬ng\", \"trans\": \"limitations\"},\n    {\"word\": \"ç‰¹åˆ«\", \"pinyin\": \"tÃ¨ biÃ©\", \"trans\": \"especially\"},\n    {\"word\": \"éæ‹‰ä¸å­—æ¯\", \"pinyin\": \"fÄ“i lÄ dÄ«ng zÃ¬ mÇ”\", \"trans\": \"non-Latin characters\"},\n    {\"word\": \"ç²¾ç¡®\", \"pinyin\": \"jÄ«ng quÃ¨\", \"trans\": \"precise\"},\n    {\"word\": \"çµæ´»\", \"pinyin\": \"lÃ­ng huÃ³\", \"trans\": \"flexible\"},\n    {\"word\": \"æ’ç‰ˆ\", \"pinyin\": \"pÃ¡i bÇn\", \"trans\": \"typesetting\"},\n    {\"word\": \"å…ƒç´ \", \"pinyin\": \"yuÃ¡n sÃ¹\", \"trans\": \"elements\"},\n    {\"word\": \"æ–¹é¢\", \"pinyin\": \"fÄng miÃ n\", \"trans\": \"aspect\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"RepText\", \"pinyin\": \"RepText\", \"trans\": \"RepText\"},\n    {\"word\": \"å‡†ç¡®\", \"pinyin\": \"zhÇ”n quÃ¨\", \"trans\": \"accurate\"},\n    {\"word\": \"æ¸²æŸ“\", \"pinyin\": \"xuÃ n rÃ¡n\", \"trans\": \"render\"},\n    {\"word\": \"å¤šè¯­è¨€\", \"pinyin\": \"duÅ yÇ” yÃ¡n\", \"trans\": \"multilingual\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"æ— éœ€\", \"pinyin\": \"wÃº xÅ«\", \"trans\": \"without needing\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understand\"},\n    {\"word\": \"é‡‡ç”¨\", \"pinyin\": \"cÇi yÃ²ng\", \"trans\": \"adopt\"},\n    {\"word\": \"ControlNet\", \"pinyin\": \"ControlNet\", \"trans\": \"ControlNet\"},\n    {\"word\": \"è®¾ç½®\", \"pinyin\": \"shÃ¨ zhÃ¬\", \"trans\": \"setting\"},\n    {\"word\": \"é¢å¤–\", \"pinyin\": \"Ã© wÃ i\", \"trans\": \"additional\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›ng hÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"è¯­è¨€æ— å…³\", \"pinyin\": \"yÇ” yÃ¡n wÃº guÄn\", \"trans\": \"language-agnostic\"},\n    {\"word\": \"å­—å½¢\", \"pinyin\": \"zÃ¬ xÃ­ng\", \"trans\": \"glyph\"},\n    {\"word\": \"ä½ç½®\", \"pinyin\": \"wÃ¨i zhÃ¬\", \"trans\": \"position\"},\n    {\"word\": \"ä½¿ç”¨æˆ·\", \"pinyin\": \"shÇ yÃ²ng hÃ¹\", \"trans\": \"enable users\"},\n    {\"word\": \"æ ¹æ®\", \"pinyin\": \"gÄ“n jÃ¹\", \"trans\": \"according to\"},\n    {\"word\": \"éœ€è¦\", \"pinyin\": \"xÅ« yÃ o\", \"trans\": \"need\"},\n    {\"word\": \"è‡ªå®šä¹‰\", \"pinyin\": \"zÃ¬ dÃ¬ng yÃ¬\", \"trans\": \"customize\"},\n    {\"word\": \"å†…å®¹\", \"pinyin\": \"nÃ¨i rÃ³ng\", \"trans\": \"content\"},\n    {\"word\": \"å­—ä½“\", \"pinyin\": \"zÃ¬ tÇ\", \"trans\": \"font\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"ä½¿ç”¨\", \"pinyin\": \"shÇ yÃ²ng\", \"trans\": \"use\"},\n    {\"word\": \"æ–‡æœ¬æ„ŸçŸ¥\", \"pinyin\": \"wÃ©n bÄ›n gÇn zhÄ«\", \"trans\": \"text-aware\"},\n    {\"word\": \"æŸå¤±\", \"pinyin\": \"sÇ”n shÄ«\", \"trans\": \"loss\"},\n    {\"word\": \"æ‰©æ•£\", \"pinyin\": \"kuÃ² sÃ n\", \"trans\": \"diffusion\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"é˜¶æ®µ\", \"pinyin\": \"jiÄ“ duÃ n\", \"trans\": \"stage\"},\n    {\"word\": \"å™ªå£°\", \"pinyin\": \"zÃ o shÄ“ng\", \"trans\": \"noise\"},\n    {\"word\": \"æ½œåœ¨\", \"pinyin\": \"qiÃ¡n zÃ i\", \"trans\": \"latent\"},\n    {\"word\": \"åˆå§‹åŒ–\", \"pinyin\": \"chÅ« shÇ huÃ \", \"trans\": \"initialization\"},\n    {\"word\": \"åŒºåŸŸ\", \"pinyin\": \"qÅ« yÃ¹\", \"trans\": \"region\"},\n    {\"word\": \"æ©ç \", \"pinyin\": \"yÇn mÇ\", \"trans\": \"mask\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"ç¨³å®šæ€§\", \"pinyin\": \"wÄ›n dÃ¬ng xÃ¬ng\", \"trans\": \"stability\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"è¡¨æ˜\", \"pinyin\": \"biÇo mÃ­ng\", \"trans\": \"indicate\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"å°é—­æº\", \"pinyin\": \"fÄ“ng bÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"},\n    {\"word\": \"ç›¸åª²ç¾\", \"pinyin\": \"xiÄng pÃ¬ mÄ›i\", \"trans\": \"compare favorably\"},\n    {\"word\": \"æ–‡ç« \", \"pinyin\": \"wÃ©n zhÄng\", \"trans\": \"article\"},\n    {\"word\": \"æœ€å\", \"pinyin\": \"zuÃ¬ hÃ²u\", \"trans\": \"finally\"},\n    {\"word\": \"è¿˜\", \"pinyin\": \"hÃ¡i\", \"trans\": \"still\"}\n]",
        "trans": "This article discusses the limitations of current text-to-image generation models, particularly in generating precise and flexible typographic elements for non-Latin scripts. To address these issues, the authors propose RepText, a model capable of accurately rendering multilingual visual text without truly understanding the text. RepText adopts the setup of ControlNet and additionally integrates language-agnostic glyphs and the positioning of rendered text, allowing users to customize text content, fonts, and positions as needed. By utilizing text-aware loss and diffusion loss, along with noisy glyph latent initialization and regional masking during the inference stage, RepText significantly improves the accuracy and stability of rendering. Experimental results demonstrate that RepText performs excellently among open-source methods and is comparable to closed-source multilingual models. The article concludes with a discussion of RepText's limitations.",
        "update_ts": "2025-04-29 09:12"
    }
}