{
    "date": {
        "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 3",
        "zh": "9æœˆ3æ—¥"
    },
    "time_utc": "2025-09-03 20:12",
    "weekday": 2,
    "issue_id": 5701,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.02547",
            "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
            "url": "https://huggingface.co/papers/2509.02547",
            "abstract": "Agentic reinforcement learning transforms large language models into autonomous decision-making agents by leveraging temporally extended POMDPs, enhancing capabilities like planning and reasoning through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.",
            "score": 67,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "04a4d0adade32d34",
            "authors": [
                "Guibin Zhang",
                "Hejia Geng",
                "Xiaohang Yu",
                "Zhenfei Yin",
                "Zaibin Zhang",
                "Zelin Tan",
                "Heng Zhou",
                "Zhongzhi Li",
                "Xiangyuan Xue",
                "Yijiang Li",
                "Yifan Zhou",
                "Yang Chen",
                "Chen Zhang",
                "Yutao Fan",
                "Zihu Wang",
                "Songtao Huang",
                "Yue Liao",
                "Hongru Wang",
                "Mengyue Yang",
                "Heng Ji",
                "Michael Littman",
                "Jun Wang",
                "Shuicheng Yan",
                "Philip Torr",
                "Lei Bai"
            ],
            "affiliations": [
                "Brown University",
                "Chinese Academy of Sciences",
                "Dalian University of Technology",
                "Fudan University",
                "Imperial College London",
                "National University of Singapore",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University College London",
                "University of Bristol",
                "University of California, San Diego",
                "University of California, Santa Barbara",
                "University of Georgia",
                "University of Illinois Urbana-Champaign",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02547.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#agi",
                    "#survey",
                    "#rl",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (Agentic RL) Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (POMDP) Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Agentic RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Transforming Language Models into Autonomous Decision-Makers",
                    "desc": "This paper introduces agentic reinforcement learning (Agentic RL), which changes how large language models (LLMs) operate by allowing them to make autonomous decisions in complex environments. It contrasts traditional single-step Markov Decision Processes (MDPs) used in LLMs with more advanced temporally extended partially observable Markov decision processes (POMDPs) that enable better planning and reasoning. The authors propose a taxonomy that categorizes agentic capabilities like memory and self-improvement, as well as their applications in various tasks. Additionally, the paper provides a resource guide for researchers, summarizing over five hundred studies to outline the current state and future directions of this emerging field."
                },
                "zh": {
                    "title": "ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼šä»è¢«åŠ¨ç”Ÿæˆåˆ°è‡ªä¸»å†³ç­–çš„è½¬å˜",
                    "desc": "ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆAgentic RLï¼‰å°†å¤§å‹è¯­è¨€æ¨¡å‹è½¬å˜ä¸ºè‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨æ—¶é—´æ‰©å±•çš„éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰ï¼Œå¢å¼ºäº†è§„åˆ’å’Œæ¨ç†ç­‰èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å•æ­¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPsï¼‰ç›¸æ¯”ï¼Œä»£ç†å¼ºåŒ–å­¦ä¹ ä½¿å¾—è¯­è¨€æ¨¡å‹ä¸å†æ˜¯è¢«åŠ¨çš„åºåˆ—ç”Ÿæˆå™¨ï¼Œè€Œæ˜¯èƒ½å¤Ÿåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­è‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„åˆ†ç±»æ³•ï¼Œå›´ç»•æ ¸å¿ƒçš„ä»£ç†èƒ½åŠ›ï¼Œå¦‚è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€è®°å¿†ã€æ¨ç†ã€è‡ªæˆ‘æ”¹è¿›å’Œæ„ŸçŸ¥è¿›è¡Œç»„ç»‡ã€‚é€šè¿‡æ•´åˆå¼€æºç¯å¢ƒã€åŸºå‡†å’Œæ¡†æ¶ï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®ç”¨çš„å‚è€ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02544",
            "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.02544",
            "abstract": "UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.",
            "score": 62,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "71474173af3c991b",
            "authors": [
                "Haoming Wang",
                "Haoyang Zou",
                "Huatong Song",
                "Jiazhan Feng",
                "Junjie Fang",
                "Junting Lu",
                "Longxiang Liu",
                "Qinyu Luo",
                "Shihao Liang",
                "Shijue Huang",
                "Wanjun Zhong",
                "Yining Ye",
                "Yujia Qin",
                "Yuwen Xiong",
                "Yuxin Song",
                "Zhiyong Wu",
                "Bo Li",
                "Chen Dun",
                "Chong Liu",
                "Fuxing Leng",
                "Hanbin Wang",
                "Hao Yu",
                "Haobin Chen",
                "Hongyi Guo",
                "Jing Su",
                "Jingjia Huang",
                "Kai Shen",
                "Kaiyu Shi",
                "Lin Yan",
                "Peiyao Zhao",
                "Pengfei Liu",
                "Qinghao Ye",
                "Renjie Zheng",
                "Wayne Xin Zhao",
                "Wen Heng",
                "Wenhao Huang",
                "Wenqian Wang",
                "Xiaobo Qin",
                "Yi Lin",
                "Youbin Wu",
                "Zehui Chen",
                "Zihao Wang",
                "Baoquan Zhong",
                "Xinchun Zhang",
                "Xujing Li",
                "Yuanfan Li",
                "Zhongkai Zhao",
                "Chengquan Jiang",
                "Faming Wu",
                "Haotian Zhou",
                "Jinlin Pang",
                "Li Han",
                "Qianli Ma",
                "Siyao Liu",
                "Songhua Cai",
                "Wenqi Fu",
                "Xin Liu",
                "Zhi Zhang",
                "Bo Zhou",
                "Guoliang Li",
                "Jiajun Shi",
                "Jiale Yang",
                "Jie Tang",
                "Li Li",
                "Taoran Lu",
                "Woyu Lin",
                "Xiaokang Tong",
                "Xinyao Li",
                "Yichi Zhang",
                "Yu Miao",
                "Zhengxuan Jiang",
                "Zili Li",
                "Ziyuan Zhao",
                "Chenxin Li",
                "Dehua Ma",
                "Feng Lin",
                "Ge Zhang",
                "Haihua Yang",
                "Hangyu Guo",
                "Hongda Zhu",
                "Jiaheng Liu",
                "Junda Du",
                "Kai Cai",
                "Kuanye Li",
                "Lichen Yuan",
                "Meilan Han",
                "Minchao Wang",
                "Shuyue Guo",
                "Tianhao Cheng",
                "Xiaobo Ma",
                "Xiaojun Xiao",
                "Xiaolong Huang",
                "Xinjie Chen",
                "Yidi Du",
                "Yilin Chen",
                "Yiwen Wang",
                "Zhaojian Li",
                "Zhenzhu Yang",
                "Zhiyuan Zeng",
                "Chaolin Jin",
                "Chen Li",
                "Hao Chen",
                "Haoli Chen",
                "Jian Chen",
                "Qinghao Zhao",
                "Guang Shi"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02544.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "UI-TARS-2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "UI-TARS-2 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¼Ğ°Ñ…Ğ¾Ğ²Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ GUI-ÑÑ€ĞµĞ´Ñƒ. UI-TARS-2 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² GUI-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» UI-TARS-2 Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with UI-TARS-2",
                    "desc": "UI-TARS-2 is a new model designed for creating intelligent agents that can interact with graphical user interfaces (GUIs). It tackles key issues like handling large amounts of data, improving learning over multiple interactions, and ensuring stable performance in different environments. The model uses innovative techniques such as a data flywheel for efficient data generation and a hybrid environment that combines various systems for better training. Its performance on various benchmarks shows significant improvements over previous models, indicating its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "UI-TARS-2ï¼šæå‡å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä½“çš„æœªæ¥",
                    "desc": "UI-TARS-2æ˜¯ä¸€ä¸ªä»¥å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°æ®å¯æ‰©å±•æ€§ã€å¤šè½®å¼ºåŒ–å­¦ä¹ å’Œç¯å¢ƒç¨³å®šæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ç³»ç»ŸåŒ–çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆã€ç¨³å®šçš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œé›†æˆæ–‡ä»¶ç³»ç»Ÿä¸ç»ˆç«¯çš„æ··åˆGUIç¯å¢ƒï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒUI-TARS-2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶å‰èº«UI-TARS-1.5å’Œå…¶ä»–å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨é•¿æ—¶é—´ä¿¡æ¯æ£€ç´¢ä»»åŠ¡å’Œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–æ™ºèƒ½ä½“ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02479",
            "title": "SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn\n  Tool-Integrated Reasoning",
            "url": "https://huggingface.co/papers/2509.02479",
            "abstract": "SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.",
            "score": 61,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "e6fb78d3f5363c7d",
            "authors": [
                "Zhenghai Xue",
                "Longtao Zheng",
                "Qian Liu",
                "Yingru Li",
                "Xiaosen Zheng",
                "Zejun Ma",
                "Bo An"
            ],
            "affiliations": [
                "Nanyang Technological University, Singapore",
                "TikTok, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02479.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ SimpleTIR, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Tool-Integrated Reasoning, TIR). ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SimpleTIR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ."
                },
                "en": {
                    "title": "Stabilizing Multi-Turn Reasoning with SimpleTIR",
                    "desc": "The paper presents SimpleTIR, an innovative algorithm designed to enhance the stability of multi-turn Tool-Integrated Reasoning (TIR) training in large language models. It addresses the issue of training instability caused by distributional drift from external tool feedback, which leads to the generation of low-probability tokens and catastrophic gradient explosions. By filtering out void turnsâ€”those that do not produce useful outputsâ€”SimpleTIR prevents harmful gradients from affecting the learning process. As a result, it achieves state-of-the-art performance on math reasoning tasks, significantly improving scores while promoting the discovery of advanced reasoning strategies."
                },
                "zh": {
                    "title": "SimpleTIRï¼šç¨³å®šå¤šè½®æ¨ç†è®­ç»ƒçš„åˆ›æ–°ç®—æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSimpleTIRçš„ç®—æ³•ï¼Œå®ƒé€šè¿‡è¿‡æ»¤æ‰æ— æ•ˆå›åˆæ¥ç¨³å®šå¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰è®­ç»ƒã€‚å¤šè½®TIRåœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ—¶å¸¸å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½å´©æºƒçš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºå¤–éƒ¨å·¥å…·åé¦ˆå¯¼è‡´çš„åˆ†å¸ƒæ¼‚ç§»ã€‚SimpleTIRçš„æ ¸å¿ƒç­–ç•¥æ˜¯è¯†åˆ«å¹¶å»é™¤é‚£äº›æ—¢æ²¡æœ‰ä»£ç å—ä¹Ÿæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆçš„å›åˆï¼Œä»è€Œæœ‰æ•ˆé˜»æ­¢æœ‰å®³çš„é«˜å¹…åº¦æ¢¯åº¦ï¼Œç¨³å®šå­¦ä¹ åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimpleTIRåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00676",
            "title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
            "url": "https://huggingface.co/papers/2509.00676",
            "abstract": "Reinforcement learning on preference-labeled critic datasets enhances a generative model's performance, creating a unified multimodal system that excels in both evaluation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.",
            "score": 56,
            "issue_id": 5686,
            "pub_date": "2025-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "804da0110302d00c",
            "authors": [
                "Xiyao Wang",
                "Chunyuan Li",
                "Jianwei Yang",
                "Kai Zhang",
                "Bo Liu",
                "Tianyi Xiong",
                "Furong Huang"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Ohio State University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00676.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#rlhf",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaVA-Critic-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 26 Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ğº Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Bridging Evaluation and Generation in Multimodal Systems",
                    "desc": "This paper introduces a novel approach to improve generative models by applying reinforcement learning (RL) on preference-labeled critic datasets. Traditionally, critic models evaluate outputs but do not generate responses, creating a divide between evaluation and generation tasks. The authors propose a unified model, LLaVA-Critic-R1, which not only serves as a high-performing critic but also functions effectively as a policy model for generating responses. Their findings demonstrate that this integrated approach enhances performance across various visual reasoning benchmarks, showcasing the potential for self-improving multimodal systems."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡ç”Ÿæˆæ¨¡å‹çš„ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†å¸¦æœ‰åå¥½çš„è¯„è®ºæ•°æ®é›†ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æå‡ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡æ–°ç»„ç»‡è¿™äº›è¯„è®ºæ•°æ®ï¼Œç›´æ¥åœ¨åŸºç¡€ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¼€å‘å‡ºLLaVA-Critic-R1ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿä¼˜åŒ–åå¥½åˆ¤æ–­çš„å¤šæ¨¡æ€è¯„è®ºæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-Critic-R1ä¸ä»…åœ¨è¯„è®ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿˜åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¸ä¸“é—¨çš„æ¨ç†æ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šå®ƒä»¬ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è¯„è®ºæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯ä»¥åˆ›å»ºä¸€ä¸ªåœ¨è¯„ä¼°å’Œç”Ÿæˆæ–¹é¢éƒ½è¡¨ç°å‡ºè‰²çš„ç»Ÿä¸€æ¨¡å‹ï¼Œæ¨åŠ¨å¤šæ¨¡æ€ç³»ç»Ÿçš„è‡ªæˆ‘æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21496",
            "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding",
            "url": "https://huggingface.co/papers/2508.21496",
            "abstract": "A benchmark for long-video hallucination identifies and investigates Semantic Aggregation Hallucination (SAH), showing its prevalence in complex and rapidly changing semantic contexts, and proposes strategies to mitigate it.  \t\t\t\t\tAI-generated summary \t\t\t\t Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.",
            "score": 48,
            "issue_id": 5691,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "ca0b1db27fcb23d9",
            "authors": [
                "Hao Lu",
                "Jiahao Wang",
                "Yaolun Zhang",
                "Ruohui Wang",
                "Xuanyu Zheng",
                "Yepeng Tang",
                "Dahua Lin",
                "Lewei Lu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21496.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#hallucinations",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ELV-Halluc Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ (Ğ¡ĞĞ“), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¡ĞĞ“ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑĞ¼ĞµĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¡ĞĞ“, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° DPO."
                },
                "en": {
                    "title": "Tackling Semantic Aggregation Hallucination in Long Videos",
                    "desc": "This paper introduces a new benchmark called ELV-Halluc, specifically designed to study Semantic Aggregation Hallucination (SAH) in long videos. SAH occurs when models incorrectly aggregate frame-level semantics into event-level groups, particularly in complex and rapidly changing contexts. The research shows that SAH is more prevalent in longer videos due to increased semantic complexity and provides strategies to mitigate this issue. By implementing a positional encoding strategy and a DPO approach, the authors demonstrate a significant reduction in SAH, improving the model's performance on video understanding tasks."
                },
                "zh": {
                    "title": "æ­ç¤ºé•¿è§†é¢‘ä¸­çš„è¯­ä¹‰èšåˆå¹»è§‰",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹é•¿è§†é¢‘å¹»è§‰çš„æ–°åŸºå‡†ï¼Œé‡ç‚¹ç ”ç©¶äº†è¯­ä¹‰èšåˆå¹»è§‰ï¼ˆSAHï¼‰ã€‚SAHåœ¨å¤æ‚å’Œå¿«é€Ÿå˜åŒ–çš„è¯­ä¹‰ç¯å¢ƒä¸­å°¤ä¸ºæ™®éï¼Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆä¸è§†é¢‘è¾“å…¥ä¸ä¸€è‡´çš„å†…å®¹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSAHçš„å‘ç”Ÿä¸è¯­ä¹‰å¤æ‚æ€§å¢åŠ æœ‰å…³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªäº‹ä»¶ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ELV-HallucåŸºå‡†ï¼Œå¹¶æ¢è®¨äº†ç¼“è§£SAHçš„ç­–ç•¥ï¼Œå¦‚ä½ç½®ç¼–ç å’ŒDPOç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01055",
            "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
            "url": "https://huggingface.co/papers/2509.01055",
            "abstract": "VerlTool is a unified and modular framework for Agentic Reinforcement Learning with Tool use, addressing inefficiencies in existing approaches and providing competitive performance across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2times speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
            "score": 44,
            "issue_id": 5687,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "8d89f7851ae1d950",
            "authors": [
                "Dongfu Jiang",
                "Yi Lu",
                "Zhuofeng Li",
                "Zhiheng Lyu",
                "Ping Nie",
                "Haozhe Wang",
                "Alex Su",
                "Hui Chen",
                "Kai Zou",
                "Chao Du",
                "Tianyu Pang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "Independent",
                "National University of Singapore",
                "NetMind.AI",
                "Sea AI Lab",
                "Shanghai University",
                "University of Toronto",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01055.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#rl",
                    "#open_source",
                    "#agi",
                    "#agents",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "VerlTool: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "VerlTool - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. VerlTool Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ VeRL, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ API, Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° 6 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… ARLT. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ARLT ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RLVR."
                },
                "en": {
                    "title": "Streamlining Agentic Reinforcement Learning with VerlTool",
                    "desc": "VerlTool is a new framework designed for Agentic Reinforcement Learning that incorporates tool use, aiming to improve efficiency and performance in various tasks. It addresses the limitations of existing methods by providing a unified system that supports multiple modalities and asynchronous execution, which speeds up processing. The framework allows for easy integration of tools through a modular architecture, making it simpler for researchers to develop and test new ideas. By demonstrating strong performance across different domains, VerlTool encourages broader adoption and innovation in the field of reinforcement learning with tools."
                },
                "zh": {
                    "title": "VerlToolï¼šæå‡ä»£ç†å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "VerlToolæ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œä¸“æ³¨äºå…·æœ‰å·¥å…·ä½¿ç”¨çš„ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ä½æ•ˆé—®é¢˜ã€‚å®ƒé€šè¿‡ç³»ç»Ÿè®¾è®¡åŸåˆ™ï¼Œæä¾›äº†å››ä¸ªå…³é”®è´¡çŒ®ï¼ŒåŒ…æ‹¬ä¸å¯éªŒè¯å¥–åŠ±çš„ä¸Šæ¸¸å¯¹é½ã€ç»Ÿä¸€çš„å·¥å…·ç®¡ç†ã€å¼‚æ­¥æ‰§è¡Œä»¥æé«˜é€Ÿåº¦ï¼Œä»¥åŠåœ¨å¤šä¸ªé¢†åŸŸçš„ç«äº‰æ€§è¡¨ç°è¯„ä¼°ã€‚è¯¥æ¡†æ¶å°†ä»£ç†å¼ºåŒ–å­¦ä¹ å½¢å¼åŒ–ä¸ºå¤šè½®è½¨è¿¹ï¼Œæ”¯æŒå¤šæ¨¡æ€è§‚å¯Ÿä»¤ç‰Œï¼Œè¶…è¶Šäº†å•è½®äº¤äº’çš„é™åˆ¶ã€‚VerlToolçš„æ¨¡å—åŒ–æ’ä»¶æ¶æ„ä½¿å¾—å·¥å…·é›†æˆå˜å¾—å¿«é€Ÿä¸”ç®€å•ï¼Œæ˜¾è‘—é™ä½äº†å¼€å‘æˆæœ¬ï¼Œä¸ºå·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01215",
            "title": "POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models\n  for Document Conversion",
            "url": "https://huggingface.co/papers/2509.01215",
            "abstract": "A framework for constructing high-quality document extraction datasets and models through synthetic data generation and iterative self-improvement outperforms existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at https://github.com/Tencent/POINTS-Reader.",
            "score": 37,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "1f731f4067d86ef7",
            "authors": [
                "Yuan Liu",
                "Zhongyin Zhao",
                "Le Tian",
                "Haicheng Wang",
                "Xubing Ye",
                "Yangxiu You",
                "Zilin Yu",
                "Chuhan Wu",
                "Xiao Zhou",
                "Yang Yu",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01215.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ POINTS-Reader Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Document Extraction with Synthetic Data and Self-Improvement",
                    "desc": "This paper presents a new framework for creating high-quality datasets and models for document extraction using synthetic data generation and iterative self-improvement. It addresses the challenges of manual data labeling and the limitations of existing models in handling complex document formats. The proposed method generates diverse synthetic data to train an initial model, which is then refined through a self-improvement process that involves annotating real documents and verifying the quality of these annotations. The resulting model, POINTS-Reader, demonstrates superior performance compared to existing models, making it a valuable tool for document conversion tasks."
                },
                "zh": {
                    "title": "åˆæˆæ•°æ®é©±åŠ¨çš„æ–‡æ¡£æå–æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ„å»ºé«˜è´¨é‡æ–‡æ¡£æå–æ•°æ®é›†å’Œæ¨¡å‹çš„æ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆæ•°æ®ç”Ÿæˆå’Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆå¤§è§„æ¨¡å¤šæ ·çš„åˆæˆæ•°æ®ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿä»¥ç»Ÿä¸€æ ¼å¼æå–å…³é”®ä¿¡æ¯ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ï¼Œå°†åˆæ­¥è®­ç»ƒçš„æ¨¡å‹é€‚åº”çœŸå®æ–‡æ¡£ã€‚é€šè¿‡å¯¹çœŸå®æ–‡æ¡£è¿›è¡Œæ ‡æ³¨ã€è´¨é‡éªŒè¯å’Œæ¨¡å‹é‡è®­ç»ƒï¼Œé€æ­¥æå‡æ¨¡å‹çš„è½¬æ¢èƒ½åŠ›å’Œç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œè®­ç»ƒå‡ºçš„POINTS-Readeræ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å…¬å…±å’Œä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02208",
            "title": "Baichuan-M2: Scaling Medical Capability with Large Verifier System",
            "url": "https://huggingface.co/papers/2509.02208",
            "abstract": "A dynamic verification framework using reinforcement learning and a novel algorithm improves the performance of a large medical language model in real-world clinical decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.",
            "score": 25,
            "issue_id": 5687,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "f78da0ee0b5088cb",
            "authors": [
                "Baichuan-M2 Team",
                ":",
                "Chengfeng Dou",
                "Chong Liu",
                "Fan Yang",
                "Fei Li",
                "Jiyuan Jia",
                "Mingyang Chen",
                "Qiang Ju",
                "Shuai Wang",
                "Shunya Dang",
                "Tianpeng Li",
                "Xiangrong Zeng",
                "Yijie Zhou",
                "Chenzheng Zhu",
                "Da Pan",
                "Fei Deng",
                "Guangwei Ai",
                "Guosheng Dong",
                "Hongda Zhang",
                "Jinyang Tai",
                "Jixiang Hong",
                "Kai Lu",
                "Linzhuang Sun",
                "Peidong Guo",
                "Qian Ma",
                "Rihui Xin",
                "Shihui Yang",
                "Shusen Zhang",
                "Yichuan Mo",
                "Zheng Liang",
                "Zhishou Zhang",
                "Hengfu Cui",
                "Zuyi Zhu",
                "Xiaochuan Wang"
            ],
            "affiliations": [
                "Baichuan-M2 Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02208.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#agents",
                    "#alignment",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Baichuan-M2 Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Baichuan-M2 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğµ HealthBench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Clinical Decision-Making with Dynamic Verification in AI",
                    "desc": "This paper presents a dynamic verification framework that enhances the performance of large medical language models (LLMs) in real-world clinical settings using reinforcement learning. The framework addresses the limitations of traditional static benchmarks by incorporating a Patient Simulator and a Clinical Rubrics Generator, which create realistic clinical scenarios and dynamic evaluation metrics. The authors introduce Baichuan-M2, a 32B-parameter model trained with an advanced Group Relative Policy Optimization algorithm, which significantly outperforms existing models on the HealthBench benchmark. This work highlights the importance of dynamic verification systems in aligning LLM capabilities with practical healthcare applications, setting a new standard for medical AI performance."
                },
                "zh": {
                    "title": "åŠ¨æ€éªŒè¯æå‡åŒ»ç–—AIå†³ç­–èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€éªŒè¯æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ–°ç®—æ³•æå‡å¤§å‹åŒ»ç–—è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸´åºŠå†³ç­–ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„é™æ€åŸºå‡†æµ‹è¯•æ— æ³•æœ‰æ•ˆåæ˜ åŒ»ç–—å’¨è¯¢çš„åŠ¨æ€äº’åŠ¨ç‰¹æ€§ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒ…å«æ‚£è€…æ¨¡æ‹Ÿå™¨å’Œä¸´åºŠè¯„åˆ†ç”Ÿæˆå™¨çš„ç³»ç»Ÿã€‚é€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥å’Œæ”¹è¿›çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬å¼€å‘äº†Baichuan-M2æ¨¡å‹ï¼Œå¹¶åœ¨HealthBenchä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„åŠ¨æ€éªŒè¯ç³»ç»Ÿå¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¸å®é™…ä¸´åºŠåº”ç”¨å¯¹é½è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01563",
            "title": "Kwai Keye-VL 1.5 Technical Report",
            "url": "https://huggingface.co/papers/2509.01563",
            "abstract": "Keye-VL-1.5 enhances video understanding through a Slow-Fast encoding strategy, progressive pre-training, and post-training reasoning improvements, outperforming existing models on video tasks while maintaining general multimodal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.",
            "score": 23,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "b033203f893abafc",
            "authors": [
                "Biao Yang",
                "Bin Wen",
                "Boyang Ding",
                "Changyi Liu",
                "Chenglong Chu",
                "Chengru Song",
                "Chongling Rao",
                "Chuan Yi",
                "Da Li",
                "Dunju Zang",
                "Fan Yang",
                "Guorui Zhou",
                "Guowang Zhang",
                "Han Shen",
                "Hao Peng",
                "Haojie Ding",
                "Hao Wang",
                "Hengrui Ju",
                "Jiaming Huang",
                "Jiangxia Cao",
                "Jiankang Chen",
                "Jingyun Hua",
                "Kaibing Chen",
                "Kaiyu Jiang",
                "Kaiyu Tang",
                "Kun Gai",
                "Muhao Wei",
                "Qiang Wang",
                "Ruitao Wang",
                "Sen Na",
                "Shengnan Zhang",
                "Siyang Mao",
                "Sui Huang",
                "Tianke Zhang",
                "Tingting Gao",
                "Wei Chen",
                "Wei Yuan",
                "Xiangyu Wu",
                "Xiao Hu",
                "Xingyu Lu",
                "Yi-Fan Zhang",
                "Yiping Yang",
                "Yulong Chen",
                "Zeyi Lu",
                "Zhenhua Wu",
                "Zhixin Ling",
                "Zhuoran Yang",
                "Ziming Li",
                "Di Xu",
                "Haixuan Gao",
                "Hang Li",
                "Jing Wang",
                "Lejian Ren",
                "Qigen Hu",
                "Qianqian Wang",
                "Shiyao Wang",
                "Xinchen Luo",
                "Yan Li",
                "Yuhang Hu",
                "Zixing Zhang"
            ],
            "affiliations": [
                "Kuaishou Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01563.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#video",
                    "#long_context",
                    "#rl",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Keye-VL-1.5 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Keye-VL-1.5 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Slow-Fast, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ - Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ¾ÑĞ»ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Keye-VL-1.5",
                    "desc": "Keye-VL-1.5 is a new model designed to improve video understanding by using a Slow-Fast encoding strategy that optimizes how videos are processed. This model employs a progressive pre-training approach that allows it to handle longer videos and more complex visual information effectively. Additionally, it includes a post-training pipeline that enhances reasoning capabilities and aligns with human preferences through advanced training techniques. Overall, Keye-VL-1.5 outperforms existing models in video tasks while still performing well in general multimodal applications."
                },
                "zh": {
                    "title": "Keye-VL-1.5ï¼šè§†é¢‘ç†è§£çš„æ–°çªç ´",
                    "desc": "Keye-VL-1.5é€šè¿‡æ…¢-å¿«ç¼–ç ç­–ç•¥ã€æ¸è¿›å¼é¢„è®­ç»ƒå’Œåè®­ç»ƒæ¨ç†æ”¹è¿›ï¼Œæå‡äº†è§†é¢‘ç†è§£èƒ½åŠ›ã€‚æ…¢-å¿«ç¼–ç ç­–ç•¥æ ¹æ®å¸§é—´ç›¸ä¼¼æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå¤„ç†å…³é”®å¸§æ—¶ä½¿ç”¨é«˜åˆ†è¾¨ç‡ï¼Œè€Œå¯¹é™æ€å¸§åˆ™ä½¿ç”¨ä½åˆ†è¾¨ç‡ä»¥å¢åŠ æ—¶é—´è¦†ç›–ã€‚æ¸è¿›å¼é¢„è®­ç»ƒæ–¹æ³•å°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä»8Kæ‰©å±•åˆ°128Kï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘å’Œæ›´å¤æ‚çš„è§†è§‰å†…å®¹ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒKeye-VL-1.5åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01363",
            "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task\n  Arithmetic",
            "url": "https://huggingface.co/papers/2509.01363",
            "abstract": "Reasoning capabilities from reinforcement learning can be extracted as a task vector and transferred to other models to improve performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: v_{reason} = theta_{GRPO} - theta_{SFT}. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.",
            "score": 20,
            "issue_id": 5688,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "1110b5cc006571ff",
            "authors": [
                "Mohammad Zbeeb",
                "Hasan Abed Al Kader Hammoud",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "American University of Beirut (AUB)",
                "King Abdullah University of Science and Technology (KAUST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01363.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#rl",
                    "#open_source",
                    "#transfer_learning",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transfer Reasoning Skills with Task Vectors!",
                    "desc": "This paper explores how reasoning skills learned through reinforcement learning can be extracted and reused in other models. The authors create a task vector that represents the reasoning capabilities gained from reinforcement learning, allowing it to be transferred to different models. By applying this vector to instruction-tuned models, they achieve significant performance improvements on various reasoning tasks. This approach demonstrates a cost-effective method to enhance model performance by leveraging previously acquired knowledge without the need for extensive retraining."
                },
                "zh": {
                    "title": "æå–æ¨ç†èƒ½åŠ›ï¼Œæå‡æ¨¡å‹è¡¨ç°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»å¼ºåŒ–å­¦ä¹ ä¸­æå–æ¨ç†èƒ½åŠ›ï¼Œå¹¶å°†å…¶ä½œä¸ºä»»åŠ¡å‘é‡è½¬ç§»åˆ°å…¶ä»–æ¨¡å‹ä¸­ï¼Œä»¥æé«˜åœ¨ä¸åŒåŸºå‡†ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç›¸åŒåˆå§‹åŒ–çš„Qwen2.5æ¨¡å‹ï¼Œä¸€ä¸ªç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¦ä¸€ä¸ªç»è¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚é€šè¿‡è®¡ç®—è¿™ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°å·®å¼‚ï¼Œæˆ‘ä»¬æå–äº†ä¸€ä¸ªæ¨ç†å‘é‡ï¼Œè¯¥å‘é‡èƒ½å¤Ÿæ•æ‰åˆ°å¼ºåŒ–å­¦ä¹ æ‰€å¸¦æ¥çš„æ¨ç†èƒ½åŠ›ã€‚å°†è¿™ä¸ªå‘é‡æ·»åŠ åˆ°å…¼å®¹çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02534",
            "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations",
            "url": "https://huggingface.co/papers/2509.02534",
            "abstract": "DARLING, a diversity-aware reinforcement learning framework, enhances both the quality and diversity of large language model outputs across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.",
            "score": 18,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "b3e51a0003bb3957",
            "authors": [
                "Tianjian Li",
                "Yiming Zhang",
                "Ping Yu",
                "Swarnadeep Saha",
                "Daniel Khashabi",
                "Jason Weston",
                "Jack Lanchantin",
                "Tianlu Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "FAIR",
                "Johns Hopkins University",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02534.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#story_generation",
                    "#rlhf",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "DARLING: ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ¸",
                    "desc": "DARLING - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. DARLING Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DARLING Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RL, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹."
                },
                "en": {
                    "title": "Enhancing Creativity with Diversity-Aware Reinforcement Learning",
                    "desc": "DARLING is a framework designed to improve the outputs of large language models by focusing on both quality and diversity. Traditional post-training methods often enhance accuracy but limit the variety of responses, which is crucial for creative tasks. DARLING addresses this by using a learned partition function to assess diversity, allowing the model to generate responses that are not only accurate but also unique. Experiments show that DARLING outperforms standard reinforcement learning approaches, leading to better quality and more diverse outputs across various tasks."
                },
                "zh": {
                    "title": "DARLINGï¼šæå‡è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è´¨é‡ä¸å¤šæ ·æ€§",
                    "desc": "DARLINGæ˜¯ä¸€ä¸ªå…³æ³¨å¤šæ ·æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¾“å‡ºè´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¼ ç»Ÿçš„åè®­ç»ƒæ–¹æ³•å¾€å¾€åªå…³æ³¨å‡†ç¡®æ€§å’Œå®ç”¨æ€§ï¼Œå¯¼è‡´è¾“å‡ºçš„å¤šæ ·æ€§é™ä½ã€‚DARLINGé€šè¿‡å¼•å…¥å­¦ä¹ çš„åˆ†åŒºå‡½æ•°æ¥è¡¡é‡å¤šæ ·æ€§ï¼Œå¹¶åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç»“åˆè´¨é‡å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¸”ç‹¬ç‰¹çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARLINGåœ¨éå¯éªŒè¯å’Œå¯éªŒè¯ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„è¾“å‡ºåœ¨è´¨é‡å’Œæ–°é¢–æ€§ä¸Šå‡ä¼˜äºä»…å…³æ³¨è´¨é‡çš„åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02522",
            "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for\n  RLVR",
            "url": "https://huggingface.co/papers/2509.02522",
            "abstract": "PACS, a novel RLVR framework, reformulates RLVR as a supervised learning task, improving stability and efficiency in training large language models for reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.",
            "score": 18,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "824c46ed359a21d1",
            "authors": [
                "Jiaming Li",
                "Longze Chen",
                "Ze Gong",
                "Yukun Chen",
                "Lu Wang",
                "Wanwei He",
                "Run Luo",
                "Min Yang"
            ],
            "affiliations": [
                "Ritzz-AI",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02522.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PACS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğ¹ RLVR",
                    "desc": "PACS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ RLVR ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. PACS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ¾Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RLVR."
                },
                "en": {
                    "title": "PACS: Transforming RLVR into Supervised Learning for Better Reasoning",
                    "desc": "PACS is a new framework that reformulates Reinforcement Learning with Verifiable Rewards (RLVR) as a supervised learning task, which enhances the stability and efficiency of training large language models (LLMs) for reasoning tasks. By treating outcome rewards as predictable labels, PACS transforms the RLVR problem into a supervised learning problem, optimizing it with cross-entropy loss. This approach allows for implicit coupling of actor and critic roles, leading to more stable policy updates and improved training outcomes. Benchmark results show that PACS significantly outperforms traditional RLVR methods like PPO and GRPO in mathematical reasoning tasks, demonstrating its effectiveness in enhancing LLM performance."
                },
                "zh": {
                    "title": "PACSï¼šç¨³å®šé«˜æ•ˆçš„æ¨ç†è®­ç»ƒæ–°æ¡†æ¶",
                    "desc": "PACSæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œå®ƒå°†RLVRé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»è€Œæé«˜äº†å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡å°†ç»“æœå¥–åŠ±è§†ä¸ºå¯é¢„æµ‹çš„æ ‡ç­¾ï¼ŒPACSå°†RLVRé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŸºäºç­–ç•¥æ¨¡å‹çš„åˆ†æ•°å‡½æ•°çš„ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚è¯¦ç»†çš„æ¢¯åº¦åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç›‘ç£å½¢å¼æœ¬è´¨ä¸Šæ¢å¤äº†ç»å…¸çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼ŒåŒæ—¶éšå¼åœ°è€¦åˆäº†æ¼”å‘˜å’Œè¯„è®ºè€…çš„è§’è‰²ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„è®­ç»ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒPACSçš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„RLVRåŸºçº¿ï¼Œå¦‚PPOå’ŒGRPOï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†æ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00605",
            "title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient\n  Sequence Modeling",
            "url": "https://huggingface.co/papers/2509.00605",
            "abstract": "Gated Associative Memory (GAM) network offers a linear complexity alternative to the Transformer architecture, improving training speed and achieving competitive validation perplexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.",
            "score": 18,
            "issue_id": 5697,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "874c41bbbff9e11e",
            "authors": [
                "Rishiraj Acharya"
            ],
            "affiliations": [
                "Independent Researcher"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00605.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GAM: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Transformer Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Gated Associative Memory (GAM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Transformer Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. GAM Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¹ self-attention Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸: ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¾Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GAM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Transformer Ğ¸ Mamba Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WikiText-2 Ğ¸ TinyStories. GAM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "GAM: A Faster, Smarter Alternative to Transformers",
                    "desc": "The Gated Associative Memory (GAM) network presents a new approach to sequence modeling that operates with linear complexity, making it faster than traditional Transformer models. By replacing the self-attention mechanism with a combination of causal convolution and associative memory retrieval, GAM effectively captures both local and global context in data. This architecture allows for dynamic integration of information, enhancing the model's ability to process sequences efficiently. Experimental results show that GAM not only improves training speed but also achieves competitive performance in terms of validation perplexity on various datasets."
                },
                "zh": {
                    "title": "GAMï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ–°é€‰æ‹©",
                    "desc": "Gated Associative Memory (GAM) ç½‘ç»œæ˜¯ä¸€ç§æ–°å‹çš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼ˆO(N)ï¼‰ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„Transformeræ¶æ„ï¼ˆO(N^2)ï¼‰ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶æ›´ä¸ºé«˜æ•ˆã€‚GAMé€šè¿‡ä¸¤ä¸ªå¹¶è¡Œè·¯å¾„æ›¿ä»£äº†è‡ªæ³¨æ„åŠ›å±‚ï¼šä¸€ä¸ªå› æœå·ç§¯ç”¨äºæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªå¹¶è¡Œçš„å…³è”è®°å¿†æ£€ç´¢æœºåˆ¶ç”¨äºå»ºæ¨¡å…¨å±€æ¨¡å¼ã€‚è¿™ç§è·¯å¾„é€šè¿‡é—¨æ§æœºåˆ¶åŠ¨æ€èåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°ç»“åˆæ¯ä¸ªæ ‡è®°çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAMåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šä¼˜äºæ ‡å‡†Transformeræ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„éªŒè¯å›°æƒ‘åº¦ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºåºåˆ—å»ºæ¨¡çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02333",
            "title": "DCPO: Dynamic Clipping Policy Optimization",
            "url": "https://huggingface.co/papers/2509.02333",
            "abstract": "DCPO, a novel reinforcement learning framework, enhances large language models by dynamically adjusting clipping bounds and standardizing rewards, leading to improved performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.",
            "score": 15,
            "issue_id": 5691,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "3a6fa32ebaa3d08e",
            "authors": [
                "Shihui Yang",
                "Chengfeng Dou",
                "Peidong Guo",
                "Kai Lu",
                "Qiang Ju",
                "Fei Deng",
                "Rihui Xin"
            ],
            "affiliations": [
                "Baichuan.inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02333.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "DCPO: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DCPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. DCPO Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Dynamic Clipping for Enhanced Learning in Language Models",
                    "desc": "DCPO is a new reinforcement learning framework designed to improve large language models by dynamically adjusting clipping bounds and standardizing rewards. This approach addresses the issue of zero gradients that can occur with fixed clipping bounds and identical rewards, which hinder effective learning. By adapting clipping strategies based on token-specific probabilities and smoothing reward standardization, DCPO enhances exploration and utilization of generated responses. The framework has demonstrated state-of-the-art performance across multiple benchmarks, significantly improving training efficiency and reducing clipping ratios compared to previous methods."
                },
                "zh": {
                    "title": "åŠ¨æ€å‰ªåˆ‡ç­–ç•¥ä¼˜åŒ–ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡",
                    "desc": "DCPOæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œå’Œæ ‡å‡†åŒ–å¥–åŠ±æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­ç”±äºå›ºå®šå‰ªåˆ‡è¾¹ç•Œå¯¼è‡´çš„é›¶æ¢¯åº¦é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ¢¯åº¦æ›´æ–°çš„æœ‰æ•ˆæ€§ã€‚DCPOå¼•å…¥äº†ä¸€ç§åŠ¨æ€å‰ªåˆ‡ç­–ç•¥ï¼Œæ ¹æ®ç‰¹å®šçš„å…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œï¼Œä¿ƒè¿›äº†ä»¤ç‰Œçº§åˆ«çš„æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆå“åº”çš„æœ‰æ•ˆåˆ©ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02460",
            "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
            "url": "https://huggingface.co/papers/2509.02460",
            "abstract": "A novel Diffusion Transformer pipeline automates video compositing by adaptively injecting identity and motion information, maintaining consistency and enabling user customization.  \t\t\t\t\tAI-generated summary \t\t\t\t Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.",
            "score": 14,
            "issue_id": 5688,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "cbe52bb6a0af85b6",
            "authors": [
                "Shuzhou Yang",
                "Xiaoyu Li",
                "Xiaodong Cun",
                "Guangzhi Wang",
                "Lingen Li",
                "Ying Shan",
                "Jian Zhang"
            ],
            "affiliations": [
                "ARC Lab, Tencent",
                "GVC Lab, Great Bay University",
                "SECE, Peking University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02460.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ½Ğ³: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ½Ğ³Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer (DiT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµÑ‚Ğ²ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ°, Ğ±Ğ»Ğ¾Ğº ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ DiT Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (ERoPE) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VideoComp, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 61 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Automating Video Compositing with Adaptive Diffusion Transformers",
                    "desc": "This paper presents a new method for automating video compositing using a Diffusion Transformer (DiT) pipeline. The approach allows for the adaptive injection of identity and motion information into videos, enabling user customization of dynamic elements. By incorporating a background preservation branch and a DiT fusion block, the method maintains video consistency while enhancing the integration of foreground and background elements. The authors also introduce a new dataset, VideoComp, to support their task, demonstrating that their method outperforms existing solutions in terms of fidelity and consistency."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–è§†é¢‘åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformerï¼‰ç®¡é“ï¼Œç”¨äºè‡ªåŠ¨åŒ–è§†é¢‘åˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æ³¨å…¥èº«ä»½å’Œè¿åŠ¨ä¿¡æ¯ï¼Œä¿æŒè§†é¢‘çš„ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸ç”¨æˆ·è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ã€‚ä¼ ç»Ÿçš„è§†é¢‘åˆæˆéœ€è¦å¤§é‡äººåŠ›å’Œä¸“ä¸šçŸ¥è¯†ï¼Œè€Œè¿™ç§ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—ç¼©çŸ­åˆ¶ä½œå‘¨æœŸï¼Œé™ä½æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘åˆæˆçš„ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01644",
            "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for\n  Multimodal Learning",
            "url": "https://huggingface.co/papers/2509.01644",
            "abstract": "OpenVision 2 simplifies the original architecture by removing the text encoder and contrastive loss, achieving competitive performance with reduced training time and memory usage, and enabling scaling to over 1 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.",
            "score": 14,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "0197aa14702197c7",
            "authors": [
                "Yanqing Liu",
                "Xianhang Li",
                "Letian Zhang",
                "Zirui Wang",
                "Zeyu Zheng",
                "Yuyin Zhou",
                "Cihang Xie"
            ],
            "affiliations": [
                "Apple",
                "University of California Berkeley",
                "University of California Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01644.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "OpenVision 2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ OpenVision, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑƒĞ´Ğ°Ğ»ĞµĞ½ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, OpenVision 2 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Streamlined Efficiency: OpenVision 2 Reimagined",
                    "desc": "OpenVision 2 is a streamlined version of the original OpenVision architecture that eliminates the text encoder and contrastive loss, focusing solely on a generative training approach with captioning loss. This simplification leads to significant improvements in training efficiency, reducing both training time and memory usage while maintaining competitive performance on multimodal benchmarks. For instance, it achieves a 1.5x reduction in training time and a 1.8x decrease in memory consumption, allowing for larger batch sizes. The architecture's ability to scale to over 1 billion parameters suggests a promising direction for future multimodal foundation models."
                },
                "zh": {
                    "title": "ç®€åŒ–æ¶æ„ï¼Œæå‡æ•ˆç‡â€”â€”OpenVision 2",
                    "desc": "OpenVision 2é€šè¿‡å»é™¤æ–‡æœ¬ç¼–ç å™¨å’Œå¯¹æ¯”æŸå¤±ï¼Œç®€åŒ–äº†åŸæœ‰æ¶æ„ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ¨¡å‹ä»…ä¿ç•™äº†ç”Ÿæˆæ€§è®­ç»ƒä¿¡å·çš„å­—å¹•æŸå¤±ï¼Œè¡¨ç°å‡ºä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚å°½ç®¡è¿›è¡Œäº†ç®€åŒ–ï¼ŒOpenVision 2åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä»ç„¶è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§è½»é‡çº§çš„ç”Ÿæˆæ€§èŒƒå¼å¯¹æœªæ¥å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01440",
            "title": "Benchmarking Optimizers for Large Language Model Pretraining",
            "url": "https://huggingface.co/papers/2509.01440",
            "abstract": "A comprehensive evaluation of recent optimization techniques for Large Language Models provides guidance on selecting the best optimizer for different pretraining scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.",
            "score": 12,
            "issue_id": 5690,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "086d91ca4e22a4be",
            "authors": [
                "Andrei Semenov",
                "Matteo Pagliardini",
                "Martin Jaggi"
            ],
            "affiliations": [
                "EPFL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01440.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ LLM: Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Optimizing Large Language Models: A Guide to Choosing the Right Optimizer",
                    "desc": "This paper evaluates various optimization techniques for Large Language Models (LLMs) to help researchers and practitioners choose the best optimizer for different pretraining scenarios. It addresses the challenges of comparing methods due to varying experimental protocols and provides a systematic analysis by varying model size, batch size, and training duration. The study offers insights into which optimizers yield faster convergence and reduced dependency on hyperparameters. Additionally, the authors release their code for reproducibility, aiming to support future research in optimization methods."
                },
                "zh": {
                    "title": "é€‰æ‹©æœ€ä½³ä¼˜åŒ–å™¨ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ—¨åœ¨ä¸ºä¸åŒçš„é¢„è®­ç»ƒåœºæ™¯é€‰æ‹©æœ€ä½³ä¼˜åŒ–å™¨æä¾›æŒ‡å¯¼ã€‚ç ”ç©¶ä¸­ç³»ç»Ÿåœ°å˜åŒ–äº†æ¨¡å‹å¤§å°ã€æ‰¹é‡å¤§å°å’Œè®­ç»ƒæ—¶é•¿ï¼Œä»¥ä¾¿å¯¹å„ç§ä¼˜åŒ–æ–¹æ³•è¿›è¡Œæ ‡å‡†åŒ–æ¯”è¾ƒã€‚é€šè¿‡å¯¹æ¯ç§æ–¹æ³•çš„ç»†è‡´è°ƒä¼˜ï¼Œæœ¬æ–‡ä¸ºå®è·µè€…æä¾›äº†åœ¨ç‰¹å®šåœºæ™¯ä¸‹é€‰æ‹©ä¼˜åŒ–å™¨çš„å»ºè®®ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºäº†æœªæ¥ä¼˜åŒ–ç ”ç©¶çš„æœ‰å¸Œæœ›æ–¹å‘ï¼Œå¹¶é€šè¿‡å‘å¸ƒä»£ç å’Œå®éªŒç»“æœï¼Œç¡®ä¿äº†ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02563",
            "title": "DynaGuard: A Dynamic Guardrail Model With User-Defined Policies",
            "url": "https://huggingface.co/papers/2509.02563",
            "abstract": "Dynamic guardian models evaluate text based on user-defined policies, offering fast and accurate detection of both static harms and free-form policy violations.  \t\t\t\t\tAI-generated summary \t\t\t\t Guardian models are used to supervise and moderate the outputs of user-facing chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian models like LlamaGuard detect predefined, static categories of harms. We propose dynamic guardian models that evaluate text based on user-defined policies, making them useful for different application domains that are not addressed by standard guardian models. Our dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs. Our dynamic guardian models match static models in detection accuracy for static harm categories while identifying violations of free-form policies with accuracy comparable to frontier reasoning models in a fraction of the time.",
            "score": 11,
            "issue_id": 5696,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "b4dda88cf231d2fa",
            "authors": [
                "Monte Hoover",
                "Vatsal Baherwani",
                "Neel Jain",
                "Khalid Saifullah",
                "Joseph Vincent",
                "Chirag Jain",
                "Melissa Kazemi Rad",
                "C. Bayan Bruss",
                "Ashwinee Panda",
                "Tom Goldstein"
            ],
            "affiliations": [
                "Capital One",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02563.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#ethics",
                    "#alignment",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½Ğ¸ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Text Evaluation with Dynamic Guardian Models",
                    "desc": "Dynamic guardian models are advanced tools that assess text according to specific rules set by users, allowing for quick and precise identification of both fixed harms and flexible policy breaches. Unlike traditional models that only recognize predefined categories of issues, these dynamic models adapt to various contexts and requirements. They not only ensure compliance with user-defined standards but also provide reasoning behind their evaluations, enhancing transparency. Furthermore, they achieve similar accuracy to static models for known harms while efficiently detecting more complex violations, making them suitable for diverse applications."
                },
                "zh": {
                    "title": "åŠ¨æ€å®ˆæŠ¤æ¨¡å‹ï¼šçµæ´»çš„æ–‡æœ¬è¯„ä¼°ä¸æ”¿ç­–æ£€æµ‹",
                    "desc": "åŠ¨æ€å®ˆæŠ¤æ¨¡å‹æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ”¿ç­–è¯„ä¼°æ–‡æœ¬ï¼Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°æ£€æµ‹é™æ€å±å®³å’Œè‡ªç”±å½¢å¼çš„æ”¿ç­–è¿è§„ã€‚ä¸æ ‡å‡†å®ˆæŠ¤æ¨¡å‹ä¸åŒï¼ŒåŠ¨æ€å®ˆæŠ¤æ¨¡å‹é€‚ç”¨äºä¸åŒçš„åº”ç”¨é¢†åŸŸï¼Œæä¾›çµæ´»çš„ç›‘ç£å’Œè°ƒèŠ‚åŠŸèƒ½ã€‚å®ƒä»¬ä¸ä»…èƒ½å¿«é€Ÿæ£€æµ‹æ”¿ç­–è¿è§„ï¼Œè¿˜èƒ½é€šè¿‡æ¨ç†é“¾æ¡æ¸…æ™°åœ°é˜è¿°å’Œè§£é‡Šæ¨¡å‹çš„è¾“å‡ºã€‚åŠ¨æ€å®ˆæŠ¤æ¨¡å‹åœ¨é™æ€å±å®³æ£€æµ‹çš„å‡†ç¡®æ€§ä¸Šä¸é™æ€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨è‡ªç”±å½¢å¼æ”¿ç­–çš„è¯†åˆ«ä¸Šä¹Ÿèƒ½è¾¾åˆ°å‰æ²¿æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02040",
            "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm\n  Simulators for Conditional Synthetic Data Generation",
            "url": "https://huggingface.co/papers/2509.02040",
            "abstract": "Genetic Prompt enhances synthetic data quality and diversity in NLP by combining genetic algorithms with LLMs, improving downstream model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.",
            "score": 11,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "085dff767b3e88ce",
            "authors": [
                "Guangzeng Han",
                "Weisi Liu",
                "Xiaolei Huang"
            ],
            "affiliations": [
                "Department of Computer Science, University of Memphis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02040.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#synthetic",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ NLP",
                    "desc": "Genetic Prompt - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞºÑ€ĞµÑ‰Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Genetic Prompt Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Genetic Prompt: Evolving Synthetic Data for NLP Excellence",
                    "desc": "The paper introduces Genetic Prompt, a framework that enhances the quality and diversity of synthetic data in natural language processing (NLP) by integrating genetic algorithms with large language models (LLMs). It treats semantic text attributes as gene sequences, applying genetic operations like crossover and mutation to generate diverse data combinations. This method improves the synthetic data's resemblance to real-world distributions, which is crucial for training effective models. Additionally, the framework incorporates an active learning strategy to optimize the selection of parent data, leading to better performance in various NLP tasks, especially in scenarios with class imbalance."
                },
                "zh": {
                    "title": "é—ä¼ æç¤ºï¼šæå‡åˆæˆæ•°æ®è´¨é‡ä¸å¤šæ ·æ€§",
                    "desc": "Genetic Promptæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å°†é—ä¼ ç®—æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œæå‡äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åˆæˆæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•å°†è¯­ä¹‰æ–‡æœ¬å±æ€§è§†ä¸ºåŸºå› åºåˆ—ï¼Œå¹¶åˆ©ç”¨LLMæ¨¡æ‹Ÿäº¤å‰å’Œçªå˜æ“ä½œï¼Œä»è€Œç”Ÿæˆæ–°çš„å±æ€§ç»„åˆã€‚é€šè¿‡è¿™ç§é—ä¼ è¿‡ç¨‹ï¼Œåˆæˆæ•°æ®çš„åˆ†å¸ƒæ›´æ¥è¿‘çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œä¼˜åŒ–äº†ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenetic Promptåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œèåˆåˆæˆæ•°æ®ä¸åŸå§‹è®­ç»ƒé›†èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01360",
            "title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via\n  Self-Supervision",
            "url": "https://huggingface.co/papers/2509.01360",
            "abstract": "M3Ret, a unified visual encoder trained on a large-scale hybrid-modality dataset, achieves state-of-the-art zero-shot image-to-image retrieval and cross-modal alignment using generative and contrastive self-supervised learning paradigms.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.",
            "score": 8,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "3dcca4eb6e49e24e",
            "authors": [
                "Che Liu",
                "Zheng Jiang",
                "Chengyu Fang",
                "Heng Guo",
                "Yan-Jie Zhou",
                "Jiaqi Qu",
                "Le Lu",
                "Minfeng Xu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Imperial College London",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01360.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#healthcare"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "M3Ret - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. M3Ret Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 867,653 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 2D Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸, Ğ£Ğ—Ğ˜, RGB Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ 3D ĞšĞ¢-ÑĞºĞ°Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DINOv3 Ğ¸ BMC-CLIP, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Unified Learning for Enhanced Medical Image Retrieval",
                    "desc": "M3Ret is a unified visual encoder designed to improve medical image retrieval by using a large dataset that includes various types of medical images. It employs generative and contrastive self-supervised learning techniques to create transferable visual representations without needing separate models for different image modalities. This approach allows M3Ret to excel in zero-shot image-to-image retrieval and cross-modal alignment, even with unseen data like MRI scans. The results indicate that M3Ret can effectively generalize across different medical imaging tasks, paving the way for more integrated solutions in medical image analysis."
                },
                "zh": {
                    "title": "M3Retï¼šç»Ÿä¸€çš„åŒ»å­¦å½±åƒæ£€ç´¢æ–°çªç ´",
                    "desc": "M3Retæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆå¼å’Œå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨2Dã€3Då’Œè§†é¢‘åŒ»å­¦æ•°æ®ä¸Šåˆ†æ•£çš„å±€é™æ€§ã€‚é€šè¿‡æ„å»º867,653ä¸ªåŒ»å­¦å½±åƒæ ·æœ¬çš„æ•°æ®é›†ï¼ŒM3Retèƒ½å¤Ÿå­¦ä¹ å¯è¿ç§»çš„è§†è§‰è¡¨ç¤ºï¼Œä¸”æ— éœ€ç‰¹å®šæ¨¡æ€çš„å®šåˆ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒM3Retåœ¨å„ä¸ªæ¨¡æ€çš„é›¶æ ·æœ¬å›¾åƒæ£€ç´¢ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»å­¦å½±åƒç†è§£ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01052",
            "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
            "url": "https://huggingface.co/papers/2509.01052",
            "abstract": "FlashAdventure benchmark and COAST framework improve GUI agents' performance in completing full story arcs in Flash-based adventure games by addressing the observation-behavior gap.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.",
            "score": 8,
            "issue_id": 5686,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "909be5826c565f9d",
            "authors": [
                "Jaewoo Ahn",
                "Junseo Kim",
                "Heeseung Yun",
                "Jaehyeon Son",
                "Dongmin Park",
                "Jaewoong Cho",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "KRAFTON",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01052.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#optimization",
                    "#games"
                ],
                "emoji": "ğŸ•¹ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…-ĞºĞ²ĞµÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FlashAdventure Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ² Flash-Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¶Ğ°Ğ½Ñ€Ğ° ĞºĞ²ĞµÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° CUA-as-a-Judge Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº COAST, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ COAST Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ½."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing GUI Agents in Adventure Games",
                    "desc": "This paper introduces the FlashAdventure benchmark and the COAST framework to enhance the performance of GUI agents in completing full story arcs in Flash-based adventure games. The research addresses the observation-behavior gap, which is the difficulty agents face in recalling and utilizing past gameplay information effectively. By providing a diverse set of 34 adventure games, the benchmark allows for a more comprehensive evaluation of agents' abilities to navigate complex narratives. The COAST framework incorporates long-term memory strategies, leading to improved task planning and milestone completion, although a significant performance gap between human players and agents remains."
                },
                "zh": {
                    "title": "æå‡æ¸¸æˆä»£ç†çš„æ•…äº‹æƒ…èŠ‚å®Œæˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†FlashAdventureåŸºå‡†å’ŒCOASTæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å®ŒæˆFlashå†’é™©æ¸¸æˆå®Œæ•´æ•…äº‹æƒ…èŠ‚æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„æ¸¸æˆåŸºå‡†ç¼ºä¹å¤šæ ·æ€§ï¼Œä¸”å¾ˆå°‘è¯„ä¼°ä»£ç†å®Œæˆæ•´ä¸ªæ•…äº‹çº¿çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è§‚å¯Ÿ-è¡Œä¸ºå·®è·çš„é—®é¢˜ï¼ŒFlashAdventureåŸºå‡†åŒ…å«34æ¬¾Flashå†’é™©æ¸¸æˆï¼Œä¸“æ³¨äºæµ‹è¯•å®Œæ•´æ•…äº‹æƒ…èŠ‚çš„å®Œæˆã€‚COASTæ¡†æ¶é€šè¿‡åˆ©ç”¨é•¿æœŸçº¿ç´¢è®°å¿†ï¼Œå¸®åŠ©ä»£ç†æ›´å¥½åœ°è§„åˆ’å’Œè§£å†³é¡ºåºä»»åŠ¡ï¼Œä»è€Œæé«˜äº†é‡Œç¨‹ç¢‘çš„å®Œæˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00425",
            "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in\n  LLMs with Camlang",
            "url": "https://huggingface.co/papers/2509.00425",
            "abstract": "Camlang, a constructed language, is used to evaluate whether LLMs can master unfamiliar languages through metalinguistic reasoning, revealing that current models lack systematic grammatical mastery compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve gold-medal performance across many benchmarks, yet it remains unclear whether such success reflects genuine reasoning or pattern matching. From a cognitive science perspective, an informative test is whether models can master an unfamiliar language through explicit metalinguistic deductive learning, a paradigm where human learners can reliably internalise grammatical systems through metalinguistic reasoning. We address this question with Camlang, a novel constructed language that exhibits naturalistic yet unattested feature combinations. Camlang consists of two explicit resources, a grammar book and a bilingual dictionary, which mirror adult second-language learning via explicit grammar rules and lexical lookup, and enable us to disentangle errors in morpho-syntax, lexical semantics, and sentence-level reasoning. Human experiments show that these resources are sufficient for participants to acquire Camlang and successfully solve Camlang tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang, creating Camlang-CSQA-v0, the first task in a broader suite where solving questions requires applying grammar rules and lexical mappings. Experimental results show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in Camlang, far below human performance at 87\\%, while other state-of-the-art reasoning LLMs perform even worse. Human verification further reveals that most model successes stem from shallow lexical alignment while GPT-5 shows emerging metalinguistic awareness to a limited extent but not systematic grammatical mastery as humans. Camlang establishes a cognitively grounded evaluation paradigm that exposes fundamental gaps between current models and human metalinguistic competence.",
            "score": 8,
            "issue_id": 5690,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "d6f6d7d86f28d385",
            "authors": [
                "Fenghua Liu",
                "Yulong Chen",
                "Yixuan Liu",
                "Zhujun Jin",
                "Solomon Tsai",
                "Ming Zhong"
            ],
            "affiliations": [
                "UIUC",
                "University of Cambridge",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00425.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#reasoning",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Camlang Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµÑ‚ÑŒ Camlang Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. GPT-5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ 47% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Camlang Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 87% Ñƒ Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Camlang Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating LLMs with Camlang",
                    "desc": "This paper investigates the ability of Large Language Models (LLMs) to learn and understand a constructed language called Camlang, which is designed to test metalinguistic reasoning. The study finds that while LLMs like GPT-5 perform well on familiar languages, they struggle significantly with Camlang, achieving only 47% accuracy compared to 87% for humans. The results indicate that current models rely on shallow pattern matching rather than true grammatical understanding, highlighting a gap in their metalinguistic competence. This research provides a new framework for evaluating LLMs by focusing on their ability to apply grammatical rules and lexical mappings in unfamiliar contexts."
                },
                "zh": {
                    "title": "æ­ç¤ºLLMsä¸äººç±»è¯­æ³•æŒæ¡çš„å·®è·",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒæ¡ä¸ç†Ÿæ‚‰è¯­è¨€æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†ä¸€ç§åä¸ºCamlangçš„æ„é€ è¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬åœ¨è¯­æ³•æŒæ¡ä¸Šä¸äººç±»ç›¸æ¯”ä»ç„¶å­˜åœ¨æ˜¾è‘—å·®è·ã€‚é€šè¿‡æä¾›è¯­æ³•ä¹¦å’ŒåŒè¯­è¯å…¸ï¼Œç ”ç©¶æ¨¡æ‹Ÿäº†äººç±»å­¦ä¹ è€…çš„æ˜¾å¼è¯­æ³•å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„æ¨¡å‹åœ¨Camlangä»»åŠ¡ä¸­çš„è¡¨ç°è¿œä½äºäººç±»ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å…ƒè¯­è¨€æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02046",
            "title": "Fantastic Pretraining Optimizers and Where to Find Them",
            "url": "https://huggingface.co/papers/2509.02046",
            "abstract": "A systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.  \t\t\t\t\tAI-generated summary \t\t\t\t AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.",
            "score": 6,
            "issue_id": 5689,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "d90554ffa1d1a865",
            "authors": [
                "Kaiyue Wen",
                "David Hall",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02046.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Muon Ğ¸ Soap, Ğ´Ğ°ÑÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰ĞµĞµÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ misleading, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° decay ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Fair Comparisons of Deep Learning Optimizers: Tuning Matters!",
                    "desc": "This paper investigates the effectiveness of various deep learning optimizers, particularly focusing on the popular AdamW. It highlights that fair comparisons of optimizers require careful hyperparameter tuning and evaluations across different model sizes and data ratios. The study reveals that matrix-based optimizers like Muon and Soap show diminishing returns in speedup as model size increases, with their advantages decreasing significantly for larger models. Ultimately, the findings suggest that many claims of speedup for alternative optimizers may be overstated, emphasizing the need for rigorous evaluation methods."
                },
                "zh": {
                    "title": "å…¬å¹³æ¯”è¾ƒæ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨çš„å…³é”®åœ¨äºè¶…å‚æ•°è°ƒä¼˜",
                    "desc": "æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨çš„å…¬å¹³æ¯”è¾ƒï¼Œå¼ºè°ƒäº†è¶…å‚æ•°è°ƒä¼˜å’Œæ¨¡å‹è§„æ¨¡ã€æ•°æ®ä¸æ¨¡å‹æ¯”ä¾‹çš„è¯„ä¼°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒçŸ©é˜µåŸºç¡€çš„ä¼˜åŒ–å™¨å¦‚Muonå’ŒSoapåœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ï¼Œé€Ÿåº¦æå‡é€æ¸å‡å°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›²ç›®è½¬ç§»è¶…å‚æ•°ä¼šå¯¼è‡´ä¸å…¬å¹³çš„æ¯”è¾ƒï¼Œè€Œåœ¨è®­ç»ƒç»“æŸæ—¶è¿›è¡Œçš„ä¸¥æ ¼è¯„ä¼°æ‰èƒ½æä¾›çœŸå®çš„æ€§èƒ½å¯¹æ¯”ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šå£°ç§°çš„é€Ÿåº¦æå‡åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€ä½äºé¢„æœŸï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00244",
            "title": "Universal Deep Research: Bring Your Own Model and Strategy",
            "url": "https://huggingface.co/papers/2509.00244",
            "abstract": "Universal Deep Research (UDR) is a flexible system that allows users to customize deep research strategies using any language model without additional training or fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.",
            "score": 6,
            "issue_id": 5685,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "641998602f4f0d54",
            "authors": [
                "Peter Belcak",
                "Pavlo Molchanov"
            ],
            "affiliations": [
                "NVIDIA Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00244.jpg",
            "data": {
                "categories": [
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Universal Deep Research (UDR) - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸. UDR Ğ¾Ğ±ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. UDR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Customize Your Research with Universal Deep Research!",
                    "desc": "Universal Deep Research (UDR) is a versatile system that empowers users to design personalized deep research strategies using any language model, eliminating the need for extra training or fine-tuning. Unlike previous deep research agents that are limited to specific strategies and tools, UDR allows for complete customization and flexibility. The system supports various research approaches, including minimal, expansive, and intensive strategies, demonstrating its adaptability. A user-friendly interface is provided to encourage experimentation and enhance the research process."
                },
                "zh": {
                    "title": "é€šç”¨æ·±åº¦ç ”ç©¶ï¼šè‡ªå®šä¹‰ä½ çš„ç ”ç©¶ç­–ç•¥",
                    "desc": "é€šç”¨æ·±åº¦ç ”ç©¶ï¼ˆUDRï¼‰æ˜¯ä¸€ä¸ªçµæ´»çš„ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨ä»»ä½•è¯­è¨€æ¨¡å‹è‡ªå®šä¹‰æ·±åº¦ç ”ç©¶ç­–ç•¥ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚ç°æœ‰çš„æ·±åº¦ç ”ç©¶å·¥å…·é€šå¸¸æ˜¯ç¡¬ç¼–ç çš„ï¼Œæ‰§è¡Œç‰¹å®šçš„ç ”ç©¶ç­–ç•¥å¹¶ä½¿ç”¨å›ºå®šçš„å·¥å…·é€‰æ‹©ã€‚UDRä½œä¸ºä¸€ä¸ªé€šç”¨çš„æ™ºèƒ½ç³»ç»Ÿï¼Œå¯ä»¥å›´ç»•ä»»ä½•è¯­è¨€æ¨¡å‹è¿›è¡Œæ„å»ºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåˆ›å»ºã€ç¼–è¾‘å’Œå®Œå–„è‡ªå·±çš„æ·±åº¦ç ”ç©¶ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜ä¸ºUDRæä¾›äº†ç¤ºä¾‹ç ”ç©¶ç­–ç•¥ï¼Œå¹¶æä¾›ç”¨æˆ·ç•Œé¢ä»¥ä¾¿äºç”¨æˆ·è¿›è¡Œå®éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01984",
            "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image\n  Editing",
            "url": "https://huggingface.co/papers/2509.01984",
            "abstract": "VARIN, a noise inversion-based editing technique for visual autoregressive models, enables precise image editing aligned with textual prompts while preserving original details.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.",
            "score": 4,
            "issue_id": 5686,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "b7fe3e5a42fa90e9",
            "authors": [
                "Quan Dao",
                "Xiaoxiao He",
                "Ligong Han",
                "Ngan Hoai Nguyen",
                "Amin Heyrani Nobar",
                "Faez Ahmed",
                "Han Zhang",
                "Viet Anh Nguyen",
                "Dimitris Metaxas"
            ],
            "affiliations": [
                "MIT",
                "Qualcomm AI Research",
                "Red Hat AI Innovation",
                "ReveAI",
                "Rutgers University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01984.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ–Œï¸",
                "ru": {
                    "title": "VARIN: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ VAR-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "VARIN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VAR). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. VARIN Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ argmax-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Location-aware Argmax Inversion (LAI), Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… ÑˆÑƒĞ¼Ğ¾Ğ² Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VARIN ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Precise Image Editing with VARIN: Text Meets Visuals",
                    "desc": "This paper presents VARIN, a novel editing technique for visual autoregressive models that allows for precise image modifications based on textual prompts. VARIN utilizes a unique method called Location-aware Argmax Inversion (LAI) to create inverse Gumbel noises, which help in reconstructing the original image while enabling targeted edits. The technique is designed to work without requiring additional training, making it practical for real-world applications. Experimental results show that VARIN successfully alters images according to prompts while maintaining important details and background structures."
                },
                "zh": {
                    "title": "VARINï¼šç²¾å‡†å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "VARINæ˜¯ä¸€ç§åŸºäºå™ªå£°åæ¼”çš„ç¼–è¾‘æŠ€æœ¯ï¼Œä¸“ä¸ºè§†è§‰è‡ªå›å½’æ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œç²¾ç¡®çš„å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ç»†èŠ‚ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¼ªé€†å‡½æ•°ï¼Œç§°ä¸ºä½ç½®æ„ŸçŸ¥çš„Argmaxåæ¼”ï¼ˆLAIï¼‰ï¼Œç”Ÿæˆé€†Gumbelå™ªå£°ã€‚è¿™äº›é€†å™ªå£°ä½¿å¾—æºå›¾åƒçš„é‡å»ºæ›´åŠ ç²¾ç¡®ï¼Œå¹¶æ”¯æŒä¸æ–‡æœ¬æç¤ºå¯¹é½çš„æœ‰é’ˆå¯¹æ€§çš„å¯æ§ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVARINèƒ½å¤Ÿæœ‰æ•ˆåœ°æ ¹æ®æŒ‡å®šæç¤ºä¿®æ”¹æºå›¾åƒï¼ŒåŒæ—¶æ˜¾è‘—ä¿ç•™åŸå§‹èƒŒæ™¯å’Œç»“æ„ç»†èŠ‚ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºå®ç”¨ç¼–è¾‘æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02133",
            "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with\n  Knowledge Augmentation for Robust Constitutional Alignment of Language Models",
            "url": "https://huggingface.co/papers/2509.02133",
            "abstract": "Ambekar framework uses a Constitution-Aware Decoding Layer to mitigate caste and religious biases in LLM outputs through speculative decoding without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/",
            "score": 2,
            "issue_id": 5688,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "a82d3bc5f04bc839",
            "authors": [
                "Snehasis Mukhopadhyay",
                "Aryan Kasat",
                "Shivam Dubey",
                "Rahul Karthikeyan",
                "Dhruv Sood",
                "Vinija Jain",
                "Aman Chadha",
                "Amitava Das"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Artificial Intelligence Institute, University of South Carolina",
                "BITS Pilani Goa",
                "DTU",
                "IIT Madras",
                "Indian Institute of Information Technology, Kalyani",
                "Meta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02133.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multilingual",
                    "#ethics",
                    "#open_source",
                    "#alignment",
                    "#inference"
                ],
                "emoji": "ğŸ‡®ğŸ‡³",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AMBEDKAR Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ñ€ĞµĞ»Ğ¸Ğ³Ğ¸Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 26.41% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "Fairness Through Constitution-Aware Decoding in AI",
                    "desc": "The Ambekar framework introduces a Constitution-Aware Decoding Layer to address caste and religious biases in Large Language Models (LLMs) without the need for retraining. It recognizes that existing bias mitigation strategies often overlook local contexts, particularly in India, where such biases are pronounced. By employing a speculative decoding algorithm, the framework actively reduces harmful outputs during the generation process. This innovative approach not only enhances fairness in AI outputs but also demonstrates a significant reduction in bias, achieving up to 26.41 percent less bias compared to traditional methods."
                },
                "zh": {
                    "title": "å…¬å¹³ä¸ä¸­ç«‹ï¼šAmbekaræ¡†æ¶çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "Ambekaræ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ä¸ªå®ªæ³•æ„è¯†è§£ç å±‚ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºä¸­çš„ç§å§“å’Œå®—æ•™åè§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µåº”ç”¨ï¼Œåˆ©ç”¨æŠ•æœºè§£ç ç®—æ³•ä¸»åŠ¨é™ä½ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åè§ã€‚ä¸ä¼ ç»Ÿçš„åè§ç¼“è§£ç­–ç•¥ä¸åŒï¼ŒAmbekaræ¡†æ¶ä¸“æ³¨äºå°åº¦ç‰¹æœ‰çš„ç¤¾ä¼šèƒŒæ™¯ï¼Œç¡®ä¿è¾“å‡ºçš„å…¬å¹³æ€§å’Œä¸­ç«‹æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å®ç°äº†ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹é«˜è¾¾26.41%çš„åè§ç»å¯¹å‡å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00581",
            "title": "SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction",
            "url": "https://huggingface.co/papers/2509.00581",
            "abstract": "A multi-agent framework decomposes the Text2SQL task into several components, using in-context learning and taxonomy-guided error modification to achieve state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting natural language queries into SQL queries is a crucial challenge in both industry and academia, aiming to increase access to databases and large-scale applications. This work examines how in-context learning and chain-of-thought can be utilized to develop a robust solution for text-to-SQL systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the Text2SQL task into schema linking, subproblem identification, query plan generation, SQL generation, and a guided correction loop. Unlike prior systems that rely only on execution-based static correction, we introduce taxonomy-guided dynamic error modification informed by in-context learning. SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its variants, combining guided error taxonomy with reasoning-based query planning.",
            "score": 2,
            "issue_id": 5688,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "9c1785e2259e0c93",
            "authors": [
                "Saumya Chaturvedi",
                "Aman Chadha",
                "Laurent Bindschaedler"
            ],
            "affiliations": [
                "AWS GenAI Santa Clara, CA, USA",
                "Max Planck Institute for Software Systems SaarbrÃ¼cken, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00581.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#reasoning",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Text2SQL",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Text2SQL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SQL-of-Thought, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸-guided ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Spider."
                },
                "en": {
                    "title": "Transforming Language to SQL with Intelligent Agents",
                    "desc": "This paper presents SQL-of-Thought, a multi-agent framework designed to improve the Text2SQL task by breaking it down into several key components. It leverages in-context learning and chain-of-thought reasoning to enhance the conversion of natural language queries into SQL queries. The framework includes processes such as schema linking, subproblem identification, query plan generation, and a dynamic error correction loop guided by a taxonomy. By integrating these elements, SQL-of-Thought achieves state-of-the-art performance on the Spider dataset, surpassing previous systems that relied solely on static correction methods."
                },
                "zh": {
                    "title": "SQLè½¬æ¢çš„æ–°æ€è·¯ï¼šå¤šæ™ºèƒ½ä½“æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSQL-of-Thoughtçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLæŸ¥è¯¢ã€‚è¯¥æ¡†æ¶å°†Text2SQLä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å¼é“¾æ¥ã€å­é—®é¢˜è¯†åˆ«ã€æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆã€SQLç”Ÿæˆå’Œå¼•å¯¼ä¿®æ­£å¾ªç¯ã€‚ä¸ä»¥å¾€ä»…ä¾èµ–é™æ€æ‰§è¡Œä¿®æ­£çš„ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„åŠ¨æ€é”™è¯¯ä¿®æ”¹ï¼Œç»“åˆäº†å¼•å¯¼é”™è¯¯åˆ†ç±»å’Œæ¨ç†åŸºç¡€çš„æŸ¥è¯¢è§„åˆ’ã€‚SQL-of-Thoughtåœ¨Spideræ•°æ®é›†åŠå…¶å˜ä½“ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬åˆ°SQLç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00531",
            "title": "MobiAgent: A Systematic Framework for Customizable Mobile Agents",
            "url": "https://huggingface.co/papers/2509.00531",
            "abstract": "MobiAgent, a comprehensive mobile agent system, achieves state-of-the-art performance in real-world mobile scenarios through its MobiMind-series models, AgentRR framework, and MobiFlow benchmarking suite, while also reducing data annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile agents have emerged as a key development direction for intelligent mobile systems. However, existing agent models continue to face significant challenges in real-world task execution, particularly in terms of accuracy and efficiency. To address these limitations, we propose MobiAgent, a comprehensive mobile agent system comprising three core components: the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. Furthermore, recognizing that the capabilities of current mobile agents are still limited by the availability of high-quality data, we have developed an AI-assisted agile data collection pipeline that significantly reduces the cost of manual annotation. Compared to both general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in real-world mobile scenarios.",
            "score": 2,
            "issue_id": 5690,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "c0685c3c2b70b6aa",
            "authors": [
                "Cheng Zhang",
                "Erhu Feng",
                "Xi Zhao",
                "Yisheng Zhao",
                "Wangbo Gong",
                "Jiahui Sun",
                "Dong Du",
                "Zhichao Hua",
                "Yubin Xia",
                "Haibo Chen"
            ],
            "affiliations": [
                "Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00531.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "MobiAgent: Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "MobiAgent - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MobiMind, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ AgentRR Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² MobiFlow. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². MobiAgent Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "MobiAgent: Revolutionizing Mobile Intelligence with Efficiency and Cost-Effectiveness",
                    "desc": "MobiAgent is a mobile agent system designed to enhance the performance of intelligent mobile applications in real-world scenarios. It consists of three main components: the MobiMind-series models for agent intelligence, the AgentRR framework for improving execution speed, and the MobiFlow suite for performance benchmarking. The system also includes an AI-assisted data collection pipeline that lowers the costs associated with data annotation, addressing a common limitation in training mobile agents. Overall, MobiAgent outperforms existing models by providing better accuracy and efficiency in task execution."
                },
                "zh": {
                    "title": "MobiAgentï¼šæ™ºèƒ½ç§»åŠ¨ä»£ç†çš„æœªæ¥",
                    "desc": "MobiAgentæ˜¯ä¸€ä¸ªå…¨é¢çš„ç§»åŠ¨ä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ç§»åŠ¨ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®ƒç”±MobiMindç³»åˆ—æ¨¡å‹ã€AgentRRåŠ é€Ÿæ¡†æ¶å’ŒMobiFlowåŸºå‡†æµ‹è¯•å¥—ä»¶ä¸‰éƒ¨åˆ†ç»„æˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ä»»åŠ¡æ‰§è¡Œçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†é™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ï¼ŒMobiAgentè¿˜å¼€å‘äº†ä¸€ä¸ªAIè¾…åŠ©çš„æ•æ·æ•°æ®æ”¶é›†ç®¡é“ã€‚ä¸é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ç”¨GUIä»£ç†æ¨¡å‹ç›¸æ¯”ï¼ŒMobiAgentåœ¨å®é™…ç§»åŠ¨åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21334",
            "title": "Stairway to Fairness: Connecting Group and Individual Fairness",
            "url": "https://huggingface.co/papers/2508.21334",
            "abstract": "Experiments reveal that highly group-fair recommendations can be individually unfair, highlighting the need for a better understanding and comparison of fairness measures in recommender systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.",
            "score": 2,
            "issue_id": 5696,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "63506e0d046b4476",
            "authors": [
                "Theresia Veronika Rampisela",
                "Maria Maistro",
                "Tuukka Ruotsalo",
                "Falk Scholer",
                "Christina Lioma"
            ],
            "affiliations": [
                "LUT University, Lahti, Finland",
                "RMIT University, Melbourne, Australia",
                "University of Copenhagen, Copenhagen, Denmark"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21334.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸: Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ vs Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Balancing Group and Individual Fairness in Recommendations",
                    "desc": "This paper investigates the relationship between group fairness and individual fairness in recommender systems (RSs). It highlights that while a recommendation may be fair for a group, it can still be unfair to individuals within that group. The authors conducted experiments across multiple datasets to compare different fairness evaluation measures, revealing that enhancing one type of fairness can negatively impact the other. This research provides valuable insights for practitioners looking to balance fairness in their recommendation algorithms."
                },
                "zh": {
                    "title": "ç¾¤ä½“å…¬å¹³ä¸ä¸ªä½“å…¬å¹³çš„å¹³è¡¡ä¹‹é“",
                    "desc": "åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå…¬å¹³æ€§é€šå¸¸åˆ†ä¸ºç¾¤ä½“å…¬å¹³å’Œä¸ªä½“å…¬å¹³ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹è¿™ä¸¤ç§å…¬å¹³æ€§ä¹‹é—´å…³ç³»çš„ç§‘å­¦ç†è§£å°šä¸æ˜ç¡®ï¼Œå› ä¸ºä»¥å¾€çš„ç ”ç©¶ä½¿ç”¨äº†ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡å’Œç›®æ ‡ï¼Œå¯¼è‡´æ— æ³•è¿›è¡Œæœ‰æ•ˆæ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†ç¾¤ä½“å…¬å¹³å’Œä¸ªä½“å…¬å¹³ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¯¹å¯ç”¨äºè¿™ä¸¤ç§å…¬å¹³æ€§çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æŸäº›æ¨èåœ¨ç¾¤ä½“å±‚é¢ä¸Šéå¸¸å…¬å¹³ï¼Œä½†åœ¨ä¸ªä½“å±‚é¢ä¸Šå¯èƒ½å­˜åœ¨ä¸¥é‡çš„ä¸å…¬å¹³ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21038",
            "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
            "url": "https://huggingface.co/papers/2508.21038",
            "abstract": "Vector embeddings face theoretical limitations in handling even simple queries, as demonstrated by a new dataset that shows state-of-the-art models fail due to the single vector paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.",
            "score": 2,
            "issue_id": 5699,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "460396ed53ab3a52",
            "authors": [
                "Orion Weller",
                "Michael Boratko",
                "Iftekhar Naim",
                "Jinhyuk Lee"
            ],
            "affiliations": [
                "Google DeepMind",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21038.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#data",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ĞµĞ»Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LIMIT, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑÑ‚Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Challenging the Limits of Vector Embeddings in Simple Queries",
                    "desc": "This paper discusses the limitations of vector embeddings in machine learning, particularly when handling simple queries. It reveals that even advanced models struggle with these tasks due to the constraints of the single vector representation. The authors introduce a new dataset, LIMIT, which tests these models and demonstrates their failures in realistic scenarios. The findings suggest that the theoretical boundaries of embedding models need to be addressed to improve their performance in various applications."
                },
                "zh": {
                    "title": "å‘é‡åµŒå…¥çš„ç†è®ºå±€é™æ€§ä¸æœªæ¥ç ”ç©¶æ–¹å‘",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å‘é‡åµŒå…¥åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶çš„ç†è®ºå±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å‘é‡åµŒå…¥åœ¨æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ä»ç„¶æ— æ³•æ»¡è¶³åŸºæœ¬éœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºåä¸ºLIMITçš„æ•°æ®é›†ï¼ŒéªŒè¯äº†å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹ç®€å•ä»»åŠ¡æ—¶ä¹Ÿä¼šå¤±è´¥ã€‚è¯¥ç ”ç©¶å‘¼åæœªæ¥çš„ç ”ç©¶åº”å¼€å‘æ–°çš„æ–¹æ³•ï¼Œä»¥å…‹æœç°æœ‰å•å‘é‡èŒƒå¼çš„æ ¹æœ¬é™åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02523",
            "title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices",
            "url": "https://huggingface.co/papers/2509.02523",
            "abstract": "Monolingual ASR models trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data outperform multilingual models for small model sizes, achieving superior error rates and enabling on-device ASR for underrepresented languages.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license.",
            "score": 1,
            "issue_id": 5698,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "0a4562a7b7ff851b",
            "authors": [
                "Evan King",
                "Adam Sabra",
                "Manjunath Kudlur",
                "James Wang",
                "Pete Warden"
            ],
            "affiliations": [
                "Moonshine AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02523.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#small_models",
                    "#low_resource",
                    "#audio",
                    "#synthetic",
                    "#open_source",
                    "#multilingual",
                    "#data"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ASR Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Flavors of Moonshine. Ğ’Ğ¾Ğ¿Ñ€ĞµĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (27 Ğ¼Ğ»Ğ½) Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ…, Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½Ğ° 48% Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Whisper Tiny Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Monolingual Models Shine for Underrepresented Languages!",
                    "desc": "This paper presents a new approach to automatic speech recognition (ASR) for underrepresented languages using monolingual models. The authors demonstrate that small ASR models, when trained on a balanced mix of high-quality, pseudo-labeled, and synthetic data, can achieve better performance than larger multilingual models. Their findings show that these monolingual models can reduce error rates significantly, making them suitable for on-device applications. The research highlights the effectiveness of tailored training strategies for improving ASR in languages that have been historically underserved."
                },
                "zh": {
                    "title": "å•è¯­æ¨¡å‹è¶…è¶Šå¤šè¯­ç§æ¨¡å‹çš„çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMoonshineçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹ä¸€äº›ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€è¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é«˜è´¨é‡äººå·¥æ ‡æ³¨ã€ä¼ªæ ‡æ³¨å’Œåˆæˆæ•°æ®çš„å•è¯­æ¨¡å‹åœ¨å°å‹æ¨¡å‹ï¼ˆ27Må‚æ•°ï¼‰ä¸­è¡¨ç°ä¼˜äºå¤šè¯­ç§æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨é”™è¯¯ç‡ä¸Šå¹³å‡æ¯”åŒç­‰å¤§å°çš„Whisper Tinyæ¨¡å‹ä½48%ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½å¤Ÿä¸æ›´å¤§æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚è¿™äº›æˆæœæ¨åŠ¨äº†å°å‹æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥ï¼Œä½¿å¾—åœ¨è®¾å¤‡ä¸Šå®ç°å‡†ç¡®çš„è¯­éŸ³è¯†åˆ«æˆä¸ºå¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02379",
            "title": "MedDINOv3: How to adapt vision foundation models for medical image\n  segmentation?",
            "url": "https://huggingface.co/papers/2509.02379",
            "abstract": "MedDINOv3, a framework adapting DINOv3 with multi-scale token aggregation, achieves state-of-the-art performance in medical image segmentation by overcoming challenges in domain adaptation and backbone performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3.",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "bbf123f2c298e53a",
            "authors": [
                "Yuheng Li",
                "Yizhou Wu",
                "Yuxiang Lai",
                "Mingzhe Hu",
                "Xiaofeng Yang"
            ],
            "affiliations": [
                "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta",
                "Department of Computer Science, Emory University, Atlanta",
                "Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta",
                "Department of Radiation Oncology, Emory University School of Medicine, Atlanta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02379.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#healthcare"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MedDINOv3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¾Ñ€ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "MedDINOv3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINOv3. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. MedDINOv3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° ĞšĞ¢ Ğ¸ ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Medical Image Segmentation with MedDINOv3",
                    "desc": "MedDINOv3 is a new framework that enhances the DINOv3 model for better medical image segmentation, particularly in CT and MRI scans. It addresses the challenges of adapting vision foundation models to medical imaging by using multi-scale token aggregation and domain-adaptive pretraining. This approach allows the model to learn robust features from a large dataset of CT images, improving its performance compared to traditional CNNs. As a result, MedDINOv3 achieves state-of-the-art results in various segmentation tasks, showcasing the effectiveness of using advanced vision models in the medical field."
                },
                "zh": {
                    "title": "MedDINOv3ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°çªç ´",
                    "desc": "MedDINOv3æ˜¯ä¸€ä¸ªå°†DINOv3ä¸å¤šå°ºåº¦æ ‡è®°èšåˆç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸé€‚åº”å’Œä¸»å¹²ç½‘ç»œæ€§èƒ½é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨CT-3Mæ•°æ®é›†ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå­¦ä¹ åˆ°å¼ºå¤§çš„å¯†é›†ç‰¹å¾ï¼Œä»è€Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹å¯ä»¥ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»Ÿä¸€ä¸»å¹²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01790",
            "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs",
            "url": "https://huggingface.co/papers/2509.01790",
            "abstract": "Modern LLMs exhibit less prompt sensitivity than previously thought, with much of the reported variability due to heuristic evaluation methods rather than inherent model weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.",
            "score": 1,
            "issue_id": 5697,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "2b3ed7c45180a009",
            "authors": [
                "Andong Hua",
                "Kenan Tang",
                "Chenhe Gu",
                "Jindong Gu",
                "Eric Wong",
                "Yao Qin"
            ],
            "affiliations": [
                "UC Irvine",
                "UC Santa Barbara",
                "University of Oxford",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01790.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ½Ğ¸Ğ¼Ğ°Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¼ĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡ĞµĞ¼ ÑÑ‡Ğ¸Ñ‚Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ½ĞµĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 7 LLM Ğ½Ğ° 6 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 12 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ½Ğµ Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Rethinking Prompt Sensitivity: It's Not the Models, It's the Evaluation!",
                    "desc": "This paper investigates the concept of prompt sensitivity in large language models (LLMs), which is the idea that changing the wording of a prompt can significantly affect the model's performance. The authors argue that much of the perceived variability in LLM responses is not due to weaknesses in the models themselves, but rather due to the evaluation methods used to assess them. By systematically testing multiple LLMs across various benchmarks and using more flexible evaluation techniques, they find that the models are actually more consistent and robust than previously thought. This suggests that the high prompt sensitivity reported in earlier studies may be an artifact of rigid evaluation processes rather than an inherent flaw in the models."
                },
                "zh": {
                    "title": "ç°ä»£LLMçš„æç¤ºæ•æ„Ÿæ€§è¢«ä½ä¼°äº†",
                    "desc": "ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºçš„æç¤ºæ•æ„Ÿæ€§æ¯”ä¹‹å‰è®¤ä¸ºçš„è¦ä½ï¼Œå¾ˆå¤šæŠ¥å‘Šçš„å˜åŒ–æ˜¯ç”±äºå¯å‘å¼è¯„ä¼°æ–¹æ³•é€ æˆçš„ï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«çš„ç¼ºé™·ã€‚æç¤ºæ•æ„Ÿæ€§æ˜¯æŒ‡é€šè¿‡ä¸åŒçš„æªè¾é‡å¤å†…å®¹æ—¶ï¼ŒLLMæ€§èƒ½å‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„ç°è±¡ã€‚æˆ‘ä»¬å¯¹7ä¸ªLLMè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å¾ˆå¤šæç¤ºæ•æ„Ÿæ€§æºäºè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œæ¯”å¦‚å¯¹è¯­ä¹‰æ­£ç¡®çš„æ›¿ä»£è¡¨è¾¾çš„å¿½è§†ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°ä»£LLMå¯¹æç¤ºæ¨¡æ¿çš„é²æ£’æ€§æ¯”ä¹‹å‰è®¤ä¸ºçš„è¦å¼ºï¼Œæç¤ºæ•æ„Ÿæ€§å¯èƒ½æ›´å¤šæ˜¯è¯„ä¼°è¿‡ç¨‹çš„äº§ç‰©ï¼Œè€Œéæ¨¡å‹çš„ç¼ºé™·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01610",
            "title": "Improving Large Vision and Language Models by Learning from a Panel of\n  Peers",
            "url": "https://huggingface.co/papers/2509.01610",
            "abstract": "A Panel-of-Peers learning framework enhances Large Vision and Language Models by simulating peer reviews, improving performance without extensive human-labeled datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "3edfce8f16cf09cb",
            "authors": [
                "Jefferson Hernandez",
                "Jing Shi",
                "Simon Jenni",
                "Vicente Ordonez",
                "Kushal Kafle"
            ],
            "affiliations": [
                "Adobe Research",
                "Rice University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01610.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#rlhf",
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Panel-of-Peers. ĞĞ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Models through Peer Learning",
                    "desc": "The paper introduces a Panel-of-Peers learning framework that enhances Large Vision and Language Models (LVLMs) by simulating peer reviews. This method addresses the challenges of relying on costly human-curated preference data and the limitations of machine-generated and self-supervised data. By allowing multiple LVLMs to evaluate and learn from each other's outputs, the framework fosters an iterative self-improvement process. The results show significant performance improvements across various benchmarks, highlighting the effectiveness of peer evaluations as a scalable alternative to traditional alignment methods."
                },
                "zh": {
                    "title": "åŒä¼´è¯„å®¡ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºâ€œåŒä¼´è¯„å®¡å­¦ä¹ æ¡†æ¶â€ï¼Œæ—¨åœ¨æå‡å¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹ŸåŒä¼´è¯„å®¡çš„è¿‡ç¨‹ï¼Œä½¿å¤šä¸ªLVLMç›¸äº’è¯„ä¼°å’Œå­¦ä¹ ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººç±»æ ‡æ³¨æ•°æ®ä¸åŒï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤§é‡äººç±»æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¹³å‡å¾—åˆ†ï¼Œå±•ç¤ºäº†åŒä¼´è¯„å®¡ä½œä¸ºä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£å¯¹é½æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01584",
            "title": "ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association",
            "url": "https://huggingface.co/papers/2509.01584",
            "abstract": "ViSTA-SLAM is a real-time monocular SLAM system that uses a lightweight STA model for pose estimation and pointmap regression, and a Sim(3) pose graph for drift correction, achieving superior tracking and reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ViSTA-SLAM as a real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs a lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35\\% that of comparable state-of-the-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct a specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/vista-slam",
            "score": 1,
            "issue_id": 5690,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "daefaefcec9f3e8c",
            "authors": [
                "Ganlin Zhang",
                "Shenhan Qian",
                "Xi Wang",
                "Daniel Cremers"
            ],
            "affiliations": [
                "ETH Zurich",
                "MCML",
                "TU Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01584.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ°Ñ SLAM-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹",
                    "desc": "ViSTA-SLAM - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½ÑƒÑ ĞºĞ°Ğ¼ĞµÑ€Ñƒ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¸ (STA) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ’ backend Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ¾Ğ· Sim(3) Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°."
                },
                "en": {
                    "title": "ViSTA-SLAM: Lightweight and Intrinsic-Free Monocular SLAM for Superior Tracking and Reconstruction",
                    "desc": "ViSTA-SLAM is a real-time monocular SLAM system that simplifies pose estimation and pointmap regression using a lightweight symmetric two-view association (STA) model. This model allows the system to operate without needing camera intrinsics, making it versatile for various camera types. The backend features a Sim(3) pose graph that effectively corrects drift by incorporating loop closures, enhancing overall tracking accuracy. Experimental results show that ViSTA-SLAM outperforms existing methods in both camera tracking and dense 3D reconstruction."
                },
                "zh": {
                    "title": "ViSTA-SLAMï¼šé«˜æ•ˆçš„å®æ—¶å•ç›®è§†è§‰SLAMç³»ç»Ÿ",
                    "desc": "ViSTA-SLAMæ˜¯ä¸€ç§å®æ—¶å•ç›®è§†è§‰SLAMç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ç›¸æœºå†…å‚çš„æƒ…å†µä¸‹è¿è¡Œï¼Œé€‚ç”¨äºå¤šç§ç›¸æœºè®¾ç½®ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¯¹ç§°åŒè§†å›¾å…³è”ï¼ˆSTAï¼‰æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ä¼°è®¡ç›¸å¯¹ç›¸æœºå§¿æ€å¹¶ä»ä¸¤å¼ RGBå›¾åƒä¸­å›å½’å±€éƒ¨ç‚¹äº‘å›¾ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹å¤æ‚åº¦æ˜¾è‘—é™ä½ï¼Œå‰ç«¯çš„å¤§å°ä»…ä¸ºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„35%ï¼ŒåŒæ—¶æé«˜äº†ç®¡é“ä¸­ä½¿ç”¨çš„åŒè§†å›¾çº¦æŸçš„è´¨é‡ã€‚åœ¨åç«¯ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç‰¹åˆ«è®¾è®¡çš„Sim(3)å§¿æ€å›¾ï¼Œç»“åˆäº†å›ç¯é—­åˆæ¥è§£å†³ç´¯ç§¯æ¼‚ç§»é—®é¢˜ï¼Œå®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸æœºè·Ÿè¸ªå’Œç¨ å¯†3Dé‡å»ºè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01250",
            "title": "Towards More Diverse and Challenging Pre-training for Point Cloud\n  Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
            "url": "https://huggingface.co/papers/2509.01250",
            "abstract": "Point-PQAE, a two-view cross-reconstruction generative paradigm, enhances 3D self-supervised learning by introducing greater diversity and variance, outperforming single-view methods in point cloud reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE.",
            "score": 1,
            "issue_id": 5685,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "3fffc03e184237e0",
            "authors": [
                "Xiangdong Zhang",
                "Shaofeng Zhang",
                "Junchi Yan"
            ],
            "affiliations": [
                "School of AI, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01250.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#synthetic"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Point-PQAE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Point-PQAE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 7% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanObjectNN."
                },
                "en": {
                    "title": "Enhancing 3D Learning with Two-View Cross-Reconstruction",
                    "desc": "Point-PQAE is a novel approach in 3D self-supervised learning that utilizes a two-view cross-reconstruction method to enhance point cloud reconstruction tasks. By generating two separate point clouds and reconstructing one from the other, it introduces greater diversity and variance compared to traditional single-view methods. This method employs a unique crop mechanism for generating views and a new positional encoding to capture the 3D relationships between the views. As a result, Point-PQAE significantly outperforms existing self-reconstruction techniques, demonstrating improved performance in various evaluation scenarios."
                },
                "zh": {
                    "title": "åŒè§†å›¾å­¦ä¹ ï¼Œæå‡3Dè‡ªç›‘ç£é‡å»ºæ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPoint-PQAEçš„è·¨é‡å»ºç”ŸæˆèŒƒå¼ï¼Œæ—¨åœ¨å¢å¼º3Dè‡ªç›‘ç£å­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åŒè§†å›¾çš„å­¦ä¹ æ–¹å¼ï¼Œå¢åŠ äº†ç‚¹äº‘é‡å»ºä»»åŠ¡ä¸­çš„å¤šæ ·æ€§å’Œæ–¹å·®ï¼Œè¶…è¶Šäº†å•è§†å›¾æ–¹æ³•çš„è¡¨ç°ã€‚æˆ‘ä»¬é¦–æ¬¡å¼€å‘äº†ä¸€ç§ç‚¹äº‘è§†å›¾ç”Ÿæˆçš„è£å‰ªæœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä½ç½®ç¼–ç æ¥è¡¨ç¤ºä¸¤ä¸ªè§£è€¦è§†å›¾ä¹‹é—´çš„3Dç›¸å¯¹ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoint-PQAEåœ¨ScanObjectNNçš„ä¸‰ä¸ªå˜ä½“ä¸­ï¼Œåˆ†åˆ«æ¯”è‡ªé‡å»ºåŸºçº¿ï¼ˆPoint-MAEï¼‰æé«˜äº†6.5%ã€7.0%å’Œ6.7%çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00578",
            "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Object Detection",
            "url": "https://huggingface.co/papers/2509.00578",
            "abstract": "Context-Aware Fusion enhances DiffusionDet by integrating global scene context with local features using cross-attention, improving performance on fine-grained object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains",
            "score": 1,
            "issue_id": 5687,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "191ffb7e3bd4bec6",
            "authors": [
                "Abdellah Zakaria Sellam",
                "Ilyes Benaissa",
                "Salah Eddine Bekhouche",
                "Abdenour Hadid",
                "Vito RenÃ³",
                "Cosimo Distante"
            ],
            "affiliations": [
                "CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy",
                "Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
                "Sorbonne University Abu Dhabi, UAE",
                "UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00578.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context-Aware Fusion (CAF) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiffusionDet Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². CAF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CarDD Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Object Detection with Context-Aware Fusion",
                    "desc": "This paper introduces Context-Aware Fusion (CAF) to enhance the DiffusionDet model for fine-grained object detection. CAF uses cross-attention mechanisms to combine global scene context with local features, addressing the limitations of local feature conditioning. By employing a dedicated encoder to capture environmental information, the model allows object proposals to utilize a broader understanding of the scene. Experimental results show that this approach significantly improves performance on the CarDD benchmark, setting new standards for context-aware object detection."
                },
                "zh": {
                    "title": "ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆæå‡ç»†ç²’åº¦ç‰©ä½“æ£€æµ‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆï¼ˆContext-Aware Fusion, CAFï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡DiffusionDetåœ¨ç»†ç²’åº¦ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚CAFé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å…¨å±€åœºæ™¯ä¸Šä¸‹æ–‡ä¸å±€éƒ¨ç‰¹å¾ç›¸ç»“åˆï¼Œä»è€Œå…‹æœäº†DiffusionDetåœ¨ä¸Šä¸‹æ–‡ä¾èµ–åœºæ™¯ä¸­çš„å±€éƒ¨ç‰¹å¾é™åˆ¶ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸“é—¨çš„ç¼–ç å™¨ç”Ÿæˆå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ•æ‰å…¨é¢çš„ç¯å¢ƒä¿¡æ¯ï¼Œä½¿æ¯ä¸ªç‰©ä½“æè®®èƒ½å¤Ÿå…³æ³¨åœºæ™¯çº§ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAFåœ¨CarDDåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸ºç»†ç²’åº¦é¢†åŸŸçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰©ä½“æ£€æµ‹è®¾ç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00404",
            "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
            "url": "https://huggingface.co/papers/2509.00404",
            "abstract": "Metis addresses training instability in low-bit quantized large language models by using spectral decomposition, adaptive learning rates, and dual-range regularization to improve performance and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization.",
            "score": 1,
            "issue_id": 5688,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "122a54575f7764a1",
            "authors": [
                "Hengjie Cao",
                "Mengyi Chen",
                "Yifeng Yang",
                "Ruijun Huang",
                "Fang Dong",
                "Jixian Zhou",
                "Anrui Chen",
                "Mingzhi Dong",
                "Yujiang Wang",
                "Jinlong Hou",
                "Yuan Cheng",
                "Fan Wu",
                "Fan Yang",
                "Tun Lu",
                "Ning Gu",
                "Li Shang"
            ],
            "affiliations": [
                "Fudan University",
                "Huawei",
                "Oxford Suzhou Centre for Advanced Research",
                "Shanghai Innovation Institute",
                "University of Bath"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00404.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Metis - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ²ÑƒÑ…Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Metis Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ñ 32-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹, Ğ° Ñ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ - ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾ Ñ 32-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹."
                },
                "en": {
                    "title": "Metis: Stabilizing Low-Bit Training for Large Language Models",
                    "desc": "This paper presents Metis, a novel training framework designed to enhance the stability and performance of low-bit quantized large language models (LLMs). It addresses the issue of anisotropic parameter distributions that hinder effective training by utilizing spectral decomposition to separate dominant and minor components, allowing for better quantization. Additionally, Metis employs adaptive learning rates to focus on underrepresented features, improving model accuracy. The framework also incorporates a dual-range regularization technique to maintain numerical precision and ensure stable training, resulting in significant performance improvements over traditional FP32 training methods."
                },
                "zh": {
                    "title": "Metisï¼šæå‡ä½æ¯”ç‰¹é‡åŒ–æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ä¸æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Metisæ˜¯ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä½æ¯”ç‰¹é‡åŒ–å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚å®ƒé€šè¿‡è°±åˆ†è§£ã€é€‚åº”æ€§å­¦ä¹ ç‡å’ŒåŒèŒƒå›´æ­£åˆ™åŒ–æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå‚æ•°åˆ†å¸ƒçš„å„å‘å¼‚æ€§æ˜¯ä½æ¯”ç‰¹é‡åŒ–è®­ç»ƒçš„ä¸»è¦éšœç¢ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å‹æ€§èƒ½ä½ä¸‹ã€‚Metisé€šè¿‡æœ‰æ•ˆåœ°åˆ†ç¦»ä¸»å¯¼æˆåˆ†å’Œé•¿å°¾æˆåˆ†ï¼Œå‹ç¼©åˆ†å¸ƒèŒƒå›´ï¼Œä»è€Œå®ç°äº†æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20586",
            "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2508.20586",
            "abstract": "FastFit, a high-speed virtual try-on framework using a cacheable diffusion architecture with a Semi-Attention mechanism, achieves significant speedup and maintains high fidelity in multi-reference outfit compositions.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.",
            "score": 1,
            "issue_id": 5691,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "fefbcacccb51e5ca",
            "authors": [
                "Zheng Chong",
                "Yanwei Lei",
                "Shiyue Zhang",
                "Zhuandi He",
                "Zhen Wang",
                "Xujie Zhang",
                "Xiao Dong",
                "Yiling Wu",
                "Dongmei Jiang",
                "Xiaodan Liang"
            ],
            "affiliations": [
                "LavieAI",
                "Pengcheng Laboratory",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20586.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#inference",
                    "#open_source",
                    "#dataset",
                    "#diffusion",
                    "#data"
                ],
                "emoji": "ğŸ‘š",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ñ FastFit",
                    "desc": "FastFit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑƒĞ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (Semi-Attention). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. FastFit Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ğ³Ğ°Ñ€Ğ´ĞµÑ€Ğ¾Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DressCode-MR Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "FastFit: Speeding Up Virtual Try-Ons with Smart Caching!",
                    "desc": "FastFit is a virtual try-on framework that enhances the speed and quality of outfit compositions by using a cacheable diffusion architecture combined with a Semi-Attention mechanism. It addresses the inefficiencies of existing methods by allowing reference features to be computed once and reused, which significantly reduces redundant calculations during the denoising process. This innovation leads to an impressive average speedup of 3.5 times compared to other techniques while maintaining high fidelity in the generated images. Additionally, the introduction of the DressCode-MR dataset supports further research in multi-reference virtual try-on applications, providing a rich resource for training and evaluation."
                },
                "zh": {
                    "title": "FastFitï¼šé«˜æ•ˆè™šæ‹Ÿè¯•è¡£çš„æ–°çªç ´",
                    "desc": "FastFitæ˜¯ä¸€ç§é«˜é€Ÿåº¦çš„è™šæ‹Ÿè¯•è¡£æ¡†æ¶ï¼Œé‡‡ç”¨å¯ç¼“å­˜çš„æ‰©æ•£æ¶æ„å’ŒåŠæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šå‚è€ƒæœè£…ç»„åˆä¸­å®ç°æ˜¾è‘—çš„åŠ é€Ÿå¹¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚è¯¥æŠ€æœ¯è§£å†³äº†å½“å‰æ–¹æ³•åœ¨å¤šå‚è€ƒæœè£…ç»„åˆæ”¯æŒå’Œæ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å†—ä½™é‡æ–°è®¡ç®—å‚è€ƒç‰¹å¾çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚é€šè¿‡å°†ä¼ ç»Ÿçš„æ—¶é—´æ­¥åµŒå…¥æ›¿æ¢ä¸ºå‚è€ƒé¡¹ç›®çš„ç±»åˆ«åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å‚è€ƒç‰¹å¾ç¼–ç ä¸å»å™ªè¿‡ç¨‹çš„å®Œå…¨è§£è€¦ï¼Œä»è€Œåœ¨æ‰€æœ‰æ­¥éª¤ä¸­ä»…è®¡ç®—ä¸€æ¬¡å‚è€ƒç‰¹å¾å¹¶æ— æŸé‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastFitåœ¨å…³é”®ä¿çœŸåº¦æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¨ç†æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-02.html",
    "link_next": "2025-09-04.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "04.09",
        "en": "09/04",
        "zh": "9æœˆ4æ—¥"
    },
    "categories": {
        "#dataset": 13,
        "#data": 8,
        "#benchmark": 21,
        "#agents": 9,
        "#cv": 6,
        "#rl": 11,
        "#rlhf": 5,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 2,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 6,
        "#healthcare": 3,
        "#training": 16,
        "#robotics": 0,
        "#agi": 2,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 14,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 0,
        "#optimization": 23,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 6,
        "#story_generation": 1,
        "#hallucinations": 2,
        "#long_context": 4,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 2
    }
}