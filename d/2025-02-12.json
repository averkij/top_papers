{
    "date": {
        "ru": "12 февраля",
        "en": "February 12",
        "zh": "2月12日"
    },
    "time_utc": "2025-02-12 09:11",
    "weekday": 2,
    "issue_id": 2169,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.06807",
            "title": "Competitive Programming with Large Reasoning Models",
            "url": "https://huggingface.co/papers/2502.06807",
            "abstract": "We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.",
            "score": 25,
            "issue_id": 2164,
            "pub_date": "2025-02-03",
            "pub_date_card": {
                "ru": "3 февраля",
                "en": "February 3",
                "zh": "2月3日"
            },
            "hash": "fd76cceb75f32321",
            "authors": [
                "OpenAI",
                ":",
                "Ahmed El-Kishky",
                "Alexander Wei",
                "Andre Saraiva",
                "Borys Minaev",
                "Daniel Selsam",
                "David Dohan",
                "Francis Song",
                "Hunter Lightman",
                "Ignasi Clavera",
                "Jakub Pachocki",
                "Jerry Tworek",
                "Lorenz Kuhn",
                "Lukasz Kaiser",
                "Mark Chen",
                "Max Schwarzer",
                "Mostafa Rohaninejad",
                "Nat McAleese",
                "o3 contributors",
                "Oleg Mürk",
                "Rhythm Garg",
                "Rui Shu",
                "Szymon Sidor",
                "Vineet Kosaraju",
                "Wenda Zhou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2502.06807.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#games",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обучение с подкреплением превосходит специализированные подходы в задачах рассуждения",
                    "desc": "Исследование показывает, что применение обучения с подкреплением к большим языковым моделям (LLM) значительно улучшает их производительность в сложных задачах программирования и рассуждения. Авторы сравнивают модели общего назначения (OpenAI o1 и o3) со специализированной системой o1-ioi, разработанной для участия в Международной олимпиаде по информатике (IOI) 2024 года. Модель o3 достигла уровня золотой медали на IOI 2024 без использования специфических стратегий или послаблений правил. Результаты указывают на то, что масштабирование обучения с подкреплением общего назначения является более эффективным подходом к созданию ИИ для задач рассуждения, чем разработка узкоспециализированных техник."
                },
                "en": {
                    "title": "Scaling General-Purpose Learning Outshines Specialized Strategies",
                    "desc": "This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming."
                },
                "zh": {
                    "title": "强化学习助力通用模型超越特定领域系统",
                    "desc": "本论文展示了强化学习在大型语言模型（LLMs）中的应用，显著提升了复杂编码和推理任务的表现。我们比较了两种通用推理模型——OpenAI的o1和o3的早期检查点，以及一个特定领域的系统o1-ioi，该系统使用手工设计的推理策略。o1-ioi在2024年国际信息学奥林匹克竞赛中表现良好，获得了第49百分位的成绩，而在放宽竞争约束的情况下则获得了金牌。我们的研究表明，尽管专门的管道如o1-ioi能带来显著提升，但扩展的通用o3模型在没有依赖手工推理启发式的情况下，超越了这些结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07316",
            "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
            "url": "https://huggingface.co/papers/2502.07316",
            "abstract": "Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.",
            "score": 11,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "fd4f34152d4de2c1",
            "authors": [
                "Junlong Li",
                "Daya Guo",
                "Dejian Yang",
                "Runxin Xu",
                "Yu Wu",
                "Junxian He"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "HKUST",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07316.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CodeI/O: Раскрытие потенциала рассуждений в больших языковых моделях через код",
                    "desc": "Статья представляет новый подход CodeI/O для улучшения способностей больших языковых моделей к рассуждениям. Метод преобразует разнообразные паттерны рассуждений, встроенные в код, в формат предсказания ввода-вывода кода на естественном языке. Это позволяет моделям изучать универсальные примитивы рассуждений, такие как планирование логического потока и модульная декомпозиция. Эксперименты показывают, что CodeI/O приводит к улучшениям в задачах символических, научных, логических и других типов рассуждений."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with CodeI/O",
                    "desc": "This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++."
                },
                "zh": {
                    "title": "CodeI/O：提升推理能力的新方法",
                    "desc": "这篇论文提出了一种新的方法，称为CodeI/O，旨在提高大型语言模型的推理能力。通过将原始代码转换为输入输出预测格式，CodeI/O系统地提炼了多样的推理模式。模型通过自然语言的链式思维（CoT）推理来预测代码的输入和输出，从而增强了逻辑流规划、状态空间搜索等推理原语的能力。实验结果表明，CodeI/O在多种推理任务上均表现出一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03492",
            "title": "Teaching Language Models to Critique via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2502.03492",
            "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.",
            "score": 9,
            "issue_id": 2165,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 февраля",
                "en": "February 5",
                "zh": "2月5日"
            },
            "hash": "a0c98706806837c6",
            "authors": [
                "Zhihui Xie",
                "Jie chen",
                "Liyu Chen",
                "Weichao Mao",
                "Jingjing Xu",
                "Lingpeng Kong"
            ],
            "affiliations": [
                "Bytedance, Seed",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03492.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самосовершенствующиеся ИИ-критики для генерации кода",
                    "desc": "Статья представляет CTRL - фреймворк для обучения моделей-критиков с помощью обучения с подкреплением. Цель - научить языковые модели (LLM) генерировать полезную обратную связь для улучшения качества генерируемого кода без участия человека. Результаты показывают, что обученные критики значительно повышают процент успешных решений и уменьшают накопление ошибок. Модели-критики также могут использоваться как генеративные модели вознаграждения и позволяют улучшать результаты во время тестирования через итеративную критику и исправления."
                },
                "en": {
                    "title": "Empowering Code Generation with Self-Critiquing Models",
                    "desc": "This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks."
                },
                "zh": {
                    "title": "通过批评训练提升代码生成性能",
                    "desc": "本文研究了大型语言模型（LLMs）在代码生成中的批评和改进能力。我们提出了CTRL框架，通过强化学习训练批评模型，生成反馈以提高固定生成模型的修正性能，而无需人工监督。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并减少了累积错误。此外，这些批评模型作为生成奖励模型，能够在测试时通过迭代批评-修订实现扩展，在具有挑战性的代码生成基准上实现了高达106.1%的相对改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07617",
            "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
            "url": "https://huggingface.co/papers/2502.07617",
            "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.",
            "score": 7,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "503a9dac2cae323c",
            "authors": [
                "Xiao Wang",
                "Ibrahim Alabdulmohsin",
                "Daniel Salz",
                "Zhe Li",
                "Keran Rong",
                "Xiaohua Zhai"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07617.jpg",
            "data": {
                "categories": [
                    "#cultural_diversity",
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Масштабное предобучение для инклюзивных мультимодальных систем",
                    "desc": "Исследование посвящено предобучению мультимодальных моделей на беспрецедентно большом наборе данных в 100 миллиардов примеров. Авторы обнаружили, что производительность модели на многих западноцентричных бенчмарках насыщается при таком масштабе. Однако задачи, связанные с культурным разнообразием, показывают значительный прирост благодаря охвату редких концепций в больших данных. Кроме того, исследование выявило улучшение производительности для низкоресурсных языков и предостерегает от чрезмерной фильтрации данных, которая может снизить культурное разнообразие."
                },
                "en": {
                    "title": "Unlocking Cultural Diversity with 100 Billion Examples",
                    "desc": "This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems."
                },
                "zh": {
                    "title": "大规模预训练助力文化多样性",
                    "desc": "本文探讨了在前所未有的规模上（1000亿个示例）对视觉-语言模型进行预训练的潜力。研究发现，在许多常见的西方分类和检索基准上，模型性能在此规模下趋于饱和。然而，对于文化多样性的任务，1000亿规模的网络数据带来了更显著的提升，因为它涵盖了长尾概念。此外，研究还分析了模型的多语言能力，显示在低资源语言上也有提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06329",
            "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
            "url": "https://huggingface.co/papers/2502.06329",
            "abstract": "We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA",
            "score": 6,
            "issue_id": 2168,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 февраля",
                "en": "February 10",
                "zh": "2月10日"
            },
            "hash": "836d77158c8a414b",
            "authors": [
                "Kiran Kamble",
                "Melisa Russak",
                "Dmytro Mozolevskyi",
                "Muayad Ali",
                "Mateusz Russak",
                "Waseem AlShikh"
            ],
            "affiliations": [
                "Writer, Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06329.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#long_context",
                    "#hallucinations"
                ],
                "emoji": "💼",
                "ru": {
                    "title": "FailSafeQA: Новый стандарт оценки надежности языковых моделей в финансах",
                    "desc": "Исследователи представили новый эталонный тест FailSafeQA для оценки устойчивости и контекстной осведомленности языковых моделей (LLM) в финансовой сфере. Тест включает шесть вариаций взаимодействия человека с системой, основанной на LLM, и фокусируется на двух сценариях: отказ запроса и отказ контекста. Используя методологию LLM-as-a-Judge и модель Qwen2.5-72B-Instruct, авторы оценили 24 готовые модели по критериям устойчивости, контекстной обоснованности и соответствия. Результаты показали, что даже высокопроизводительные модели имеют значительный потенциал для улучшения, и подчеркнули важность FailSafeQA как инструмента для разработки более надежных LLM в финансовых приложениях."
                },
                "en": {
                    "title": "Enhancing LLM Reliability in Finance with FailSafeQA",
                    "desc": "The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts."
                },
                "zh": {
                    "title": "FailSafeQA：提升金融领域LLM的鲁棒性与上下文意识",
                    "desc": "我们提出了一个新的长上下文金融基准，FailSafeQA，旨在测试大型语言模型（LLMs）在金融领域中对人机交互的鲁棒性和上下文意识。我们关注两个案例研究：查询失败和上下文失败。在查询失败场景中，我们通过改变领域专业性、完整性和语言准确性来扰动原始查询。在上下文失败案例中，我们模拟上传降级、无关和空文档的情况。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07701",
            "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
            "url": "https://huggingface.co/papers/2502.07701",
            "abstract": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
            "score": 6,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "7212b752112bcd5a",
            "authors": [
                "Hongwei Yi",
                "Shitong Shao",
                "Tian Ye",
                "Jiantong Zhao",
                "Qingyu Yin",
                "Michael Lingelbach",
                "Li Yuan",
                "Yonghong Tian",
                "Enze Xie",
                "Daquan Zhou"
            ],
            "affiliations": [
                "Hedra Inc.",
                "Nvidia",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07701.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#inference",
                    "#multimodal",
                    "#open_source",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективная генерация видео: от текста к кадрам за секунды",
                    "desc": "Magic 1-For-1 (Magic141) - это эффективная модель генерации видео с оптимизированным потреблением памяти и латентностью вывода. Ключевая идея заключается в разделении задачи генерации видео по тексту на две более простые задачи: генерацию изображения по тексту и генерацию видео по изображению. Авторы применяют ряд оптимизационных приемов, включая многомодальное введение условий, состязательную дистилляцию шагов и разреживание параметров. В результате модель способна генерировать 5-секундные видеоклипы менее чем за 3 секунды, а минутное видео - за одну минуту с улучшенным качеством и динамикой."
                },
                "en": {
                    "title": "Efficient Video Generation: Simplifying with Magic141",
                    "desc": "The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality."
                },
                "zh": {
                    "title": "高效视频生成，轻松实现！",
                    "desc": "本文介绍了一种高效的视频生成模型Magic 1-For-1（Magic141），该模型优化了内存消耗和推理延迟。其核心思想是将文本到视频生成任务分解为两个更简单的任务：文本到图像生成和图像到视频生成。研究表明，使用相同的优化算法，图像到视频任务的收敛速度确实优于文本到视频任务。通过多种优化技巧，模型能够在短时间内生成高质量的视频片段，显著降低计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07527",
            "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
            "url": "https://huggingface.co/papers/2502.07527",
            "abstract": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
            "score": 5,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "a6e947f52bde9a9c",
            "authors": [
                "Yingce Xia",
                "Peiran Jin",
                "Shufang Xie",
                "Liang He",
                "Chuan Cao",
                "Renqian Luo",
                "Guoqing Liu",
                "Yue Wang",
                "Zequn Liu",
                "Yuan-Jyue Chen",
                "Zekun Guo",
                "Yeqi Bai",
                "Pan Deng",
                "Yaosen Min",
                "Ziheng Lu",
                "Hongxia Hao",
                "Han Yang",
                "Jielan Li",
                "Chang Liu",
                "Jia Zhang",
                "Jianwei Zhu",
                "Kehan Wu",
                "Wei Zhang",
                "Kaiyuan Gao",
                "Qizhi Pei",
                "Qian Wang",
                "Xixian Liu",
                "Yanting Li",
                "Houtian Zhu",
                "Yeqing Lu",
                "Mingqian Ma",
                "Zun Wang",
                "Tian Xie",
                "Krzysztof Maziarz",
                "Marwin Segler",
                "Zhao Yang",
                "Zilong Chen",
                "Yu Shi",
                "Shuxin Zheng",
                "Lijun Wu",
                "Chen Hu",
                "Peggy Dai",
                "Tie-Yan Liu",
                "Haiguang Liu",
                "Tao Qin"
            ],
            "affiliations": [
                "Microsoft Research AI for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07527.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#architecture",
                    "#science",
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "NatureLM: единая модель для научных открытий во множестве доменов",
                    "desc": "Исследователи разработали NatureLM - языковую модель для научных открытий, объединяющую различные научные домены. Эта фундаментальная модель обучена на данных из нескольких областей, включая молекулы, материалы, белки, ДНК и РНК. NatureLM способна генерировать и оптимизировать объекты из разных доменов, а также выполнять кросс-доменные задачи. Модель демонстрирует высокую производительность в различных научных задачах и доступна в нескольких размерах, от 1 до 46,7 миллиардов параметров."
                },
                "en": {
                    "title": "NatureLM: Unifying Science Through Language Models",
                    "desc": "This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design."
                },
                "zh": {
                    "title": "自然语言模型：科学发现的新工具",
                    "desc": "基础模型在自然语言处理和人工智能领域带来了革命性的变化，显著提升了机器理解和生成自然语言的能力。受基础模型成功的启发，研究人员为各个科学领域开发了相应的基础模型，但这些模型通常是孤立训练的，缺乏跨领域整合的能力。我们提出了自然语言模型（NatureLM），这是一个基于序列的科学基础模型，旨在促进科学发现。NatureLM经过多领域数据的预训练，能够支持小分子、蛋白质、RNA和材料的生成与优化，并在多个科学任务中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06589",
            "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
            "url": "https://huggingface.co/papers/2502.06589",
            "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
            "score": 5,
            "issue_id": 2164,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 февраля",
                "en": "February 10",
                "zh": "2月10日"
            },
            "hash": "4273eabfdc59b328",
            "authors": [
                "Yuchen Zhuang",
                "Jingfeng Yang",
                "Haoming Jiang",
                "Xin Liu",
                "Kewei Cheng",
                "Sanket Lokegaonkar",
                "Yifan Gao",
                "Qing Ping",
                "Tianyi Liu",
                "Binxuan Huang",
                "Zheng Li",
                "Zhengyang Wang",
                "Pei Chen",
                "Ruijie Wang",
                "Rongzhi Zhang",
                "Nasser Zalmout",
                "Priyanka Nigam",
                "Bing Yin",
                "Chao Zhang"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06589.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#transfer_learning",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Кузница агентов: улучшение LLM через специализированное предобучение",
                    "desc": "Исследователи представили Hephaestus-Forge - первый крупномасштабный корпус для предобучения, направленный на улучшение фундаментальных способностей агентов на основе больших языковых моделей (LLM). Корпус содержит 103 миллиарда специфичных для агентов данных, охватывающих 76,537 API, включая документацию и траектории вызовов функций. Применение Hephaestus-Forge в процессе дообучения позволило модели Hephaestus превзойти открытые LLM малого и среднего масштаба и конкурировать с коммерческими LLM в трех тестах для агентов. Это демонстрирует эффективность предложенного подхода в улучшении базовых агентных возможностей и обобщающей способности LLM для новых задач и сред."
                },
                "en": {
                    "title": "Empowering LLM Agents with Hephaestus-Forge",
                    "desc": "This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability."
                },
                "zh": {
                    "title": "Hephaestus-Forge：提升LLM代理能力的创新预训练语料库",
                    "desc": "由于缺乏面向代理的预训练数据，基于大型语言模型（LLM）的自主代理通常依赖复杂的提示或广泛的微调，这往往无法在保持强泛化能力的同时引入新功能。我们提出了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强LLM代理在API功能调用、内在推理和规划以及适应环境反馈方面的基本能力。Hephaestus-Forge包含1030亿个特定于代理的数据，涵盖76,537个API，包括工具文档以介绍API功能的知识和功能调用轨迹以增强内在推理。通过在Hephaestus-Forge上持续预训练，Hephaestus在三个代理基准测试中超越了小到中型的开源LLM，并与商业LLM相媲美，证明了我们的预训练语料库在增强代理基本能力和LLM对新任务或环境的泛化能力方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07374",
            "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
            "url": "https://huggingface.co/papers/2502.07374",
            "abstract": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
            "score": 5,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "4df9e17df3250cb4",
            "authors": [
                "Dacheng Li",
                "Shiyi Cao",
                "Tyler Griggs",
                "Shu Liu",
                "Xiangxi Mo",
                "Shishir G. Patil",
                "Matei Zaharia",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "affiliations": [
                "Department of Electrical Engineering and Computer Sciences, University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07374.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение длинным цепочкам рассуждений в больших языковых моделях",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) могут эффективно обучаться длинным цепочкам рассуждений (Long CoT) с помощью контролируемой дообучки на небольшом наборе данных. Модель Qwen2.5-32B-Instruct, обученная на 17 тысячах примеров Long CoT, значительно улучшила результаты в задачах по математике и программированию. Структура Long CoT оказалась критически важной для обучения, в то время как содержание отдельных шагов рассуждения имело минимальное влияние. Эти выводы углубляют понимание того, как развивать способности к рассуждению в LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning: Structure Over Content in Large Models",
                    "desc": "This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples."
                },
                "zh": {
                    "title": "长链思维：推理模型的关键结构",
                    "desc": "大型推理模型（LRMs）通过长链思维（Long CoT）解决复杂的推理问题，这种思维方式包括反思、回溯和自我验证。我们发现，大型语言模型（LLM）可以通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）有效学习长链思维。仅使用17,000个长链思维训练样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中取得了显著提升。研究表明，长链思维的结构对学习过程至关重要，而单个推理步骤的内容对性能影响较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03997",
            "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
            "url": "https://huggingface.co/papers/2502.03997",
            "abstract": "Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.",
            "score": 4,
            "issue_id": 2165,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 февраля",
                "en": "February 6",
                "zh": "2月6日"
            },
            "hash": "69bfc4de9cf7106d",
            "authors": [
                "Yu Yuan",
                "Shizhao Sun",
                "Qi Liu",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03997.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "CAD-Editor: Революция в текстовом редактировании CAD-моделей",
                    "desc": "CAD-Editor - это первая система для редактирования CAD-моделей на основе текстовых инструкций. Она использует автоматизированный конвейер для синтеза обучающих данных, сочетая модели вариаций дизайна и большие мультимодальные языковые модели. Система применяет подход 'locate-then-infill', разбивая задачу на локализацию областей для изменения и их заполнение соответствующими правками. CAD-Editor опирается на большие языковые модели для понимания естественного языка и знаний о CAD, демонстрируя превосходные результаты в экспериментах."
                },
                "en": {
                    "title": "Revolutionizing CAD Editing with Text Instructions",
                    "desc": "This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "文本驱动的CAD编辑新纪元",
                    "desc": "计算机辅助设计（CAD）在各个行业中至关重要。基于文本的CAD编辑可以根据文本指令自动修改CAD模型，但这一领域尚未得到充分探索。现有方法主要集中在设计变体生成或基于文本的CAD生成，缺乏对文本控制的支持或忽视了现有CAD模型的约束。我们提出了CAD-Editor，这是第一个用于基于文本的CAD编辑的框架，利用大型语言模型（LLMs）和自动化数据合成管道，实现了高效的编辑指令生成和模型修改。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07508",
            "title": "Enhance-A-Video: Better Generated Video for Free",
            "url": "https://huggingface.co/papers/2502.07508",
            "abstract": "DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.",
            "score": 3,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "e02d3082d3b21016",
            "authors": [
                "Yang Luo",
                "Xuanlei Zhao",
                "Mengzhao Chen",
                "Kaipeng Zhang",
                "Wenqi Shao",
                "Kai Wang",
                "Zhangyang Wang",
                "Yang You"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07508.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Enhance-A-Video: Повышение качества генерации видео без переобучения",
                    "desc": "Данная статья представляет новый подход к улучшению качества видео, генерируемого моделями на основе DiT (Diffusion Transformer), без необходимости дополнительного обучения. Метод, названный Enhance-A-Video, фокусируется на усилении межкадровых корреляций с использованием недиагональных распределений временного внимания. Подход легко применим к большинству существующих фреймворков генерации видео на основе DiT без переобучения или дополнительной настройки. Результаты демонстрируют значительное улучшение как временной согласованности, так и визуального качества генерируемых видео."
                },
                "en": {
                    "title": "Enhancing DiT Video Generation Without Retraining",
                    "desc": "This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques."
                },
                "zh": {
                    "title": "提升视频生成质量的新方法",
                    "desc": "本研究提出了一种名为 Enhance-A-Video 的方法，旨在提高基于 DiT 的视频生成模型的连贯性和质量。该方法通过增强跨帧相关性，利用非对角时间注意力分布来实现。由于其设计简单，该方法可以轻松应用于大多数基于 DiT 的视频生成框架，而无需重新训练或微调。我们的实验表明，该方法在时间一致性和视觉质量方面都取得了显著的改善。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.04465",
            "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
            "url": "https://huggingface.co/papers/2502.04465",
            "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.",
            "score": 2,
            "issue_id": 2167,
            "pub_date": "2025-02-06",
            "pub_date_card": {
                "ru": "6 февраля",
                "en": "February 6",
                "zh": "2月6日"
            },
            "hash": "a6ebe3d69cd8bcc2",
            "authors": [
                "Luca Della Libera",
                "Francesco Paissan",
                "Cem Subakan",
                "Mirco Ravanelli"
            ],
            "affiliations": [
                "Concordia University, Montreal, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.04465.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "FocalCodec: Эффективное сжатие речи с сохранением семантики и акустики",
                    "desc": "Исследователи представили FocalCodec - эффективный аудиокодек с низким битрейтом, основанный на фокальной модуляции. Он использует единый бинарный кодбук для сжатия речи до 0.16-0.65 кбит/с, что ниже, чем у современных аналогов. FocalCodec показывает конкурентоспособные результаты в ресинтезе речи и преобразовании голоса, сохраняя при этом семантическую и акустическую информацию. Кодек хорошо справляется с многоязычной речью и шумными условиями, а также подходит для генеративного моделирования."
                },
                "en": {
                    "title": "FocalCodec: Efficient Speech Compression with Single Binary Codebook",
                    "desc": "This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing."
                },
                "zh": {
                    "title": "FocalCodec：高效低比特率语音编解码器",
                    "desc": "大型语言模型通过在海量数据集上进行自监督预训练，彻底改变了自然语言处理。受到这一成功的启发，研究人员尝试将这些方法应用于语音处理，通过神经音频编解码器将连续音频离散化为标记。然而，现有方法面临高比特率、语义或声学信息丢失以及多代码本设计的复杂性等限制。为了解决这些问题，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，能够在0.16到0.65 kbps之间压缩语音，同时在语音重合成和语音转换中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07445",
            "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
            "url": "https://huggingface.co/papers/2502.07445",
            "abstract": "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.",
            "score": 2,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "3e6282dd3913750a",
            "authors": [
                "Nurit Cohen-Inger",
                "Yehonatan Elisha",
                "Bracha Shapira",
                "Lior Rokach",
                "Seffi Cohen"
            ],
            "affiliations": [
                "Ben Gurion University",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07445.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🦎",
                "ru": {
                    "title": "Разоблачение иллюзии понимания: как языковые модели маскируют переобучение",
                    "desc": "Статья представляет новый метод оценки языковых моделей под названием C-BOD. Этот метод выявляет переобучение моделей путем систематического искажения входных данных с сохранением их семантического содержания. Исследование показало, что многие модели, включая крупные ЯМ, демонстрируют значительное снижение производительности при небольших изменениях формулировок. Авторы призывают сообщество уделять больше внимания устойчивости и обобщающей способности моделей, а не только показателям в рейтингах."
                },
                "en": {
                    "title": "Beyond Scores: Evaluating True Language Understanding in LLMs",
                    "desc": "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."
                },
                "zh": {
                    "title": "超越分数，关注模型的鲁棒性与泛化能力",
                    "desc": "大型语言模型（LLMs）在公共基准测试中表现优异，但这些高分可能掩盖了模型对特定数据集表面特征的过度依赖，而非真正的语言理解。我们提出了变色龙基准过拟合检测器（C-BOD），这是一个通过参数变换系统性扭曲基准提示的元评估框架，用于检测LLMs的过拟合。C-BOD通过重新表述输入，同时保持其语义内容和标签，揭示模型性能是否受到记忆模式的驱动。我们的研究表明，经过适度扰动后，26个领先的LLM在MMLU基准上的平均性能下降了2.15%，这表明模型在评估时需要关注更强的鲁棒性和泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07531",
            "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.07531",
            "abstract": "Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.",
            "score": 2,
            "issue_id": 2165,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "dea5fd89dd98f3b1",
            "authors": [
                "Sixiao Zheng",
                "Zimian Peng",
                "Yanpeng Zhou",
                "Yi Zhu",
                "Hang Xu",
                "Xiangru Huang",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University, China",
                "Huawei Noahs Ark Lab, China",
                "Westlake University, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07531.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Точный контроль над ключевыми элементами при генерации видео из изображений",
                    "desc": "VidCRAFT3 - это новая система генерации видео из изображений, позволяющая одновременно контролировать движение камеры, движение объектов и направление освещения. В основе системы лежит Spatial Triple-Attention Transformer, который интегрирует информацию об освещении, тексте и изображении. Для обучения был создан синтетический датасет VideoLightingDirection с аннотациями направления освещения. Предложенная трехэтапная стратегия обучения позволяет обойтись без данных с одновременными аннотациями всех визуальных элементов."
                },
                "en": {
                    "title": "VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation",
                    "desc": "This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods."
                },
                "zh": {
                    "title": "VidCRAFT3：多元素控制的图像到视频生成新框架",
                    "desc": "本文介绍了一种新的图像到视频生成框架VidCRAFT3，能够同时控制相机运动、物体运动和光照方向。为了更好地解耦每个视觉元素的控制，提出了空间三重注意力变换器，能够对光照方向、文本和图像进行对称整合。由于大多数真实世界视频数据集缺乏光照注释，研究者构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），该数据集包含光照方向注释和多样化外观的物体。通过广泛的实验，VidCRAFT3在生成高质量视频内容方面表现优异，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07776",
            "title": "Auditing Prompt Caching in Language Model APIs",
            "url": "https://huggingface.co/papers/2502.07776",
            "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.",
            "score": 2,
            "issue_id": 2164,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "48f7472ef1c86b27",
            "authors": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Rohith Kuditipudi",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07776.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#leakage",
                    "#inference",
                    "#ethics",
                    "#security",
                    "#data"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Кэширование промптов в LLM: скрытая угроза приватности",
                    "desc": "Статья исследует проблему кэширования промптов в больших языковых моделях (LLM) и связанные с этим риски утечки данных. Авторы разработали методы статистического аудита для обнаружения кэширования промптов у реальных API-провайдеров LLM. Они обнаружили глобальное разделение кэша между пользователями у семи провайдеров, включая OpenAI, что может привести к утечке информации о промптах пользователей. Кроме того, временные различия из-за кэширования могут раскрывать информацию об архитектуре модели, например, авторы нашли доказательства того, что модель встраивания OpenAI является декодер-ориентированным трансформером."
                },
                "en": {
                    "title": "Timing Variations: A Privacy Risk in Prompt Caching for LLMs",
                    "desc": "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."
                },
                "zh": {
                    "title": "提示缓存的隐私风险与透明性",
                    "desc": "在大型语言模型（LLMs）中，提示缓存会导致数据依赖的时间变化：缓存的提示处理速度比非缓存的提示快。这些时间差异可能引发侧信道攻击的风险，例如，如果缓存被多个用户共享，攻击者可以通过快速的API响应时间识别出缓存的提示，从而获取其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存政策透明度非常重要。我们开发并进行统计审计，以检测现实世界中LLM API提供商的提示缓存情况，发现七个API提供商（包括OpenAI）之间存在全球缓存共享，可能导致用户提示的隐私泄露。"
                }
            }
        }
    ],
    "link_prev": "2025-02-11.html",
    "link_next": "2025-02-13.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "11.02",
        "en": "02/11",
        "zh": "2月11日"
    },
    "short_date_next": {
        "ru": "13.02",
        "en": "02/13",
        "zh": "2月13日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 4,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1,
        "#cultural_diversity": 1
    },
    "zh": {
        "text": "我们展示了强化学习应用于大型语言模型（LLMs）可以显著提升复杂编程和推理任务的表现。我们比较了两个通用推理模型 - OpenAI o1 和 o3 的早期版本，以及一个针对2024年国际信息学奥林匹克（IOI）设计的特定领域系统 o1-ioi。我们在IOI 2024上使用 o1-ioi 参赛，并在放宽的比赛约束下获得了金牌。然而，后续模型如 o3 在没有手工制定的特定领域策略或放宽约束的情况下也能获得金牌。我们的发现表明，虽然专门的流水线如 o1-ioi 带来了显著的改进，但扩展的通用 o3 模型在不依赖手工制定的推理启发式的情况下超越了这些结果。",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "Wǒmen zhǎnshìle qiáng huà xuéxí yìngyòng yú dàxíng yǔyán móxíng (LLMs) kěyǐ xiǎnzhù tíshēng fùzá bǐan chéng yǔ tuīlǐ rènwù de biǎoxiàn. Wǒmen bǐjiàole liǎng gè tōngyòng tuīlǐ móxíng - OpenAI o1 hé o3 de zǎoqī bǎnběn, yǐjià yīgè zhǐduì 2024 nián guójì xìnxī xué àolínpǐkè (IOI) shèjì de tèdìng yùyí xìtǒng o1-ioi. Wǒmen zài IOI 2024 shàng shǐyòng o1-ioi cānsài, bìng zài fàngkuān de bǐsài yuēshù xià huòdéle jīnpái. Rán'ér, hòuxù móxíng rú o3 zài méiyǒu shǒugōng zhìdìng de tèdìng yùyí cèlüè huò fàngkuān yuēshù de qíngkuàng xià yě néng huòdé jīnpái. Wǒmen de fāxiàn biǎomíng, suīrán zhuānmén de liúshuǐxiàn rú o1-ioi dàilái le xiǎnzhù de gǎijìn, dàn kuòzhǎn de tōngyòng o3 móxíng zài bù yīlài shǒugōng zhìdìng de tuīlǐ qǐfǎshì de qíngkuàng xià chāoyuèle zhèxiē jiéguǒ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'}, {'word': '强化学习', 'pinyin': 'qiáng​huà​xué​xí', 'trans': 'reinforcement learning'}, {'word': '应用于', 'pinyin': 'yìng​yòng​yú', 'trans': 'apply to'}, {'word': '大型语言模型', 'pinyin': 'dà​xíng​yǔ​yán​mó​xíng', 'trans': 'large language model'}, {'word': '显著', 'pinyin': 'xiǎn​zhù', 'trans': 'significant'}, {'word': '提升', 'pinyin': 'tí​shēng', 'trans': 'improve'}, {'word': '复杂', 'pinyin': 'fù​zá', 'trans': 'complex'}, {'word': '编程', 'pinyin': 'biān​chéng', 'trans': 'programming'}, {'word': '推理', 'pinyin': 'tuī​lǐ', 'trans': 'reasoning'}, {'word': '表现', 'pinyin': 'biǎo​xiàn', 'trans': 'performance'}, {'word': '比较', 'pinyin': 'bǐ​jiào', 'trans': 'compare'}, {'word': '通用', 'pinyin': 'tōng​yòng', 'trans': 'general-purpose'}, {'word': '针对', 'pinyin': 'zhēn​duì', 'trans': 'target'}, {'word': '特定领域', 'pinyin': 'tè​dìng​lǐng​yù', 'trans': 'specific domain'}, {'word': '系统', 'pinyin': 'xì​tǒng', 'trans': 'system'}, {'word': '国际信息学奥林匹克', 'pinyin': 'guó​jì​xìn​xī​xué​ào​lín​pǐ​kè', 'trans': 'International Olympiad in Informatics'}, {'word': '设计', 'pinyin': 'shè​jì', 'trans': 'design'}, {'word': '参赛', 'pinyin': 'cān​sài', 'trans': 'compete'}, {'word': '放宽', 'pinyin': 'fàng​kuān', 'trans': 'relax'}, {'word': '约束', 'pinyin': 'yuē​shù', 'trans': 'constraint'}, {'word': '获得', 'pinyin': 'huò​dé', 'trans': 'obtain'}, {'word': '金牌', 'pinyin': 'jīn​pái', 'trans': 'gold medal'}, {'word': '后续', 'pinyin': 'hòu​xù', 'trans': 'subsequent'}, {'word': '手工制定', 'pinyin': 'shǒu​gōng​zhì​dìng', 'trans': 'manually specified'}, {'word': '策略', 'pinyin': 'cè​lüè', 'trans': 'strategy'}, {'word': '启发式', 'pinyin': 'qǐ​fā​shì', 'trans': 'heuristic'}, {'word': '依赖', 'pinyin': 'yī​lài', 'trans': 'rely on'}, {'word': '扩展', 'pinyin': 'kuò​zhǎn', 'trans': 'extend'}, {'word': '超越', 'pinyin': 'chāo​yuè', 'trans': 'surpass'}, {'word': '结果', 'pinyin': 'jié​guǒ', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning models—early versions of OpenAI o1 and o3—and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}