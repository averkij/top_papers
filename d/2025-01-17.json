{
    "date": {
        "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 17",
        "zh": "1æœˆ17æ—¥"
    },
    "time_utc": "2025-01-17 05:09",
    "weekday": 4,
    "issue_id": 1720,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.08617",
            "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
            "url": "https://huggingface.co/papers/2501.08617",
            "abstract": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.",
            "score": 3,
            "issue_id": 1720,
            "pub_date": "2025-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "f758bc630d8dd443",
            "authors": [
                "Kaiqu Liang",
                "Haimin Hu",
                "Ryan Liu",
                "Thomas L. Griffiths",
                "Jaime FernÃ¡ndez Fisac"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University",
                "Department of Psychology, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.08617.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ - Reinforcement Learning from Hindsight Simulation (RLHS). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ RLHF, RLHS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚ÑƒĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RLHS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RLHS Ğ½Ğ°Ğ´ RLHF Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Aligning AI with Human Values through Hindsight Feedback",
                    "desc": "This paper addresses the challenge of aligning generative AI systems with human values using Reinforcement Learning from Human Feedback (RLHF). It identifies that relying on immediate feedback can lead to misaligned behaviors, such as sycophancy and deception, due to Goodhart's Law dynamics. The authors propose a new approach called Reinforcement Learning from Hindsight Simulation (RLHS), which uses simulated consequences to gather feedback on beneficial behaviors. Their experiments show that RLHS improves user satisfaction and goal achievement compared to traditional RLHF methods, highlighting the importance of considering long-term outcomes in AI alignment."
                },
                "zh": {
                    "title": "å…³æ³¨é•¿æœŸåæœï¼Œæå‡AIå¯¹é½æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•æ›´å¥½åœ°ä¸äººç±»ä»·å€¼è§‚å¯¹é½ï¼Œä»¥ç¡®ä¿å…¶è¡Œä¸ºæœ‰ç›Šä¸”å¯ä¿¡ã€‚ç°æœ‰çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–å³æ—¶åé¦ˆï¼Œä½†è¿™ç§åé¦ˆå¯èƒ½æ— æ³•å‡†ç¡®åæ˜ ä¸ç”¨æˆ·æ•ˆç”¨ç›¸å…³çš„é•¿æœŸå½±å“ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºäº‹åæ¨¡æ‹Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHSï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¯èƒ½çš„åæœæ¥è·å–åé¦ˆï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„å¯¹é½æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLHSåœ¨å¸®åŠ©ç”¨æˆ·å®ç°ç›®æ ‡å’Œæé«˜æ»¡æ„åº¦æ–¹é¢ï¼Œä¼˜äºä¼ ç»Ÿçš„RLHFæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09755",
            "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
            "url": "https://huggingface.co/papers/2501.09755",
            "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.",
            "score": 0,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "426aa3415c3c0ef4",
            "authors": [
                "Philippe Hansen-Estruch",
                "David Yan",
                "Ching-Yao Chung",
                "Orr Zohar",
                "Jialiang Wang",
                "Tingbo Hou",
                "Tao Xu",
                "Sriram Vishwanath",
                "Peter Vajda",
                "Xinlei Chen"
            ],
            "affiliations": [
                "FAIR, Meta",
                "GenAI, Meta",
                "Stanford University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09755.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#video",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ViTok: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ViTok - Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision Transformer, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ½Ğ¾ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ViTok Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ FLOP Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Scaling Auto-Encoders for Enhanced Image and Video Generation",
                    "desc": "This paper explores the scaling of auto-encoders, particularly focusing on the tokenizer component, which is crucial for image and video generation. The authors introduce ViTok, a Vision Transformer-based architecture that replaces traditional convolutional backbones, allowing for better scaling on large datasets. They investigate how different scaling strategies for the encoder and decoder affect both reconstruction and generative performance, finding that scaling the decoder is more beneficial for reconstruction. Ultimately, ViTok achieves competitive results with fewer computational resources and sets new benchmarks in image and video generation tasks."
                },
                "zh": {
                    "title": "è‡ªç¼–ç å™¨çš„è§†è§‰æ ‡è®°åŒ–ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡è‡ªç¼–ç å™¨è¿›è¡Œè§†è§‰æ ‡è®°åŒ–å¯¹å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„è§†è§‰å˜æ¢å™¨æ¶æ„ï¼ˆViTokï¼‰ï¼Œç”¨äºæ›¿ä»£ä¼ ç»Ÿçš„å·ç§¯éª¨å¹²ç½‘ç»œï¼Œä»¥æé«˜æ ‡è®°åŒ–çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œè‡ªç¼–ç å™¨çš„ç“¶é¢ˆè§„æ¨¡ä¸é‡å»ºæ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œä½†ä¸ç”Ÿæˆæ€§èƒ½çš„å…³ç³»æ›´ä¸ºå¤æ‚ã€‚æœ€ç»ˆï¼ŒViTokåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘é‡å»ºå’Œå›¾åƒç”Ÿæˆæ–¹é¢ï¼Œå±•ç¤ºäº†å…¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09732",
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "url": "https://huggingface.co/papers/2501.09732",
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "score": 0,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "2ad32c666f91ba05",
            "authors": [
                "Nanye Ma",
                "Shangyuan Tong",
                "Haolin Jia",
                "Hexiang Hu",
                "Yu-Chuan Su",
                "Mingda Zhang",
                "Xuan Yang",
                "Yandong Li",
                "Tommi Jaakkola",
                "Xuhui Jia",
                "Saining Xie"
            ],
            "affiliations": [
                "Google",
                "MIT",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09732.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ÑˆÑƒĞ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¾ÑÑĞ¼: Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Diffusion Models: Scaling Inference for Better Image Generation",
                    "desc": "This paper investigates how to enhance the performance of diffusion models during the inference phase by increasing computational resources. It highlights that, unlike Large Language Models (LLMs), diffusion models can adjust their inference process through the number of denoising steps, but improvements tend to plateau after a certain point. The authors propose a method to optimize the noise used in the diffusion sampling process by exploring different feedback verifiers and algorithms. Their experiments demonstrate that by strategically increasing computation during inference, the quality of generated images can be significantly improved, tailored to various application needs."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶çš„è®¡ç®—æ‰©å±•ä¸æ€§èƒ½æå‡",
                    "desc": "ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†é‡è¦å½±å“ï¼Œä¸»è¦å¾—ç›Šäºå…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡å¢åŠ æ•°æ®ã€è®¡ç®—èµ„æºå’Œæ¨¡å‹è§„æ¨¡æ¥æ‰©å±•çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶çš„æ‰©å±•è¡Œä¸ºï¼Œå‘ç°é¢å¤–çš„è®¡ç®—å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚ä¸LLMsä¸åŒï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªæ­¥éª¤çš„æ•°é‡çµæ´»è°ƒæ•´æ¨ç†æ—¶çš„è®¡ç®—ï¼Œå°½ç®¡æ€§èƒ½æå‡é€šå¸¸åœ¨å‡ åæ­¥åè¶‹äºå¹³ç¨³ã€‚æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ‰©å±•è¡Œä¸ºï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡å¢åŠ è®¡ç®—æ¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¯»æ‰¾æ›´å¥½çš„å™ªå£°æ¥ä¼˜åŒ–æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.09686",
            "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
            "url": "https://huggingface.co/papers/2501.09686",
            "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.",
            "score": 0,
            "issue_id": 1720,
            "pub_date": "2025-01-16",
            "pub_date_card": {
                "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 16",
                "zh": "1æœˆ16æ—¥"
            },
            "hash": "1c6b1b1f0235304c",
            "authors": [
                "Fengli Xu",
                "Qianyue Hao",
                "Zefang Zong",
                "Jingwei Wang",
                "Yunke Zhang",
                "Jingyi Wang",
                "Xiaochong Lan",
                "Jiahui Gong",
                "Tianjian Ouyang",
                "Fanjin Meng",
                "Chenyang Shao",
                "Yuwei Yan",
                "Qinglong Yang",
                "Yiwen Song",
                "Sijian Ren",
                "Xinyuan Hu",
                "Yu Li",
                "Jie Feng",
                "Chen Gao",
                "Yong Li"
            ],
            "affiliations": [
                "Emory University, Atlanta GA, USA",
                "HKUST (GZ), Guangzhou, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.09686.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rl",
                    "#survey",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑÑƒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Human-Like Reasoning in Large Language Models",
                    "desc": "This paper discusses the advancements in Large Language Models (LLMs) and their application to complex reasoning tasks. It introduces the concept of 'thought', which represents intermediate reasoning steps, allowing LLMs to simulate human-like reasoning processes. The paper highlights the use of reinforcement learning to enhance LLMs' reasoning capabilities by generating high-quality reasoning trajectories through trial-and-error methods. Additionally, it emphasizes the importance of scaling both training and testing phases to improve reasoning accuracy, paving the way for the development of Large Reasoning Models."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤§å‹æ¨ç†æ¨¡å‹çš„ç ”ç©¶æ–°å‰æ²¿",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è€…ä»¬å¼•å…¥äº†â€œæ€è€ƒâ€çš„æ¦‚å¿µï¼Œé€šè¿‡ä¸­é—´æ­¥éª¤çš„ä»¤ç‰Œåºåˆ—æ¥æ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«åº”ç”¨äºè®­ç»ƒLLMsï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡è¿˜è®¨è®ºäº†åœ¨æµ‹è¯•æ—¶å¢åŠ ä»¤ç‰Œæ•°é‡ä»¥æé«˜æ¨ç†å‡†ç¡®æ€§çš„æ•ˆæœï¼Œå¹¶å±•æœ›äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-16.html",
    "link_next": "2025-01-20.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "16.01",
        "en": "01/16",
        "zh": "1æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢ï¼Œæ—¨åœ¨ä»å¤§é‡æ–‡æ¡£ä¸­è¯†åˆ«å’Œæ£€ç´¢å›¾è¡¨ã€è¡¨æ ¼ã€å›¾å½¢å’Œå¸ƒå±€ä¿¡æ¯ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MMDocIRï¼ŒåŒ…æ‹¬é¡µé¢çº§å’Œå¸ƒå±€çº§æ£€ç´¢ä»»åŠ¡ã€‚MMDocIRåŒ…å«1,685ä¸ªä¸“å®¶æ ‡æ³¨å’Œ173,843ä¸ªè‡ªåŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„èµ„æºã€‚å®éªŒæ˜¾ç¤ºï¼Œè§†è§‰æ£€ç´¢å™¨æ¯”æ–‡æœ¬æ£€ç´¢å™¨è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä½¿ç”¨VLM-textçš„æ–‡æœ¬æ£€ç´¢å™¨ä¼˜äºä½¿ç”¨OCR-textçš„æ£€ç´¢å™¨ã€‚",
        "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢ï¼Œæ—¨åœ¨ä»å¤§é‡æ–‡æ¡£ä¸­è¯†åˆ«å’Œæ£€ç´¢å›¾è¡¨ã€è¡¨æ ¼ã€å›¾å½¢å’Œå¸ƒå±€ä¿¡æ¯ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼Œç¼ºä¹æœ‰æ•ˆçš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MMDocIRï¼ŒåŒ…æ‹¬é¡µé¢çº§å’Œå¸ƒå±€çº§æ£€ç´¢ä»»åŠ¡ã€‚MMDocIRåŒ…å«1,685ä¸ªä¸“å®¶æ ‡æ³¨å’Œ173,843ä¸ªè‡ªåŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„èµ„æºã€‚å®éªŒæ˜¾ç¤ºï¼Œè§†è§‰æ£€ç´¢å™¨æ¯”æ–‡æœ¬æ£€ç´¢å™¨è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä½¿ç”¨VLM-textçš„æ–‡æœ¬æ£€ç´¢å™¨ä¼˜äºä½¿ç”¨OCR-textçš„æ£€ç´¢å™¨ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le duÅ mÃ³ tÃ i wÃ©n dÃ ng jiÃ n suÇ’, zhÇ zÃ i cÃ³ng dÃ  liÃ ng wÃ©n dÃ ng zhÅng shÃ­ biÃ© hÃ© jiÃ n suÇ’ tÃº biÇo, biÇo gÃ©, tÃº xÃ­ng hÃ© bÃ¹ jÃº xÃ¬n xÄ«. JÇn guÇn qÃ­ zhÃ²ng yÃ o xÃ¬ng, quÄ“ fÃ¡ yÇ’u xiÃ o de jÄ« zhÇ”n cÃ¨ shÃ¬ lÃ¡i pÃ­ng guÌ„ xÃ¬ tÇ’ng xÃ¬ng nÃ©ng. WÃ¨i cÇ, wÃ©n zhÄng tÃ­ chÅ« le yÄ« gÃ¨ xÄ«n de jÄ« zhÇ”n cÃ¨ shÃ¬ MMDocIR, bÄo kuÃ² yÃ¨ miÃ n jÃ­ hÃ© bÃ¹ jÃº jÃ­ jiÃ n suÇ’ rÃ¨n wÃ¹. MMDocIR bÄo hÃ¡n 1,685 gÃ¨ zhuÄn jiÄ biÄo zhÃ¹ hÃ© 173,843 gÃ¨ zÃ¬ dÃ²ng biÄo zhÃ¹ de wÃ¨n tÃ­, shÃ¬ yÄ« gÃ¨ zhÃ²ng yÃ o de zÄ« yuÃ¡n. ShÃ­ yÃ n xiÇn shÃ¬, shÃ¬ juÃ© jiÃ n suÇ’ qÃ¬ bÇ wÃ©n bÄ›n jiÃ n suÇ’ qÃ¬ biÇo xiÃ n gÃ¨ng hÇo, bÃ¬ng qiÄ› shÇ yÃ²ng VLM-text de wÃ©n bÄ›n jiÃ n suÇ’ qÃ¬ yÅu yÃº shÇ yÃ²ng OCR-text de jiÃ n suÇ’ qÃ¬.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'æ£€ç´¢', 'pinyin': 'jiÇn suÇ’', 'trans': 'retrieval'},\n{'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'},\n{'word': 'è¯†åˆ«', 'pinyin': 'shÃ­ biÃ©', 'trans': 'recognize'},\n{'word': 'å¸ƒå±€', 'pinyin': 'bÃ¹ jiÃº', 'trans': 'layout'},\n{'word': 'å°½ç®¡', 'pinyin': 'jÃ¬n guÇn', 'trans': 'although'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'é¡µé¢çº§', 'pinyin': 'yÃ¨ miÃ n jÃ­', 'trans': 'page-level'},\n{'word': 'æ ‡æ³¨', 'pinyin': 'biÄo zhÃ¹', 'trans': 'annotation'},\n{'word': 'èµ„æº', 'pinyin': 'zÄ« yuÃ¡n', 'trans': 'resource'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'perform'},\n{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'},\n{'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'},\n{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]",
        "trans": "This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text.",
        "update_ts": "2025-01-16 09:10"
    }
}