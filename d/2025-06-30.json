{
    "date": {
        "ru": "30 Ğ¸ÑĞ½Ñ",
        "en": "June 30",
        "zh": "6æœˆ30æ—¥"
    },
    "time_utc": "2025-06-30 21:10",
    "weekday": 0,
    "issue_id": 4566,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.17450",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "url": "https://huggingface.co/papers/2506.17450",
            "abstract": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.",
            "score": 44,
            "issue_id": 4550,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "b5bb4470d500be10",
            "authors": [
                "Jiacheng Chen",
                "Ramin Mehran",
                "Xuhui Jia",
                "Saining Xie",
                "Sanghyun Woo"
            ],
            "affiliations": [
                "Google DeepMind",
                "New York University",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17450.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "BlenderFusion - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ñ€Ğ¾Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². BlenderFusion Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing Scene Editing with BlenderFusion",
                    "desc": "BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods."
                },
                "zh": {
                    "title": "ç”Ÿæˆè§†è§‰åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "BlenderFusionæ˜¯ä¸€ä¸ªç”Ÿæˆè§†è§‰åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é‡æ–°ç»„åˆå¯¹è±¡ã€ç›¸æœºå’ŒèƒŒæ™¯æ¥åˆæˆæ–°åœºæ™¯ã€‚å®ƒé‡‡ç”¨åˆ†å±‚-ç¼–è¾‘-åˆæˆçš„æµç¨‹ï¼Œé¦–å…ˆå°†è§†è§‰è¾“å…¥åˆ†å‰²å¹¶è½¬æ¢ä¸ºå¯ç¼–è¾‘çš„3Då®ä½“ï¼Œç„¶ååœ¨Blenderä¸­è¿›è¡Œ3Dæ§åˆ¶çš„ç¼–è¾‘ï¼Œæœ€åä½¿ç”¨ç”Ÿæˆåˆæˆå™¨å°†å®ƒä»¬èåˆæˆä¸€ä¸ªè¿è´¯çš„åœºæ™¯ã€‚è¯¥ç”Ÿæˆåˆæˆå™¨æ‰©å±•äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åŸå§‹åœºæ™¯å’Œç¼–è¾‘åçš„åœºæ™¯ã€‚BlenderFusionåœ¨å¤æ‚çš„åˆæˆåœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21862",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "url": "https://huggingface.co/papers/2506.21862",
            "abstract": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "score": 29,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "b9ad171aa3fb5bbf",
            "authors": [
                "Boyuan Sun",
                "Jiaxing Zhao",
                "Xihan Wei",
                "Qibin Hou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#dataset",
                    "#video"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LLaVA-Scissor - ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² (SCC) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ. LLaVA-Scissor Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ SCC ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaVA-Scissor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Video Understanding with Semantic Token Compression",
                    "desc": "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."
                },
                "zh": {
                    "title": "LLaVA-Scissorï¼šé«˜æ•ˆçš„è§†é¢‘ä»¤ç‰Œå‹ç¼©ç­–ç•¥",
                    "desc": "LLaVA-Scissoræ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œå‹ç¼©ç­–ç•¥ã€‚å®ƒåˆ©ç”¨è¯­ä¹‰è¿é€šç»„ä»¶ï¼ˆSCCï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å°†ä»¤ç‰Œåˆ†é…åˆ°ä¸åŒçš„è¯­ä¹‰åŒºåŸŸï¼Œä»è€Œç¡®ä¿å…¨é¢çš„è¯­ä¹‰è¦†ç›–ã€‚ä¸ä»¥å¾€åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒLLaVA-Scissorèƒ½å¤Ÿå‡å°‘ä»¤ç‰Œå†—ä½™ï¼Œå¹¶åœ¨ç©ºé—´å’Œæ—¶é—´åŸŸä¸­è¿›è¡Œä¸¤æ­¥å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä½ä»¤ç‰Œä¿ç•™æ¯”ç‡ä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21416",
            "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
            "url": "https://huggingface.co/papers/2506.21416",
            "abstract": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.",
            "score": 23,
            "issue_id": 4551,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "4c3c383901d9306f",
            "authors": [
                "Bowen Chen",
                "Mengyi Zhao",
                "Haomiao Sun",
                "Li Chen",
                "Xu Wang",
                "Kang Du",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21416.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "XVerse - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. XVerse Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "XVerse: Mastering Multi-Subject Control in Image Generation",
                    "desc": "XVerse is a new model that improves text-to-image generation by allowing users to control multiple subjects in an image with high precision. It uses a technique called token-specific text-stream modulation to manage the identity and attributes of each subject, such as pose and lighting, without losing image quality. Traditional methods often create unwanted artifacts or mix up attributes, but XVerse avoids these issues by transforming reference images into specific adjustments. This leads to better coherence and fidelity in generated images, making it easier to create complex scenes with distinct and editable subjects."
                },
                "zh": {
                    "title": "XVerseï¼šç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡å›¾åƒç”Ÿæˆçš„åˆ›æ–°",
                    "desc": "XVerseæ˜¯ä¸€ç§å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹å¤šä¸ªå¯¹è±¡è¿›è¡Œç²¾ç¡®å’Œç‹¬ç«‹çš„æ§åˆ¶ã€‚å®ƒé€šè¿‡ç‰¹å®šçš„æ–‡æœ¬æµè°ƒåˆ¶ï¼Œè§£å†³äº†åœ¨ç”Ÿæˆå¤šå¯¹è±¡å›¾åƒæ—¶å¸¸è§çš„ç¼–è¾‘æ€§å’Œä¸€è‡´æ€§é—®é¢˜ã€‚XVerseå°†å‚è€ƒå›¾åƒè½¬æ¢ä¸ºåç§»é‡ï¼Œä»è€Œå®ç°å¯¹ç‰¹å®šå¯¹è±¡çš„æ§åˆ¶ï¼Œè€Œä¸å¹²æ‰°å›¾åƒçš„æ½œåœ¨ç‰¹å¾ã€‚è¿™ä¸€åˆ›æ–°æ˜¾è‘—æé«˜äº†ä¸ªæ€§åŒ–å’Œå¤æ‚åœºæ™¯ç”Ÿæˆçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21356",
            "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
            "url": "https://huggingface.co/papers/2506.21356",
            "abstract": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.",
            "score": 19,
            "issue_id": 4551,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "5a54508ae68df265",
            "authors": [
                "Hongbo Liu",
                "Jingwen He",
                "Yi Jin",
                "Dian Zheng",
                "Yuhao Dong",
                "Fan Zhang",
                "Ziqi Huang",
                "Yinan He",
                "Yangguang Li",
                "Weichao Chen",
                "Yu Qiao",
                "Wanli Ouyang",
                "Shengjie Zhao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21356.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° ĞºĞ¸Ğ½Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ShotBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ShotQA Ñ 70 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ShotQA Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ShotVL, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ´ÑˆĞ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° ĞºĞ¸Ğ½Ğ¾. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing AI's Cinematic Language Comprehension with ShotBench and ShotQA",
                    "desc": "This paper introduces ShotBench and ShotQA datasets, along with the ShotVL model, to improve AI's ability to understand and generate cinematic language. Cinematography is a complex visual language that conveys stories and emotions, but current Vision-Language Models (VLMs) struggle with its nuances. The ShotBench benchmark evaluates VLMs on their comprehension of cinematic grammar, revealing significant limitations in their performance. By developing ShotQA and fine-tuning the ShotVL model, the authors achieve state-of-the-art results, providing valuable resources for advancing AI in cinematic understanding and generation."
                },
                "zh": {
                    "title": "æå‡AIç”µå½±è¯­è¨€ç†è§£çš„çªç ´æ€§è¿›å±•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ShotBenchå’ŒShotQAæ•°æ®é›†ä»¥åŠShotVLæ¨¡å‹ï¼Œæ—¨åœ¨æå‡äººå·¥æ™ºèƒ½å¯¹ç”µå½±è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç”µå½±æ‘„å½±æ˜¯ä¼ è¾¾å™äº‹ã€æƒ…æ„Ÿå’Œç¾å­¦è´¨é‡çš„åŸºæœ¬è§†è§‰è¯­è¨€ï¼Œä½†ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç»†è…»çš„ç”µå½±è¯­æ³•æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬æ„å»ºäº†ShotBenchåŸºå‡†ï¼ŒåŒ…å«3500å¤šä¸ªä¸“å®¶æ³¨é‡Šçš„é—®ç­”å¯¹ï¼Œè¯„ä¼°äº†24ä¸ªé¢†å…ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå®ƒä»¬åœ¨ç»†ç²’åº¦è§†è§‰çº¿ç´¢å’Œå¤æ‚ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡æ„å»ºShotQAæ•°æ®é›†å¹¶å¼€å‘ShotVLæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ShotBenchä¸Šå–å¾—äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œæ¨åŠ¨äº†AIåœ¨ç”µå½±ç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20279",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2506.20279",
            "abstract": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj",
            "score": 15,
            "issue_id": 4550,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "8382f71877fe1997",
            "authors": [
                "Changliang Xia",
                "Chengyou Jia",
                "Zhuohang Dang",
                "Minnan Luo"
            ],
            "affiliations": [
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20279.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DenseDiT - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². DenseDiT Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¾Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 0,01% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "DenseDiT: Revolutionizing Dense Prediction with Minimal Data",
                    "desc": "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."
                },
                "zh": {
                    "title": "DenseDiTï¼šç”¨æœ€å°‘æ•°æ®å®ç°å¯†é›†é¢„æµ‹çš„çªç ´",
                    "desc": "DenseDiTæ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ä»¥æœ€å°‘çš„è®­ç»ƒæ•°æ®å®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚å¯†é›†é¢„æµ‹ä»»åŠ¡åœ¨è®¡ç®—æœºè§†è§‰ä¸­éå¸¸é‡è¦ï¼Œæ—¨åœ¨ä¸ºè¾“å…¥å›¾åƒå­¦ä¹ é€åƒç´ çš„æ ‡æ³¨æ ‡ç­¾ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œç¼ºä¹å¯¹çœŸå®åœºæ™¯çš„å¹¿æ³›é€‚åº”æ€§ï¼Œä¸”é¢ä¸´çœŸå®æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚DenseDiTé€šè¿‡æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è§†è§‰å…ˆéªŒï¼Œç»“åˆå‚æ•°é‡ç”¨æœºåˆ¶å’Œè½»é‡çº§åˆ†æ”¯ï¼Œèƒ½å¤Ÿåœ¨å¤šç§çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21628",
            "title": "Ark: An Open-source Python-based Framework for Robot Learning",
            "url": "https://huggingface.co/papers/2506.21628",
            "abstract": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots.",
            "score": 11,
            "issue_id": 4555,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "19bc247d9dffd525",
            "authors": [
                "Magnus Dierking",
                "Christopher E. Mower",
                "Sarthak Das",
                "Huang Helong",
                "Jiacheng Qiu",
                "Cody Reading",
                "Wei Chen",
                "Huidong Liang",
                "Huang Guowei",
                "Jan Peters",
                "Quan Xingyue",
                "Jun Wang",
                "Haitham Bou-Ammar"
            ],
            "affiliations": [
                "Huawei Noahs Ark",
                "Imperial College London",
                "Technical University of Darmstadt",
                "University College London",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21628.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#games",
                    "#agents",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ARK: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¸ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ´ ĞºÑ€Ñ‹Ğ»Ğ¾Ğ¼ Python",
                    "desc": "ARK - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Python-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Gym Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ARK Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, SLAM, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ROS. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Python, ARK ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Simplifying Robotics with Python: ARK Framework",
                    "desc": "ARK is an open-source framework that simplifies robotics development by integrating imitation-learning algorithms with Python, making it more accessible for developers. It provides a Gym-style interface for data collection, preprocessing, and training policies, allowing seamless transitions between simulation and real-world robots. The framework includes a client-server architecture for efficient communication and offers modules for various robotics tasks like control and motion planning. By bridging the gap between robotics and AI, ARK enhances rapid prototyping and deployment of autonomous systems."
                },
                "zh": {
                    "title": "ARKï¼šç®€åŒ–æœºå™¨äººå¼€å‘çš„Pythonæ¡†æ¶",
                    "desc": "ARKæ˜¯ä¸€ä¸ªå¼€æºçš„Pythonä¼˜å…ˆæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–æœºå™¨äººå¼€å‘å’Œéƒ¨ç½²ã€‚å®ƒé›†æˆäº†ç°ä»£æ¨¡ä»¿å­¦ä¹ ç®—æ³•å’Œæ— ç¼çš„ä»¿çœŸä¸ç‰©ç†æœºå™¨äººäº¤äº’ï¼Œæä¾›äº†ä¸€ä¸ªç±»ä¼¼Gymçš„ç¯å¢ƒæ¥å£ï¼Œæ–¹ä¾¿ç”¨æˆ·æ”¶é›†æ•°æ®ã€é¢„å¤„ç†å’Œè®­ç»ƒç­–ç•¥ã€‚ARKè¿˜å…·æœ‰è½»é‡çº§çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œæ”¯æŒç½‘ç»œé€šä¿¡ï¼Œå¹¶æä¾›C/C++ç»‘å®šä»¥ç¡®ä¿å®æ—¶æ€§èƒ½ã€‚é€šè¿‡ç»Ÿä¸€æœºå™¨äººå’Œäººå·¥æ™ºèƒ½çš„å®è·µï¼ŒARKé™ä½äº†å…¥é—¨é—¨æ§›ï¼ŒåŠ é€Ÿäº†è‡ªä¸»æœºå™¨äººçš„ç ”ç©¶å’Œå•†ä¸šéƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21411",
            "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
            "url": "https://huggingface.co/papers/2505.21411",
            "abstract": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.",
            "score": 11,
            "issue_id": 4552,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "e4bcbe7787b328fa",
            "authors": [
                "Yehui Tang",
                "Xiaosong Li",
                "Fangcheng Liu",
                "Wei Guo",
                "Hang Zhou",
                "Yaoyuan Wang",
                "Kai Han",
                "Xianzhi Yu",
                "Jinpeng Li",
                "Hui Zang",
                "Fei Mi",
                "Xiaojun Meng",
                "Zhicheng Liu",
                "Hanting Chen",
                "Binfan Zheng",
                "Can Chen",
                "Youliang Yan",
                "Ruiming Tang",
                "Peifeng Qin",
                "Xinghao Chen",
                "Dacheng Tao",
                "Yunhe Wang"
            ],
            "affiliations": [
                "Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21411.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoGE: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mixture of Grouped Experts (MoGE) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MoGE Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MoGE Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Pangu Pro MoE Ñ 72 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ NPU Ascend. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MoGE Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Balancing Experts for Efficient Language Model Execution",
                    "desc": "The paper introduces Mixture of Grouped Experts (MoGE), a novel approach to improve the efficiency of large language models by enhancing expert load balancing. MoGE ensures that an equal number of experts are activated for each input token, which addresses the issue of uneven expert activation seen in traditional Mixture of Experts (MoE) models. This architectural design allows for better distribution of computational load across multiple devices, significantly increasing throughput during inference. The results demonstrate that MoGE leads to superior performance and cost-effectiveness on Ascend NPUs, particularly with the Pangu Pro MoE model, which achieves impressive inference speeds and outperforms existing dense models."
                },
                "zh": {
                    "title": "æ··åˆåˆ†ç»„ä¸“å®¶ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ€§èƒ½",
                    "desc": "æ··åˆåˆ†ç»„ä¸“å®¶ï¼ˆMoGEï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸“å®¶è´Ÿè½½å¹³è¡¡å’Œæ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡å°†ä¸“å®¶åˆ†ç»„é€‰æ‹©ï¼ŒMoGEç¡®ä¿æ¯ä¸ªè¾“å…¥ä»¤ç‰Œæ¿€æ´»çš„ä¸“å®¶æ•°é‡ç›¸ç­‰ï¼Œä»è€Œå‡å°‘äº†ç³»ç»Ÿåœ¨å¹¶è¡Œè¿è¡Œæ—¶çš„æ•ˆç‡æŸå¤±ã€‚æˆ‘ä»¬åœ¨Ascend NPUä¸Šæ„å»ºäº†Pangu Pro MoEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMoGEçš„ç¨€ç–æ¨¡å‹ï¼Œå…·æœ‰720äº¿ä¸ªå‚æ•°ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoGEåœ¨æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸­éƒ½èƒ½å®ç°æ›´å¥½çš„ä¸“å®¶è´Ÿè½½å¹³è¡¡å’Œæ›´é«˜çš„æ‰§è¡Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22434",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "url": "https://huggingface.co/papers/2506.22434",
            "abstract": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.",
            "score": 9,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "d7e89f248d4c331e",
            "authors": [
                "Xi Chen",
                "Mingkang Zhu",
                "Shaoteng Liu",
                "Xiaoyang Wu",
                "Xiaogang Xu",
                "Yu Liu",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HUST",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22434.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Supervised Image Triplet Learning",
                    "desc": "This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä½¿ç”¨å›¾åƒä¸‰å…ƒç»„çš„è‡ªç›‘ç£å­¦ä¹ æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šå›¾åƒä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ç ”ç©¶è€…ä»¬æ„å»ºäº†ç”±åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒç»„æˆçš„å›¾åƒä¸‰å…ƒç»„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«è¦æ±‚ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œä»¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³åˆ¤æ–­ç›¸åŒæˆ–ä¸åŒï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä»…åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šè®­ç»ƒï¼Œä½†å…¶å­¦ä¹ åˆ°çš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§é—®é¢˜ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22432",
            "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
            "url": "https://huggingface.co/papers/2506.22432",
            "abstract": "A novel framework integrates 3D proxy meshes and a decoupled video diffusion model to achieve precise and consistent video editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/",
            "score": 9,
            "issue_id": 4561,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "99840d39a4880400",
            "authors": [
                "Yuhao Liu",
                "Tengfei Wang",
                "Fang Liu",
                "Zhenwei Wang",
                "Rynson W. H. Lau"
            ],
            "affiliations": [
                "City University of Hong Kong, Hong Kong SAR, China",
                "Tencent, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22432.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "3D-Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Shape-for-Motion Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-Ğ¿Ñ€Ğ¾ĞºÑĞ¸-ÑĞµÑ‚ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, Ğ¿Ğ¾ÑĞ»Ğµ Ñ‡ĞµĞ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. Shape-for-Motion Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ñ‹, Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Shape-for-Motion: Precise Video Editing with 3D Proxies",
                    "desc": "This paper introduces Shape-for-Motion, a new framework that enhances video editing by using 3D proxy meshes. It allows users to edit a 3D representation of an object in a video, ensuring that changes are consistent across all frames. The framework employs a Dual-Propagation Strategy, which simplifies the editing process by automatically applying edits made to one frame's mesh to the meshes of other frames. This method supports various manipulations like pose editing and texture modification, leading to high-quality and controllable video outputs."
                },
                "zh": {
                    "title": "ç²¾ç¡®ä¸€è‡´çš„è§†é¢‘ç¼–è¾‘æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç»“åˆäº†3Dä»£ç†ç½‘æ ¼å’Œè§£è€¦è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°ç²¾ç¡®ä¸”ä¸€è‡´çš„è§†é¢‘ç¼–è¾‘ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è¾“å…¥è§†é¢‘ä¸­çš„ç›®æ ‡å¯¹è±¡è½¬æ¢ä¸ºæ—¶é—´ä¸€è‡´çš„3Dç½‘æ ¼ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ç›´æ¥åœ¨ä»£ç†ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œå¹¶å°†ç¼–è¾‘ç»“æœæ¨æ–­å›è§†é¢‘å¸§ã€‚ä¸ºäº†ç®€åŒ–ç¼–è¾‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åŒä¼ æ’­ç­–ç•¥ï¼Œå…è®¸ç”¨æˆ·åœ¨å•å¸§çš„3Dç½‘æ ¼ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œå¹¶è‡ªåŠ¨å°†è¿™äº›ç¼–è¾‘ä¼ æ’­åˆ°å…¶ä»–å¸§çš„3Dç½‘æ ¼ä¸Šã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒå¤šç§ç²¾ç¡®ä¸”ç‰©ç†ä¸€è‡´çš„æ“ä½œï¼ŒåŒ…æ‹¬å§¿æ€ç¼–è¾‘ã€æ—‹è½¬ã€ç¼©æ”¾ã€å¹³ç§»ã€çº¹ç†ä¿®æ”¹å’Œå¯¹è±¡ç»„åˆï¼Œæ ‡å¿—ç€é«˜è´¨é‡ã€å¯æ§è§†é¢‘ç¼–è¾‘å·¥ä½œæµç¨‹çš„é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21656",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "url": "https://huggingface.co/papers/2506.21656",
            "abstract": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.",
            "score": 9,
            "issue_id": 4548,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "8d063b13fc555964",
            "authors": [
                "Yifan Shen",
                "Yuanzhe Liu",
                "Jingyuan Zhu",
                "Xu Cao",
                "Xiaofeng Zhang",
                "Yixiao He",
                "Wenming Ye",
                "James Matthew Rehg",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "Google",
                "Shanghai Jiao Tong University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21656.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ",
                    "desc": "SpatialReasoner-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. SpatialReasoner-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SPATIALRGPT-Bench, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 9.8% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Elevating Spatial Reasoning with SpatialReasoner-R1",
                    "desc": "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."
                },
                "zh": {
                    "title": "ç©ºé—´æ¨ç†çš„æ–°çªç ´",
                    "desc": "SpatialReasoner-R1æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šæ¨¡å‹è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·ä¸”é€»è¾‘ä¸€è‡´çš„é•¿é“¾æ€ç»´æ¨ç†è½¨è¿¹ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒSpatialReasoner-R1è¿˜å¼•å…¥äº†ç»†ç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆfDPOï¼‰ï¼Œé€šè¿‡ç©ºé—´å¥–åŠ±æœºåˆ¶å¯¹å€™é€‰å“åº”è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæé«˜æè¿°æ€§åŸºç¡€å’Œé€»è¾‘æ¨ç†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€å¼ºåŸºçº¿æé«˜äº†9.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22419",
            "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
            "url": "https://huggingface.co/papers/2506.22419",
            "abstract": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.",
            "score": 8,
            "issue_id": 4553,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "179a2fbf84ed5e98",
            "authors": [
                "Bingchen Zhao",
                "Despoina Magka",
                "Minqi Jiang",
                "Xian Li",
                "Roberta Raileanu",
                "Tatiana Shavrina",
                "Jean-Christophe Gagnon-Audet",
                "Kelvin Niu",
                "Shagun Sodhani",
                "Michael Shvartsman",
                "Andrei Lupu",
                "Alisia Lupidi",
                "Edan Toledo",
                "Karen Hambardzumyan",
                "Martin Josifoski",
                "Thomas Foster",
                "Lucia Cipolina-Kun",
                "Abhishek Charnalia",
                "Derek Dunfield",
                "Alexander H. Miller",
                "Oisin Mac Aodha",
                "Jakob Foerster",
                "Yoram Bachrach"
            ],
            "affiliations": [
                "Meta",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22419.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#science",
                    "#agents",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸƒâ€â™‚ï¸",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ğ¸Ğ´Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ½Ğ³Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¿Ğ¸Ğ´Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ¯Ğ‘Ğœ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¿Ğ¸Ğ´Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ° NanoGPT, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-2. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ°."
                },
                "en": {
                    "title": "Benchmarking LLMs: Can They Reproduce Scientific Results?",
                    "desc": "The paper introduces the Automated LLM Speedrunning Benchmark, which assesses the ability of large language models (LLMs) to reproduce scientific results. It utilizes tasks from the NanoGPT speedrun competition, where AI agents attempt to train a GPT-2 model as quickly as possible. Despite advancements in reasoning capabilities, recent LLMs struggle to replicate known improvements in the benchmark, even with detailed hints provided. This benchmark serves as a straightforward measure of an LLM's capacity for automating scientific reproduction, an essential skill for future autonomous research agents."
                },
                "zh": {
                    "title": "è¯„ä¼°LLMé‡ç°ç§‘å­¦æˆæœçš„è‡ªåŠ¨åŒ–åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„LLMé€Ÿåº¦æµ‹è¯•åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç§‘å­¦ç ”ç©¶ä¸­é‡ç°ç»“æœçš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åˆ©ç”¨NanoGPTé€Ÿåº¦æµ‹è¯•ä»»åŠ¡ï¼Œæä¾›19ä¸ªä»»åŠ¡ï¼Œå¸®åŠ©ä»£ç†åœ¨æœ€çŸ­æ—¶é—´å†…è®­ç»ƒGPT-2æ¨¡å‹ã€‚å°½ç®¡æä¾›äº†è¯¦ç»†çš„æç¤ºï¼Œæœ€æ–°çš„æ¨ç†LLMä»ç„¶éš¾ä»¥é‡æ–°å®ç°å·²çŸ¥çš„æ”¹è¿›ã€‚è¿™ä¸€åŸºå‡†ä¸ºè¯„ä¼°LLMåœ¨ç§‘å­¦é‡ç°ä¸­çš„è‡ªåŠ¨åŒ–èƒ½åŠ›æä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„æµ‹é‡æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21876",
            "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic\n  Evaluation",
            "url": "https://huggingface.co/papers/2506.21876",
            "abstract": "A benchmark framework evaluates the world modeling capabilities of Vision-Language Models, highlighting their limitations in perception and prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.",
            "score": 7,
            "issue_id": 4565,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "db21a8520a375546",
            "authors": [
                "Qiyue Gao",
                "Xinyu Pi",
                "Kevin Liu",
                "Junrong Chen",
                "Ruolan Yang",
                "Xinqi Huang",
                "Xinyu Fang",
                "Lu Sun",
                "Gautham Kishore",
                "Bo Ai",
                "Stone Tao",
                "Mengyang Liu",
                "Jiaxi Yang",
                "Chao-Jung Lai",
                "Chuanyang Jin",
                "Jiannan Xiang",
                "Benhao Huang",
                "Zeming Chen",
                "David Danks",
                "Hao Su",
                "Tianmin Shu",
                "Ziqiao Ma",
                "Lianhui Qin",
                "Zhiting Hu"
            ],
            "affiliations": [
                "Maitrix.org",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21876.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#cv",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ VLM Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ (Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ) Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´). Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WM-ABench Ñ 23 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² 6 ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Evaluating Vision-Language Models: Uncovering Limitations in World Modeling",
                    "desc": "This paper presents a benchmark framework to evaluate the world modeling capabilities of Vision-Language Models (VLMs). It identifies significant limitations in these models' abilities to perceive and predict various aspects of the world, such as motion and spatial relationships. The authors introduce WM-ABench, a comprehensive evaluation tool that tests VLMs across multiple dimensions in controlled environments. The findings reveal that current VLMs struggle with basic world modeling tasks, often performing poorly compared to human-level understanding."
                },
                "zh": {
                    "title": "è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ„ŸçŸ¥å’Œé¢„æµ‹æ–¹é¢çš„å±€é™æ€§ã€‚å†…éƒ¨ä¸–ç•Œæ¨¡å‹ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿç†è§£ä¸–ç•ŒçŠ¶æ€å¹¶é¢„æµ‹å˜åŒ–ï¼Œæ˜¯é«˜çº§æ¨ç†çš„åŸºç¡€ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ†åˆ«è¯„ä¼°æ„ŸçŸ¥å’Œé¢„æµ‹èƒ½åŠ›ï¼Œä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°VLMsçš„åŸºæœ¬ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡å¯¹15ä¸ªæœ€æ–°çš„å•†ä¸šå’Œå¼€æºVLMsè¿›è¡Œ660æ¬¡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨åŸºæœ¬çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—çš„ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21355",
            "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning",
            "url": "https://huggingface.co/papers/2506.21355",
            "abstract": "Current multimodal large language models show moderate to poor performance in multimodal in-context learning for medical tasks, with sensitivity to example relevance and ordering.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, example ordering exhibits a recency bias, i.e., placing the most relevant example last can lead to substantial performance improvements by up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context.",
            "score": 5,
            "issue_id": 4561,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "99f9cb46fa313c8b",
            "authors": [
                "Melanie Rieff",
                "Maya Varma",
                "Ossian Rabow",
                "Subathra Adithan",
                "Julie Kim",
                "Ken Chang",
                "Hannah Lee",
                "Nidhi Rohatgi",
                "Christian Bluethgen",
                "Mohamed S. Muneer",
                "Jean-Benoit Delbrouck",
                "Michael Moor"
            ],
            "affiliations": [
                "ETH Zurich",
                "HOPPR",
                "Jawaharlal Institute of Postgraduate Medical Education and Research",
                "Lund University",
                "Stanford University",
                "UCSF",
                "University Hospital Zurich",
                "University of Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21355.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#healthcare",
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SMMILE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ MLLM Ğ¿Ñ€Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Multimodal Learning in Medicine: The SMMILE Challenge",
                    "desc": "This paper discusses the limitations of current multimodal large language models (MLLMs) in performing in-context learning (ICL) for medical tasks. The authors introduce SMMILE, a benchmark created with input from medical experts, which includes a variety of multimodal queries and examples to assess MLLMs' capabilities. The evaluation reveals that most models struggle with ICL, showing only slight improvements over zero-shot learning and being significantly affected by irrelevant examples and the order of presented information. The study emphasizes the need for better understanding and enhancement of MLLMs in the context of medical applications, particularly in how they learn from multimodal data."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ¨¡æ€å­¦ä¹ åœ¨åŒ»å­¦ä¸­çš„å±€é™æ€§",
                    "desc": "å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦ä»»åŠ¡çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­è¡¨ç°ä¸€èˆ¬ï¼Œä¸”å¯¹ç¤ºä¾‹çš„ç›¸å…³æ€§å’Œé¡ºåºæ•æ„Ÿã€‚å°½ç®¡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨åŒ»å­¦é¢†åŸŸå…·æœ‰é‡è¦æ½œåŠ›ï¼Œä½†ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†SMMILEï¼Œè¿™æ˜¯é¦–ä¸ªç”±ä¸“å®¶é©±åŠ¨çš„åŒ»å­¦ä»»åŠ¡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ åŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§åŒ»å­¦ä¸“ä¸šå’Œæˆåƒæ–¹å¼ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨åŒ»å­¦ä»»åŠ¡çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸”ç¤ºä¾‹çš„é¡ºåºå’Œç›¸å…³æ€§å¯¹æ€§èƒ½å½±å“æ˜¾è‘—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21594",
            "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
            "url": "https://huggingface.co/papers/2506.21594",
            "abstract": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Gazal-R1, a 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Built upon Qwen3 32B, our model demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains. We developed a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset of 107,033 synthetic medical reasoning examples that teaches structured clinical thinking, enhanced by advanced parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation (DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system that refines accuracy, format adherence, and reasoning quality. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12x larger. Beyond its strong empirical results, this work provides detailed insights into the challenges of training reasoning-capable models in specialized domains, including issues with reward hacking, training instability, and the fundamental tension between factual recall and detailed reasoning. Our methodology offers a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability.",
            "score": 5,
            "issue_id": 4555,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "02b5127629bc134b",
            "authors": [
                "Ahmed M. Adly",
                "Mostafa Samy",
                "Amr Fawzy"
            ],
            "affiliations": [
                "TachyHealth, Riyadh 13316, Saudi Arabia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21594.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#rl",
                    "#optimization",
                    "#survey",
                    "#training",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹: Gazal-R1 - Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Gazal-R1 - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DoRA Ğ¸ rsLoRA. Gazal-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Medical Reasoning with Gazal-R1",
                    "desc": "Gazal-R1 is a 32-billion-parameter language model designed for medical reasoning, achieving top performance through innovative training methods. It utilizes a two-stage training pipeline that includes supervised fine-tuning on a large dataset of synthetic medical examples and reinforcement learning with a multi-component reward system. This model demonstrates that mid-sized models can outperform larger ones in specialized tasks by employing advanced techniques like Weight-Decomposed Low-Rank Adaptation. Gazal-R1 not only excels in accuracy but also provides clear explanations for its clinical decisions, addressing challenges in training reasoning-capable models."
                },
                "zh": {
                    "title": "Gazal-R1ï¼šåŒ»å­¦æ¨ç†çš„æ–°æ ‡æ†",
                    "desc": "Gazal-R1æ˜¯ä¸€ç§æ‹¥æœ‰320äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨åŒ»å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡æˆ˜ç•¥æ€§è®­ç»ƒï¼ŒåŒ…æ‹¬å…ˆè¿›çš„å‚æ•°é«˜æ•ˆæŠ€æœ¯å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæä¾›ä¸´åºŠå†³ç­–çš„è¯¦ç»†è§£é‡Šã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨ç²¾å¿ƒç­–åˆ’çš„107,033ä¸ªåˆæˆåŒ»å­¦æ¨ç†ç¤ºä¾‹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚Gazal-R1åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œå±•ç¤ºäº†ä¸­å‹æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸè¶…è¶Šæ›´å¤§æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19741",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "url": "https://huggingface.co/papers/2506.19741",
            "abstract": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT",
            "score": 4,
            "issue_id": 4548,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "288a2c7ef1ba6865",
            "authors": [
                "Yihong Luo",
                "Shuchen Xue",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "NUS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Noise Consistency Training (NCT) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NCT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑˆÑƒĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NCT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Control in AI Content Generation with Noise Consistency Training",
                    "desc": "This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content."
                },
                "zh": {
                    "title": "å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼šé«˜æ•ˆå¯æ§ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼ˆNCTï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†æ–°çš„æ§åˆ¶ä¿¡å·æ•´åˆåˆ°é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„ä¿®æ”¹ï¼Œè€ŒNCTé€šè¿‡å¼•å…¥é€‚é…æ¨¡å—å’Œå™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œåœ¨ç”Ÿæˆå™¨çš„å™ªå£°ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¤šæ­¥å’Œè’¸é¦æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯æ§ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚NCTçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶åœ¨æ•°æ®ä½¿ç”¨ä¸Šæ›´åŠ é«˜æ•ˆï¼Œæ˜“äºéƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18330",
            "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
            "url": "https://huggingface.co/papers/2506.18330",
            "abstract": "Confucius3-Math, a 14B parameter large language model, achieves state-of-the-art performance on mathematical reasoning tasks using reinforcement learning techniques and is optimized for education in China.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.",
            "score": 4,
            "issue_id": 4557,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 Ğ¸ÑĞ½Ñ",
                "en": "June 23",
                "zh": "6æœˆ23æ—¥"
            },
            "hash": "7f069bbbc7caa337",
            "authors": [
                "Lixin Wu",
                "Na Cai",
                "Qiao Cheng",
                "Jiachen Wang",
                "Yitao Duan"
            ],
            "affiliations": [
                "NetEase Youdao, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18330.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Confucius3-Math: ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞšĞ¸Ñ‚Ğ°Ğµ",
                    "desc": "Confucius3-Math - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞšĞ¸Ñ‚Ğ°Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ñ†ĞµĞ»ĞµĞ²ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ GPU Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Math Education with Advanced AI",
                    "desc": "Confucius3-Math is a large language model with 14 billion parameters designed to excel in mathematical reasoning tasks, particularly for K-12 education in China. It utilizes reinforcement learning techniques to optimize its performance, achieving state-of-the-art results while being efficient enough to run on a single consumer-grade GPU. The model incorporates innovative methods such as Targeted Entropy Regularization and Policy-Specific Hardness Weighting to enhance training stability and data efficiency. By focusing on the national curriculum, Confucius3-Math aims to support mathematics learning for students and educators at a low cost."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›æ•°å­¦æ•™è‚²çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "Confucius3-Mathæ˜¯ä¸€ä¸ªæ‹¥æœ‰140äº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ¨¡å‹åœ¨ä¸­å›½çš„æ•™è‚²é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹K-12å­¦ç”Ÿçš„æ•°å­¦å­¦ä¹ ã€‚é€šè¿‡åæœŸè®­ç»ƒå’Œåˆ›æ–°æŠ€æœ¯ï¼ŒConfucius3-Mathåœ¨è§£å†³ä¸»æµæ•°å­¦é—®é¢˜æ—¶å±•ç°äº†é«˜æ•ˆæ€§å’Œä½æˆæœ¬ã€‚æˆ‘ä»¬åˆ†äº«äº†å¼€å‘è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨ç‰¹å®šé¢†åŸŸæ„å»ºå¼ºå¤§æ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17859",
            "title": "In-Context Learning Strategies Emerge Rationally",
            "url": "https://huggingface.co/papers/2506.17859",
            "abstract": "A hierarchical Bayesian framework explains in-context learning behavior by modeling it as a tradeoff between strategy loss and complexity, offering both explanatory power and predictive insights.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.",
            "score": 4,
            "issue_id": 4560,
            "pub_date": "2025-06-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ½Ñ",
                "en": "June 21",
                "zh": "6æœˆ21æ—¥"
            },
            "hash": "919b375c089d97fa",
            "authors": [
                "Daniel Wurgaft",
                "Ekdeep Singh Lubana",
                "Core Francisco Park",
                "Hidenori Tanaka",
                "Gautam Reddy",
                "Noah D. Goodman"
            ],
            "affiliations": [
                "CBS-NTT Program in Physics of Intelligence, Harvard University",
                "Department of Computer Science, Stanford University",
                "Department of Psychology, Stanford University",
                "Joseph Henry Laboratories of Physics, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17859.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL) Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ICL ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ ĞµÑ‘ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ñ‹ ICL Ğ¸ Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¾ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Balancing Strategy Loss and Complexity in In-Context Learning",
                    "desc": "This paper presents a hierarchical Bayesian framework to understand in-context learning (ICL) in machine learning models. It models ICL as a balance between the loss incurred by different strategies and their complexity. The authors propose that models learn various strategies based on their training on a mixture of tasks, using predictors that either memorize or generalize from the data. By analyzing these strategies through a Bayesian lens, the framework provides insights into how models adapt their learning behavior based on computational constraints and task diversity."
                },
                "zh": {
                    "title": "ç­–ç•¥æŸå¤±ä¸å¤æ‚æ€§çš„æƒè¡¡è§£é‡Šä¸Šä¸‹æ–‡å­¦ä¹ ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡è´å¶æ–¯æ¡†æ¶ï¼Œç”¨äºè§£é‡Šä¸Šä¸‹æ–‡å­¦ä¹ è¡Œä¸ºï¼Œè®¤ä¸ºè¿™æ˜¯ç­–ç•¥æŸå¤±ä¸å¤æ‚æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹å­¦ä¹ å¤šç§ä»»åŠ¡æ—¶ï¼Œå¯ä»¥é€šè¿‡è´å¶æ–¯é¢„æµ‹å™¨æ¥æ•æ‰å…¶å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬è®°å¿†å‹é¢„æµ‹å™¨å’Œæ³›åŒ–å‹é¢„æµ‹å™¨ã€‚é€šè¿‡ç†æ€§åˆ†æçš„è§†è§’ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸ä¾èµ–æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œå‡ ä¹å®Œç¾åœ°é¢„æµ‹Transformerçš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚è¯¥æ¡†æ¶å¼ºè°ƒäº†åœ¨å€™é€‰ç­–ç•¥ä¸­ï¼Œæ¨¡å‹å¯¹ç­–ç•¥çš„åå¥½ä¸ä»…å–å†³äºå…¶å¯¹æ•°æ®çš„è§£é‡Šèƒ½åŠ›ï¼Œè¿˜ä¸ç­–ç•¥çš„å¤æ‚æ€§æœ‰å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21458",
            "title": "Spatial Mental Modeling from Limited Views",
            "url": "https://huggingface.co/papers/2506.21458",
            "abstract": "A new benchmark, MindCube, shows that VLMs can improve their understanding of unseen spaces by forming internal spatial representations and reasoning over them.  \t\t\t\t\tAI-generated summary \t\t\t\t Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new MindCube benchmark with 21,154 questions across 3,268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using MindCube, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for \"what-if\" movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from a synergistic approach, \"map-then-reason\", that jointly trains the model to first generate a cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space.",
            "score": 3,
            "issue_id": 4563,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "2d558cf4535a9561",
            "authors": [
                "Baiqiao Yin",
                "Qineng Wang",
                "Pingyue Zhang",
                "Jianshu Zhang",
                "Kangrui Wang",
                "Zihan Wang",
                "Jieyu Zhang",
                "Keshigeyan Chandrasegaran",
                "Han Liu",
                "Ranjay Krishna",
                "Saining Xie",
                "Manling Li",
                "Jiajun Wu",
                "Li Fei-Fei"
            ],
            "affiliations": [
                "New York University",
                "Northwestern University",
                "Stanford University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21458.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rl",
                    "#cv",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ¾Ñ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MindCube, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'map-then-reason', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞµĞµ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ 37.8% Ğ´Ğ¾ 60.8%, Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 70.7%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing VLMs with Spatial Mental Models for Better Scene Understanding",
                    "desc": "The paper introduces MindCube, a benchmark designed to evaluate how well Vision Language Models (VLMs) can create internal spatial representations to understand unseen environments. It highlights that current VLMs struggle with this task, often performing at near-random levels when faced with spatial reasoning questions. The authors propose a 'map-then-reason' approach, where models first generate cognitive maps of the space and then use these maps for reasoning, leading to significant performance improvements. By incorporating reinforcement learning, the models achieved even higher accuracy, demonstrating that structured spatial representations enhance the understanding of complex spatial scenarios."
                },
                "zh": {
                    "title": "æ„å»ºç©ºé—´å¿ƒç†æ¨¡å‹ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MindCubeï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£æœªè§ç©ºé—´æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„VLMsåœ¨å¤„ç†ç©ºé—´æ¨ç†æ—¶è¡¨ç°æ¥è¿‘éšæœºï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ„å»ºå†…éƒ¨ç©ºé—´è¡¨å¾æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡MindCubeï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æ–¹æ³•æ¥å¸®åŠ©VLMsæ›´å¥½åœ°è¿‘ä¼¼ç©ºé—´å¿ƒç†æ¨¡å‹ï¼Œå…¶ä¸­â€œå…ˆæ„å›¾å†æ¨ç†â€çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹çš„æ€§èƒ½ä»37.8%æå‡è‡³70.7%ï¼Œè¯æ˜äº†æ„å»ºå’Œåˆ©ç”¨å†…éƒ¨ç»“æ„åŒ–ç©ºé—´è¡¨å¾çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21718",
            "title": "Performance Prediction for Large Systems via Text-to-Text Regression",
            "url": "https://huggingface.co/papers/2506.21718",
            "abstract": "A text-to-text regression model achieves high accuracy in predicting resource efficiency for Google's Borg system, surpassing tabular methods, and demonstrates adaptability and uncertainty quantification.  \t\t\t\t\tAI-generated summary \t\t\t\t In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes.",
            "score": 2,
            "issue_id": 4560,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "e8bc4fba805dea4f",
            "authors": [
                "Yash Akhauri",
                "Bryan Lewandowski",
                "Cheng-Hsi Lin",
                "Adrian N. Reyes",
                "Grant C. Forbes",
                "Arissa Wongpanich",
                "Bangding Yang",
                "Mohamed S. Abdelfattah",
                "Sagi Perel",
                "Xingyou Song"
            ],
            "affiliations": [
                "Cornell University",
                "Google",
                "North Carolina State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21718.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#architecture",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Google Borg. ĞœĞ¾Ğ´ĞµĞ»ÑŒ encoder-decoder Ñ 60 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ 0.99 Ğ¸ Ğ² 100 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 500 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Resource Efficiency Prediction with Text-to-Text Regression",
                    "desc": "This paper presents a text-to-text regression model that effectively predicts resource efficiency in Google's Borg system, outperforming traditional tabular regression methods. The model, which is a 60 million parameter encoder-decoder, achieves a high rank correlation of 0.99, indicating its accuracy in predicting outcomes. It demonstrates adaptability by requiring only 500 few-shot examples to learn new tasks and provides uncertainty quantification for complex data distributions. The research suggests that this approach could lead to the development of universal simulators for predicting real-world outcomes across various industries."
                },
                "zh": {
                    "title": "æ–‡æœ¬åˆ°æ–‡æœ¬å›å½’ï¼šæå‡èµ„æºæ•ˆç‡é¢„æµ‹çš„æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ°æ–‡æœ¬çš„å›å½’æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹è°·æ­ŒBorgç³»ç»Ÿçš„èµ„æºæ•ˆç‡ï¼Œè¡¨ç°å‡ºæ¯”ä¼ ç»Ÿçš„è¡¨æ ¼å›å½’æ–¹æ³•æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†ä¸€ä¸ª6000ä¸‡å‚æ•°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„æ•°æ®ç¯å¢ƒä¸­æœ‰æ•ˆå·¥ä½œï¼Œå°¤å…¶æ˜¯åœ¨é…ç½®æ–‡ä»¶å’Œç³»ç»Ÿæ—¥å¿—ç­‰åœºæ™¯ä¸­ã€‚é€šè¿‡å°‘é‡çš„ç¤ºä¾‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œå¹¶ä¸”èƒ½å¤Ÿé‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç¼–ç å™¨å’Œå¢åŠ åºåˆ—é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸ºç°å®ä¸–ç•Œç»“æœçš„é€šç”¨æ¨¡æ‹Ÿå™¨å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22149",
            "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
            "url": "https://huggingface.co/papers/2506.22149",
            "abstract": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rise of imaging techniques such as optical coherence tomography (OCT) and advances in deep learning (DL) have enabled clinicians and researchers to streamline retinal disease staging. A popular DL approach is self-supervised learning (SSL), where models learn from vast amounts of unlabeled data, avoiding costly annotation. SSL has allowed the development of foundation models (FMs), large models that can be used for a variety of downstream tasks. However, existing FMs for OCT, trained solely on image data, lack a comprehensive and robust semantic understanding of images, as evidenced by their downstream performance (especially for complex tasks), and thus require supervised fine-tuning (which may be unfeasible) to better adapt to specific applications and populations. To address this, we propose RetFiner, an SSL vision-language refinement scheme that improves the representations of existing FMs and enables their efficient and direct adaptation to specific populations for improved downstream performance. Our method uses a diverse set of training objectives which take advantage of the rich supervisory signal found in textual data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM, showing significant improvements in linear probing performance on seven highly diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1 percentage points over their baselines, respectively. Our code and model weights are publicly available at https://github.com/ronnief1/RetFiner.",
            "score": 1,
            "issue_id": 4555,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "89abca6b475fc5b6",
            "authors": [
                "Ronald Fecso",
                "JosÃ© Morano",
                "Ursula Schmidt-Erfurth",
                "Hrvoje BogunoviÄ‡"
            ],
            "affiliations": [
                "Christian Doppler Lab for Artificial Intelligence in Retina, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria",
                "Comprehensive Center for AI in Medicine, Medical University of Vienna, Austria",
                "Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria",
                "OPTIMA Lab, Dept. of Ophthalmology, Medical University of Vienna, Austria"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22149.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#healthcare",
                    "#data"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "RetFiner: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞĞšĞ¢ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "RetFiner - ÑÑ‚Ğ¾ ÑÑ…ĞµĞ¼Ğ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ (ĞĞšĞ¢). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ ÑĞµÑ‚Ñ‡Ğ°Ñ‚ĞºĞ¸. RetFiner Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… RETFound, UrFound Ğ¸ VisionFM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞĞšĞ¢."
                },
                "en": {
                    "title": "Enhancing OCT Models with Vision-Language Refinement",
                    "desc": "RetFiner is a novel vision-language refinement method designed to enhance self-supervised foundation models (FMs) for optical coherence tomography (OCT) by incorporating textual data. This approach addresses the limitations of existing FMs that rely solely on image data, which often struggle with complex retinal disease classification tasks. By utilizing a variety of training objectives that leverage rich textual supervisory signals, RetFiner improves the semantic understanding of the models. Our experiments demonstrate that RetFiner significantly boosts the performance of several retinal FMs across diverse classification tasks, achieving notable increases in accuracy."
                },
                "zh": {
                    "title": "RetFinerï¼šæå‡OCTæ¨¡å‹æ€§èƒ½çš„è§†è§‰-è¯­è¨€æ–¹æ¡ˆ",
                    "desc": "RetFineræ˜¯ä¸€ç§è§†è§‰-è¯­è¨€ç²¾ç»†åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æ–‡æœ¬æ•°æ®æ¥å¢å¼ºè‡ªç›‘ç£åŸºç¡€æ¨¡å‹åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•æ”¹å–„äº†ç°æœ‰æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‚åº”ç‰¹å®šäººç¾¤ï¼Œä»è€Œæé«˜åœ¨è§†ç½‘è†œç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¸‹æ¸¸æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨å¤šæ ·åŒ–çš„è®­ç»ƒç›®æ ‡ï¼ŒRetFinerå……åˆ†åˆ©ç”¨äº†æ–‡æœ¬æ•°æ®ä¸­çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetFineråœ¨å¤šä¸ªOCTåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹³å‡æå‡äº†5.8ã€3.9å’Œ2.1ä¸ªç™¾åˆ†ç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21476",
            "title": "Global and Local Entailment Learning for Natural World Imagery",
            "url": "https://huggingface.co/papers/2506.21476",
            "abstract": "Radial Cross-Modal Embeddings enable explicit modeling of transitive entailment in vision-language models, leading to improved performance in hierarchical species classification and retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning the hierarchical structure of data in vision-language models is a significant challenge. Previous works have attempted to address this challenge by employing entailment learning. However, these approaches fail to model the transitive nature of entailment explicitly, which establishes the relationship between order and semantics within a representation space. In this work, we introduce Radial Cross-Modal Embeddings (RCME), a framework that enables the explicit modeling of transitivity-enforced entailment. Our proposed framework optimizes for the partial order of concepts within vision-language models. By leveraging our framework, we develop a hierarchical vision-language foundation model capable of representing the hierarchy in the Tree of Life. Our experiments on hierarchical species classification and hierarchical retrieval tasks demonstrate the enhanced performance of our models compared to the existing state-of-the-art models. Our code and models are open-sourced at https://vishu26.github.io/RCME/index.html.",
            "score": 1,
            "issue_id": 4560,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "d621e5f99b239efd",
            "authors": [
                "Srikumar Sastry",
                "Aayush Dhakal",
                "Eric Xing",
                "Subash Khanal",
                "Nathan Jacobs"
            ],
            "affiliations": [
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21476.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¹ Ğ² AI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Radial Cross-Modal Embeddings (RCME), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸. RCME Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Transitive Entailment",
                    "desc": "This paper presents Radial Cross-Modal Embeddings (RCME), a new framework designed to improve vision-language models by explicitly modeling transitive entailment. Traditional methods struggle with the hierarchical structure of data, particularly in understanding the relationships between concepts. RCME addresses this by optimizing the partial order of concepts, allowing for better representation of hierarchical relationships, such as those found in the Tree of Life. The results show that models using RCME outperform existing state-of-the-art approaches in tasks like hierarchical species classification and retrieval."
                },
                "zh": {
                    "title": "æ˜¾å¼å»ºæ¨¡ä¼ é€’è•´å«ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå¾„å‘è·¨æ¨¡æ€åµŒå…¥ï¼ˆRCMEï¼‰ï¼Œç”¨äºåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­æ˜¾å¼å»ºæ¨¡ä¼ é€’è•´å«å…³ç³»ã€‚é€šè¿‡ä¼˜åŒ–æ¦‚å¿µçš„éƒ¨åˆ†é¡ºåºï¼ŒRCMEèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æ•°æ®çš„å±‚æ¬¡ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ç§åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­ã€‚ä¸ä¹‹å‰çš„è•´å«å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒRCMEèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰è¯­ä¹‰ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RCMEçš„æ¨¡å‹åœ¨å±‚æ¬¡ç‰©ç§åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22049",
            "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
            "url": "https://huggingface.co/papers/2506.22049",
            "abstract": "Gradient-Preserving Activation Scaling (GPAS) mitigates activation variance issues in Pre-LayerNorm Transformers and enhances training dynamics across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings.",
            "score": 0,
            "issue_id": 4560,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "89839b0e292e17a5",
            "authors": [
                "Tianhao Chen",
                "Xin Xu",
                "Zijing Liu",
                "Pengxiang Li",
                "Xinyuan Song",
                "Ajay Kumar Jaiswal",
                "Fan Zhang",
                "Jishan Hu",
                "Yang Wang",
                "Hao Chen",
                "Shizhe Diao",
                "Shiwei Liu",
                "Yu Li",
                "Yin Lu",
                "Can Yang"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Emory University",
                "International Digital Economy Academy",
                "NVIDIA",
                "The Hong Kong University of Science and Technology",
                "University of Oxford",
                "University of Surrey",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22049.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "GPAS: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Gradient-Preserving Activation Scaling (GPAS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»Ğ¾ĞµĞ² (Pre-LayerNorm). GPAS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ…, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GPAS ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ñ‚ 71 Ğ¼Ğ»Ğ½ Ğ´Ğ¾ 1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Sandwich-LN Ğ¸ DeepNorm."
                },
                "en": {
                    "title": "Enhancing Transformer Training with GPAS",
                    "desc": "The paper introduces Gradient-Preserving Activation Scaling (GPAS), a technique designed to address activation variance issues in Pre-LayerNorm Transformers. Pre-LN Transformers, while effective for large models, experience increasing activation variance that hampers the learning ability of deeper layers. GPAS mitigates this by scaling down the activations without altering their gradients, preserving essential information and preventing gradient vanishing. The method has been tested across various model sizes and shows significant performance improvements, indicating its potential applicability to other architectures as well."
                },
                "zh": {
                    "title": "æ¢¯åº¦ä¿æŒæ¿€æ´»ç¼©æ”¾ï¼šæå‡å˜æ¢å™¨è®­ç»ƒçš„å…³é”®",
                    "desc": "æ¢¯åº¦ä¿æŒæ¿€æ´»ç¼©æ”¾ï¼ˆGPASï¼‰æ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³é¢„å±‚å½’ä¸€åŒ–å˜æ¢å™¨ä¸­çš„æ¿€æ´»æ–¹å·®é—®é¢˜ã€‚å®ƒé€šè¿‡ç¼©å°ä¸­é—´æ¿€æ´»å€¼çš„å¤§å°ï¼ŒåŒæ—¶ä¿æŒå…¶æ¢¯åº¦ä¸å˜ï¼Œä»è€Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚GPASåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨è®­ç»ƒåŠ¨æ€ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…å¢å¼ºäº†é¢„å±‚å½’ä¸€åŒ–å˜æ¢å™¨ï¼Œè¿˜åœ¨å…¶ä»–æ¶æ„ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-27.html",
    "link_next": "2025-07-01.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 2,
        "#cv": 10,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 3,
        "#training": 14,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 4,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}