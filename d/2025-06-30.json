{
    "date": {
        "ru": "30 –∏—é–Ω—è",
        "en": "June 30",
        "zh": "6Êúà30Êó•"
    },
    "time_utc": "2025-06-30 03:51",
    "weekday": 0,
    "issue_id": 4548,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.21862",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "url": "https://huggingface.co/papers/2506.21862",
            "abstract": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "score": 7,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 –∏—é–Ω—è",
                "en": "June 27",
                "zh": "6Êúà27Êó•"
            },
            "hash": "b9ad171aa3fb5bbf",
            "authors": [
                "Boyuan Sun",
                "Jiaxing Zhao",
                "Xihan Wei",
                "Qibin Hou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#dataset",
                    "#video"
                ],
                "emoji": "‚úÇÔ∏è",
                "ru": {
                    "title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —É–º–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π",
                    "desc": "LLaVA-Scissor - —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –°–≤—è–∑–∞–Ω–Ω—ã—Ö –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (SCC) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ. LLaVA-Scissor –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Å–∂–∞—Ç–∏—é —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è SCC –∫–∞–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π, —Ç–∞–∫ –∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLaVA-Scissor –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Efficient Video Understanding with Semantic Token Compression",
                    "desc": "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."
                },
                "zh": {
                    "title": "LLaVA-ScissorÔºöÈ´òÊïàÁöÑËßÜÈ¢ë‰ª§ÁâåÂéãÁº©Á≠ñÁï•",
                    "desc": "LLaVA-ScissorÊòØ‰∏ÄÁßçÈíàÂØπËßÜÈ¢ëÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ª§ÁâåÂéãÁº©Á≠ñÁï•„ÄÇÂÆÉÂà©Áî®ËØ≠‰πâËøûÈÄöÁªÑ‰ª∂ÔºàSCCÔºâÊñπÊ≥ïÔºåÊúâÊïàÂú∞Â∞Ü‰ª§ÁâåÂàÜÈÖçÂà∞‰∏çÂêåÁöÑËØ≠‰πâÂå∫ÂüüÔºå‰ªéËÄåÁ°Æ‰øùÂÖ®Èù¢ÁöÑËØ≠‰πâË¶ÜÁõñ„ÄÇ‰∏é‰ª•ÂæÄÂü∫‰∫éÊ≥®ÊÑèÂäõÂàÜÊï∞ÁöÑÂéãÁº©ÊñπÊ≥ï‰∏çÂêåÔºåLLaVA-ScissorËÉΩÂ§üÂáèÂ∞ë‰ª§ÁâåÂÜó‰ΩôÔºåÂπ∂Âú®Á©∫Èó¥ÂíåÊó∂Èó¥Âüü‰∏≠ËøõË°å‰∏§Ê≠•ÂéãÁº©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®‰Ωé‰ª§Áâå‰øùÁïôÊØîÁéá‰∏ã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21656",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "url": "https://huggingface.co/papers/2506.21656",
            "abstract": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.",
            "score": 2,
            "issue_id": 4548,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 –∏—é–Ω—è",
                "en": "June 26",
                "zh": "6Êúà26Êó•"
            },
            "hash": "8d063b13fc555964",
            "authors": [
                "Yifan Shen",
                "Yuanzhe Liu",
                "Jingyuan Zhu",
                "Xu Cao",
                "Xiaofeng Zhang",
                "Yixiao He",
                "Wenming Ye",
                "James Matthew Rehg",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "Google",
                "Shanghai Jiao Tong University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21656.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å",
                    "desc": "SpatialReasoner-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –ø–æ –¥–µ—Ä–µ–≤—É –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä—è–º—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. SpatialReasoner-R1 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ SPATIALRGPT-Bench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 9.8% –ø–æ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."
                },
                "en": {
                    "title": "Elevating Spatial Reasoning with SpatialReasoner-R1",
                    "desc": "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."
                },
                "zh": {
                    "title": "Á©∫Èó¥Êé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "SpatialReasoner-R1ÊòØ‰∏ÄÁßçËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥ÂΩìÂâçËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®ÁªÜÁ≤íÂ∫¶Á©∫Èó¥Êé®ÁêÜÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇËØ•Ê®°ÂûãÈááÁî®Â§öÊ®°ÂûãËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàM3CTSÔºâÊñπÊ≥ïÔºåÁîüÊàêÂ§öÊ†∑‰∏îÈÄªËæë‰∏ÄËá¥ÁöÑÈïøÈìæÊÄùÁª¥Êé®ÁêÜËΩ®ËøπÔºå‰ª•ÊûÑÂª∫È´òË¥®ÈáèÁöÑÁ©∫Èó¥Êé®ÁêÜÁõëÁù£„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåSpatialReasoner-R1ËøòÂºïÂÖ•‰∫ÜÁªÜÁ≤íÂ∫¶Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàfDPOÔºâÔºåÈÄöËøáÁ©∫Èó¥Â•ñÂä±Êú∫Âà∂ÂØπÂÄôÈÄâÂìçÂ∫îËøõË°åËØÑ‰º∞Ôºå‰ªéËÄåÊèêÈ´òÊèèËø∞ÊÄßÂü∫Á°ÄÂíåÈÄªËæëÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSpatialReasoner-R1Âú®SPATIALRGPT-Bench‰∏äËÆæÂÆö‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºåÂπ≥ÂùáÂáÜÁ°ÆÁéáÊØîÊúÄÂº∫Âü∫Á∫øÊèêÈ´ò‰∫Ü9.8%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22434",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "url": "https://huggingface.co/papers/2506.22434",
            "abstract": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.",
            "score": 1,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 –∏—é–Ω—è",
                "en": "June 27",
                "zh": "6Êúà27Êó•"
            },
            "hash": "d7e89f248d4c331e",
            "authors": [
                "Xi Chen",
                "Mingkang Zhu",
                "Shaoteng Liu",
                "Xiaoyang Wu",
                "Xiaogang Xu",
                "Yu Liu",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HUST",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22434.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—Ä–∏–ø–ª–µ—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ç–æ–Ω–∫–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –æ–±–æ–±—â–∞—é—Ç—Å—è –Ω–∞ —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Supervised Image Triplet Learning",
                    "desc": "This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks."
                },
                "zh": {
                    "title": "Ëá™ÁõëÁù£Â≠¶‰π†ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøá‰ΩøÁî®ÂõæÂÉè‰∏âÂÖÉÁªÑÁöÑËá™ÁõëÁù£Â≠¶‰π†Êù•Â¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂú®Â§öÂõæÂÉè‰ªªÂä°‰∏äÁöÑÊé®ÁêÜËÉΩÂäõÔºåËÄåÊó†ÈúÄ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπ„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ÊûÑÂª∫‰∫ÜÁî±Âêå‰∏ÄÂõæÂÉèÁöÑ‰∏§‰∏™Â¢ûÂº∫ËßÜÂõæÂíå‰∏Ä‰∏™Áõ∏‰ºº‰ΩÜ‰∏çÂêåÁöÑÂõæÂÉèÁªÑÊàêÁöÑÂõæÂÉè‰∏âÂÖÉÁªÑ„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÁîüÊàêÊé®ÁêÜËøáÁ®ãÔºå‰ª•ÊØîËæÉËøô‰∫õÂõæÂÉèÔºàÂç≥Âà§Êñ≠Áõ∏ÂêåÊàñ‰∏çÂêåÔºâ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂ∞ΩÁÆ°Ê®°Âûã‰ªÖÂú®ËßÜËßâÊØîËæÉ‰ªªÂä°‰∏äËÆ≠ÁªÉÔºå‰ΩÜÂÖ∂Â≠¶‰π†Âà∞ÁöÑÊé®ÁêÜËÉΩÂäõËÉΩÂ§üÊúâÊïàÂú∞Êé®ÂπøÂà∞ÂêÑÁßçÈóÆÈ¢ò‰∏ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19741",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "url": "https://huggingface.co/papers/2506.19741",
            "abstract": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT",
            "score": 1,
            "issue_id": 4548,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 –∏—é–Ω—è",
                "en": "June 24",
                "zh": "6Êúà24Êó•"
            },
            "hash": "288a2c7ef1ba6865",
            "authors": [
                "Yihong Luo",
                "Shuchen Xue",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "NUS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üéõÔ∏è",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Noise Consistency Training (NCT) –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. NCT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä–Ω—ã–π –º–æ–¥—É–ª—å –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —à—É–º–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —à—É–º–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º —É—Å–ª–æ–≤–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —É–∫–∞–∑–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NCT –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Efficient Control in AI Content Generation with Noise Consistency Training",
                    "desc": "This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content."
                },
                "zh": {
                    "title": "Âô™Â£∞‰∏ÄËá¥ÊÄßËÆ≠ÁªÉÔºöÈ´òÊïàÂèØÊéßÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂô™Â£∞‰∏ÄËá¥ÊÄßËÆ≠ÁªÉÔºàNCTÔºâÊñπÊ≥ïÔºåËÉΩÂ§üÈ´òÊïàÂú∞Â∞ÜÊñ∞ÁöÑÊéßÂà∂‰ø°Âè∑Êï¥ÂêàÂà∞È¢ÑËÆ≠ÁªÉÁöÑ‰∏ÄÊ≠•ÁîüÊàêÂô®‰∏≠ÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂØπÂü∫Á°ÄÊ®°ÂûãËøõË°åÊòÇË¥µÁöÑ‰øÆÊîπÔºåËÄåNCTÈÄöËøáÂºïÂÖ•ÈÄÇÈÖçÊ®°ÂùóÂíåÂô™Â£∞‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºåÂú®ÁîüÊàêÂô®ÁöÑÂô™Â£∞Á©∫Èó¥‰∏≠Áõ¥Êé•ËøõË°åË∞ÉÊï¥„ÄÇËØ•ÊñπÊ≥ïÂú®ÁîüÊàêË¥®ÈáèÂíåËÆ°ÁÆóÊïàÁéá‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂ§öÊ≠•ÂíåËí∏È¶èÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂèØÊéßÁîüÊàêÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇNCTÁöÑÊ®°ÂùóÂåñËÆæËÆ°‰ΩøÂÖ∂Âú®Êï∞ÊçÆ‰ΩøÁî®‰∏äÊõ¥Âä†È´òÊïàÔºåÊòì‰∫éÈÉ®ÁΩ≤„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-06-27.html",
    "link_next": "2025-07-01.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6Êúà27Êó•"
    },
    "short_date_next": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7Êúà1Êó•"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}