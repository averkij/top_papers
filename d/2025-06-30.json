{
    "date": {
        "ru": "30 Ğ¸ÑĞ½Ñ",
        "en": "June 30",
        "zh": "6æœˆ30æ—¥"
    },
    "time_utc": "2025-06-30 05:15",
    "weekday": 0,
    "issue_id": 4550,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.17450",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "url": "https://huggingface.co/papers/2506.17450",
            "abstract": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.",
            "score": 14,
            "issue_id": 4550,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "b5bb4470d500be10",
            "authors": [
                "Jiacheng Chen",
                "Ramin Mehran",
                "Xuhui Jia",
                "Saining Xie",
                "Sanghyun Woo"
            ],
            "affiliations": [
                "Google DeepMind",
                "New York University",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17450.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "BlenderFusion - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½. ĞĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ñ€Ğ¾Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². BlenderFusion Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing Scene Editing with BlenderFusion",
                    "desc": "BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods."
                },
                "zh": {
                    "title": "ç”Ÿæˆè§†è§‰åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "BlenderFusionæ˜¯ä¸€ä¸ªç”Ÿæˆè§†è§‰åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é‡æ–°ç»„åˆå¯¹è±¡ã€ç›¸æœºå’ŒèƒŒæ™¯æ¥åˆæˆæ–°åœºæ™¯ã€‚å®ƒé‡‡ç”¨åˆ†å±‚-ç¼–è¾‘-åˆæˆçš„æµç¨‹ï¼Œé¦–å…ˆå°†è§†è§‰è¾“å…¥åˆ†å‰²å¹¶è½¬æ¢ä¸ºå¯ç¼–è¾‘çš„3Då®ä½“ï¼Œç„¶ååœ¨Blenderä¸­è¿›è¡Œ3Dæ§åˆ¶çš„ç¼–è¾‘ï¼Œæœ€åä½¿ç”¨ç”Ÿæˆåˆæˆå™¨å°†å®ƒä»¬èåˆæˆä¸€ä¸ªè¿è´¯çš„åœºæ™¯ã€‚è¯¥ç”Ÿæˆåˆæˆå™¨æ‰©å±•äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åŸå§‹åœºæ™¯å’Œç¼–è¾‘åçš„åœºæ™¯ã€‚BlenderFusionåœ¨å¤æ‚çš„åˆæˆåœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21862",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "url": "https://huggingface.co/papers/2506.21862",
            "abstract": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "score": 6,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "b9ad171aa3fb5bbf",
            "authors": [
                "Boyuan Sun",
                "Jiaxing Zhao",
                "Xihan Wei",
                "Qibin Hou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21862.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#dataset",
                    "#video"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LLaVA-Scissor - ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² (SCC) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ. LLaVA-Scissor Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ SCC ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaVA-Scissor Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Efficient Video Understanding with Semantic Token Compression",
                    "desc": "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."
                },
                "zh": {
                    "title": "LLaVA-Scissorï¼šé«˜æ•ˆçš„è§†é¢‘ä»¤ç‰Œå‹ç¼©ç­–ç•¥",
                    "desc": "LLaVA-Scissoræ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œå‹ç¼©ç­–ç•¥ã€‚å®ƒåˆ©ç”¨è¯­ä¹‰è¿é€šç»„ä»¶ï¼ˆSCCï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å°†ä»¤ç‰Œåˆ†é…åˆ°ä¸åŒçš„è¯­ä¹‰åŒºåŸŸï¼Œä»è€Œç¡®ä¿å…¨é¢çš„è¯­ä¹‰è¦†ç›–ã€‚ä¸ä»¥å¾€åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„å‹ç¼©æ–¹æ³•ä¸åŒï¼ŒLLaVA-Scissorèƒ½å¤Ÿå‡å°‘ä»¤ç‰Œå†—ä½™ï¼Œå¹¶åœ¨ç©ºé—´å’Œæ—¶é—´åŸŸä¸­è¿›è¡Œä¸¤æ­¥å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ä½ä»¤ç‰Œä¿ç•™æ¯”ç‡ä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22434",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "url": "https://huggingface.co/papers/2506.22434",
            "abstract": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.",
            "score": 5,
            "issue_id": 4548,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "d7e89f248d4c331e",
            "authors": [
                "Xi Chen",
                "Mingkang Zhu",
                "Shaoteng Liu",
                "Xiaoyang Wu",
                "Xiaogang Xu",
                "Yu Liu",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "CUHK",
                "HKU",
                "HUST",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22434.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Supervised Image Triplet Learning",
                    "desc": "This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä½¿ç”¨å›¾åƒä¸‰å…ƒç»„çš„è‡ªç›‘ç£å­¦ä¹ æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šå›¾åƒä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ç ”ç©¶è€…ä»¬æ„å»ºäº†ç”±åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒç»„æˆçš„å›¾åƒä¸‰å…ƒç»„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«è¦æ±‚ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œä»¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³åˆ¤æ–­ç›¸åŒæˆ–ä¸åŒï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä»…åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šè®­ç»ƒï¼Œä½†å…¶å­¦ä¹ åˆ°çš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§é—®é¢˜ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21656",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "url": "https://huggingface.co/papers/2506.21656",
            "abstract": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.",
            "score": 3,
            "issue_id": 4548,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "8d063b13fc555964",
            "authors": [
                "Yifan Shen",
                "Yuanzhe Liu",
                "Jingyuan Zhu",
                "Xu Cao",
                "Xiaofeng Zhang",
                "Yixiao He",
                "Wenming Ye",
                "James Matthew Rehg",
                "Ismini Lourentzou"
            ],
            "affiliations": [
                "Google",
                "Shanghai Jiao Tong University",
                "University of Illinois Urbana-Champaign",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21656.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#rlhf",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ",
                    "desc": "SpatialReasoner-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. SpatialReasoner-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SPATIALRGPT-Bench, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 9.8% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Elevating Spatial Reasoning with SpatialReasoner-R1",
                    "desc": "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."
                },
                "zh": {
                    "title": "ç©ºé—´æ¨ç†çš„æ–°çªç ´",
                    "desc": "SpatialReasoner-R1æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šæ¨¡å‹è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·ä¸”é€»è¾‘ä¸€è‡´çš„é•¿é“¾æ€ç»´æ¨ç†è½¨è¿¹ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒSpatialReasoner-R1è¿˜å¼•å…¥äº†ç»†ç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆfDPOï¼‰ï¼Œé€šè¿‡ç©ºé—´å¥–åŠ±æœºåˆ¶å¯¹å€™é€‰å“åº”è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæé«˜æè¿°æ€§åŸºç¡€å’Œé€»è¾‘æ¨ç†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€å¼ºåŸºçº¿æé«˜äº†9.8%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.20279",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "url": "https://huggingface.co/papers/2506.20279",
            "abstract": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj",
            "score": 2,
            "issue_id": 4550,
            "pub_date": "2025-06-25",
            "pub_date_card": {
                "ru": "25 Ğ¸ÑĞ½Ñ",
                "en": "June 25",
                "zh": "6æœˆ25æ—¥"
            },
            "hash": "8382f71877fe1997",
            "authors": [
                "Changliang Xia",
                "Chengyou Jia",
                "Zhuohang Dang",
                "Minnan Luo"
            ],
            "affiliations": [
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.20279.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DenseDiT - ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². DenseDiT Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¾Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 0,01% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "DenseDiT: Revolutionizing Dense Prediction with Minimal Data",
                    "desc": "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."
                },
                "zh": {
                    "title": "DenseDiTï¼šç”¨æœ€å°‘æ•°æ®å®ç°å¯†é›†é¢„æµ‹çš„çªç ´",
                    "desc": "DenseDiTæ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ä»¥æœ€å°‘çš„è®­ç»ƒæ•°æ®å®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚å¯†é›†é¢„æµ‹ä»»åŠ¡åœ¨è®¡ç®—æœºè§†è§‰ä¸­éå¸¸é‡è¦ï¼Œæ—¨åœ¨ä¸ºè¾“å…¥å›¾åƒå­¦ä¹ é€åƒç´ çš„æ ‡æ³¨æ ‡ç­¾ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œç¼ºä¹å¯¹çœŸå®åœºæ™¯çš„å¹¿æ³›é€‚åº”æ€§ï¼Œä¸”é¢ä¸´çœŸå®æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚DenseDiTé€šè¿‡æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è§†è§‰å…ˆéªŒï¼Œç»“åˆå‚æ•°é‡ç”¨æœºåˆ¶å’Œè½»é‡çº§åˆ†æ”¯ï¼Œèƒ½å¤Ÿåœ¨å¤šç§çœŸå®ä¸–ç•Œçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.19741",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "url": "https://huggingface.co/papers/2506.19741",
            "abstract": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT",
            "score": 2,
            "issue_id": 4548,
            "pub_date": "2025-06-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ½Ñ",
                "en": "June 24",
                "zh": "6æœˆ24æ—¥"
            },
            "hash": "288a2c7ef1ba6865",
            "authors": [
                "Yihong Luo",
                "Shuchen Xue",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "NUS",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.19741.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Noise Consistency Training (NCT) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NCT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑˆÑƒĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NCT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Control in AI Content Generation with Noise Consistency Training",
                    "desc": "This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content."
                },
                "zh": {
                    "title": "å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼šé«˜æ•ˆå¯æ§ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼ˆNCTï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†æ–°çš„æ§åˆ¶ä¿¡å·æ•´åˆåˆ°é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä¸­ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„ä¿®æ”¹ï¼Œè€ŒNCTé€šè¿‡å¼•å…¥é€‚é…æ¨¡å—å’Œå™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œåœ¨ç”Ÿæˆå™¨çš„å™ªå£°ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¤šæ­¥å’Œè’¸é¦æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¯æ§ç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚NCTçš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶åœ¨æ•°æ®ä½¿ç”¨ä¸Šæ›´åŠ é«˜æ•ˆï¼Œæ˜“äºéƒ¨ç½²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-27.html",
    "link_next": "2025-07-01.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "27.06",
        "en": "06/27",
        "zh": "6æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "01.07",
        "en": "07/01",
        "zh": "7æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}