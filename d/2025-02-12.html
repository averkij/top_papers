
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. February 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 февраля</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-11.html">⬅️ <span id="prev-date">11.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-13.html">➡️ <span id="next-date">13.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 февраля', 'en': 'February 12', 'zh': '2月12日'};
        let feedDateNext = {'ru': '13.02', 'en': '02/13', 'zh': '2月13日'};
        let feedDatePrev = {'ru': '11.02', 'en': '02/11', 'zh': '2月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'score': 34, 'issue_id': 2164, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fd76cceb75f32321', 'authors': ['OpenAI', ':', 'Ahmed El-Kishky', 'Alexander Wei', 'Andre Saraiva', 'Borys Minaev', 'Daniel Selsam', 'David Dohan', 'Francis Song', 'Hunter Lightman', 'Ignasi Clavera', 'Jakub Pachocki', 'Jerry Tworek', 'Lorenz Kuhn', 'Lukasz Kaiser', 'Mark Chen', 'Max Schwarzer', 'Mostafa Rohaninejad', 'Nat McAleese', 'o3 contributors', 'Oleg Mürk', 'Rhythm Garg', 'Rui Shu', 'Szymon Sidor', 'Vineet Kosaraju', 'Wenda Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.06807.jpg', 'data': {'categories': ['#rlhf', '#rl', '#games', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением превосходит специализированные подходы в задачах рассуждения', 'desc': 'Исследование показывает, что применение обучения с подкреплением к большим языковым моделям (LLM) значительно улучшает их производительность в сложных задачах программирования и рассуждения. Авторы сравнивают модели общего назначения (OpenAI o1 и o3) со специализированной системой o1-ioi, разработанной для участия в Международной олимпиаде по информатике (IOI) 2024 года. Модель o3 достигла уровня золотой медали на IOI 2024 без использования специфических стратегий или послаблений правил. Результаты указывают на то, что масштабирование обучения с подкреплением общего назначения является более эффективным подходом к созданию ИИ для задач рассуждения, чем разработка узкоспециализированных техник.'}, 'en': {'title': 'Scaling General-Purpose Learning Outshines Specialized Strategies', 'desc': 'This paper demonstrates that applying reinforcement learning to large language models (LLMs) enhances their ability to tackle complex coding and reasoning challenges. It compares two reasoning models, OpenAI o1 and an early version of o3, against a specialized model, o1-ioi, which uses tailored inference strategies for the 2024 International Olympiad in Informatics (IOI). The results show that while o1-ioi performed well with its hand-crafted strategies, the later model o3 achieved superior results without such specific techniques. This suggests that scaling general-purpose reinforcement learning is a more effective approach for achieving high performance in reasoning tasks, like competitive programming.'}, 'zh': {'title': '强化学习助力通用模型超越特定领域系统', 'desc': '本论文展示了强化学习在大型语言模型（LLMs）中的应用，显著提升了复杂编码和推理任务的表现。我们比较了两种通用推理模型——OpenAI的o1和o3的早期检查点，以及一个特定领域的系统o1-ioi，该系统使用手工设计的推理策略。o1-ioi在2024年国际信息学奥林匹克竞赛中表现良好，获得了第49百分位的成绩，而在放宽竞争约束的情况下则获得了金牌。我们的研究表明，尽管专门的管道如o1-ioi能带来显著提升，但扩展的通用o3模型在没有依赖手工推理启发式的情况下，超越了这些结果。'}}}, {'id': 'https://huggingface.co/papers/2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'url': 'https://huggingface.co/papers/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.', 'score': 19, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'fd4f34152d4de2c1', 'authors': ['Junlong Li', 'Daya Guo', 'Dejian Yang', 'Runxin Xu', 'Yu Wu', 'Junxian He'], 'affiliations': ['DeepSeek-AI', 'HKUST', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07316.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'CodeI/O: Раскрытие потенциала рассуждений в больших языковых моделях через код', 'desc': 'Статья представляет новый подход CodeI/O для улучшения способностей больших языковых моделей к рассуждениям. Метод преобразует разнообразные паттерны рассуждений, встроенные в код, в формат предсказания ввода-вывода кода на естественном языке. Это позволяет моделям изучать универсальные примитивы рассуждений, такие как планирование логического потока и модульная декомпозиция. Эксперименты показывают, что CodeI/O приводит к улучшениям в задачах символических, научных, логических и других типов рассуждений.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with CodeI/O', 'desc': 'This paper introduces CodeI/O, a new method designed to enhance reasoning capabilities in Large Language Models (LLMs) by transforming code into a format that predicts inputs and outputs. The approach focuses on training models using natural language Chain-of-Thought (CoT) rationales, which helps the models learn universal reasoning patterns without being tied to specific coding syntax. By exposing models to various reasoning tasks, such as logic flow and state-space searching, CodeI/O improves performance across multiple domains, including math and commonsense reasoning. The results show that this method not only boosts reasoning accuracy but also allows for verification and refinement of predictions through a multi-turn revision process, leading to even better outcomes with CodeI/O++.'}, 'zh': {'title': 'CodeI/O：提升推理能力的新方法', 'desc': '这篇论文提出了一种新的方法，称为CodeI/O，旨在提高大型语言模型的推理能力。通过将原始代码转换为输入输出预测格式，CodeI/O系统地提炼了多样的推理模式。模型通过自然语言的链式思维（CoT）推理来预测代码的输入和输出，从而增强了逻辑流规划、状态空间搜索等推理原语的能力。实验结果表明，CodeI/O在多种推理任务上均表现出一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07701', 'title': 'Magic 1-For-1: Generating One Minute Video Clips within One Minute', 'url': 'https://huggingface.co/papers/2502.07701', 'abstract': 'In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.', 'score': 14, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '7212b752112bcd5a', 'authors': ['Hongwei Yi', 'Shitong Shao', 'Tian Ye', 'Jiantong Zhao', 'Qingyu Yin', 'Michael Lingelbach', 'Li Yuan', 'Yonghong Tian', 'Enze Xie', 'Daquan Zhou'], 'affiliations': ['Hedra Inc.', 'Nvidia', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07701.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#multimodal', '#open_source', '#diffusion', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео: от текста к кадрам за секунды', 'desc': 'Magic 1-For-1 (Magic141) - это эффективная модель генерации видео с оптимизированным потреблением памяти и латентностью вывода. Ключевая идея заключается в разделении задачи генерации видео по тексту на две более простые задачи: генерацию изображения по тексту и генерацию видео по изображению. Авторы применяют ряд оптимизационных приемов, включая многомодальное введение условий, состязательную дистилляцию шагов и разреживание параметров. В результате модель способна генерировать 5-секундные видеоклипы менее чем за 3 секунды, а минутное видео - за одну минуту с улучшенным качеством и динамикой.'}, 'en': {'title': 'Efficient Video Generation: Simplifying with Magic141', 'desc': 'The paper introduces Magic 1-For-1 (Magic141), a video generation model designed to optimize memory usage and reduce inference time. It simplifies the text-to-video generation process by breaking it down into two tasks: generating images from text and then creating videos from those images. The authors demonstrate that the image-to-video task converges more easily than the direct text-to-video approach, allowing for faster training. They implement various optimization techniques to enhance model performance, achieving impressive video generation speeds while maintaining high visual quality.'}, 'zh': {'title': '高效视频生成，轻松实现！', 'desc': '本文介绍了一种高效的视频生成模型Magic 1-For-1（Magic141），该模型优化了内存消耗和推理延迟。其核心思想是将文本到视频生成任务分解为两个更简单的任务：文本到图像生成和图像到视频生成。研究表明，使用相同的优化算法，图像到视频任务的收敛速度确实优于文本到视频任务。通过多种优化技巧，模型能够在短时间内生成高质量的视频片段，显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2502.06329', 'title': 'Expect the Unexpected: FailSafe Long Context QA for Finance', 'url': 'https://huggingface.co/papers/2502.06329', 'abstract': 'We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA', 'score': 12, 'issue_id': 2168, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '836d77158c8a414b', 'authors': ['Kiran Kamble', 'Melisa Russak', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Mateusz Russak', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc'], 'pdf_title_img': 'assets/pdf/title_img/2502.06329.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#long_context', '#hallucinations'], 'emoji': '💼', 'ru': {'title': 'FailSafeQA: Новый стандарт оценки надежности языковых моделей в финансах', 'desc': 'Исследователи представили новый эталонный тест FailSafeQA для оценки устойчивости и контекстной осведомленности языковых моделей (LLM) в финансовой сфере. Тест включает шесть вариаций взаимодействия человека с системой, основанной на LLM, и фокусируется на двух сценариях: отказ запроса и отказ контекста. Используя методологию LLM-as-a-Judge и модель Qwen2.5-72B-Instruct, авторы оценили 24 готовые модели по критериям устойчивости, контекстной обоснованности и соответствия. Результаты показали, что даже высокопроизводительные модели имеют значительный потенциал для улучшения, и подчеркнули важность FailSafeQA как инструмента для разработки более надежных LLM в финансовых приложениях.'}, 'en': {'title': 'Enhancing LLM Reliability in Finance with FailSafeQA', 'desc': 'The paper introduces FailSafeQA, a benchmark aimed at evaluating the robustness and context-awareness of large language models (LLMs) in financial query-answer systems. It focuses on two main failure scenarios: Query Failure, where the input query is altered in terms of expertise and clarity, and Context Failure, where irrelevant or degraded documents are introduced. The authors utilize the LLM-as-a-Judge methodology to assess various models based on their Robustness, Context Grounding, and Compliance scores. The findings reveal that while some models perform well under input variations, they struggle with maintaining accuracy, indicating a need for further enhancements in LLM reliability for financial contexts.'}, 'zh': {'title': 'FailSafeQA：提升金融领域LLM的鲁棒性与上下文意识', 'desc': '我们提出了一个新的长上下文金融基准，FailSafeQA，旨在测试大型语言模型（LLMs）在金融领域中对人机交互的鲁棒性和上下文意识。我们关注两个案例研究：查询失败和上下文失败。在查询失败场景中，我们通过改变领域专业性、完整性和语言准确性来扰动原始查询。在上下文失败案例中，我们模拟上传降级、无关和空文档的情况。'}}}, {'id': 'https://huggingface.co/papers/2502.03492', 'title': 'Teaching Language Models to Critique via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.03492', 'abstract': 'Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose CTRL, a framework for Critic Training via Reinforcement Learning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with CTRL significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.', 'score': 12, 'issue_id': 2165, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'a0c98706806837c6', 'authors': ['Zhihui Xie', 'Jie chen', 'Liyu Chen', 'Weichao Mao', 'Jingjing Xu', 'Lingpeng Kong'], 'affiliations': ['Bytedance, Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.03492.jpg', 'data': {'categories': ['#rlhf', '#training', '#benchmark', '#reasoning', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствующиеся ИИ-критики для генерации кода', 'desc': 'Статья представляет CTRL - фреймворк для обучения моделей-критиков с помощью обучения с подкреплением. Цель - научить языковые модели (LLM) генерировать полезную обратную связь для улучшения качества генерируемого кода без участия человека. Результаты показывают, что обученные критики значительно повышают процент успешных решений и уменьшают накопление ошибок. Модели-критики также могут использоваться как генеративные модели вознаграждения и позволяют улучшать результаты во время тестирования через итеративную критику и исправления.'}, 'en': {'title': 'Empowering Code Generation with Self-Critiquing Models', 'desc': 'This paper introduces CTRL, a framework that trains large language models (LLMs) to act as critics for code generation tasks. By using reinforcement learning, CTRL enables these critic models to provide feedback that helps improve the performance of a fixed generator model without needing human input. The study shows that critics trained with CTRL can significantly increase the success rates of code generation and reduce errors. Additionally, these critics function as effective generative reward models, allowing for iterative improvements during testing, leading to substantial performance gains on difficult benchmarks.'}, 'zh': {'title': '通过批评训练提升代码生成性能', 'desc': '本文研究了大型语言模型（LLMs）在代码生成中的批评和改进能力。我们提出了CTRL框架，通过强化学习训练批评模型，生成反馈以提高固定生成模型的修正性能，而无需人工监督。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并减少了累积错误。此外，这些批评模型作为生成奖励模型，能够在测试时通过迭代批评-修订实现扩展，在具有挑战性的代码生成基准上实现了高达106.1%的相对改进。'}}}, {'id': 'https://huggingface.co/papers/2502.07617', 'title': 'Scaling Pre-training to One Hundred Billion Data for Vision Language Models', 'url': 'https://huggingface.co/papers/2502.07617', 'abstract': "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.", 'score': 12, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '503a9dac2cae323c', 'authors': ['Xiao Wang', 'Ibrahim Alabdulmohsin', 'Daniel Salz', 'Zhe Li', 'Keran Rong', 'Xiaohua Zhai'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.07617.jpg', 'data': {'categories': ['#cultural_diversity', '#multilingual', '#dataset', '#low_resource', '#data', '#multimodal', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Масштабное предобучение для инклюзивных мультимодальных систем', 'desc': 'Исследование посвящено предобучению мультимодальных моделей на беспрецедентно большом наборе данных в 100 миллиардов примеров. Авторы обнаружили, что производительность модели на многих западноцентричных бенчмарках насыщается при таком масштабе. Однако задачи, связанные с культурным разнообразием, показывают значительный прирост благодаря охвату редких концепций в больших данных. Кроме того, исследование выявило улучшение производительности для низкоресурсных языков и предостерегает от чрезмерной фильтрации данных, которая может снизить культурное разнообразие.'}, 'en': {'title': 'Unlocking Cultural Diversity with 100 Billion Examples', 'desc': 'This paper investigates the effects of pre-training vision-language models using a massive dataset of 100 billion examples. The authors find that while performance on common benchmarks tends to plateau, tasks that involve cultural diversity show significant improvements due to the extensive coverage of diverse concepts in the dataset. Additionally, the study highlights the benefits of this large-scale data for enhancing multilingual capabilities, particularly for low-resource languages. However, it also warns that applying quality filters to reduce dataset size can diminish the representation of cultural diversity, emphasizing the importance of large-scale data for inclusive multimodal systems.'}, 'zh': {'title': '大规模预训练助力文化多样性', 'desc': '本文探讨了在前所未有的规模上（1000亿个示例）对视觉-语言模型进行预训练的潜力。研究发现，在许多常见的西方分类和检索基准上，模型性能在此规模下趋于饱和。然而，对于文化多样性的任务，1000亿规模的网络数据带来了更显著的提升，因为它涵盖了长尾概念。此外，研究还分析了模型的多语言能力，显示在低资源语言上也有提升。'}}}, {'id': 'https://huggingface.co/papers/2502.07374', 'title': 'LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!', 'url': 'https://huggingface.co/papers/2502.07374', 'abstract': "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.", 'score': 12, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '4df9e17df3250cb4', 'authors': ['Dacheng Li', 'Shiyi Cao', 'Tyler Griggs', 'Shu Liu', 'Xiangxi Mo', 'Shishir G. Patil', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.07374.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#math', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение длинным цепочкам рассуждений в больших языковых моделях', 'desc': 'Исследование показывает, что большие языковые модели (LLM) могут эффективно обучаться длинным цепочкам рассуждений (Long CoT) с помощью контролируемой дообучки на небольшом наборе данных. Модель Qwen2.5-32B-Instruct, обученная на 17 тысячах примеров Long CoT, значительно улучшила результаты в задачах по математике и программированию. Структура Long CoT оказалась критически важной для обучения, в то время как содержание отдельных шагов рассуждения имело минимальное влияние. Эти выводы углубляют понимание того, как развивать способности к рассуждению в LLM.'}, 'en': {'title': 'Unlocking Reasoning: Structure Over Content in Large Models', 'desc': 'This paper explores how Large Reasoning Models (LRMs) can improve their reasoning abilities by learning from structured long chain-of-thought (Long CoT) examples. It demonstrates that a Large Language Model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning and low-rank adaptation techniques. The study reveals that the structure of Long CoT is crucial for learning, while the specific content of reasoning steps has a minimal effect on performance. The findings suggest that maintaining logical consistency in reasoning steps is vital for accuracy, even when training on incorrect samples.'}, 'zh': {'title': '长链思维：推理模型的关键结构', 'desc': '大型推理模型（LRMs）通过长链思维（Long CoT）解决复杂的推理问题，这种思维方式包括反思、回溯和自我验证。我们发现，大型语言模型（LLM）可以通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）有效学习长链思维。仅使用17,000个长链思维训练样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中取得了显著提升。研究表明，长链思维的结构对学习过程至关重要，而单个推理步骤的内容对性能影响较小。'}}}, {'id': 'https://huggingface.co/papers/2502.05878', 'title': 'Retrieval-augmented Large Language Models for Financial Time Series Forecasting', 'url': 'https://huggingface.co/papers/2502.05878', 'abstract': 'Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.', 'score': 11, 'issue_id': 2172, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'bb6c947ce857a2db', 'authors': ['Mengxi Xiao', 'Zihao Jiang', 'Lingfei Qian', 'Zhengyu Chen', 'Yueru He', 'Yijing Xu', 'Yuecheng Jiang', 'Dong Li', 'Ruey-Ling Weng', 'Min Peng', 'Jimin Huang', 'Sophia Ananiadou', 'Qianqian Xie'], 'affiliations': ['School of Computer Science, Wuhan University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05878.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#rag'], 'emoji': '📈', 'ru': {'title': 'FinSeer: Умный поиск для точного прогноза акций', 'desc': 'Статья представляет новую систему для прогнозирования движения акций на основе технологии извлечения и генерации (RAG). Ключевые инновации включают использование крупной языковой модели StockLLM, новый метод отбора кандидатов с обратной связью от LLM и специальную цель обучения для поиска значимых исторических последовательностей. Разработанная система FinSeer превосходит существующие методы извлечения информации, достигая на 8% более высокой точности на датасете BIGDATA22. Работа подчеркивает важность специализированных моделей извлечения данных в финансовом прогнозировании.'}, 'en': {'title': 'Revolutionizing Stock Prediction with RAG Framework', 'desc': 'This paper introduces a new approach to predicting stock movements by using a retrieval-augmented generation (RAG) framework specifically designed for financial time-series data. The framework employs a large language model called StockLLM, which has been fine-tuned to better understand financial contexts. It also features a unique candidate selection method that utilizes feedback from the language model to improve the relevance of retrieved data. The results show that this method significantly enhances prediction accuracy and uncovers important patterns in complex financial datasets, outperforming traditional retrieval techniques.'}, 'zh': {'title': '金融预测的新框架：检索增强生成', 'desc': '本文提出了一种用于金融时间序列预测的检索增强生成（RAG）框架，旨在从大量时间序列数据中识别和提取关键影响因素。我们使用了一个经过微调的1B参数大型语言模型（StockLLM）作为基础，并引入了一种新颖的候选选择方法，利用LLM反馈来优化检索过程。通过最大化查询与历史重要序列之间的相似性，我们的检索器FinSeer能够在复杂的金融数据中发现有意义的模式，同时减少噪声。实验结果表明，该RAG框架在准确性上优于传统方法，强调了定制检索模型在金融预测中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.07508', 'title': 'Enhance-A-Video: Better Generated Video for Free', 'url': 'https://huggingface.co/papers/2502.07508', 'abstract': 'DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.', 'score': 9, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'e02d3082d3b21016', 'authors': ['Yang Luo', 'Xuanlei Zhao', 'Mengzhao Chen', 'Kaipeng Zhang', 'Wenqi Shao', 'Kai Wang', 'Zhangyang Wang', 'Yang You'], 'affiliations': ['National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.07508.jpg', 'data': {'categories': ['#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'Enhance-A-Video: Повышение качества генерации видео без переобучения', 'desc': 'Данная статья представляет новый подход к улучшению качества видео, генерируемого моделями на основе DiT (Diffusion Transformer), без необходимости дополнительного обучения. Метод, названный Enhance-A-Video, фокусируется на усилении межкадровых корреляций с использованием недиагональных распределений временного внимания. Подход легко применим к большинству существующих фреймворков генерации видео на основе DiT без переобучения или дополнительной настройки. Результаты демонстрируют значительное улучшение как временной согласованности, так и визуального качества генерируемых видео.'}, 'en': {'title': 'Enhancing DiT Video Generation Without Retraining', 'desc': 'This paper presents a novel method called Enhance-A-Video, aimed at improving the coherence and quality of videos generated by DiT-based models. The approach focuses on enhancing cross-frame correlations using non-diagonal temporal attention distributions, which helps maintain consistency across frames. Importantly, Enhance-A-Video does not require any retraining or fine-tuning, making it easy to integrate into existing DiT frameworks. The results show significant improvements in both temporal consistency and visual quality, paving the way for further advancements in video generation techniques.'}, 'zh': {'title': '提升视频生成质量的新方法', 'desc': '本研究提出了一种名为 Enhance-A-Video 的方法，旨在提高基于 DiT 的视频生成模型的连贯性和质量。该方法通过增强跨帧相关性，利用非对角时间注意力分布来实现。由于其设计简单，该方法可以轻松应用于大多数基于 DiT 的视频生成框架，而无需重新训练或微调。我们的实验表明，该方法在时间一致性和视觉质量方面都取得了显著的改善。'}}}, {'id': 'https://huggingface.co/papers/2502.07527', 'title': 'NatureLM: Deciphering the Language of Nature for Scientific Discovery', 'url': 'https://huggingface.co/papers/2502.07527', 'abstract': 'Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.', 'score': 8, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'a6e947f52bde9a9c', 'authors': ['Yingce Xia', 'Peiran Jin', 'Shufang Xie', 'Liang He', 'Chuan Cao', 'Renqian Luo', 'Guoqing Liu', 'Yue Wang', 'Zequn Liu', 'Yuan-Jyue Chen', 'Zekun Guo', 'Yeqi Bai', 'Pan Deng', 'Yaosen Min', 'Ziheng Lu', 'Hongxia Hao', 'Han Yang', 'Jielan Li', 'Chang Liu', 'Jia Zhang', 'Jianwei Zhu', 'Kehan Wu', 'Wei Zhang', 'Kaiyuan Gao', 'Qizhi Pei', 'Qian Wang', 'Xixian Liu', 'Yanting Li', 'Houtian Zhu', 'Yeqing Lu', 'Mingqian Ma', 'Zun Wang', 'Tian Xie', 'Krzysztof Maziarz', 'Marwin Segler', 'Zhao Yang', 'Zilong Chen', 'Yu Shi', 'Shuxin Zheng', 'Lijun Wu', 'Chen Hu', 'Peggy Dai', 'Tie-Yan Liu', 'Haiguang Liu', 'Tao Qin'], 'affiliations': ['Microsoft Research AI for Science'], 'pdf_title_img': 'assets/pdf/title_img/2502.07527.jpg', 'data': {'categories': ['#healthcare', '#architecture', '#science', '#optimization', '#transfer_learning', '#training', '#dataset', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'NatureLM: единая модель для научных открытий во множестве доменов', 'desc': 'Исследователи разработали NatureLM - языковую модель для научных открытий, объединяющую различные научные домены. Эта фундаментальная модель обучена на данных из нескольких областей, включая молекулы, материалы, белки, ДНК и РНК. NatureLM способна генерировать и оптимизировать объекты из разных доменов, а также выполнять кросс-доменные задачи. Модель демонстрирует высокую производительность в различных научных задачах и доступна в нескольких размерах, от 1 до 46,7 миллиардов параметров.'}, 'en': {'title': 'NatureLM: Unifying Science Through Language Models', 'desc': 'This paper introduces Nature Language Model (NatureLM), a foundation model designed to enhance scientific discovery by integrating knowledge across various scientific domains. Unlike traditional models that operate in isolation, NatureLM is pre-trained on data from multiple fields, allowing it to understand and generate sequences related to small molecules, proteins, RNA, and materials. The model supports diverse applications, such as generating new compounds and optimizing existing ones, while also excelling in specific tasks like translating chemical notations. With different sizes available, NatureLM demonstrates improved performance with larger models, showcasing its potential as a versatile tool in drug discovery and material design.'}, 'zh': {'title': '自然语言模型：科学发现的新工具', 'desc': '基础模型在自然语言处理和人工智能领域带来了革命性的变化，显著提升了机器理解和生成自然语言的能力。受基础模型成功的启发，研究人员为各个科学领域开发了相应的基础模型，但这些模型通常是孤立训练的，缺乏跨领域整合的能力。我们提出了自然语言模型（NatureLM），这是一个基于序列的科学基础模型，旨在促进科学发现。NatureLM经过多领域数据的预训练，能够支持小分子、蛋白质、RNA和材料的生成与优化，并在多个科学任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.06589', 'title': 'Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training', 'url': 'https://huggingface.co/papers/2502.06589', 'abstract': 'Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.', 'score': 8, 'issue_id': 2164, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '4273eabfdc59b328', 'authors': ['Yuchen Zhuang', 'Jingfeng Yang', 'Haoming Jiang', 'Xin Liu', 'Kewei Cheng', 'Sanket Lokegaonkar', 'Yifan Gao', 'Qing Ping', 'Tianyi Liu', 'Binxuan Huang', 'Zheng Li', 'Zhengyang Wang', 'Pei Chen', 'Ruijie Wang', 'Rongzhi Zhang', 'Nasser Zalmout', 'Priyanka Nigam', 'Bing Yin', 'Chao Zhang'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.06589.jpg', 'data': {'categories': ['#agents', '#transfer_learning', '#optimization', '#training', '#reasoning', '#dataset', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'Кузница агентов: улучшение LLM через специализированное предобучение', 'desc': 'Исследователи представили Hephaestus-Forge - первый крупномасштабный корпус для предобучения, направленный на улучшение фундаментальных способностей агентов на основе больших языковых моделей (LLM). Корпус содержит 103 миллиарда специфичных для агентов данных, охватывающих 76,537 API, включая документацию и траектории вызовов функций. Применение Hephaestus-Forge в процессе дообучения позволило модели Hephaestus превзойти открытые LLM малого и среднего масштаба и конкурировать с коммерческими LLM в трех тестах для агентов. Это демонстрирует эффективность предложенного подхода в улучшении базовых агентных возможностей и обобщающей способности LLM для новых задач и сред.'}, 'en': {'title': 'Empowering LLM Agents with Hephaestus-Forge', 'desc': 'This paper presents Hephaestus-Forge, a large-scale pre-training dataset specifically designed for enhancing the capabilities of large language model (LLM) agents. It includes 103 billion agent-specific data points, featuring 76,537 APIs, which provide both documentation and function calling examples to improve reasoning and planning skills. The authors explore different training protocols and data mixing ratios to optimize the pre-training process. Results show that agents trained on Hephaestus-Forge outperform smaller open-source LLMs and compete with commercial models, highlighting its effectiveness in improving agent performance and adaptability.'}, 'zh': {'title': 'Hephaestus-Forge：提升LLM代理能力的创新预训练语料库', 'desc': '由于缺乏面向代理的预训练数据，基于大型语言模型（LLM）的自主代理通常依赖复杂的提示或广泛的微调，这往往无法在保持强泛化能力的同时引入新功能。我们提出了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强LLM代理在API功能调用、内在推理和规划以及适应环境反馈方面的基本能力。Hephaestus-Forge包含1030亿个特定于代理的数据，涵盖76,537个API，包括工具文档以介绍API功能的知识和功能调用轨迹以增强内在推理。通过在Hephaestus-Forge上持续预训练，Hephaestus在三个代理基准测试中超越了小到中型的开源LLM，并与商业LLM相媲美，证明了我们的预训练语料库在增强代理基本能力和LLM对新任务或环境的泛化能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04223', 'title': 'Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents', 'url': 'https://huggingface.co/papers/2502.04223', 'abstract': "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.", 'score': 7, 'issue_id': 2170, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '98e15ce7f5732b9f', 'authors': ['Ilia Karmanov', 'Amala Sanjay Deshmukh', 'Lukas Voegtle', 'Philipp Fischer', 'Kateryna Chumachenko', 'Timo Roman', 'Jarno Seppänen', 'Jupinder Parmar', 'Joseph Jennings', 'Andrew Tao', 'Karan Sapra'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04223.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#dataset', '#optimization', '#cv'], 'emoji': '📄', 'ru': {'title': 'Éclair: Передовой инструмент OCR для комплексного анализа структуры документов', 'desc': 'Статья представляет Éclair - инструмент для извлечения текста из изображений документов. Éclair не только извлекает текст, но и понимает структуру документа, включая форматирование, формулы, таблицы и порядок чтения. Инструмент также способен определять семантические элементы, такие как сноски и подписи к изображениям. Авторы представляют новый разнообразный бенчмарк для оценки OCR на уровне документов и семантической классификации, на котором Éclair показывает наилучшие результаты.'}, 'en': {'title': 'Eclair: Revolutionizing Document Understanding with Advanced OCR', 'desc': "This paper presents 'Eclair', an advanced Optical Character Recognition (OCR) tool designed to extract not just text, but also the structural and semantic elements of complex documents. It recognizes formatting, tables, and reading order, which are essential for understanding multi-page documents. 'Eclair' provides bounding boxes and semantic classes for extracted text, enhancing its utility for tasks like document retrieval and question answering. The tool demonstrates state-of-the-art performance on a newly introduced benchmark, showcasing its effectiveness compared to existing methods."}, 'zh': {'title': 'Eclair：全面理解文档的OCR工具', 'desc': "光学字符识别（OCR）技术广泛应用于从文档图像中提取文本，促进高效的数字化和数据检索。然而，仅仅提取文本对于处理复杂文档是不够的。全面理解这些文档需要了解其结构，包括格式、公式、表格以及跨多个页面的多个块和列的阅读顺序，以及检测脚注和图像标题等元素的语义信息。为了解决这个问题，我们提出了'Eclair'，这是一种通用的文本提取工具，专门设计用于处理各种文档类型，并在文档级OCR和语义分类的基准测试中实现了最先进的准确性。"}}}, {'id': 'https://huggingface.co/papers/2502.03997', 'title': 'CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing', 'url': 'https://huggingface.co/papers/2502.03997', 'abstract': 'Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.', 'score': 7, 'issue_id': 2165, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '69bfc4de9cf7106d', 'authors': ['Yu Yuan', 'Shizhao Sun', 'Qi Liu', 'Jiang Bian'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.03997.jpg', 'data': {'categories': ['#data', '#dataset', '#multimodal', '#architecture'], 'emoji': '🛠️', 'ru': {'title': 'CAD-Editor: Революция в текстовом редактировании CAD-моделей', 'desc': "CAD-Editor - это первая система для редактирования CAD-моделей на основе текстовых инструкций. Она использует автоматизированный конвейер для синтеза обучающих данных, сочетая модели вариаций дизайна и большие мультимодальные языковые модели. Система применяет подход 'locate-then-infill', разбивая задачу на локализацию областей для изменения и их заполнение соответствующими правками. CAD-Editor опирается на большие языковые модели для понимания естественного языка и знаний о CAD, демонстрируя превосходные результаты в экспериментах."}, 'en': {'title': 'Revolutionizing CAD Editing with Text Instructions', 'desc': 'This paper presents CAD-Editor, a novel framework for text-based editing of Computer Aided Design (CAD) models. It addresses the limitations of existing methods by integrating automated data synthesis and Large Vision-Language Models (LVLMs) to generate editing instructions from original and modified CAD models. The framework employs a locate-then-infill approach, breaking down the editing process into identifying areas for change and applying the necessary modifications. Experimental results demonstrate that CAD-Editor outperforms previous techniques in both quantitative metrics and qualitative assessments.'}, 'zh': {'title': '文本驱动的CAD编辑新纪元', 'desc': '计算机辅助设计（CAD）在各个行业中至关重要。基于文本的CAD编辑可以根据文本指令自动修改CAD模型，但这一领域尚未得到充分探索。现有方法主要集中在设计变体生成或基于文本的CAD生成，缺乏对文本控制的支持或忽视了现有CAD模型的约束。我们提出了CAD-Editor，这是第一个用于基于文本的CAD编辑的框架，利用大型语言模型（LLMs）和自动化数据合成管道，实现了高效的编辑指令生成和模型修改。'}}}, {'id': 'https://huggingface.co/papers/2502.06428', 'title': 'CoS: Chain-of-Shot Prompting for Long Video Understanding', 'url': 'https://huggingface.co/papers/2502.06428', 'abstract': 'Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.', 'score': 3, 'issue_id': 2173, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': '8302e2d276dc5929', 'authors': ['Jian Hu', 'Zixu Cheng', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2502.06428.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умный выбор кадров для понимания длинных видео искусственным интеллектом', 'desc': 'Статья представляет новый метод Chain-of-Shot prompting (CoS) для улучшения понимания длинных видео мультимодальными большими языковыми моделями (MLLM). CoS решает проблему выбора релевантных кадров путем оптимизации визуальных промптов во время тестирования, адаптивно выбирая кадры в соответствии с задачей понимания видео. Метод включает механизм бинарного обобщения видео и модуль совместных рассуждений, которые помогают идентифицировать и сопоставлять релевантные и нерелевантные кадры. Эксперименты на пяти датасетах показали эффективность и адаптивность предложенного подхода.'}, 'en': {'title': 'Optimizing Video Understanding with Chain-of-Shot Prompting', 'desc': "This paper addresses the challenge that Multi-modal Large Language Models (MLLMs) face when processing long videos, which often contain too many visual tokens. These tokens can overwhelm the model with irrelevant information, making it difficult to understand the video's content. The authors propose a method called Chain-of-Shot prompting (CoS) that optimizes shot selection based on the specific task at hand, improving the alignment between selected shots and the semantic understanding required. CoS includes a binary video summary mechanism and a video co-reasoning module to enhance the model's focus on relevant shots, leading to better video comprehension."}, 'zh': {'title': '优化镜头选择，提升视频理解', 'desc': '多模态大型语言模型（MLLMs）在处理长视频时面临挑战，因为需要过多的视觉标记。这些标记超出了MLLMs的上下文长度，导致填充了大量与任务无关的镜头。如何选择镜头是一个未解决的关键问题：稀疏采样可能会错过关键细节，而全面采样则会使模型被无关内容淹没，从而导致视频理解错误。为了解决这个问题，我们提出了镜头链提示（CoS），通过优化镜头与任务的对齐来选择适合视频理解的镜头。'}}}, {'id': 'https://huggingface.co/papers/2502.07490', 'title': 'Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More', 'url': 'https://huggingface.co/papers/2502.07490', 'abstract': "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.", 'score': 3, 'issue_id': 2172, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '2d86c4124095af30', 'authors': ['Xialie Zhuang', 'Zhikai Jia', 'Jianjin Li', 'Zhenyu Zhang', 'Li Shen', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China', 'South China Normal University, China', 'Sun YatSen University, China', 'University of Oxford, UK', 'University of Texas at Austin, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.07490.jpg', 'data': {'categories': ['#training', '#reasoning', '#long_context', '#architecture', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'MEAP: Улучшение извлечения информации в больших языковых моделях', 'desc': 'Исследователи предложили новый метод обучения больших языковых моделей под названием MEAP (Mask-Enhanced Autoregressive Prediction). MEAP объединяет маскированное языковое моделирование (MLM) с предсказанием следующего токена (NTP) для улучшения способности модели извлекать ключевую информацию из контекста. Эксперименты показали, что MEAP значительно превосходит NTP в задачах извлечения ключевой информации и рассуждений на основе длинного контекста. Метод также демонстрирует преимущества при дообучении на размеченных данных, особенно в сценариях с потерей информации в середине последовательности.'}, 'en': {'title': 'Enhancing Information Retrieval in LLMs with MEAP', 'desc': 'This paper introduces Mask-Enhanced Autoregressive Prediction (MEAP), a new training method for Large Language Models (LLMs) that improves their ability to retrieve important information. MEAP combines Masked Language Modeling (MLM) with Next-Token Prediction (NTP) by masking some input tokens and then predicting the next token using a decoder-only Transformer. This approach avoids the complexity of bidirectional attention and encoder-decoder structures, making it computationally efficient during training and inference. Experimental results show that MEAP significantly enhances performance in information retrieval and reasoning tasks, especially in scenarios where context is crucial, while also benefiting supervised fine-tuning.'}, 'zh': {'title': '掩码增强自回归预测：提升语言模型的信息检索能力', 'desc': '大型语言模型（LLMs）在准确检索关键信息方面存在问题。为了解决这个问题，我们提出了一种名为掩码增强自回归预测（MEAP）的训练范式，它将掩码语言建模（MLM）与下一个标记预测（NTP）无缝结合，以增强后者的上下文检索能力。MEAP通过随机掩盖输入标记的一小部分，然后使用仅解码器的Transformer直接进行标准的下一个标记预测。实验表明，MEAP在关键信息检索和长上下文推理任务上显著优于NTP，同时在常识推理任务上表现相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'url': 'https://huggingface.co/papers/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'score': 3, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '3e6282dd3913750a', 'authors': ['Nurit Cohen-Inger', 'Yehonatan Elisha', 'Bracha Shapira', 'Lior Rokach', 'Seffi Cohen'], 'affiliations': ['Ben Gurion University', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07445.jpg', 'data': {'categories': ['#interpretability', '#training', '#dataset', '#hallucinations', '#benchmark', '#optimization'], 'emoji': '🦎', 'ru': {'title': 'Разоблачение иллюзии понимания: как языковые модели маскируют переобучение', 'desc': 'Статья представляет новый метод оценки языковых моделей под названием C-BOD. Этот метод выявляет переобучение моделей путем систематического искажения входных данных с сохранением их семантического содержания. Исследование показало, что многие модели, включая крупные ЯМ, демонстрируют значительное снижение производительности при небольших изменениях формулировок. Авторы призывают сообщество уделять больше внимания устойчивости и обобщающей способности моделей, а не только показателям в рейтингах.'}, 'en': {'title': 'Beyond Scores: Evaluating True Language Understanding in LLMs', 'desc': "This paper discusses the limitations of large language models (LLMs) in truly understanding language, as they often rely on specific patterns in datasets rather than genuine comprehension. The authors introduce the Chameleon Benchmark Overfit Detector (C-BOD), a tool that modifies benchmark prompts to test if LLMs are overfitting to memorized cues. By analyzing the performance of 26 leading LLMs on the MMLU benchmark, they find that many models show a decline in performance when faced with slight changes in input, indicating a reliance on fixed patterns. The study emphasizes the need for better evaluation methods that focus on a model's ability to generalize and understand language, rather than just achieving high scores on benchmarks."}, 'zh': {'title': '超越分数，关注模型的鲁棒性与泛化能力', 'desc': '大型语言模型（LLMs）在公共基准测试中表现优异，但这些高分可能掩盖了模型对特定数据集表面特征的过度依赖，而非真正的语言理解。我们提出了变色龙基准过拟合检测器（C-BOD），这是一个通过参数变换系统性扭曲基准提示的元评估框架，用于检测LLMs的过拟合。C-BOD通过重新表述输入，同时保持其语义内容和标签，揭示模型性能是否受到记忆模式的驱动。我们的研究表明，经过适度扰动后，26个领先的LLM在MMLU基准上的平均性能下降了2.15%，这表明模型在评估时需要关注更强的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.07531', 'title': 'VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.07531', 'abstract': 'Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.', 'score': 3, 'issue_id': 2165, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'dea5fd89dd98f3b1', 'authors': ['Sixiao Zheng', 'Zimian Peng', 'Yanpeng Zhou', 'Yi Zhu', 'Hang Xu', 'Xiangru Huang', 'Yanwei Fu'], 'affiliations': ['Fudan University, China', 'Huawei Noahs Ark Lab, China', 'Westlake University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.07531.jpg', 'data': {'categories': ['#video', '#training', '#open_source', '#dataset', '#benchmark', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль над ключевыми элементами при генерации видео из изображений', 'desc': 'VidCRAFT3 - это новая система генерации видео из изображений, позволяющая одновременно контролировать движение камеры, движение объектов и направление освещения. В основе системы лежит Spatial Triple-Attention Transformer, который интегрирует информацию об освещении, тексте и изображении. Для обучения был создан синтетический датасет VideoLightingDirection с аннотациями направления освещения. Предложенная трехэтапная стратегия обучения позволяет обойтись без данных с одновременными аннотациями всех визуальных элементов.'}, 'en': {'title': 'VidCRAFT3: Mastering Multi-Element Control in Image-to-Video Generation', 'desc': 'This paper presents VidCRAFT3, a new framework for generating videos from images with enhanced control over multiple visual elements, including camera motion, object motion, and lighting direction. The authors introduce the Spatial Triple-Attention Transformer, which allows for better separation and management of these elements during the generation process. To support this framework, they created the VideoLightingDirection (VLD) dataset, which includes detailed lighting annotations to improve the realism of generated videos. The proposed three-stage training strategy enables effective learning without requiring simultaneous annotations for all visual elements, leading to superior video quality and coherence compared to existing methods.'}, 'zh': {'title': 'VidCRAFT3：多元素控制的图像到视频生成新框架', 'desc': '本文介绍了一种新的图像到视频生成框架VidCRAFT3，能够同时控制相机运动、物体运动和光照方向。为了更好地解耦每个视觉元素的控制，提出了空间三重注意力变换器，能够对光照方向、文本和图像进行对称整合。由于大多数真实世界视频数据集缺乏光照注释，研究者构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），该数据集包含光照方向注释和多样化外观的物体。通过广泛的实验，VidCRAFT3在生成高质量视频内容方面表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2502.06755', 'title': 'Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models', 'url': 'https://huggingface.co/papers/2502.06755', 'abstract': 'To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V.', 'score': 2, 'issue_id': 2175, 'pub_date': '2025-02-10', 'pub_date_card': {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'}, 'hash': 'cf5a833e93ca6f88', 'authors': ['Samuel Stevens', 'Wei-Lun Chao', 'Tanya Berger-Wolf', 'Yu Su'], 'affiliations': ['The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2502.06755.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#open_source', '#interpretability'], 'emoji': '🔬', 'ru': {'title': 'Разреженные автоэнкодеры: ключ к пониманию и контролю моделей компьютерного зрения', 'desc': 'Статья представляет унифицированную систему на основе разреженных автоэнкодеров (SAE) для интерпретации и контроля визуальных моделей машинного обучения. Метод позволяет обнаруживать интерпретируемые человеком визуальные признаки и точно манипулировать ими для проверки гипотез о поведении модели. Исследователи применили свой подход к современным моделям компьютерного зрения и выявили ключевые различия в семантических абстракциях, изученных моделями с разными целями предварительного обучения. Авторы демонстрируют практическое применение своей системы через контролируемые вмешательства в различных задачах компьютерного зрения.'}, 'en': {'title': 'Bridging Interpretation and Control in Vision Models with Sparse Autoencoders', 'desc': 'This paper introduces a new framework that uses sparse autoencoders (SAEs) to interpret and manipulate visual features in vision models. It addresses the challenge of validating the causal influence of these features through controlled experiments. By applying this framework, the authors uncover significant differences in the semantic abstractions learned by various models based on their pre-training objectives. The framework allows for reliable identification and manipulation of interpretable features without needing to retrain the models, enhancing our understanding of their behavior.'}, 'zh': {'title': '用稀疏自编码器理解和操控视觉模型', 'desc': '本文提出了一种统一框架，利用稀疏自编码器（SAE）来理解视觉模型的特征。该框架不仅可以发现人类可解释的视觉特征，还能精确操控这些特征，以测试模型行为的假设。通过对先进视觉模型的应用，我们揭示了不同预训练目标模型所学习的语义抽象之间的关键差异。我们的研究表明，SAE能够在不重新训练模型的情况下，可靠地识别和操控可解释的视觉特征，为理解和控制视觉模型行为提供了强大的工具。'}}}, {'id': 'https://huggingface.co/papers/2502.06857', 'title': 'Gemstones: A Model Suite for Multi-Faceted Scaling Laws', 'url': 'https://huggingface.co/papers/2502.06857', 'abstract': 'Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws', 'score': 2, 'issue_id': 2172, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '8f1b246a774832da', 'authors': ['Sean McLeish', 'John Kirchenbauer', 'David Yu Miller', 'Siddharth Singh', 'Abhinav Bhatele', 'Micah Goldblum', 'Ashwinee Panda', 'Tom Goldstein'], 'affiliations': ['Columbia University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.06857.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#architecture', '#optimization'], 'emoji': '💎', 'ru': {'title': 'Переосмысление законов масштабирования в глубоком обучении', 'desc': 'Статья исследует законы масштабирования в машинном обучении, используя широкий спектр архитектурных и гиперпараметрических вариаций. Авторы представляют набор данных Gemstones, содержащий более 4000 контрольных точек трансформеров с различными параметрами обучения. Исследование показывает, что рекомендации, основанные на законах масштабирования, могут сильно зависеть от экспериментального дизайна и выбранных моделей. Работа предлагает более сложный подход к изучению масштабирования, включая закон, предсказывающий производительность языкового моделирования в зависимости от ширины и глубины модели.'}, 'en': {'title': 'Unlocking Scaling Laws with Gemstones Dataset', 'desc': 'This paper explores scaling laws in machine learning by analyzing a diverse set of models with various hyper-parameters. The authors introduce the Gemstones dataset, which includes over 4000 transformer model checkpoints, allowing for extensive experimentation on scaling effects. They demonstrate that the performance of language models can be predicted based on their architecture, specifically width and depth. The findings reveal that scaling law prescriptions are sensitive to the design of experiments and the specific models used, emphasizing the importance of comprehensive datasets in understanding scaling behavior.'}, 'zh': {'title': '探索缩放法则的多样性与敏感性', 'desc': '本文研究了缩放法则，使用了多种架构和超参数选择，强调了这些选择对结果的影响。我们发布了Gemstones数据集，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个来自变换器模型的检查点，参数量高达20亿。通过这些检查点，我们能够进行更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。研究发现，缩放法则的适用性对实验设计过程和使用的特定模型检查点非常敏感。'}}}, {'id': 'https://huggingface.co/papers/2502.05932', 'title': 'Skill Expansion and Composition in Parameter Space', 'url': 'https://huggingface.co/papers/2502.05932', 'abstract': "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.", 'score': 2, 'issue_id': 2170, 'pub_date': '2025-02-09', 'pub_date_card': {'ru': '9 февраля', 'en': 'February 9', 'zh': '2月9日'}, 'hash': 'd4dd3a87e6ed9712', 'authors': ['Tenglong Liu', 'Jianxiong Li', 'Yinan Zheng', 'Haoyi Niu', 'Yixing Lan', 'Xin Xu', 'Xianyuan Zhan'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'National University of Defense Technology', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05932.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюция навыков ИИ-агентов через параметрическое расширение и композицию', 'desc': 'Статья представляет новый фреймворк под названием Parametric Skill Expansion and Composition (PSEC) для итеративного развития возможностей автономных агентов. PSEC использует управляемую библиотеку навыков, интегрируя примитивы навыков как модули Low-Rank Adaptation (LoRA) для эффективной донастройки параметров. Фреймворк позволяет напрямую комбинировать навыки в пространстве параметров путем объединения модулей LoRA, кодирующих различные навыки. PSEC демонстрирует превосходную способность использовать предыдущие знания для эффективного решения новых задач и расширения библиотеки навыков.'}, 'en': {'title': 'Empowering Autonomous Agents with Efficient Skill Evolution', 'desc': "This paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enhances the ability of autonomous agents to learn new skills by building on existing knowledge. PSEC maintains a skill library that allows for efficient integration of skill primitives using Low-Rank Adaptation (LoRA) modules, which support parameter-efficient finetuning. The framework also enables the merging of these modules to create new skills by leveraging shared information, promoting flexibility in skill development. Experimental results demonstrate that PSEC significantly improves the agents' performance in adapting to new challenges while expanding their skill sets effectively."}, 'zh': {'title': '智能体技能的高效扩展与组合', 'desc': '本文提出了一种新的框架，称为参数化技能扩展与组合（PSEC），旨在提高自主智能体在面对新挑战时的能力。该框架通过维护一个可管理的技能库，逐步整合技能原语，支持高效的参数微调，从而实现灵活的技能扩展。PSEC还允许在参数空间中直接组合技能，通过合并不同技能的LoRA模块，利用共享信息有效编程新技能。实验结果表明，PSEC在利用先前知识应对新挑战方面表现出色，并能够扩展其技能库以进化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04465', 'title': 'FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks', 'url': 'https://huggingface.co/papers/2502.04465', 'abstract': 'Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.', 'score': 2, 'issue_id': 2167, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a6ebe3d69cd8bcc2', 'authors': ['Luca Della Libera', 'Francesco Paissan', 'Cem Subakan', 'Mirco Ravanelli'], 'affiliations': ['Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.04465.jpg', 'data': {'categories': ['#multilingual', '#audio', '#architecture'], 'emoji': '🎙️', 'ru': {'title': 'FocalCodec: Эффективное сжатие речи с сохранением семантики и акустики', 'desc': 'Исследователи представили FocalCodec - эффективный аудиокодек с низким битрейтом, основанный на фокальной модуляции. Он использует единый бинарный кодбук для сжатия речи до 0.16-0.65 кбит/с, что ниже, чем у современных аналогов. FocalCodec показывает конкурентоспособные результаты в ресинтезе речи и преобразовании голоса, сохраняя при этом семантическую и акустическую информацию. Кодек хорошо справляется с многоязычной речью и шумными условиями, а также подходит для генеративного моделирования.'}, 'en': {'title': 'FocalCodec: Efficient Speech Compression with Single Binary Codebook', 'desc': 'This paper presents FocalCodec, a novel low-bitrate codec designed for speech processing, which addresses the limitations of existing methods that often require complex multi-codebook architectures. By utilizing focal modulation and a single binary codebook, FocalCodec achieves efficient compression of speech at bitrates between 0.16 and 0.65 kbps. The model demonstrates strong performance in tasks like speech resynthesis and voice conversion, while maintaining the integrity of both semantic and acoustic information. Additionally, FocalCodec is effective in handling multilingual speech and noisy environments, making it a promising tool for generative modeling in natural language processing.'}, 'zh': {'title': 'FocalCodec：高效低比特率语音编解码器', 'desc': '大型语言模型通过在海量数据集上进行自监督预训练，彻底改变了自然语言处理。受到这一成功的启发，研究人员尝试将这些方法应用于语音处理，通过神经音频编解码器将连续音频离散化为标记。然而，现有方法面临高比特率、语义或声学信息丢失以及多代码本设计的复杂性等限制。为了解决这些问题，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，能够在0.16到0.65 kbps之间压缩语音，同时在语音重合成和语音转换中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.07776', 'title': 'Auditing Prompt Caching in Language Model APIs', 'url': 'https://huggingface.co/papers/2502.07776', 'abstract': "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.", 'score': 2, 'issue_id': 2164, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': '48f7472ef1c86b27', 'authors': ['Chenchen Gu', 'Xiang Lisa Li', 'Rohith Kuditipudi', 'Percy Liang', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07776.jpg', 'data': {'categories': ['#healthcare', '#leakage', '#inference', '#ethics', '#security', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Кэширование промптов в LLM: скрытая угроза приватности', 'desc': 'Статья исследует проблему кэширования промптов в больших языковых моделях (LLM) и связанные с этим риски утечки данных. Авторы разработали методы статистического аудита для обнаружения кэширования промптов у реальных API-провайдеров LLM. Они обнаружили глобальное разделение кэша между пользователями у семи провайдеров, включая OpenAI, что может привести к утечке информации о промптах пользователей. Кроме того, временные различия из-за кэширования могут раскрывать информацию об архитектуре модели, например, авторы нашли доказательства того, что модель встраивания OpenAI является декодер-ориентированным трансформером.'}, 'en': {'title': 'Timing Variations: A Privacy Risk in Prompt Caching for LLMs', 'desc': "This paper discusses how prompt caching in large language models (LLMs) can lead to timing variations that depend on the data being processed. When prompts are cached, they are handled more quickly than those that are not, which can create vulnerabilities for side-channel attacks. The authors highlight the risks of privacy breaches, especially when cache is shared among users, allowing attackers to infer information about others' prompts based on response times. To address these concerns, the paper presents statistical audits that reveal global cache sharing in several API providers, including OpenAI, and even uncovers details about the model architecture that were previously undisclosed."}, 'zh': {'title': '提示缓存的隐私风险与透明性', 'desc': '在大型语言模型（LLMs）中，提示缓存会导致数据依赖的时间变化：缓存的提示处理速度比非缓存的提示快。这些时间差异可能引发侧信道攻击的风险，例如，如果缓存被多个用户共享，攻击者可以通过快速的API响应时间识别出缓存的提示，从而获取其他用户提示的信息。由于提示缓存可能导致隐私泄露，因此API提供商的缓存政策透明度非常重要。我们开发并进行统计审计，以检测现实世界中LLM API提供商的提示缓存情况，发现七个API提供商（包括OpenAI）之间存在全球缓存共享，可能导致用户提示的隐私泄露。'}}}, {'id': 'https://huggingface.co/papers/2502.07640', 'title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2502.07640', 'abstract': 'We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.', 'score': 0, 'issue_id': 2175, 'pub_date': '2025-02-11', 'pub_date_card': {'ru': '11 февраля', 'en': 'February 11', 'zh': '2月11日'}, 'hash': 'f0efbd784e8053e1', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Jiayun Wu', 'Hongzhou Lin', 'Kaiyu Yang', 'Jia Li', 'Mengzhou Xia', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'Numina', 'Princeton Language and Intelligence, Princeton University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.07640.jpg', 'data': {'categories': ['#training', '#math', '#data', '#benchmark', '#reasoning', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматизированном доказательстве теорем с помощью LLM', 'desc': 'Представлен Goedel-Prover - модель большого языка (LLM) с открытым исходным кодом для автоматизированной генерации формальных математических доказательств. Основная проблема в этой области - нехватка формализованных математических утверждений и доказательств, которую авторы решают путем создания датасета из 1,64 миллиона формальных утверждений. Используется итеративный подход для построения большого набора формальных доказательств, обучая серию доказывающих моделей. Goedel-Prover превосходит все существующие модели с открытым исходным кодом в генерации полных доказательств, достигая 57,6% успеха на бенчмарке miniF2F.'}, 'en': {'title': 'Revolutionizing Formal Proof Generation with Goedel-Prover', 'desc': 'Goedel-Prover is an advanced open-source large language model designed for generating formal proofs in mathematics. It addresses the challenge of limited formalized math statements by creating a dataset of 1.64 million formal statements from natural language problems. The model employs iterative training of provers, where each new prover builds on the successes of its predecessors, leading to improved proof generation capabilities. As a result, Goedel-Prover achieves state-of-the-art performance, surpassing previous models in both proof generation success rates and the volume of formal proofs produced.'}, 'zh': {'title': 'Goedel-Prover：数学证明生成的新突破', 'desc': 'Goedel-Prover 是一个开源的大型语言模型，专注于自动化形式证明生成，特别是在数学问题上表现出色。我们通过训练语句形式化器，将自然语言数学问题转换为正式语言（Lean 4），创建了一个包含164万条正式语句的数据集。使用大型语言模型来验证这些正式语句是否准确保留了原始自然语言问题的内容。最终的证明者在整个证明生成方面超越了所有现有的开源模型，成功率显著提高。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi', '#alignment', '#architecture (6)', '#audio (1)', '#benchmark (10)', '#cv (2)', '#data (6)', '#dataset (12)', '#diffusion (1)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (2)', '#inference (2)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (5)', '#open_source (5)', '#optimization (14)', '#plp', '#rag (1)', '#reasoning (7)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (2)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (14)', '#transfer_learning (4)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-12 15:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-12 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-12 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    