
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. December 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-04.html">â¬…ï¸ <span id="prev-date">04.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-06.html">â¡ï¸ <span id="next-date">06.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'};
        let feedDateNext = {'ru': '06.12', 'en': '12/06', 'zh': '12æœˆ6æ—¥'};
        let feedDatePrev = {'ru': '04.12', 'en': '12/04', 'zh': '12æœˆ4æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.02687', 'title': 'SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance', 'url': 'https://huggingface.co/papers/2412.02687', 'abstract': "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.", 'score': 32, 'issue_id': 961, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'd766bad745d5f322', 'authors': ['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran'], 'affiliations': ['Posts & Telecom. Inst. of Tech.', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.02687.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization', '#inference', '#training', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SNOOPI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PG-SB Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ±ĞµÑĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ NASA Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ° HPSv2 Ğ² 31.08 Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance', 'desc': 'This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08.'}, 'zh': {'title': 'SNOOPIï¼šæå‡ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§ä¸ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶SNOOPIï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡Proper Guidance-SwiftBrush (PG-SB)æ–¹æ³•å¢å¼ºäº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé‡‡ç”¨éšæœºå°ºåº¦çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•Negative-Away Steer Attention (NASA)ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†è´Ÿæç¤ºé›†æˆåˆ°ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æŠ‘åˆ¶ç”Ÿæˆå›¾åƒä¸­çš„ä¸å¿…è¦å…ƒç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ›é€ äº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„æ–°æ ‡æ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03552', 'title': 'Imagine360: Immersive 360 Video Generation from Perspective Anchor', 'url': 'https://huggingface.co/papers/2412.03552', 'abstract': '360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.', 'score': 18, 'issue_id': 958, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '90dc986cabb575af', 'authors': ['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03552.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² 360Â°: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‹Ñ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Imagine360 - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ñ‚Ğ¸Ğ¿Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Transforming Perspective Videos into Immersive 360Â° Experiences', 'desc': 'The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos.'}, 'zh': {'title': 'Imagine360ï¼šä¸ªæ€§åŒ–æ²‰æµ¸å¼360åº¦è§†é¢‘åˆ›ä½œçš„æœªæ¥', 'desc': '360åº¦è§†é¢‘æä¾›äº†ä¸€ç§è¶…æ²‰æµ¸å¼ä½“éªŒï¼Œè®©è§‚ä¼—å¯ä»¥ä»å…¨æ–¹ä½æ¢ç´¢åŠ¨æ€åœºæ™¯ã€‚ä¸ºå®ç°æ›´å‹å¥½å’Œä¸ªæ€§åŒ–çš„360åº¦è§†é¢‘å†…å®¹åˆ›ä½œï¼Œæˆ‘ä»¬æå‡ºäº†Imagine360ï¼Œè¿™æ˜¯é¦–ä¸ªå°†æ ‡å‡†è§†è§’è§†é¢‘è½¬æ¢ä¸º360åº¦è§†é¢‘çš„æ¡†æ¶ã€‚Imagine360é€šè¿‡æœ‰é™çš„360åº¦è§†é¢‘æ•°æ®å­¦ä¹ ç»†è‡´çš„çƒé¢è§†è§‰å’Œè¿åŠ¨æ¨¡å¼ï¼Œé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡æ¥æä¾›å±€éƒ¨å’Œå…¨å±€çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼ŒImagine360åœ¨å›¾å½¢è´¨é‡å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„360åº¦è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03515', 'title': 'Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2412.03515', 'abstract': 'Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.', 'score': 17, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '6e733cf9c0a1b851', 'authors': ['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03515.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#3d', '#open_source'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ScoreLiDAR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞŸĞ¾Ñ‚ĞµÑ€Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ 3D LiDAR-ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ScoreLiDAR ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with ScoreLiDAR', 'desc': "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."}, 'zh': {'title': 'é«˜æ•ˆ3D LiDARåœºæ™¯è¡¥å…¨çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„è®­ç»ƒç¨³å®šæ€§å’Œé«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨è€Œè¢«åº”ç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚ç„¶è€Œï¼Œæ…¢é€Ÿé‡‡æ ·é€Ÿåº¦é™åˆ¶äº†åŸºäºæ‰©æ•£çš„åœºæ™¯è¡¥å…¨æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œå› ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦é«˜æ•ˆæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºScoreLiDARï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ã€‚é€šè¿‡å¼•å…¥ç»“æ„æŸå¤±ï¼ŒScoreLiDARèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰3D LiDARåœºæ™¯çš„å‡ ä½•ç»“æ„ï¼ŒåŒæ—¶æ˜¾è‘—åŠ å¿«äº†æ¯å¸§çš„è¡¥å…¨æ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03555', 'title': 'PaliGemma 2: A Family of Versatile VLMs for Transfer', 'url': 'https://huggingface.co/papers/2412.03555', 'abstract': 'PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.', 'score': 15, 'issue_id': 964, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '12d0d9bcc8060099', 'authors': ['Andreas Steiner', 'AndrÃ© Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.03555.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#training', '#cv', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PaliGemma 2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'PaliGemma 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PaliGemma, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SigLIP-So400m Ñ Ñ€ÑĞ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ‚ 2B Ğ´Ğ¾ 27B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (224px, 448px Ğ¸ 896px) Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. PaliGemma 2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'PaliGemma 2: Advancing Vision-Language Understanding', 'desc': 'PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning.'}, 'zh': {'title': 'PaliGemma 2ï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ', 'desc': 'PaliGemma 2 æ˜¯åŸºäº Gemma 2 è¯­è¨€æ¨¡å‹å®¶æ—çš„ PaliGemma å¼€æ”¾è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬ç»“åˆäº† SigLIP-So400m è§†è§‰ç¼–ç å™¨å’Œä¸åŒè§„æ¨¡çš„ Gemma 2 æ¨¡å‹ï¼Œè¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸‰ç§åˆ†è¾¨ç‡ä¸‹è®­ç»ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç ”ç©¶å½±å“è¿ç§»æ€§èƒ½çš„å› ç´ ï¼Œå¦‚å­¦ä¹ ç‡ï¼Œå¹¶åˆ†æä»»åŠ¡ç±»å‹ã€æ¨¡å‹å¤§å°å’Œåˆ†è¾¨ç‡ä¹‹é—´çš„å…³ç³»ã€‚PaliGemma 2 æ‰©å±•äº†è¿ç§»ä»»åŠ¡çš„æ•°é‡å’ŒèŒƒå›´ï¼Œæ¶µç›–äº†å¤šç§å…‰å­¦å­—ç¬¦è¯†åˆ«ç›¸å…³ä»»åŠ¡ï¼Œå¹¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03069', 'title': 'TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2412.03069', 'abstract': "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.", 'score': 13, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '820e62e1bd498d55', 'authors': ['Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Daniel K. Du', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.03069.jpg', 'data': {'categories': ['#multimodal', '#cv', '#architecture'], 'emoji': 'ğŸ”€', 'ru': {'title': 'TokenFlow: ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'TokenFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. TokenFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 7.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ LLaVA-1.5 13B. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'TokenFlow: Bridging Understanding and Generation in Image Processing', 'desc': 'TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics.'}, 'zh': {'title': 'TokenFlowï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å›¾åƒæ ‡è®°å™¨TokenFlowï¼Œå®ƒå¼¥åˆäº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç†è§£å’Œç”Ÿæˆä»»åŠ¡éœ€è¦ä¸åŒç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œä¼ ç»Ÿçš„å•ä¸€é‡å»ºç›®æ ‡å‘é‡é‡åŒ–ç¼–ç å™¨æ— æ³•æœ‰æ•ˆå¤„ç†è¿™ä¸€é—®é¢˜ã€‚TokenFlowé€šè¿‡åˆ›æ–°çš„åŒä»£ç æœ¬æ¶æ„ï¼Œè§£è€¦äº†è¯­ä¹‰å’Œåƒç´ çº§ç‰¹å¾å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«æ˜ å°„æœºåˆ¶ä¿æŒå®ƒä»¬çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenFlowåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œé¦–æ¬¡è¯æ˜ç¦»æ•£è§†è§‰è¾“å…¥åœ¨ç†è§£æ€§èƒ½ä¸Šè¶…è¶Šäº†LLaVA-1.5 13Bã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19103', 'title': 'VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.19103', 'abstract': "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.", 'score': 11, 'issue_id': 964, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '4507a3a2ac0bc8b5', 'authors': ['Jeongho Ju', 'Daeyoung Kim', 'SunYoung Park', 'Youngjune Kim'], 'affiliations': ['NC Research, NCSOFT'], 'pdf_title_img': 'assets/pdf/title_img/2411.19103.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#training', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'VARCO-VISION: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ VARCO-VISION Ğ´Ğ»Ñ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. VARCO-VISION Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞµĞµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'VARCO-VISION: Bridging Korean and English through Vision-Language Learning', 'desc': 'This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area.'}, 'zh': {'title': 'VARCO-VISIONï¼šåŒè¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°é‡Œç¨‹ç¢‘', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æºçš„éŸ©è‹±è§†è§‰è¯­è¨€æ¨¡å‹VARCO-VISIONã€‚æˆ‘ä»¬é‡‡ç”¨é€æ­¥è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ è¯­è¨€å’Œè§†è§‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ã€‚ä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼ŒVARCO-VISIONåœ¨åŒè¯­å›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹è¿˜å…·å¤‡å®šä½ã€å¼•ç”¨å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰åŠŸèƒ½ï¼Œæ‰©å±•äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03517', 'title': 'NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images', 'url': 'https://huggingface.co/papers/2412.03517', 'abstract': 'Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.', 'score': 11, 'issue_id': 960, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '9d51bf0b60be344b', 'authors': ['Lingen Li', 'Zhaoyang Zhang', 'Yaowei Li', 'Jiale Xu', 'Xiaoyu Li', 'Wenbo Hu', 'Weihao Cheng', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03517.jpg', 'data': {'categories': ['#optimization', '#3d', '#diffusion', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ²', 'desc': 'NVComposer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NVComposer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ².'}, 'en': {'title': 'NVComposer: Generating Novel Views Without External Alignment', 'desc': 'This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis.'}, 'zh': {'title': 'NVComposerï¼šæ— é¡»å¤–éƒ¨å¯¹é½çš„ç”Ÿæˆæ–°è§†å›¾åˆæˆ', 'desc': 'æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šè§†å›¾æ•°æ®çš„æ–°çš„è§†å›¾åˆæˆï¼ˆNVSï¼‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤–éƒ¨çš„å¤šè§†å›¾å¯¹é½è¿‡ç¨‹ï¼Œå¦‚æ˜¾å¼çš„å§¿æ€ä¼°è®¡æˆ–é¢„é‡å»ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§å’Œå¯è®¿é—®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è§†å›¾ä¹‹é—´é‡å ä¸è¶³æˆ–é®æŒ¡æ—¶å¯¹é½ä¸ç¨³å®šçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†NVComposerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼å¤–éƒ¨å¯¹é½çš„éœ€æ±‚ã€‚NVComposeré€šè¿‡å¼•å…¥ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä½¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿéšå¼æ¨æ–­å¤šä¸ªæ¡ä»¶è§†å›¾ä¹‹é—´çš„ç©ºé—´å’Œå‡ ä½•å…³ç³»ï¼Œä»è€Œåœ¨ç”Ÿæˆå¤šè§†å›¾NVSä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00493', 'title': 'Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.00493', 'abstract': "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.", 'score': 10, 'issue_id': 964, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': '10c214b548697656', 'authors': ['Duo Zheng', 'Shijia Huang', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.00493.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Video-3D LLM: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Video-3D LLM', 'desc': 'This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding.'}, 'zh': {'title': 'æå‡3Dåœºæ™¯ç†è§£çš„åˆ›æ–°æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç§°ä¸ºVideo-3D LLMï¼Œæ—¨åœ¨æé«˜3Dåœºæ™¯ç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„MLLMä¸»è¦åŸºäº2Dæ•°æ®è®­ç»ƒï¼Œå¯¼è‡´å®ƒä»¬åœ¨å¤„ç†3Dç¯å¢ƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚é€šè¿‡å°†3Dåœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘ï¼Œå¹¶å¼•å…¥3Dä½ç½®ç¼–ç ï¼ŒVideo-3D LLMèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å¯¹é½è§†é¢‘è¡¨ç¤ºä¸ç°å®ä¸–ç•Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03439', 'title': 'CleanDIFT: Diffusion Features without Noise', 'url': 'https://huggingface.co/papers/2412.03439', 'abstract': 'Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.', 'score': 8, 'issue_id': 963, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'cd474064bf17503a', 'authors': ['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'BjÃ¶rn Ommer'], 'affiliations': ['CompVis @ LMU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2412.03439.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#training', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Noise-Free Semantic Features from Diffusion Models', 'desc': 'This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient.'}, 'zh': {'title': 'æ— å™ªå£°çš„é«˜è´¨é‡è¯­ä¹‰ç‰¹å¾æå–', 'desc': 'æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾è¢«ç¡®ç«‹ä¸ºå¼ºå¤§çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸ï¼Œè¿™äº›ç‰¹å¾éœ€è¦åœ¨å›¾åƒä¸­æ·»åŠ å™ªå£°åæ‰èƒ½æå–ï¼Œå› ä¸ºæ¨¡å‹åœ¨å¤„ç†å‡ ä¹æ²¡æœ‰å™ªå£°çš„å›¾åƒæ—¶ï¼Œæä¾›çš„ç‰¹å¾æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬å‘ç°å™ªå£°å¯¹ç‰¹å¾çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ï¼Œä¸”é€šè¿‡ä¸åŒéšæœºå™ªå£°çš„é›†æˆæ— æ³•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ— ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæä¾›é«˜è´¨é‡ã€æ— å™ªå£°çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£ç‰¹å¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03558', 'title': 'MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation', 'url': 'https://huggingface.co/papers/2412.03558', 'abstract': 'This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '5e1a4c1e1017e7af', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Xingqiao An', 'Yunhan Yang', 'Yangguang Li', 'Zi-Xin Zou', 'Ding Liang', 'Xihui Liu', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03558.jpg', 'data': {'categories': ['#cv', '#synthetic', '#diffusion', '#training', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'MIDI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MIDI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. MIDI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸Ñ… Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. MIDI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'MIDI: Revolutionizing 3D Scene Generation from Single Images', 'desc': 'This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes.'}, 'zh': {'title': 'MIDIï¼šä»å•å›¾åƒç”Ÿæˆ3Dåœºæ™¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMIDIçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆç»„åˆ3Dåœºæ™¯ã€‚ä¸ç°æœ‰ä¾èµ–é‡å»ºæˆ–æ£€ç´¢æŠ€æœ¯çš„æ–¹æ³•ä¸åŒï¼ŒMIDIæ‰©å±•äº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Då¯¹è±¡ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨å¤šå®ä¾‹æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¤šä¸ª3Då®ä¾‹çš„åŒæ—¶ç”Ÿæˆã€‚MIDIçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šå®ä¾‹æ³¨æ„æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¯¹è±¡é—´çš„äº¤äº’å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ°åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç»è¿‡åˆæˆæ•°æ®ã€çœŸå®åœºæ™¯æ•°æ®å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–åœºæ™¯å›¾åƒçš„è¯„ä¼°éªŒè¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01106', 'title': 'One Shot, One Talk: Whole-body Talking Avatar from a Single Image', 'url': 'https://huggingface.co/papers/2412.01106', 'abstract': 'Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '13d96f9bb346e344', 'authors': ['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01106.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#diffusion', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒÑÑ, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¶ĞµÑÑ‚Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑƒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'From One Image to a Lifelike Talking Avatar!', 'desc': "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."}, 'zh': {'title': 'ä»å•å¼ å›¾åƒç”Ÿæˆå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒæ„å»ºå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬è§£å†³äº†å¤æ‚åŠ¨æ€å»ºæ¨¡å’Œå¯¹æ–°æ‰‹åŠ¿ä¸è¡¨æƒ…çš„æ³›åŒ–è¿™ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å§¿æ€å¼•å¯¼çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸å®Œç¾çš„è§†é¢‘å¸§ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä»¥å®ç°æ— ç¼æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å•å¼ å›¾åƒåˆ›å»ºå‡ºé€¼çœŸã€å¯ç²¾ç¡®åŠ¨ç”»å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„å…¨èº«è™šæ‹Ÿå¤´åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02030', 'title': 'NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training', 'url': 'https://huggingface.co/papers/2412.02030', 'abstract': 'We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.', 'score': 5, 'issue_id': 966, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '4c749ff913210111', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.02030.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#architecture', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'NitroFusion: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'NitroFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿ÑƒĞ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑˆÑƒĞ¼Ğ°. NitroFusion Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 4 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators', 'desc': 'NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used.'}, 'zh': {'title': 'NitroFusionï¼šé«˜æ•ˆä¸é«˜è´¨é‡ç”Ÿæˆçš„å®Œç¾ç»“åˆ', 'desc': 'NitroFusionæ˜¯ä¸€ç§å…¨æ–°çš„å•æ­¥æ‰©æ•£ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å¯¹æŠ—æ¡†æ¶å®ç°é«˜è´¨é‡ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„å•æ­¥æ–¹æ³•ç›¸æ¯”ï¼ŒNitroFusionåœ¨ç”Ÿæˆè´¨é‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°½ç®¡å•æ­¥æ–¹æ³•åœ¨é€Ÿåº¦ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šä¸ªä¸“ä¸šçš„åˆ¤åˆ«å™¨ç»„ï¼Œé’ˆå¯¹ä¸åŒçš„å™ªå£°æ°´å¹³æä¾›å¤šæ ·åŒ–çš„åé¦ˆï¼Œä»è€Œæé«˜ç”Ÿæˆçš„ä¿çœŸåº¦ã€‚é€šè¿‡çµæ´»çš„éƒ¨ç½²æœºåˆ¶ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦åœ¨1åˆ°4ä¸ªå»å™ªæ­¥éª¤ä¹‹é—´åŠ¨æ€é€‰æ‹©ï¼Œå®ç°è´¨é‡ä¸é€Ÿåº¦çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03085', 'title': 'Mimir: Improving Video Diffusion Models for Precise Text Understanding', 'url': 'https://huggingface.co/papers/2412.03085', 'abstract': 'Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/', 'score': 5, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'a065164e5fdadf2c', 'authors': ['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang'], 'affiliations': ['Ant Group', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03085.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Mimir: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mimir - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Mimir - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ 'token fuser', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mimir ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."}, 'en': {'title': 'Mimir: Bridging Text Understanding and Video Generation', 'desc': 'This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements.'}, 'zh': {'title': 'Mimirï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMimirçš„ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»¤ç‰Œèåˆå™¨ï¼Œè§£å†³äº†æ–‡æœ¬ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚Mimirèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å­¦ä¹ åˆ°çš„è§†é¢‘å…ˆéªŒï¼ŒåŒæ—¶å¢å¼ºLLMsåœ¨æ–‡æœ¬ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMimiråœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶ï¼Œå°¤å…¶åœ¨å¤„ç†çŸ­æ–‡æœ¬å’ŒåŠ¨æ€å˜åŒ–æ—¶ï¼Œè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03187', 'title': 'Weighted-Reward Preference Optimization for Implicit Model Fusion', 'url': 'https://huggingface.co/papers/2412.03187', 'abstract': 'While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.', 'score': 4, 'issue_id': 961, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '6da11fbf4e1ea7d9', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Tianyuan Shi', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.03187.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#open_source', '#architecture', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'WRPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ - Weighted-Reward Preference Optimization (WRPO). WRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Effortless Fusion of LLMs with WRPO!', 'desc': 'This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods.'}, 'zh': {'title': 'åŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼šé«˜æ•ˆèåˆå¤šç§å¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§éšå¼èåˆæ–¹æ³•ï¼Œç§°ä¸ºåŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼ˆWRPOï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆä¸åŒæ¶æ„å’Œè§„æ¨¡çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚WRPOé€šè¿‡ä¼˜åŒ–æºæ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åå¥½ï¼Œé¿å…äº†è¯æ±‡å¯¹é½å’ŒçŸ©é˜µèåˆçš„å¤æ‚æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œé€æ­¥è°ƒæ•´å¯¹ç›®æ ‡æ¨¡å‹å’Œæºæ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œè§£å†³äº†åˆ†å¸ƒåå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„çŸ¥è¯†èåˆæ–¹æ³•å’Œå¾®è°ƒåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03565', 'title': 'Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning', 'url': 'https://huggingface.co/papers/2412.03565', 'abstract': 'Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.', 'score': 2, 'issue_id': 967, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '72af31b504d0aac1', 'authors': ['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.03565.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#alignment', '#dataset', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ² LMM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Inst-IT - Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GPT-4 Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Inst-IT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Instance Understanding in Multimodal Models with Inst-IT', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."}, 'zh': {'title': 'æå‡å®ä¾‹ç†è§£èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æŒ‡ä»¤è°ƒä¼˜æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†åœ¨å®ä¾‹çº§ç†è§£ä¸Šä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚å®ä¾‹çº§ç†è§£å…³æ³¨ç‰¹å®šå…ƒç´ ï¼Œè¿™å¯¹äºæ·±å…¥ç†è§£å›¾åƒå’Œè§†é¢‘è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ³¨é‡Šç®¡é“ï¼Œåˆ©ç”¨GPT-4oé€šè¿‡æ˜ç¡®çš„è§†è§‰æç¤ºæå–å®ä¾‹çº§ä¿¡æ¯ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†Inst-ITï¼Œé€šè¿‡æ˜ç¡®çš„è§†è§‰æç¤ºæŒ‡ä»¤è°ƒä¼˜æ¥å¢å¼ºLMMsçš„å®ä¾‹ç†è§£èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒInst-ITæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šç§å›¾åƒå’Œè§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (5)', '#cv (8)', '#data (1)', '#dataset (3)', '#diffusion (8)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (8)', '#open_source (3)', '#optimization (11)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (11)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-05 13:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-05 13:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-05 13:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    