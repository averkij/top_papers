
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. December 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 –¥–µ–∫–∞–±—Ä—è</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-04.html">‚¨ÖÔ∏è <span id="prev-date">04.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-06.html">‚û°Ô∏è <span id="next-date">06.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'};
        let feedDateNext = {'ru': '06.12', 'en': '12/06', 'zh': '12Êúà6Êó•'};
        let feedDatePrev = {'ru': '04.12', 'en': '12/04', 'zh': '12Êúà4Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.02687', 'title': 'SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance', 'url': 'https://huggingface.co/papers/2412.02687', 'abstract': "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.", 'score': 36, 'issue_id': 961, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': 'd766bad745d5f322', 'authors': ['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran'], 'affiliations': ['Posts & Telecom. Inst. of Tech.', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.02687.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization', '#inference', '#training', '#benchmark', '#diffusion'], 'emoji': 'üé®', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–∏–±–∫–æ—Å—Ç–∏ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SNOOPI - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ PG-SB –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø—É—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –º–µ—Ç–æ–¥ NASA –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–æ–≤–æ–≥–æ —Ä–µ–∫–æ—Ä–¥–∞ HPSv2 –≤ 31.08 –¥–ª—è –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance', 'desc': 'This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08.'}, 'zh': {'title': 'SNOOPIÔºöÊèêÂçá‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÁ®≥ÂÆöÊÄß‰∏éÁîüÊàêË¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂SNOOPIÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞Êúâ‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáProper Guidance-SwiftBrush (PG-SB)ÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÔºåÈááÁî®ÈöèÊú∫Â∞∫Â∫¶ÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÁ≠ñÁï•„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïNegative-Away Steer Attention (NASA)ÔºåÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÂ∞ÜË¥üÊèêÁ§∫ÈõÜÊàêÂà∞‰∏ÄÊ≠•Êâ©Êï£Ê®°Âûã‰∏≠Ôºå‰ª•ÊäëÂà∂ÁîüÊàêÂõæÂÉè‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÂÖÉÁ¥†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÂü∫Á∫øÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂàõÈÄ†‰∫Ü‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÊñ∞Ê†áÊùÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03555', 'title': 'PaliGemma 2: A Family of Versatile VLMs for Transfer', 'url': 'https://huggingface.co/papers/2412.03555', 'abstract': 'PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.', 'score': 35, 'issue_id': 964, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '12d0d9bcc8060099', 'authors': ['Andreas Steiner', 'Andr√© Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.03555.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#training', '#cv', '#transfer_learning'], 'emoji': 'üß†', 'ru': {'title': 'PaliGemma 2: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò', 'desc': 'PaliGemma 2 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç–∫—Ä—ã—Ç–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ PaliGemma, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Å–µ–º–µ–π—Å—Ç–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Gemma 2. –ú–æ–¥–µ–ª—å —Å–æ—á–µ—Ç–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä SigLIP-So400m —Å —Ä—è–¥–æ–º –º–æ–¥–µ–ª–µ–π Gemma 2 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –æ—Ç 2B –¥–æ 27B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (224px, 448px –∏ 896px) –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤ –¥–ª—è –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è —à–∏—Ä–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. PaliGemma 2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü, –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, –Ω–æ—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤.'}, 'en': {'title': 'PaliGemma 2: Advancing Vision-Language Understanding', 'desc': 'PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning.'}, 'zh': {'title': 'PaliGemma 2ÔºöËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'PaliGemma 2 ÊòØÂü∫‰∫é Gemma 2 ËØ≠Ë®ÄÊ®°ÂûãÂÆ∂ÊóèÁöÑ PaliGemma ÂºÄÊîæËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂçáÁ∫ßÁâà„ÄÇÊàë‰ª¨ÁªìÂêà‰∫Ü SigLIP-So400m ËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏çÂêåËßÑÊ®°ÁöÑ Gemma 2 Ê®°ÂûãÔºåËøõË°åÂ§öÈò∂ÊÆµËÆ≠ÁªÉÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªËÉΩÂäõ„ÄÇÈÄöËøáÂú®‰∏âÁßçÂàÜËæ®Áéá‰∏ãËÆ≠ÁªÉÔºåÊàë‰ª¨ËÉΩÂ§üÁ†îÁ©∂ÂΩ±ÂìçËøÅÁßªÊÄßËÉΩÁöÑÂõ†Á¥†ÔºåÂ¶ÇÂ≠¶‰π†ÁéáÔºåÂπ∂ÂàÜÊûê‰ªªÂä°Á±ªÂûã„ÄÅÊ®°ÂûãÂ§ßÂ∞èÂíåÂàÜËæ®Áéá‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇPaliGemma 2 Êâ©Â±ï‰∫ÜËøÅÁßª‰ªªÂä°ÁöÑÊï∞ÈáèÂíåËåÉÂõ¥ÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´Áõ∏ÂÖ≥‰ªªÂä°ÔºåÂπ∂Âú®Ëøô‰∫õ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03552', 'title': 'Imagine360: Immersive 360 Video Generation from Perspective Anchor', 'url': 'https://huggingface.co/papers/2412.03552', 'abstract': '360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.', 'score': 23, 'issue_id': 958, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '90dc986cabb575af', 'authors': ['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03552.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'üåê', 'ru': {'title': '–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ 360¬∞: –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∫ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–º—É –æ–ø—ã—Ç—É', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Imagine360 - –ø–µ—Ä–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 360-–≥—Ä–∞–¥—É—Å–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –æ–±—ã—á–Ω—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –º–æ–¥—É–ª—è–º–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∞ —Ç–∞–∫–∂–µ –∞–Ω—Ç–∏–ø–æ–¥–∞–ª—å–Ω—É—é –º–∞—Å–∫—É –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –¥–∞–ª—å–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–≤–∏–∂–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Transforming Perspective Videos into Immersive 360¬∞ Experiences', 'desc': 'The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos.'}, 'zh': {'title': 'Imagine360Ôºö‰∏™ÊÄßÂåñÊ≤âÊµ∏Âºè360Â∫¶ËßÜÈ¢ëÂàõ‰ΩúÁöÑÊú™Êù•', 'desc': '360Â∫¶ËßÜÈ¢ëÊèê‰æõ‰∫Ü‰∏ÄÁßçË∂ÖÊ≤âÊµ∏Âºè‰ΩìÈ™åÔºåËÆ©ËßÇ‰ºóÂèØ‰ª•‰ªéÂÖ®Êñπ‰ΩçÊé¢Á¥¢Âä®ÊÄÅÂú∫ÊôØ„ÄÇ‰∏∫ÂÆûÁé∞Êõ¥ÂèãÂ•ΩÂíå‰∏™ÊÄßÂåñÁöÑ360Â∫¶ËßÜÈ¢ëÂÜÖÂÆπÂàõ‰ΩúÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜImagine360ÔºåËøôÊòØÈ¶ñ‰∏™Â∞ÜÊ†áÂáÜËßÜËßíËßÜÈ¢ëËΩ¨Êç¢‰∏∫360Â∫¶ËßÜÈ¢ëÁöÑÊ°ÜÊû∂„ÄÇImagine360ÈÄöËøáÊúâÈôêÁöÑ360Â∫¶ËßÜÈ¢ëÊï∞ÊçÆÂ≠¶‰π†ÁªÜËá¥ÁöÑÁêÉÈù¢ËßÜËßâÂíåËøêÂä®Ê®°ÂºèÔºåÈááÁî®ÂèåÂàÜÊîØËÆæËÆ°Êù•Êèê‰æõÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁ∫¶Êùü„ÄÇÂÆûÈ™åË°®ÊòéÔºåImagine360Âú®ÂõæÂΩ¢Ë¥®ÈáèÂíåËøêÂä®‰∏ÄËá¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑ360Â∫¶ËßÜÈ¢ëÁîüÊàêÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03515', 'title': 'Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2412.03515', 'abstract': 'Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.', 'score': 21, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '6e733cf9c0a1b851', 'authors': ['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03515.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#3d', '#open_source'], 'emoji': 'üöó', 'ru': {'title': '–ë—ã—Å—Ç—Ä–æ–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ 3D LiDAR-—Å—Ü–µ–Ω –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è 3D LiDAR-—Å—Ü–µ–Ω –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ScoreLiDAR. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –ü–æ—Ç–µ—Ä—é, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É 3D LiDAR-—Å—Ü–µ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ScoreLiDAR —É—Å–∫–æ—Ä—è–µ—Ç –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–æ–ª–µ–µ —á–µ–º –≤ 5 —Ä–∞–∑ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è 3D LiDAR-—Å—Ü–µ–Ω.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with ScoreLiDAR', 'desc': "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."}, 'zh': {'title': 'È´òÊïà3D LiDARÂú∫ÊôØË°•ÂÖ®ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂõ†ÂÖ∂Âº∫Â§ßÁöÑËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÈ´òË¥®ÈáèÁöÑÂú∫ÊôØË°•ÂÖ®ËÄåË¢´Â∫îÁî®‰∫é3D LiDARÂú∫ÊôØË°•ÂÖ®„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÈÄüÈááÊ†∑ÈÄüÂ∫¶ÈôêÂà∂‰∫ÜÂü∫‰∫éÊâ©Êï£ÁöÑÂú∫ÊôØË°•ÂÖ®Ê®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®ÔºåÂõ†‰∏∫Ëá™Âä®È©æÈ©∂ËΩ¶ËæÜÈúÄË¶ÅÈ´òÊïàÊÑüÁü•Âë®Âõ¥ÁéØÂ¢É„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËí∏È¶èÊñπÊ≥ïÔºåÁß∞‰∏∫ScoreLiDARÔºåÊó®Âú®ÂÆûÁé∞È´òÊïà‰∏îÈ´òË¥®ÈáèÁöÑÂú∫ÊôØË°•ÂÖ®„ÄÇÈÄöËøáÂºïÂÖ•ÁªìÊûÑÊçüÂ§±ÔºåScoreLiDARËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ3D LiDARÂú∫ÊôØÁöÑÂá†‰ΩïÁªìÊûÑÔºåÂêåÊó∂ÊòæËëóÂä†Âø´‰∫ÜÊØèÂ∏ßÁöÑË°•ÂÖ®Êó∂Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03069', 'title': 'TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2412.03069', 'abstract': "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.", 'score': 16, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '820e62e1bd498d55', 'authors': ['Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Daniel K. Du', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.03069.jpg', 'data': {'categories': ['#multimodal', '#cv', '#architecture'], 'emoji': 'üîÄ', 'ru': {'title': 'TokenFlow: –µ–¥–∏–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'TokenFlow - —ç—Ç–æ –Ω–æ–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∑–∞–¥–∞—á–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –¥–≤–æ–π–Ω—ã–º –∫–æ–¥–±—É–∫–æ–º, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø–∏–∫—Å–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. TokenFlow –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 7.2% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LLaVA-1.5 13B. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'TokenFlow: Bridging Understanding and Generation in Image Processing', 'desc': 'TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics.'}, 'zh': {'title': 'TokenFlowÔºöÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàêÁöÑÊ°•Ê¢Å', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõæÂÉèÊ†áËÆ∞Âô®TokenFlowÔºåÂÆÉÂº•Âêà‰∫ÜÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàê‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°ÈúÄË¶Å‰∏çÂêåÁ≤íÂ∫¶ÁöÑËßÜËßâ‰ø°ÊÅØÔºå‰º†ÁªüÁöÑÂçï‰∏ÄÈáçÂª∫ÁõÆÊ†áÂêëÈáèÈáèÂåñÁºñÁ†ÅÂô®Êó†Ê≥ïÊúâÊïàÂ§ÑÁêÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇTokenFlowÈÄöËøáÂàõÊñ∞ÁöÑÂèå‰ª£Á†ÅÊú¨Êû∂ÊûÑÔºåËß£ËÄ¶‰∫ÜËØ≠‰πâÂíåÂÉèÁ¥†Á∫ßÁâπÂæÅÂ≠¶‰π†ÔºåÂêåÊó∂ÈÄöËøáÂÖ±‰∫´Êò†Â∞ÑÊú∫Âà∂‰øùÊåÅÂÆÉ‰ª¨ÁöÑÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTokenFlowÂú®Â§öÈ°π‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÈ¶ñÊ¨°ËØÅÊòéÁ¶ªÊï£ËßÜËßâËæìÂÖ•Âú®ÁêÜËß£ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜLLaVA-1.5 13B„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03517', 'title': 'NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images', 'url': 'https://huggingface.co/papers/2412.03517', 'abstract': 'Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.', 'score': 13, 'issue_id': 960, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '9d51bf0b60be344b', 'authors': ['Lingen Li', 'Zhaoyang Zhang', 'Yaowei Li', 'Jiale Xu', 'Xiaoyu Li', 'Wenbo Hu', 'Weihao Cheng', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03517.jpg', 'data': {'categories': ['#optimization', '#3d', '#diffusion', '#cv'], 'emoji': 'üé•', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –±–µ–∑ —è–≤–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–¥–æ–≤', 'desc': 'NVComposer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–æ –≤–Ω–µ—à–Ω–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –≤–∏–¥–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–ø–æ—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π –∫–∞–º–µ—Ä. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑ –ø–ª–æ—Ç–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NVComposer –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤.'}, 'en': {'title': 'NVComposer: Generating Novel Views Without External Alignment', 'desc': 'This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis.'}, 'zh': {'title': 'NVComposerÔºöÊó†È°ªÂ§ñÈÉ®ÂØπÈΩêÁöÑÁîüÊàêÊñ∞ËßÜÂõæÂêàÊàê', 'desc': 'ÊúÄËøëÁîüÊàêÊ®°ÂûãÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öËßÜÂõæÊï∞ÊçÆÁöÑÊñ∞ÁöÑËßÜÂõæÂêàÊàêÔºàNVSÔºâËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§ñÈÉ®ÁöÑÂ§öËßÜÂõæÂØπÈΩêËøáÁ®ãÔºåÂ¶ÇÊòæÂºèÁöÑÂßøÊÄÅ‰º∞ËÆ°ÊàñÈ¢ÑÈáçÂª∫ÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØËÆøÈóÆÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜÂõæ‰πãÈó¥ÈáçÂè†‰∏çË∂≥ÊàñÈÅÆÊå°Êó∂ÂØπÈΩê‰∏çÁ®≥ÂÆöÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜNVComposerÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÊ∂àÈô§‰∫ÜÂØπÊòæÂºèÂ§ñÈÉ®ÂØπÈΩêÁöÑÈúÄÊ±Ç„ÄÇNVComposerÈÄöËøáÂºïÂÖ•‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂Ôºå‰ΩøÁîüÊàêÊ®°ÂûãËÉΩÂ§üÈöêÂºèÊé®Êñ≠Â§ö‰∏™Êù°‰ª∂ËßÜÂõæ‰πãÈó¥ÁöÑÁ©∫Èó¥ÂíåÂá†‰ΩïÂÖ≥Á≥ªÔºå‰ªéËÄåÂú®ÁîüÊàêÂ§öËßÜÂõæNVS‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03205', 'title': 'U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs', 'url': 'https://huggingface.co/papers/2412.03205', 'abstract': "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.   To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release mu-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.   The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on mu-MATH.", 'score': 12, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '8df63a02d444d462', 'authors': ['Konstantin Chernyshev', 'Vitaliy Polshkov', 'Ekaterina Artemova', 'Alex Myasnikov', 'Vlad Stepanov', 'Alexei Miasnikov', 'Sergei Tilga'], 'affiliations': ['Gradarius', 'Stevens Institute of Technology', 'Toloka AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.03205.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#math', '#science', '#benchmark'], 'emoji': 'üßÆ', 'ru': {'title': 'U-MATH: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ U-MATH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 1100 –∑–∞–¥–∞—á —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ —à–µ—Å—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, –≤–∫–ª—é—á–∞—è 20% –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—à–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –¥–ª—è —á–µ–≥–æ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç mu-MATH. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ª–∏—à—å 63% –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ 45% –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö U-MATH, –∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫ –∏–º–µ–µ—Ç F1-–º–µ—Ä—É 80% –Ω–∞ mu-MATH.'}, 'en': {'title': 'U-MATH: Elevating Math Evaluation for LLMs', 'desc': 'This paper presents U-MATH, a new benchmark designed to evaluate the mathematical skills of large language models (LLMs) using 1,100 open-ended university-level problems. The benchmark covers six core subjects and includes multimodal tasks, which incorporate visual elements, addressing the limitations of existing evaluations. To assess the performance of LLMs on these problems, the authors introduce mu-MATH, a dataset specifically for evaluating the correctness of solutions generated by LLMs. The results indicate that LLMs struggle with these tasks, achieving only 63% accuracy on text-based problems and 45% on visual ones, highlighting the need for further advancements in LLM capabilities.'}, 'zh': {'title': 'U-MATHÔºöÊèêÂçáLLMsÊï∞Â≠¶ËÉΩÂäõËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'ÁõÆÂâçÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊï∞Â≠¶ÊäÄËÉΩÁöÑËØÑ‰º∞Â≠òÂú®Â±ÄÈôêÊÄßÔºåÁé∞ÊúâÂü∫ÂáÜÊµãËØïÁõ∏ÂØπËæÉÂ∞èÔºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âü∫Á°ÄÂíåÈ´ò‰∏≠ÈóÆÈ¢ò‰∏äÔºå‰∏îÁº∫‰πè‰∏ªÈ¢òÂ§öÊ†∑ÊÄß„ÄÇÊ≠§Â§ñÔºå‰ªªÂä°‰∏≠ËßÜËßâÂÖÉÁ¥†ÁöÑÂåÖÂê´‰ªçÁÑ∂Êú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜU-MATHÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´1100‰∏™Êú™ÂèëË°®ÁöÑÂºÄÊîæÂºèÂ§ßÂ≠¶Á∫ßÈóÆÈ¢òÁöÑÊñ∞Âü∫ÂáÜÔºåÊ∂µÁõñÂÖ≠‰∏™Ê†∏ÂøÉÂ≠¶ÁßëÔºåÂÖ∂‰∏≠20%ÁöÑÈóÆÈ¢ò‰∏∫Â§öÊ®°ÊÄÅÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLLMsÂú®ÊñáÊú¨‰ªªÂä°‰∏äÁöÑÊúÄÈ´òÂáÜÁ°ÆÁéá‰ªÖ‰∏∫63%ÔºåËÄåÂú®ËßÜËßâÈóÆÈ¢ò‰∏äÁöÑÂáÜÁ°ÆÁéáÊõ¥‰ΩéÔºå‰ªÖ‰∏∫45%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00493', 'title': 'Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.00493', 'abstract': "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.", 'score': 12, 'issue_id': 964, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 –Ω–æ—è–±—Ä—è', 'en': 'November 30', 'zh': '11Êúà30Êó•'}, 'hash': '10c214b548697656', 'authors': ['Duo Zheng', 'Shijia Huang', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.00493.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': 'üé•', 'ru': {'title': 'Video-3D LLM: –ü—Ä–æ—Ä—ã–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å Video-3D LLM –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. –ú–æ–¥–µ–ª—å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç 3D-—Å—Ü–µ–Ω—ã –∫–∞–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫—É –≤—ã–±–æ—Ä–∫–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –ø–æ–Ω–∏–º–∞–Ω–∏—é 3D-—Å—Ü–µ–Ω.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Video-3D LLM', 'desc': 'This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding.'}, 'zh': {'title': 'ÊèêÂçá3DÂú∫ÊôØÁêÜËß£ÁöÑÂàõÊñ∞Ê®°Âûã', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÁß∞‰∏∫Video-3D LLMÔºåÊó®Âú®ÊèêÈ´ò3DÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑMLLM‰∏ªË¶ÅÂü∫‰∫é2DÊï∞ÊçÆËÆ≠ÁªÉÔºåÂØºËá¥ÂÆÉ‰ª¨Âú®Â§ÑÁêÜ3DÁéØÂ¢ÉÊó∂Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇÈÄöËøáÂ∞Ü3DÂú∫ÊôØËßÜ‰∏∫Âä®ÊÄÅËßÜÈ¢ëÔºåÂπ∂ÂºïÂÖ•3D‰ΩçÁΩÆÁºñÁ†ÅÔºåVideo-3D LLMËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂØπÈΩêËßÜÈ¢ëË°®Á§∫‰∏éÁé∞ÂÆû‰∏ñÁïåÁöÑÁ©∫Èó¥‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúÄÂ§ßË¶ÜÁõñÈááÊ†∑ÊäÄÊúØÔºå‰ª•‰ºòÂåñËÆ°ÁÆóÊàêÊú¨ÂíåÊÄßËÉΩÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19103', 'title': 'VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.19103', 'abstract': "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.", 'score': 11, 'issue_id': 964, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '4507a3a2ac0bc8b5', 'authors': ['Jeongho Ju', 'Daeyoung Kim', 'SunYoung Park', 'Youngjune Kim'], 'affiliations': ['NC Research, NCSOFT'], 'pdf_title_img': 'assets/pdf/title_img/2411.19103.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#training', '#low_resource'], 'emoji': 'üåè', 'ru': {'title': 'VARCO-VISION: –ü—Ä–æ—Ä—ã–≤ –≤ –¥–≤—É—è–∑—ã—á–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è VARCO-VISION –¥–ª—è –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –ø–æ—à–∞–≥–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –º–æ–¥–µ–ª–∏ —É—Å–≤–∞–∏–≤–∞—Ç—å –∫–∞–∫ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. VARCO-VISION –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–≤—É—è–∑—ã—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –µ–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'VARCO-VISION: Bridging Korean and English through Vision-Language Learning', 'desc': 'This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area.'}, 'zh': {'title': 'VARCO-VISIONÔºöÂèåËØ≠ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÈáåÁ®ãÁ¢ë', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂºÄÊ∫êÁöÑÈü©Ëã±ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãVARCO-VISION„ÄÇÊàë‰ª¨ÈááÁî®ÈÄêÊ≠•ËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂêåÊó∂Â≠¶‰π†ËØ≠Ë®ÄÂíåËßÜËßâ‰ø°ÊÅØÔºåÂêåÊó∂‰øùÁïôÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ‰∏éÂêåÁ±ªÊ®°ÂûãÁõ∏ÊØîÔºåVARCO-VISIONÂú®ÂèåËØ≠ÂõæÂÉèÊñáÊú¨ÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°ÂûãËøòÂÖ∑Â§áÂÆö‰Ωç„ÄÅÂºïÁî®ÂíåÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâÂäüËÉΩÔºåÊâ©Â±ï‰∫ÜÂÖ∂Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03558', 'title': 'MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation', 'url': 'https://huggingface.co/papers/2412.03558', 'abstract': 'This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.', 'score': 10, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '5e1a4c1e1017e7af', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Xingqiao An', 'Yunhan Yang', 'Yangguang Li', 'Zi-Xin Zou', 'Ding Liang', 'Xihui Liu', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03558.jpg', 'data': {'categories': ['#cv', '#synthetic', '#diffusion', '#training', '#3d'], 'emoji': 'üèôÔ∏è', 'ru': {'title': 'MIDI: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MIDI - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. MIDI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∏—Ö –¥–æ –º–Ω–æ–≥–æ—ç–∫–∑–µ–º–ø–ª—è—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏. –ö–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —è–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ—ç–∫–∑–µ–º–ø–ª—è—Ä–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. MIDI –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞–º–∏ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'MIDI: Revolutionizing 3D Scene Generation from Single Images', 'desc': 'This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes.'}, 'zh': {'title': 'MIDIÔºö‰ªéÂçïÂõæÂÉèÁîüÊàê3DÂú∫ÊôØÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫MIDIÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÁªÑÂêà3DÂú∫ÊôØ„ÄÇ‰∏éÁé∞Êúâ‰æùËµñÈáçÂª∫ÊàñÊ£ÄÁ¥¢ÊäÄÊúØÁöÑÊñπÊ≥ï‰∏çÂêåÔºåMIDIÊâ©Â±ï‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂõæÂÉèÂà∞3DÂØπË±°ÁîüÊàêÊ®°ÂûãÔºåÈááÁî®Â§öÂÆû‰æãÊâ©Êï£Ê®°ÂûãÔºåÂÆûÁé∞‰∫ÜÂ§ö‰∏™3DÂÆû‰æãÁöÑÂêåÊó∂ÁîüÊàê„ÄÇMIDIÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÂÆû‰æãÊ≥®ÊÑèÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÂØπË±°Èó¥ÁöÑ‰∫§‰∫íÂíåÁ©∫Èó¥‰∏ÄËá¥ÊÄßÔºåÁÆÄÂåñ‰∫ÜÁîüÊàêËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂà∞Âú∫ÊôØÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÁªèËøáÂêàÊàêÊï∞ÊçÆ„ÄÅÁúüÂÆûÂú∫ÊôØÊï∞ÊçÆÂíåÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁîüÊàêÁöÑÈ£éÊ†ºÂåñÂú∫ÊôØÂõæÂÉèÁöÑËØÑ‰º∞È™åËØÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02030', 'title': 'NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training', 'url': 'https://huggingface.co/papers/2412.02030', 'abstract': 'We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.', 'score': 9, 'issue_id': 966, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '4c749ff913210111', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.02030.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#architecture', '#benchmark'], 'emoji': 'üé®', 'ru': {'title': 'NitroFusion: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'NitroFusion - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –±–æ–ª—å—à–æ–π –ø—É–ª —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –∞—Å–ø–µ–∫—Ç–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —à—É–º–∞. NitroFusion –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ—Ç 1 –¥–æ 4 —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é.'}, 'en': {'title': 'NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators', 'desc': 'NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used.'}, 'zh': {'title': 'NitroFusionÔºöÈ´òÊïà‰∏éÈ´òË¥®ÈáèÁîüÊàêÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'NitroFusionÊòØ‰∏ÄÁßçÂÖ®Êñ∞ÁöÑÂçïÊ≠•Êâ©Êï£ÁîüÊàêÊñπÊ≥ïÔºåÈÄöËøáÂä®ÊÄÅÂØπÊäóÊ°ÜÊû∂ÂÆûÁé∞È´òË¥®ÈáèÁîüÊàê„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊ≠•ÊñπÊ≥ïÁõ∏ÊØîÔºåNitroFusionÂú®ÁîüÊàêË¥®Èáè‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂ∞ΩÁÆ°ÂçïÊ≠•ÊñπÊ≥ïÂú®ÈÄüÂ∫¶‰∏äÂÖ∑Êúâ‰ºòÂäø„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Â§ö‰∏™‰∏ì‰∏öÁöÑÂà§Âà´Âô®ÁªÑÔºåÈíàÂØπ‰∏çÂêåÁöÑÂô™Â£∞Ê∞¥Âπ≥Êèê‰æõÂ§öÊ†∑ÂåñÁöÑÂèçÈ¶àÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑ‰øùÁúüÂ∫¶„ÄÇÈÄöËøáÁÅµÊ¥ªÁöÑÈÉ®ÁΩ≤Êú∫Âà∂ÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÂú®1Âà∞4‰∏™ÂéªÂô™Ê≠•È™§‰πãÈó¥Âä®ÊÄÅÈÄâÊã©ÔºåÂÆûÁé∞Ë¥®Èáè‰∏éÈÄüÂ∫¶ÁöÑÂπ≥Ë°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03439', 'title': 'CleanDIFT: Diffusion Features without Noise', 'url': 'https://huggingface.co/papers/2412.03439', 'abstract': 'Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.', 'score': 9, 'issue_id': 963, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'cd474064bf17503a', 'authors': ['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'Bj√∂rn Ommer'], 'affiliations': ['CompVis @ LMU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2412.03439.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#training', '#optimization'], 'emoji': 'üé®', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —à—É–º–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º—ã–µ –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —è–≤–ª—è—é—Ç—Å—è –º–æ—â–Ω—ã–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á. –û–¥–Ω–∞–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –ø–µ—Ä–µ–¥ –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–æ–¥–µ–ª—å—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–π –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –ø–æ–ª—É—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ —à—É–º–∞. –≠—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.'}, 'en': {'title': 'Unlocking Noise-Free Semantic Features from Diffusion Models', 'desc': 'This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient.'}, 'zh': {'title': 'Êó†Âô™Â£∞ÁöÑÈ´òË¥®ÈáèËØ≠‰πâÁâπÂæÅÊèêÂèñ', 'desc': 'ÊúÄËøëÔºåÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÁöÑÂÜÖÈÉ®ÁâπÂæÅË¢´Á°ÆÁ´ã‰∏∫Âº∫Â§ßÁöÑËØ≠‰πâÊèèËø∞Á¨¶ÔºåÈÄÇÁî®‰∫éÂ§öÁßç‰∏ãÊ∏∏‰ªªÂä°„ÄÇÈÄöÂ∏∏ÔºåËøô‰∫õÁâπÂæÅÈúÄË¶ÅÂú®ÂõæÂÉè‰∏≠Ê∑ªÂä†Âô™Â£∞ÂêéÊâçËÉΩÊèêÂèñÔºåÂõ†‰∏∫Ê®°ÂûãÂú®Â§ÑÁêÜÂá†‰πéÊ≤°ÊúâÂô™Â£∞ÁöÑÂõæÂÉèÊó∂ÔºåÊèê‰æõÁöÑÁâπÂæÅÊïàÊûú‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÂèëÁé∞Âô™Â£∞ÂØπÁâπÂæÅÁöÑÊúâÊïàÊÄßÊúâÈáçË¶ÅÂΩ±ÂìçÔºå‰∏îÈÄöËøá‰∏çÂêåÈöèÊú∫Âô™Â£∞ÁöÑÈõÜÊàêÊó†Ê≥ïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÊó†ÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÔºå‰ΩøÊâ©Êï£Ê®°ÂûãËÉΩÂ§üÊèê‰æõÈ´òË¥®Èáè„ÄÅÊó†Âô™Â£∞ÁöÑËØ≠‰πâÁâπÂæÅÔºåÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊâ©Êï£ÁâπÂæÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03085', 'title': 'Mimir: Improving Video Diffusion Models for Precise Text Understanding', 'url': 'https://huggingface.co/papers/2412.03085', 'abstract': 'Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'a065164e5fdadf2c', 'authors': ['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang'], 'affiliations': ['Ant Group', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03085.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training'], 'emoji': 'üé¨', 'ru': {'title': 'Mimir: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mimir - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç Mimir - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π 'token fuser', –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Ö–æ–¥—ã —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Mimir —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ –¥–∏–Ω–∞–º–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω."}, 'en': {'title': 'Mimir: Bridging Text Understanding and Video Generation', 'desc': 'This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements.'}, 'zh': {'title': 'MimirÔºöÊèêÂçáÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÁöÑÊô∫ËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MimirÁöÑÁ´ØÂà∞Á´ØËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁî®‰∫éÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÔºàT2VÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑ‰ª§ÁâåËûçÂêàÂô®ÔºåËß£ÂÜ≥‰∫ÜÊñáÊú¨ÁºñÁ†ÅÂô®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰πãÈó¥ÁöÑÁâπÂæÅÂàÜÂ∏ÉÂ∑ÆË∑ù„ÄÇMimirËÉΩÂ§üÂÖÖÂàÜÂà©Áî®Â≠¶‰π†Âà∞ÁöÑËßÜÈ¢ëÂÖàÈ™åÔºåÂêåÊó∂Â¢ûÂº∫LLMsÂú®ÊñáÊú¨ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMimirÂú®ÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÊó∂ÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜÁü≠ÊñáÊú¨ÂíåÂä®ÊÄÅÂèòÂåñÊó∂ÔºåË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01106', 'title': 'One Shot, One Talk: Whole-body Talking Avatar from a Single Image', 'url': 'https://huggingface.co/papers/2412.01106', 'abstract': 'Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '13d96f9bb346e344', 'authors': ['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01106.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#diffusion', '#3d'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –≥–æ–≤–æ—Ä—è—â–∏–π –∞–≤–∞—Ç–∞—Ä –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ç–æ', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –¥–≤–∏–≥–∞—Ç—å—Å—è, –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–≤–∏–¥–µ–æ –∫–∞–¥—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∞–≤–∞—Ç–∞—Ä–∞, —Å–æ—á–µ—Ç–∞—é—â–µ–µ 3D –≥–∞—É—Å—Å–æ–≤—ã —Å–ø–ª–∞—Ç—ã –∏ –ø–æ–ª–∏–≥–æ–Ω–∞–ª—å–Ω—É—é —Å–µ—Ç–∫—É. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∂–µ—Å—Ç—ã –∏ –º–∏–º–∏–∫—É –∞–≤–∞—Ç–∞—Ä–∞, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤.'}, 'en': {'title': 'From One Image to a Lifelike Talking Avatar!', 'desc': "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."}, 'zh': {'title': '‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÂÖ®Ë∫´‰ºöËØ¥ËØùÁöÑËôöÊãüÂ§¥ÂÉè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªéÂçïÂº†ÂõæÂÉèÊûÑÂª∫ÂÖ®Ë∫´‰ºöËØ¥ËØùÁöÑËôöÊãüÂ§¥ÂÉèÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜÂ§çÊùÇÂä®ÊÄÅÂª∫Ê®°ÂíåÂØπÊñ∞ÊâãÂäø‰∏éË°®ÊÉÖÁöÑÊ≥õÂåñËøô‰∏§‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®ÂßøÊÄÅÂºïÂØºÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÊàë‰ª¨ÁîüÊàê‰∫Ü‰∏çÂÆåÁæéÁöÑËßÜÈ¢ëÂ∏ß‰Ωú‰∏∫‰º™Ê†áÁ≠æÔºå‰ª•ÂÆûÁé∞Êó†ÁºùÊ≥õÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§ü‰ªéÂçïÂº†ÂõæÂÉèÂàõÂª∫Âá∫ÈÄºÁúü„ÄÅÂèØÁ≤æÁ°ÆÂä®ÁîªÂíåÂØåÊúâË°®Áé∞ÂäõÁöÑÂÖ®Ë∫´ËôöÊãüÂ§¥ÂÉè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03565', 'title': 'Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning', 'url': 'https://huggingface.co/papers/2412.03565', 'abstract': 'Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.', 'score': 4, 'issue_id': 967, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '72af31b504d0aac1', 'authors': ['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.03565.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#alignment', '#dataset', '#training'], 'emoji': 'üîç', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ LMM —Å –ø–æ–º–æ—â—å—é —è–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Inst-IT - —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM) —Å –ø–æ–º–æ—â—å—é —è–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT-4 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. Inst-IT –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –ø–∞—Ä–∞–¥–∏–≥–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞–∫ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, —Ç–∞–∫ –∏ –≤ –æ–±—â–µ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Instance Understanding in Multimodal Models with Inst-IT', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."}, 'zh': {'title': 'ÊèêÂçáÂÆû‰æãÁêÜËß£ËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êåá‰ª§Ë∞É‰ºòÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÁ™ÅÁ†¥Ôºå‰ΩÜÂú®ÂÆû‰æãÁ∫ßÁêÜËß£‰∏ä‰ªçÁÑ∂Â≠òÂú®ÊåëÊàò„ÄÇÂÆû‰æãÁ∫ßÁêÜËß£ÂÖ≥Ê≥®ÁâπÂÆöÂÖÉÁ¥†ÔºåËøôÂØπ‰∫éÊ∑±ÂÖ•ÁêÜËß£ÂõæÂÉèÂíåËßÜÈ¢ëËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Âä®Ê≥®ÈáäÁÆ°ÈÅìÔºåÂà©Áî®GPT-4oÈÄöËøáÊòéÁ°ÆÁöÑËßÜËßâÊèêÁ§∫ÊèêÂèñÂÆû‰æãÁ∫ß‰ø°ÊÅØ„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜInst-ITÔºåÈÄöËøáÊòéÁ°ÆÁöÑËßÜËßâÊèêÁ§∫Êåá‰ª§Ë∞É‰ºòÊù•Â¢ûÂº∫LMMsÁöÑÂÆû‰æãÁêÜËß£ËÉΩÂäõÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInst-ITÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§öÁßçÂõæÂÉèÂíåËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜ‰∏äÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03187', 'title': 'Weighted-Reward Preference Optimization for Implicit Model Fusion', 'url': 'https://huggingface.co/papers/2412.03187', 'abstract': 'While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.', 'score': 4, 'issue_id': 961, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '6da11fbf4e1ea7d9', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Tianyuan Shi', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.03187.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#open_source', '#architecture', '#training', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'WRPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä—è–º–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º - Weighted-Reward Preference Optimization (WRPO). WRPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, —É—Å—Ç—Ä–∞–Ω—è—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä–µ–π –∏ —Å–ª–∏—è–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑–ª–∏—á–∏–π –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ WRPO –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Å–ª–∏—è–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Effortless Fusion of LLMs with WRPO!', 'desc': 'This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods.'}, 'zh': {'title': 'Âä†ÊùÉÂ•ñÂä±ÂÅèÂ•Ω‰ºòÂåñÔºöÈ´òÊïàËûçÂêàÂ§öÁßçÂ§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈöêÂºèËûçÂêàÊñπÊ≥ïÔºåÁß∞‰∏∫Âä†ÊùÉÂ•ñÂä±ÂÅèÂ•Ω‰ºòÂåñÔºàWRPOÔºâÔºåÊó®Âú®ÊúâÊïàÊï¥Âêà‰∏çÂêåÊû∂ÊûÑÂíåËßÑÊ®°ÁöÑÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇWRPOÈÄöËøá‰ºòÂåñÊ∫êÊ®°Âûã‰∏éÁõÆÊ†áÊ®°Âûã‰πãÈó¥ÁöÑÂÅèÂ•ΩÔºåÈÅøÂÖç‰∫ÜËØçÊ±áÂØπÈΩêÂíåÁü©ÈòµËûçÂêàÁöÑÂ§çÊùÇÊÄß„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊ∏êËøõÈÄÇÂ∫îÁ≠ñÁï•ÔºåÈÄêÊ≠•Ë∞ÉÊï¥ÂØπÁõÆÊ†áÊ®°ÂûãÂíåÊ∫êÊ®°ÂûãÁöÑ‰æùËµñÔºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÂàÜÂ∏ÉÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWRPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÁü•ËØÜËûçÂêàÊñπÊ≥ïÂíåÂæÆË∞ÉÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02980', 'title': 'Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models', 'url': 'https://huggingface.co/papers/2412.02980', 'abstract': 'Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.', 'score': 3, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '8055d4be8211be80', 'authors': ['Alex Havrilla', 'Andrew Dai', "Laura O'Mahony", 'Koen Oostermeijer', 'Vera Zisler', 'Alon Albalak', 'Fabrizio Milo', 'Sharath Chandra Raparthy', 'Kanishk Gandhi', 'Baber Abbasi', 'Duy Phung', 'Maia Iyer', 'Dakota Mahan', 'Chase Blagden', 'Srishti Gureja', 'Mohammed Hamdy', 'Wen-Ding Li', 'Giovanni Paolini', 'Pawan Sasanka Ammanamanchi', 'Elliot Meyerson'], 'affiliations': ['Aleph Alpha @ IPAI', 'Cognizant AI Labs', 'Cohere for AI Community', 'Cornell University', 'Eleuther AI', 'Georgia Tech', 'IBM', 'Independent', 'Reka AI', 'Sakana AI', 'Stanford University', 'SynthLabs', 'University of Bologna', 'University of Limerick'], 'pdf_title_img': 'assets/pdf/title_img/2412.02980.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#dataset', '#rl', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –±–∞–ª–∞–Ω—Å–∞ —ç—Ç–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è.'}, 'en': {'title': 'Balancing Quality, Diversity, and Complexity in Synthetic Data Generation', 'desc': 'This paper discusses the generation of synthetic data using Large Language Models (LLMs) and its importance in enhancing natural data for various tasks. It evaluates synthetic data generation algorithms based on three key characteristics: quality, diversity, and complexity, which are crucial for the performance of downstream models. The authors highlight the trade-offs between these characteristics, noting that while quality is vital for in-distribution generalization, diversity is necessary for out-of-distribution generalization. The paper emphasizes the need for a balanced approach to these trade-offs to improve the effectiveness of reinforcement learning and self-improvement algorithms.'}, 'zh': {'title': 'ÂêàÊàêÊï∞ÊçÆÁîüÊàêÔºöÂπ≥Ë°°Ë¥®Èáè‰∏éÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂêàÊàêÊï∞ÊçÆÁöÑÊΩúÂäõÔºåÂº∫Ë∞É‰∫ÜÂêàÊàêÊï∞ÊçÆÂú®Ëá™ÁÑ∂Êï∞ÊçÆÂ¢ûÂº∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫ÈÄöËøáÊï∞ÊçÆË¥®Èáè„ÄÅÂ§öÊ†∑ÊÄßÂíåÂ§çÊùÇÊÄßÊù•ËØÑ‰º∞ÂêàÊàêÊï∞ÊçÆÁîüÊàêÁÆóÊ≥ïÔºåËøô‰∏âËÄÖÂØπ‰∏ãÊ∏∏Ê®°ÂûãÁöÑËÉΩÂäõÊúâÊòæËëóÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊï∞ÊçÆË¥®ÈáèÂØπÊ®°ÂûãÁöÑÂàÜÂ∏ÉÂÜÖÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶ÅÔºåËÄåÂ§öÊ†∑ÊÄßÂàôÂØπÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ§çÊùÇÊÄßÂØπ‰∏§ËÄÖÈÉΩÊúâÁõä„ÄÇÊàë‰ª¨ËøòÂº∫Ë∞É‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑË¥®Èáè-Â§öÊ†∑ÊÄßÊùÉË°°ÂèäÂÖ∂ÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåËÆ§‰∏∫Âú®Êú™Êù•ÁöÑËá™ÊàëÊîπËøõÁÆóÊ≥ï‰∏≠Âπ≥Ë°°Ëøô‰∫õÊùÉË°°ÊòØËá≥ÂÖ≥ÈáçË¶ÅÁöÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00177', 'title': 'LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting', 'url': 'https://huggingface.co/papers/2412.00177', 'abstract': "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.", 'score': 1, 'issue_id': 969, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '210b042d1a430116', 'authors': ['Xiaoyan Xing', 'Konrad Groh', 'Sezer Karaoglu', 'Theo Gevers', 'Anand Bhattad'], 'affiliations': ['BCAI-Bosch', 'Toyota Technological Institute at Chicago', 'UvA-Bosch Delta Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.00177.jpg', 'data': {'categories': ['#data', '#optimization', '#cv', '#diffusion', '#architecture'], 'emoji': 'üí°', 'ru': {'title': 'LumiNet: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –æ—Å–≤–µ—â–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π', 'desc': 'LumiNet - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ StyleGAN –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é ControlNet, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â—É—é –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏ —Ü–µ–ª–µ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. LumiNet –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –æ—Å–≤–µ—â–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞, –≤–Ω–µ–¥—Ä—è—é—â–µ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç —Å–ª–æ–∂–Ω—ã–µ —Å–≤–µ—Ç–æ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞–º–∏ —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–µ–π –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏.'}, 'en': {'title': 'LumiNet: Mastering Lighting Transfer with Generative Models', 'desc': 'LumiNet is a new machine learning architecture designed for transferring lighting effects from one image to another. It uses generative models to create a relit version of a source image by applying the lighting characteristics of a target image. The model incorporates a unique data curation strategy and a modified diffusion-based ControlNet that processes both intrinsic and extrinsic properties of the images. By utilizing cross-attention and fine-tuning, LumiNet effectively captures complex lighting phenomena, outperforming traditional methods in challenging indoor environments.'}, 'zh': {'title': 'LumiNetÔºöÈ´òÊïàÂÖâÁÖßËΩ¨ÁßªÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'LumiNetÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÔºåÂà©Áî®ÁîüÊàêÊ®°ÂûãÂíåÊΩúÂú®ÂÜÖÂú®Ë°®Á§∫Êù•ÂÆûÁé∞ÊúâÊïàÁöÑÂÖâÁÖßËΩ¨Áßª„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËæìÂÖ•Ê∫êÂõæÂÉèÂíåÁõÆÊ†áÂÖâÁÖßÂõæÂÉèÔºåÂêàÊàêÂá∫‰∏Ä‰∏™ÊçïÊçâÁõÆÊ†áÂÖâÁÖßÁöÑÈáçÊñ∞ÁÖßÊòéÁâàÊú¨„ÄÇLumiNetÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆË¥°ÁåÆÂåÖÊã¨Âü∫‰∫éStyleGANÁöÑÈáçÊñ∞ÁÖßÊòéÊ®°ÂûãÁöÑÊï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•Ôºå‰ª•ÂèäÂ§ÑÁêÜÊ∫êÂõæÂÉèÁöÑÊΩúÂú®ÂÜÖÂú®Â±ûÊÄßÂíåÁõÆÊ†áÂõæÂÉèÁöÑÊΩúÂú®Â§ñÂú®Â±ûÊÄßÁöÑÊîπËøõÊâ©Êï£ÊéßÂà∂ÁΩëÁªú„ÄÇÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÂíåÂæÆË∞ÉÔºåLumiNetËøõ‰∏ÄÊ≠•ÈÄöËøáÂ≠¶‰π†ÈÄÇÈÖçÂô®ÔºàMLPÔºâÊ≥®ÂÖ•ÁõÆÊ†áÁöÑÊΩúÂú®Â§ñÂú®Â±ûÊÄßÔºå‰ªéËÄåÊîπÂñÑÂÖâÁÖßËΩ¨ÁßªÊïàÊûú„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (7)', '#cv (9)', '#data (3)', '#dataset (5)', '#diffusion (9)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (9)', '#open_source (3)', '#optimization (13)', '#plp', '#rag', '#reasoning', '#rl (1)', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (12)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-12-05 17:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-05 17:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-05 17:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    