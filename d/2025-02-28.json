{
    "date": {
        "ru": "28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 28",
        "zh": "2æœˆ28æ—¥"
    },
    "time_utc": "2025-02-28 04:13",
    "weekday": 4,
    "issue_id": 2456,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.19613",
            "title": "Self-rewarding correction for mathematical reasoning",
            "url": "https://huggingface.co/papers/2502.19613",
            "abstract": "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.",
            "score": 28,
            "issue_id": 2455,
            "pub_date": "2025-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "e2535efc8aadcc9d",
            "authors": [
                "Wei Xiong",
                "Hanning Zhang",
                "Chenlu Ye",
                "Lichang Chen",
                "Nan Jiang",
                "Tong Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.19613.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering LLMs with Self-Rewarding Reasoning and Self-Correction",
                    "desc": "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¥–åŠ±æ¨ç†ï¼šæ¨¡å‹çš„ç‹¬ç«‹æ€è€ƒä¸ä¿®æ­£",
                    "desc": "æˆ‘ä»¬ç ”ç©¶äº†è‡ªæˆ‘å¥–åŠ±æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŒæ—¶ç”Ÿæˆé€æ­¥æ¨ç†å¹¶è¯„ä¼°è¾“å‡ºçš„æ­£ç¡®æ€§ï¼Œè€Œæ— éœ€å¤–éƒ¨åé¦ˆã€‚è¿™ç§é›†æˆæ–¹æ³•ä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿç‹¬ç«‹å¼•å¯¼å…¶æ¨ç†è¿‡ç¨‹ï¼Œä¸ºæ¨¡å‹éƒ¨ç½²æä¾›äº†è®¡ç®—ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨è‡ªæˆ‘ä¿®æ­£çš„ä»»åŠ¡ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ£€æµ‹å“åº”ä¸­çš„é”™è¯¯ï¼Œä¿®æ­£è¾“å‡ºï¼Œå¹¶å†³å®šä½•æ—¶ç»ˆæ­¢è¿­ä»£ä¼˜åŒ–å¾ªç¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ç®—æ³•æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªç”Ÿæˆçš„æ•°æ®æ„å»ºè‡ªæˆ‘å¥–åŠ±æ¨ç†æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20082",
            "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
            "url": "https://huggingface.co/papers/2502.20082",
            "abstract": "LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by \"needle-driven\" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.",
            "score": 8,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "ee15387b2b27d4c6",
            "authors": [
                "Ning Shang",
                "Li Lyna Zhang",
                "Siyuan Wang",
                "Gaokai Zhang",
                "Gilsinia Lopez",
                "Fan Yang",
                "Weizhu Chen",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "LongRoPE2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğµ Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… RoPE Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE. LongRoPE2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LongRoPE2 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ LLaMA3-8B Ğ´Ğ¾ 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Extending Context Length Without Compromise",
                    "desc": "LongRoPE2 is a new method that enhances the context length of large language models (LLMs) while maintaining their performance on shorter contexts. It introduces a hypothesis that inadequate training in higher dimensions of RoPE leads to out-of-distribution issues in existing models. The method employs a RoPE rescaling algorithm that uses evolutionary search to improve training effectiveness. Additionally, it utilizes a mixed context window training strategy to fine-tune model weights, allowing for long-context sequences without sacrificing short-context performance."
                },
                "zh": {
                    "title": "æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œä¿æŒæ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "LongRoPE2æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ‰©å±•é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ï¼ŒåŒæ—¶ä¿æŒåœ¨åŸå§‹è¾ƒçŸ­ä¸Šä¸‹æ–‡çª—å£ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªè´¡çŒ®å®ç°ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼Œè®¤ä¸ºåœ¨æ›´é«˜RoPEç»´åº¦ä¸Šçš„è®­ç»ƒä¸è¶³å¯¼è‡´äº†ç°æœ‰æ–¹æ³•ä¸­æŒç»­å­˜åœ¨çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰é—®é¢˜ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„RoPEé‡ç¼©æ”¾ç®—æ³•ï¼Œé€šè¿‡â€œé’ˆé©±åŠ¨â€çš„å›°æƒ‘åº¦æŒ‡å¯¼çš„è¿›åŒ–æœç´¢æ¥è§£å†³è®­ç»ƒä¸è¶³çš„é—®é¢˜ï¼›æœ€åï¼Œé‡‡ç”¨æ··åˆä¸Šä¸‹æ–‡çª—å£è®­ç»ƒæ–¹æ³•ï¼Œå¾®è°ƒæ¨¡å‹æƒé‡ä»¥é€‚åº”é•¿ä¸Šä¸‹æ–‡åºåˆ—çš„é‡ç¼©æ”¾RoPEï¼ŒåŒæ—¶ä¿æŒçŸ­ä¸Šä¸‹æ–‡çš„åŸå§‹RoPEæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20395",
            "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
            "url": "https://huggingface.co/papers/2502.20395",
            "abstract": "In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.",
            "score": 5,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "e8862deee761c4d0",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20395.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Re-Routing in Test-Time (R2-T2) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ end-to-end, Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². R2-T2 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ ĞµĞ³Ğ¾ Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ R2-T2 Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑĞµĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Multimodal Performance with Test-Time Re-Routing",
                    "desc": "This paper addresses the performance gap in large multimodal models (LMMs) when processing non-language data compared to large language models (LLMs). The authors introduce a mixture-of-experts (MoE) approach to enhance the vision encoder, allowing for richer and more diverse representations. They identify that the router, which determines how to mix these expert representations, often fails to optimize routing weights effectively during testing. To solve this, they propose a method called Re-Routing in Test-Time (R2-T2), which fine-tunes routing weights based on nearby correctly predicted samples, significantly boosting the performance of LMMs on various challenging tasks without retraining the base model."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­ï¼Œè§†è§‰è¡¨ç¤ºçš„æ„ŸçŸ¥èƒ½åŠ›é€šå¸¸ä¸å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å½±å“äº†LMMsåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ€è¿‘ï¼Œé€šè¿‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ›¿æ¢è§†è§‰ç¼–ç å™¨ï¼Œç¼“è§£äº†è¿™ä¸€å¼±ç‚¹ï¼Œæä¾›äº†ä¸°å¯Œä¸”å¤šæ ·çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç°ï¼Œç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å¹¶ä¸æ€»èƒ½ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆæœ€ä½³çš„è·¯ç”±æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•â€œæµ‹è¯•æ—¶é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰â€ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶ä¼˜åŒ–è·¯ç”±æƒé‡å‘é‡ï¼Œæ˜¾è‘—æå‡äº†LMMsåœ¨å¤šæ ·åŒ–ä»»åŠ¡åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16645",
            "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
            "url": "https://huggingface.co/papers/2502.16645",
            "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.",
            "score": 4,
            "issue_id": 2456,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 23",
                "zh": "2æœˆ23æ—¥"
            },
            "hash": "616cf3f6f2ab1d17",
            "authors": [
                "Chenlong Wang",
                "Zhaoyang Chu",
                "Zhengxiang Cheng",
                "Xuyi Yang",
                "Kaiyue Qiu",
                "Yao Wan",
                "Zhou Zhao",
                "Xuanhua Shi",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Wuhuan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16645.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CODESYNC - Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ¸Ğ· ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CODESYNC Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ CODESYNCBENCH - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM) ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 220 API Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Python. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "CODESYNC: Keeping Code Knowledge Fresh for LLMs",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in adapting to changes in third-party library APIs, which can lead to outdated or inefficient code. It introduces CODESYNC, a data engine designed to identify outdated code patterns and gather real-time updates from Python libraries. Additionally, the authors present CODESYNCBENCH, a benchmark for evaluating LLMs' performance in keeping up with code evolution, featuring 3,300 test cases across various tasks. The findings indicate that even advanced LLMs struggle with dynamic code changes, highlighting the need for improved methods for real-time code knowledge updating."
                },
                "zh": {
                    "title": "å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„åŸºå‡†æµ‹è¯•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ä»£ç çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚ç”±äºé™æ€çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™ç§é™åˆ¶å¸¸å¸¸å¯¼è‡´ç”Ÿæˆçš„ä»£ç æ— æ³•æ‰§è¡Œæˆ–å®ç°çš„å®‰å…¨æ€§å’Œæ•ˆç‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CODESYNCï¼Œä¸€ä¸ªç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶æ”¶é›†æ¥è‡ªPythonç¬¬ä¸‰æ–¹åº“çš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°çš„æ•°æ®å¼•æ“ã€‚åŸºäºCODESYNCï¼Œæˆ‘ä»¬å¼€å‘äº†CODESYNCBENCHï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä»£ç æ¼”å˜ä¸­çš„åŒæ­¥èƒ½åŠ›ï¼Œæ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„çœŸå®æ›´æ–°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20127",
            "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
            "url": "https://huggingface.co/papers/2502.20127",
            "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",
            "score": 2,
            "issue_id": 2456,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 27",
                "zh": "2æœˆ27æ—¥"
            },
            "hash": "ad848cf98c7468a7",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Pengfei Gao",
                "Xiangxin Meng",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "ByteDance",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20127.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#rlhf",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "SoRFT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¯Ğœ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ - Subtask-oriented Reinforced Fine-Tuning (SoRFT). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. SoRFT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SoRFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Issue Resolution with SoRFT: A Cost-Effective Approach",
                    "desc": "This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a new method designed to improve the issue-resolving capabilities of large language models (LLMs). It breaks down the issue resolution process into specific subtasks, such as file and function localization, and code editing. The training process involves two stages: first, a supervised fine-tuning phase that uses filtered data, and second, a reinforcement learning phase that applies Proximal Policy Optimization (PPO) with rewards based on ground-truth data. The results show that models trained with SoRFT outperform existing open-source models, achieving state-of-the-art results while being more cost-effective and maintaining better privacy."
                },
                "zh": {
                    "title": "æå‡é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºå­ä»»åŠ¡å¯¼å‘å¼ºåŒ–å¾®è°ƒï¼ˆSoRFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†é—®é¢˜è§£å†³åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡ä»¶å®šä½ã€åŠŸèƒ½å®šä½ã€è¡Œå®šä½å’Œä»£ç ç¼–è¾‘ç”Ÿæˆã€‚SoRFTåŒ…å«ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šé¦–å…ˆæ˜¯åŸºäºæ‹’ç»é‡‡æ ·çš„ç›‘ç£å¾®è°ƒï¼Œå…¶æ¬¡æ˜¯åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨åŸºäºçœŸå®æ•°æ®çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoRFTæ˜¾è‘—æå‡äº†é—®é¢˜è§£å†³æ€§èƒ½ï¼Œæ”¹å–„äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºå•†ä¸šæ¨¡å‹æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-27.html",
    "link_next": "2025-03-03.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "27.02",
        "en": "02/27",
        "zh": "2æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "03.03",
        "en": "03/03",
        "zh": "3æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†Kananaï¼Œä¸€ç³»åˆ—åœ¨éŸ©è¯­å’Œè‹±è¯­ä¸­è¡¨ç°å‡ºè‰²çš„åŒè¯­æ¨¡å‹ã€‚Kananaçš„è®¡ç®—æˆæœ¬æ˜¾è‘—ä½äºç±»ä¼¼è§„æ¨¡çš„é¡¶çº§æ¨¡å‹ã€‚æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®è¿‡æ»¤ã€åˆ†é˜¶æ®µé¢„è®­ç»ƒã€æ·±åº¦æ‰©å±•ã€å‰ªæå’Œè’¸é¦ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜æ¦‚è¿°äº†Kananaæ¨¡å‹åœ¨è®­ç»ƒåä½¿ç”¨çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ï¼Œä»¥æé«˜å…¶ä¸ç”¨æˆ·äº’åŠ¨çš„èƒ½åŠ›ã€‚æœ€åï¼ŒæŠ¥å‘Šè¿˜è®¨è®ºäº†è¯­è¨€æ¨¡å‹é€‚åº”ç‰¹å®šåœºæ™¯çš„å¯èƒ½æ–¹æ³•ï¼Œå¦‚åµŒå…¥ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œå‡½æ•°è°ƒç”¨ã€‚Kananaæ¨¡å‹ç³»åˆ—ä»2.1Båˆ°32.5Bå‚æ•°ä¸ç­‰ï¼Œ2.1Bæ¨¡å‹ï¼ˆåŸºç¡€ã€æŒ‡ä»¤ã€åµŒå…¥ï¼‰å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›éŸ©è¯­æ¨¡å‹ç ”ç©¶ã€‚",
        "title": "Kanana: Compute-efficient Bilingual Language Models",
        "pinyin": "WÇ’men jiÃ¨shÃ o le Kanana, yÄ« xÃ¬liÃ¨ zÃ i hÃ¡nyÇ” hÃ© yÄ«ngyÇ” zhÅng biÇoxiÃ n chÅ«sÃ¨ de shuÄngyÇ” mÃ³xÃ­ng. Kanana de jÃ¬suÃ n chÃ©ngbÄ›n xiÇnzhÃ¹ dÄ«yÃº lÃ¨isÃ¬ guÄ«mÃ³ de dÇngjÃ­ mÃ³xÃ­ng. BÃ ogÃ o xiÃ¡ngxÃ¬ jiÃ¨shÃ o le yÃ¹xÃ¹nliÃ n zhÅng shÇyÃ²ng de jÃ¬shÃ¹, bÄokuÃ² gÄo zhÃ¬liÃ ng shÃ¹jÃ¹ guÃ²lÇœ, fÄ“n jiÄ“duÃ n yÃ¹xÃ¹nliÃ n, shÄ“ndÃ¹ kuÃ²zhÇn, jiÇnzhÄ« hÃ© zhÄ“ngliÃº. CÇwÃ i, bÃ ogÃ o hÃ¡i gÃ ikuÃ ng le Kanana mÃ³xÃ­ng zÃ i xÃ¹nliÃ n hÃ²u shÇyÃ²ng de fÄngfÇ, bÄokuÃ² jiÃ ndÅ« wÄ“itiÃ¡o hÃ© piÄnhÇo yÅuhuÃ , yÇ tÃ­gÄo qÃ­ yÇ” yÃ²nghÃ¹ hÃ¹dÃ²ng de nÃ©nglÃ¬. ZÃ¹ihÃ²u, bÃ ogÃ o hÃ¡i tÇolÃ¹n le yÇ”yÃ¡n mÃ³xÃ­ng shÃ¬yÃ¬ng tÃ¨dÃ¬ng chÇngjÄ«ng de kÄ›nÃ©ng fÄngfÇ, rÃº qiÃ nrÃ¹, jiÇnsuÇ’ zÄ“ngqiÃ¡ng shÄ“ngchÃ©ng hÃ© hÃ¡nshÃ¹ diÃ oyÃ²ng. Kanana mÃ³xÃ­ng xÃ¬liÃ¨ cÃ³ng 2.1B dÃ o 32.5B cÄnshÃ¹ bÃ¹dÄ›ng, 2.1B mÃ³xÃ­ng (jÄ«chÇ”, zhÇlÃ¬ng, qiÃ nrÃ¹) yÇ gÅngkÄi fÄbÃ¹, yÇ cÃ¹jÃ¬n hÃ¡nyÇ” mÃ³xÃ­ng yÃ¡njiÅ«.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨shÃ o', 'trans': 'introduce'},\n{'word': 'åŒè¯­', 'pinyin': 'shuÄngyÇ”', 'trans': 'bilingual'},\n{'word': 'è®¡ç®—æˆæœ¬', 'pinyin': 'jÃ¬suÃ n chÃ©ngbÄ›n', 'trans': 'computational cost'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-training'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬liÃ ng', 'trans': 'high-quality'},\n{'word': 'è¿‡æ»¤', 'pinyin': 'guÃ²lÇœ', 'trans': 'filter'},\n{'word': 'åˆ†é˜¶æ®µ', 'pinyin': 'fÄ“n jiÄ“duÃ n', 'trans': 'phased'},\n{'word': 'æ·±åº¦æ‰©å±•', 'pinyin': 'shÄ“ndÃ¹ kuÃ²zhÇn', 'trans': 'deep expansion'},\n{'word': 'å‰ªæ', 'pinyin': 'jiÇnzhÄ«', 'trans': 'pruning'},\n{'word': 'è’¸é¦', 'pinyin': 'zhÄ“ngliÃº', 'trans': 'distillation'},\n{'word': 'ç›‘ç£å¾®è°ƒ', 'pinyin': 'jiÃ ndÅ« wÄ“itiÃ¡o', 'trans': 'supervised fine-tuning'},\n{'word': 'åå¥½ä¼˜åŒ–', 'pinyin': 'piÄnhÃ o yÅuhuÃ ', 'trans': 'preference optimization'},\n{'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹dÃ²ng', 'trans': 'interaction'},\n{'word': 'åµŒå…¥', 'pinyin': 'qiÃ nrÃ¹', 'trans': 'embedding'},\n{'word': 'æ£€ç´¢å¢å¼ºç”Ÿæˆ', 'pinyin': 'jiÇnsuÇ’ zÄ“ngqiÃ¡ng shÄ“ngchÃ©ng', 'trans': 'retrieval-augmented generation'},\n{'word': 'å‡½æ•°è°ƒç”¨', 'pinyin': 'hÃ¡nshÃ¹ diÃ oyÃ²ng', 'trans': 'function call'},\n{'word': 'å‚æ•°', 'pinyin': 'cÄnshÃ¹', 'trans': 'parameters'},\n{'word': 'å…¬å¼€å‘å¸ƒ', 'pinyin': 'gÅngkÄi fÄbÃ¹', 'trans': 'publicly released'},\n{'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹jÃ¬n', 'trans': 'promote'}]",
        "trans": "We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models.",
        "update_ts": "2025-02-27 09:11"
    }
}