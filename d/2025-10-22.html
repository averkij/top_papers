
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. October 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-21.html">â¬…ï¸ <span id="prev-date">21.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-23.html">â¡ï¸ <span id="next-date">23.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 22', 'zh': '10æœˆ22æ—¥'};
        let feedDateNext = {'ru': '23.10', 'en': '10/23', 'zh': '10æœˆ23æ—¥'};
        let feedDatePrev = {'ru': '21.10', 'en': '10/21', 'zh': '10æœˆ21æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.18135', 'title': 'World-in-World: World Models in a Closed-Loop World', 'url': 'https://huggingface.co/papers/2510.18135', 'abstract': 'World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.', 'score': 58, 'issue_id': 6545, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': 'd41a575be6992504', 'authors': ['Jiahan Zhang', 'Muqing Jiang', 'Nanru Dai', 'Taiming Lu', 'Arda Uzunoglu', 'Shunchi Zhang', 'Yana Wei', 'Jiahao Wang', 'Vishal M. Patel', 'Paul Pu Liang', 'Daniel Khashabi', 'Cheng Peng', 'Rama Chellappa', 'Tianmin Shu', 'Alan Yuille', 'Yilun Du', 'Jieneng Chen'], 'affiliations': ['Harvard', 'JHU', 'MIT', 'PKU', 'Princeton'], 'pdf_title_img': 'assets/pdf/title_img/2510.18135.jpg', 'data': {'categories': ['#games', '#benchmark', '#optimization', '#dataset', '#agents'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ° Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ: Ğ²Ğ°Ğ¶Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ½Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ World-in-World Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… world models Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ÑƒÑĞ¿ĞµÑ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Ğ’Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ â€” ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ ÑÑ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ world models Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ.'}, 'en': {'title': 'Prioritizing Task Success Over Visual Quality in Generative World Models', 'desc': 'The paper introduces World-in-World, a new platform for evaluating generative world models (WMs) in closed-loop environments, focusing on their effectiveness in helping agents complete tasks rather than just their visual quality. It highlights that traditional benchmarks often overlook the practical utility of WMs in real-world interactions, which this study aims to address. The research reveals that controllability is more crucial than visual fidelity for task success, and that scaling data post-training is more beneficial than simply enhancing visual generators. Additionally, it shows that increasing computational resources during inference can significantly boost the performance of WMs in these environments.'}, 'zh': {'title': 'é—­ç¯ç¯å¢ƒä¸­çš„ç”Ÿæˆä¸–ç•Œæ¨¡å‹è¯„ä¼°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†World-in-Worldï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°ç”Ÿæˆä¸–ç•Œæ¨¡å‹çš„å¼€æ”¾å¹³å°ï¼Œä¸“æ³¨äºåœ¨é—­ç¯ç¯å¢ƒä¸­è¿›è¡Œä»»åŠ¡æˆåŠŸçš„è¯„ä¼°ï¼Œè€Œéå•çº¯çš„è§†è§‰è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰è´¨é‡å¹¶ä¸èƒ½ä¿è¯ä»»åŠ¡çš„æˆåŠŸï¼Œæ§åˆ¶èƒ½åŠ›æ›´ä¸ºé‡è¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¡ŒåŠ¨-è§‚å¯Ÿæ•°æ®è¿›è¡ŒåæœŸè®­ç»ƒçš„æ•ˆæœä¼˜äºå‡çº§é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå™¨ã€‚æœ€åï¼Œå¢åŠ æ¨ç†æ—¶é—´çš„è®¡ç®—èµ„æºå¯ä»¥æ˜¾è‘—æå‡ç”Ÿæˆä¸–ç•Œæ¨¡å‹åœ¨é—­ç¯ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18866', 'title': 'LightMem: Lightweight and Efficient Memory-Augmented Generation', 'url': 'https://huggingface.co/papers/2510.18866', 'abstract': 'LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.', 'score': 56, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'cbff2ef26dc05177', 'authors': ['Jizhan Fang', 'Xinle Deng', 'Haoming Xu', 'Ziyan Jiang', 'Yuqi Tang', 'Ziwen Xu', 'Shumin Deng', 'Yunzhi Yao', 'Mengru Wang', 'Shuofei Qiao', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18866.jpg', 'data': {'categories': ['#architecture', '#optimization', '#data', '#training', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ AI: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ', 'desc': 'LightMem â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞÑ‚ĞºĞ¸Ğ½ÑĞ¾Ğ½Ğ°-Ğ¨Ğ¸Ñ„Ñ„Ñ€Ğ¸Ğ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğ°Ğ¼, Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 10.9% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 117 Ñ€Ğ°Ğ· Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 12 Ñ€Ğ°Ğ·. LightMem Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'LightMem: Enhancing LLMs with Efficient Memory Management', 'desc': 'LightMem is a novel memory system designed to enhance Large Language Models (LLMs) by efficiently managing historical interaction data. It incorporates a three-stage memory architecture inspired by human memory, which includes sensory memory for filtering information, short-term memory for organizing topics, and long-term memory for offline updates. This approach allows LLMs to utilize past interactions more effectively, leading to improved accuracy and reduced computational costs. Experimental results demonstrate that LightMem significantly outperforms existing memory systems, achieving notable gains in accuracy while drastically lowering resource usage.'}, 'zh': {'title': 'LightMemï¼šé«˜æ•ˆçš„è®°å¿†ç³»ç»Ÿæå‡LLMsæ€§èƒ½', 'desc': 'LightMemæ˜¯ä¸€ç§å—äººç±»è®°å¿†å¯å‘çš„å†…å­˜ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡æœ‰æ•ˆç®¡ç†å†å²äº¤äº’ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹åœ¨åŠ¨æ€å¤æ‚çš„ç¯å¢ƒä¸­æ›´å¥½åœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚LightMemå°†å†…å­˜ç»„ç»‡ä¸ºä¸‰ä¸ªäº’è¡¥çš„é˜¶æ®µï¼šæ„ŸçŸ¥è®°å¿†ã€çŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†ï¼Œä»è€Œå®ç°ä¿¡æ¯çš„å¿«é€Ÿè¿‡æ»¤ã€ç»„ç»‡å’Œæ€»ç»“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLightMemåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå¼ºåŸºçº¿ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18701', 'title': 'UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2510.18701', 'abstract': "UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.", 'score': 44, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'a88009f51e46a1e5', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Jiazi Bu', 'Yujie Zhou', 'Yi Xin', 'Junjun He', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18701.jpg', 'data': {'categories': ['#science', '#multilingual', '#multimodal', '#benchmark', '#survey'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'UniGenBench++ â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ text-to-image Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 600 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ 5 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞ¼Ğ°Ğ¼ Ğ¸ 20 Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼Ğ°Ğ¼, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°Ñ 10 Ğ¿ĞµÑ€Ğ²Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 27 Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ² ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ LLM Gemini-2.5-Pro, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'UniGenBench++: Elevating Text-to-Image Evaluation with Semantic Precision', 'desc': "UniGenBench++ is a new benchmark designed to evaluate text-to-image (T2I) generation models by measuring how well the generated images match the meanings of their corresponding text prompts. It addresses previous benchmarks' shortcomings by including a wide variety of scenarios and supporting multiple languages, specifically English and Chinese. The benchmark features a hierarchical structure with 600 prompts that cover 5 main themes and 20 subthemes, allowing for detailed assessments across 10 primary and 27 sub-evaluation criteria. Additionally, it utilizes a Multi-modal Large Language Model to create a reliable evaluation pipeline, enabling thorough analysis of both open- and closed-source T2I models."}, 'zh': {'title': 'UniGenBench++ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å…¨é¢è¯„ä¼°åŸºå‡†', 'desc': 'UniGenBench++ æ˜¯ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥åŸºå‡†é‡‡ç”¨åˆ†å±‚æç¤ºç»“æ„ï¼Œæ¶µç›–å¤šç§åœºæ™¯å’Œè¯­è¨€ï¼Œè§£å†³äº†ç°æœ‰åŸºå‡†ç¼ºä¹å¤šæ ·æ€§å’Œç»†ç²’åº¦è¯„ä¼°çš„é—®é¢˜ã€‚å®ƒåŒ…å«600ä¸ªæç¤ºï¼Œç»„ç»‡æˆ5ä¸ªä¸»è¦ä¸»é¢˜å’Œ20ä¸ªå­ä¸»é¢˜ï¼Œèƒ½å¤Ÿå…¨é¢æ¢æµ‹T2Iæ¨¡å‹åœ¨10ä¸ªä¸»è¦å’Œ27ä¸ªå­è¯„ä¼°æ ‡å‡†ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ŒUniGenBench++ æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è¯„ä¼°ç®¡é“ï¼Œå¸®åŠ©ç¤¾åŒºæ›´å¥½åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒçš„T2Iæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.16880', 'title': 'Chem-R: Learning to Reason as a Chemist', 'url': 'https://huggingface.co/papers/2510.16880', 'abstract': "Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.", 'score': 40, 'issue_id': 6544, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 19', 'zh': '10æœˆ19æ—¥'}, 'hash': '1cb1fbcf81423b67', 'authors': ['Weida Wang', 'Benteng Chen', 'Di Zhang', 'Wanhao Liu', 'Shuchen Pu', 'Ben Gao', 'Jin Zeng', 'Lei Bai', 'Wanli Ouyang', 'Xiaoyong Wei', 'Tianshu Yu', 'Tianfan Fu', 'Shuzhou Sun', 'Jiatong Li', 'Zifu Wang', 'Yuqiang Li', 'Shufei Zhang'], 'affiliations': ['Fudan University', 'Hong Kong Polytechnic University', 'Nanjing University', 'Shanghai AI Lab', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Tongji University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.16880.jpg', 'data': {'categories': ['#dataset', '#science', '#reasoning', '#architecture', '#benchmark', '#optimization', '#interpretability', '#training'], 'emoji': 'âš—ï¸', 'ru': {'title': 'Chem-R: LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ°Ğº Ñ…Ğ¸Ğ¼Ğ¸Ğº', 'desc': 'Chem-R - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ…Ğ¸Ğ¼Ğ¸Ğ¸. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ñ…Ğ¸Ğ¼Ğ¸Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… reasoning Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ† Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Multi-task Group Relative Policy Optimization. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Gemini-2.5-Pro Ğ¸ DeepSeek-R1, Ğ½Ğ° 46% Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ° 66% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµĞ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸. Chem-R Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Chem-R: Revolutionizing Chemical Reasoning with Expert Insights', 'desc': "Chem-R is a novel Chemical Reasoning model that enhances chemical task performance by integrating essential chemical knowledge and expert reasoning. It employs a three-phase training approach: first, it builds a solid foundation of chemical knowledge; second, it distills expert reasoning protocols to improve problem-solving reliability; and third, it optimizes performance across various tasks through multi-task learning. This structured training allows Chem-R to outperform existing large language models and chemical foundation models significantly. The results demonstrate Chem-R's strong generalization capabilities and its potential to drive advancements in AI-assisted chemical discovery."}, 'zh': {'title': 'Chem-Rï¼šåŒ–å­¦æ¨ç†çš„æ–°çºªå…ƒ', 'desc': 'Chem-Ræ˜¯ä¸€ç§ç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒçš„åŒ–å­¦æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨åŒ–å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡æ•´åˆæ ¸å¿ƒçŸ¥è¯†ã€ä¸“å®¶æ¨ç†å’Œå¤šä»»åŠ¡ä¼˜åŒ–ï¼Œæ¨¡æ‹ŸåŒ–å­¦å®¶çš„æ€ç»´è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒåŒ…æ‹¬å»ºç«‹åŒ–å­¦åŸºç¡€çŸ¥è¯†ã€å¼•å…¥ç»“æ„åŒ–çš„ä¸“å®¶æ¨ç†è½¨è¿¹ä»¥åŠä¼˜åŒ–å¤šä»»åŠ¡æ€§èƒ½ã€‚Chem-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§ï¼Œæˆä¸ºä¸‹ä¸€ä»£AIé©±åŠ¨åŒ–å­¦å‘ç°çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18692', 'title': 'MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation', 'url': 'https://huggingface.co/papers/2510.18692', 'abstract': 'Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.', 'score': 26, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'c596d2b417fc08c8', 'authors': ['Weinan Jia', 'Yuning Lu', 'Mengqi Huang', 'Hualiang Wang', 'Binyuan Huang', 'Nan Chen', 'Mu Liu', 'Jidong Jiang', 'Zhendong Mao'], 'affiliations': ['FanqieAI, ByteDance China', 'Hong Kong University of Science and Technology', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18692.jpg', 'data': {'categories': ['#video', '#architecture', '#diffusion', '#training', '#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mixture-of-Groups Attention (MoGA) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Diffusion Transformers. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, MoGA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ FlashAttention Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 480p Ğ¿Ñ€Ğ¸ 24 fps Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 580 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient Long Video Generation with MoGA', 'desc': 'The paper presents Mixture-of-Groups Attention (MoGA), a novel approach to improve long video generation using Diffusion Transformers by solving the quadratic scaling problem of full attention. MoGA reduces redundancy in attention mechanisms by employing a learnable token router that accurately matches tokens, eliminating the need for blockwise estimation. This method allows for effective long-range interactions while maintaining efficiency, integrating well with existing attention frameworks. The results demonstrate that MoGA can generate high-quality, long videos at a significant scale, showcasing its potential in video generation tasks.'}, 'zh': {'title': 'é«˜æ•ˆé•¿è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'Mixture-of-Groups Attentionï¼ˆMoGAï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£å˜æ¢å™¨ä¸­å…¨æ³¨æ„åŠ›çš„äºŒæ¬¡æ‰©å±•é—®é¢˜ï¼Œä»è€Œå®ç°é•¿è§†é¢‘ç”Ÿæˆã€‚ç”±äºæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨é«˜åº¦å†—ä½™ï¼Œè¾“å‡ºä¸»è¦ç”±å°‘é‡æŸ¥è¯¢-é”®å¯¹ä¸»å¯¼ï¼ŒMoGAé€šè¿‡è½»é‡çº§çš„å¯å­¦ä¹ ä»¤ç‰Œè·¯ç”±å™¨ç²¾ç¡®åŒ¹é…ä»¤ç‰Œï¼Œé¿å…äº†å—çŠ¶ä¼°è®¡çš„é™åˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥è·¯ç”±å®ç°äº†æœ‰æ•ˆçš„é•¿è·ç¦»äº¤äº’ï¼Œå¹¶ä¸”ä½œä¸ºæ— æ ¸æ–¹æ³•ï¼ŒMoGAèƒ½å¤Ÿä¸ç°ä»£æ³¨æ„åŠ›å †æ ˆæ— ç¼é›†æˆã€‚åŸºäºMoGAï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„é•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥24å¸§æ¯ç§’çš„é€Ÿåº¦ç”Ÿæˆåˆ†é’Ÿçº§ã€480pçš„å¤šé•œå¤´è§†é¢‘ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å„ç§è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18876', 'title': 'Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs', 'url': 'https://huggingface.co/papers/2510.18876', 'abstract': 'Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.', 'score': 23, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'f12731f1d8090440', 'authors': ['Haochen Wang', 'Yuhao Wang', 'Tao Zhang', 'Yikang Zhou', 'Yanwei Li', 'Jiacong Wang', 'Ye Tian', 'Jiahao Meng', 'Zilong Huang', 'Guangcan Mai', 'Anran Wang', 'Yunhai Tong', 'Zhuochen Wang', 'Xiangtai Li', 'Zhaoxiang Zhang'], 'affiliations': ['ByteDance', 'NLPR, MAIS, CASIA', 'PKU', 'UCAS', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2510.18876.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#games', '#benchmark', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ»ÑĞ±Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GAR (Grasp Any Region), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, GAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ RoI-aligned feature replay Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹, Ğ½Ğ¾ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ¸Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. GAR-1B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ°Ğ¶Ğµ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Grasp Any Region: Enhancing Visual Understanding through Global Contexts and Interactions', 'desc': 'The paper introduces Grasp Any Region (GAR), a model designed to improve region-level visual understanding by integrating global contexts and modeling interactions between different visual prompts. Unlike previous models that focused on isolated regions, GAR utilizes a RoI-aligned feature replay technique to enhance perception and reasoning capabilities. This allows GAR to answer complex questions about specific regions while considering their relationships with other regions. The model has shown superior performance in tasks like captioning and video reference, demonstrating its effectiveness in both single-region comprehension and multi-region interactions.'}, 'zh': {'title': 'æŒæ¡ä»»ä½•åŒºåŸŸï¼Œæå‡è§†è§‰ç†è§£ï¼', 'desc': 'Grasp Any Region (GAR) æ˜¯ä¸€ç§å¢å¼ºåŒºåŸŸçº§è§†è§‰ç†è§£çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ•´åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå»ºæ¨¡äº¤äº’æ¥å®ç°æ›´é«˜çº§çš„æ¨ç†ã€‚GAR é‡‡ç”¨æœ‰æ•ˆçš„ RoI å¯¹é½ç‰¹å¾é‡æ”¾æŠ€æœ¯ï¼Œæ”¯æŒç²¾ç¡®æ„ŸçŸ¥å’Œå¤šæç¤ºä¹‹é—´çš„äº¤äº’å»ºæ¨¡ã€‚é€šè¿‡è¿™äº›åŠŸèƒ½ï¼ŒGAR èƒ½å¤Ÿå›ç­”å…³äºä»»ä½•åŒºåŸŸçš„å…·ä½“è‡ªç”±å½¢å¼é—®é¢˜ï¼Œä»è¢«åŠ¨æè¿°è½¬å˜ä¸ºä¸»åŠ¨å¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAR åœ¨å›¾åƒæè¿°å’Œè§†é¢‘å‚è€ƒä»»åŠ¡ä¸­å‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18726', 'title': 'IF-VidCap: Can Video Caption Models Follow Instructions?', 'url': 'https://huggingface.co/papers/2510.18726', 'abstract': 'A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.', 'score': 22, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': '02c47b36a2a8b616', 'authors': ['Shihao Li', 'Yuanxing Zhang', 'Jiangtao Wu', 'Zhide Lei', 'Yiwen He', 'Runzhe Wen', 'Chenxi Liao', 'Chengkang Jiang', 'An Ping', 'Shuo Gao', 'Suhan Wang', 'Zhaozhou Bian', 'Zijun Zhou', 'Jingyi Xie', 'Jiayi Zhou', 'Jing Wang', 'Yifan Yao', 'Weihao Xie', 'Yingshui Tan', 'Yanghai Wang', 'Qianqian Xie', 'Zhaoxiang Zhang', 'Jiaheng Liu'], 'affiliations': ['CASIA', 'Kuaishou Technology', 'Nanjing University', 'Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18726.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IF-VidCap Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1400 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ open-source Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ÑĞ»Ğ¸ÑÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Evaluating Instruction-Following in Video Captioning', 'desc': 'The paper introduces IF-VidCap, a new benchmark designed to evaluate video captioning models based on their ability to follow user instructions. Unlike existing benchmarks that focus mainly on descriptive accuracy, IF-VidCap assesses both format correctness and content correctness in captions. The study reveals that while proprietary models still lead in performance, top open-source models are rapidly closing the gap. Additionally, it highlights that models optimized for dense captioning struggle with complex instructions, suggesting a need for future advancements in both descriptive and instruction-following capabilities.'}, 'zh': {'title': 'æ–°åŸºå‡†IF-VidCapï¼šæå‡è§†é¢‘å­—å¹•ç”Ÿæˆçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•IF-VidCapï¼Œç”¨äºè¯„ä¼°è§†é¢‘å­—å¹•ç”Ÿæˆæ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨æè¿°çš„å…¨é¢æ€§ï¼Œè€Œå¿½è§†äº†æ¨¡å‹åœ¨éµå¾ªç”¨æˆ·ç‰¹å®šæŒ‡ä»¤æ–¹é¢çš„è¡¨ç°ã€‚IF-VidCapåŒ…å«1400ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œè¯„ä¼°å­—å¹•çš„æ ¼å¼æ­£ç¡®æ€§å’Œå†…å®¹æ­£ç¡®æ€§ä¸¤ä¸ªç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸“æœ‰æ¨¡å‹ä»å ä¸»å¯¼åœ°ä½ï¼Œä½†é¡¶å°–çš„å¼€æºæ¨¡å‹æ­£åœ¨ç¼©å°ä¸ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18849', 'title': 'Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.18849', 'abstract': 'A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.', 'score': 18, 'issue_id': 6545, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'd16733b11aed5d27', 'authors': ['Chenghao Zhu', 'Meiling Tao', 'Tiannan Wang', 'Dongyi Ding', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'South China Agricultural University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.18849.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Critique-Post-Edit Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Generative Reward Model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°Ñ‘Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ reward hacking. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ PPO, Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Qwen2.5-14B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4.1.'}, 'en': {'title': 'Revolutionizing Personalization in Language Models with Critique-Post-Edit', 'desc': 'This paper introduces the Critique-Post-Edit framework, which enhances the personalization of large language models (LLMs) by using a multi-dimensional reward model and a self-revision mechanism. Traditional methods like supervised fine-tuning and reinforcement learning from human feedback often fail to capture the complexities of individual user preferences. The proposed framework includes a Personalized Generative Reward Model that provides detailed feedback to prevent reward hacking, and a Critique-Post-Edit mechanism that allows the model to improve its outputs based on this feedback. The results show significant improvements in personalization performance, outperforming standard methods and even surpassing the capabilities of existing models like GPT-4.1.'}, 'zh': {'title': 'æå‡ä¸ªæ€§åŒ–çš„Critique-Post-Editæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCritique-Post-Editçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šç»´åº¦å¥–åŠ±æ¨¡å‹å’Œè‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨ä¸ªæ€§åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å¼•å…¥ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹å’Œæ‰¹è¯„åç¼–è¾‘æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¹¶ç”Ÿæˆç¬¦åˆç”¨æˆ·åå¥½çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18855', 'title': 'Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model', 'url': 'https://huggingface.co/papers/2510.18855', 'abstract': 'Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.', 'score': 12, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': '8c4e98c6e6663cb4', 'authors': ['Ling Team', 'Anqi Shen', 'Baihui Li', 'Bin Hu', 'Bin Jing', 'Cai Chen', 'Chao Huang', 'Chao Zhang', 'Chaokun Yang', 'Cheng Lin', 'Chengyao Wen', 'Congqi Li', 'Deng Zhao', 'Dingbo Yuan', 'Donghai You', 'Fagui Mao', 'Fanzhuang Meng', 'Feng Xu', 'Guojie Li', 'Guowei Wang', 'Hao Dai', 'Haonan Zheng', 'Hong Liu', 'Jia Guo', 'Jiaming Liu', 'Jian Liu', 'Jianhao Fu', 'Jiannan Shi', 'Jianwen Wang', 'Jianxin Lai', 'Jin Yang', 'Jun Mei', 'Jun Zhou', 'Junbo Zhao', 'Junping Zhao', 'Kuan Xu', 'Le Su', 'Lei Chen', 'Li Tang', 'Liang Jiang', 'Liangcheng Fu', 'Lianhao Xu', 'Linfeng Shi', 'Lisha Liao', 'Longfei Zheng', 'Meng Li', 'Mingchun Chen', 'Qi Zuo', 'Qiang Cheng', 'Qianggang Cao', 'Qitao Shi', 'Quanrui Guo', 'Senlin Zhu', 'Shaofei Wang', 'Shaomian Zheng', 'Shuaicheng Li', 'Shuwei Gu', 'Siba Chen', 'Tao Wu', 'Tao Zhang', 'Tianyu Zhang', 'Tianyu Zhou', 'Tiwei Bie', 'Tongkai Yang', 'Wang Hong', 'Wang Ren', 'Weihua Chen', 'Wenbo Yu', 'Wengang Zheng', 'Xiangchun Wang', 'Xiaodong Yan', 'Xiaopei Wan', 'Xin Zhao', 'Xinyu Kong', 'Xinyu Tang', 'Xudong Han', 'Xudong Wang', 'Xuemin Yang', 'Xueyu Hu', 'Yalin Zhang', 'Yan Sun', 'Yicheng Shan', 'Yilong Wang', 'Yingying Xu', 'Yongkang Liu', 'Yongzhen Guo', 'Yuanyuan Wang', 'Yuchen Yan', 'Yuefan Wang', 'Yuhong Guo', 'Zehuan Li', 'Zhankai Xu', 'Zhe Li', 'Zhenduo Zhang', 'Zhengke Gui', 'Zhenxuan Pan', 'Zhenyu Huang', 'Zhenzhong Lan', 'Zhiqiang Ding', 'Zhiqiang Zhang', 'Zhixun Li', 'Zhizhen Liu', 'Zihao Wang', 'Zujie Wen'], 'affiliations': ['Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.18855.jpg', 'data': {'categories': ['#agi', '#reasoning', '#architecture', '#benchmark', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑ…: Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ AI-Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ring-1T â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ thinking-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 50 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸: IcePop Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, C3PO++ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ASystem â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ reinforcement learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞµÑ€ĞµĞ±Ñ€ÑĞ½Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° IMO-2025. ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ reasoning-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Democratizing Intelligence with a Trillion-Parameter Model', 'desc': 'Ring-1T is a groundbreaking open-source thinking model that boasts a trillion parameters, making it one of the largest models available. It tackles significant training challenges through three innovative techniques: IcePop for stabilizing reinforcement learning, C3PO++ for optimizing resource use during long rollouts, and ASystem to eliminate bottlenecks in training. These advancements enable Ring-1T to achieve impressive benchmark scores, demonstrating its superior reasoning abilities. By making this model accessible to the research community, it aims to democratize advanced reasoning intelligence and set a new standard for open-source models.'}, 'zh': {'title': 'ä¸€ä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œæ¨åŠ¨æ¨ç†æ™ºèƒ½çš„æ°‘ä¸»åŒ–', 'desc': 'Ring-1Tæ˜¯ä¸€ä¸ªå¼€æºçš„æ€ç»´æ¨¡å‹ï¼Œæ‹¥æœ‰ä¸€ä¸‡äº¿ä¸ªå‚æ•°ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒä¸­çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯IcePopã€C3PO++å’ŒASystemï¼Œå…‹æœäº†è®­ç»ƒæ¨ç†ä¸ä¸€è‡´ã€èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹å’Œç³»ç»Ÿç“¶é¢ˆç­‰é—®é¢˜ã€‚Ring-1Tåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å‘ç¤¾åŒºå‘å¸ƒå®Œæ•´çš„ä¸€ä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å‰æ²¿æ¨ç†èƒ½åŠ›çš„ç›´æ¥è®¿é—®ï¼Œæ¨åŠ¨äº†å¤§è§„æ¨¡æ¨ç†æ™ºèƒ½çš„æ°‘ä¸»åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17722', 'title': 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues', 'url': 'https://huggingface.co/papers/2510.17722', 'abstract': "MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.", 'score': 12, 'issue_id': 6544, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': '4f3ffcd4b03ee354', 'authors': ['Yaning Pan', 'Zekun Wang', 'Qianqian Xie', 'Yongqian Wen', 'Yuanxing Zhang', 'Guohui Zhang', 'Haoxuan Hu', 'Zhiyu Pan', 'Yibing Huang', 'Zhidong Gan', 'Yonghong Lin', 'An Ping', 'Tianhao Peng', 'Jiaheng Liu'], 'affiliations': ['Fudan University', 'Kuaishou Technology', 'Nanjing University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.17722.jpg', 'data': {'categories': ['#multimodal', '#science', '#video', '#benchmark', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ AI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MT-Video-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 987 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… open-source Ğ¸ closed-source Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸.'}, 'en': {'title': "Evaluating AI's Dialogue Skills in Video Contexts", 'desc': "The paper introduces MT-Video-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in the context of multi-turn video dialogues. Unlike previous benchmarks that focused on single-turn interactions, MT-Video-Bench assesses the models' abilities to perceive and interact across complex dialogues. It includes 987 carefully curated dialogues from various domains, ensuring relevance to real-world applications like sports analysis and intelligent tutoring. The evaluation reveals notable performance differences among MLLMs, highlighting their strengths and weaknesses in handling multi-turn interactions in video content."}, 'zh': {'title': 'å¤šè½®è§†é¢‘å¯¹è¯çš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'MT-Video-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šè½®è§†é¢‘å¯¹è¯ä¸­çš„èƒ½åŠ›çš„åŸºå‡†ã€‚å®ƒå…³æ³¨æ„ŸçŸ¥èƒ½åŠ›å’Œäº’åŠ¨æ€§ï¼Œæ¶µç›–äº†æ¥è‡ªä¸åŒé¢†åŸŸçš„987ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šè½®å¯¹è¯ã€‚è¯¥åŸºå‡†æ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°å·¥å…·åœ¨å¤šè½®å¯¹è¯å¤æ‚æ€§æ–¹é¢çš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚äº’åŠ¨ä½“è‚²åˆ†æå’ŒåŸºäºè§†é¢‘çš„æ™ºèƒ½è¾…å¯¼ã€‚é€šè¿‡å¯¹å¤šç§æœ€å…ˆè¿›çš„ MLLMs è¿›è¡Œè¯„ä¼°ï¼ŒMT-Video-Bench æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ—¶çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18250', 'title': 'ssToken: Self-modulated and Semantic-aware Token Selection for LLM\n  Fine-tuning', 'url': 'https://huggingface.co/papers/2510.18250', 'abstract': 'ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.', 'score': 10, 'issue_id': 6546, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'b1590a8489248052', 'authors': ['Xiaohan Qin', 'Xiaoxing Wang', 'Ning Liao', 'Cancheng Zhang', 'Xiangdong Zhang', 'Mingquan Feng', 'Jingzhi Wang', 'Junchi Yan'], 'affiliations': ['Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18250.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ fine-tuning LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ssToken - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ supervised fine-tuning Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ reference Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ loss Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ loss-based Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ semantic-aware Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ token-level ÑĞµĞ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ fine-tuning Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Token Selection with ssToken for Better Language Model Fine-Tuning', 'desc': "The paper introduces ssToken, a novel approach for selecting tokens during the supervised fine-tuning of large language models. It addresses limitations of existing methods by using a self-modulated signal derived from the model's own history, allowing for adaptive token selection without needing an additional reference model. Additionally, ssToken incorporates a semantic-aware metric that evaluates token importance based on their meaning, rather than just loss metrics. The results show that ssToken significantly improves performance and efficiency compared to traditional token selection methods."}, 'zh': {'title': 'è‡ªæˆ‘è°ƒèŠ‚ä¸è¯­ä¹‰æ„ŸçŸ¥çš„ä»¤ç‰Œé€‰æ‹©æ–°æ–¹æ³•', 'desc': 'ssTokenæ˜¯ä¸€ç§è‡ªæˆ‘è°ƒèŠ‚å’Œè¯­ä¹‰æ„ŸçŸ¥çš„ä»¤ç‰Œé€‰æ‹©æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒã€‚å®ƒé€šè¿‡è‡ªé€‚åº”é€‰æ‹©ä»¤ç‰Œå¹¶æä¾›è¡¥å……çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒssTokenåˆ©ç”¨å†å²æ¨¡å‹è®¡ç®—æ¯ä¸ªä»¤ç‰Œçš„æŸå¤±å·®å¼‚ï¼Œä½œä¸ºè‡ªæˆ‘è°ƒèŠ‚ä¿¡å·ï¼Œä¼˜åŒ–ä»¤ç‰Œé€‰æ‹©è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒssTokenå¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„è¯­ä¹‰æ„ŸçŸ¥ä»¤ç‰Œé‡è¦æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œè¿›ä¸€æ­¥æé«˜äº†é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18795', 'title': 'ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder', 'url': 'https://huggingface.co/papers/2510.18795', 'abstract': "ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP", 'score': 7, 'issue_id': 6545, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'c294c8d308c352e1', 'authors': ['Xiaoxing Hu', 'Kaicheng Yang', 'Ziyong Feng', 'Qi Ming', 'Zonghao Guo', 'Xiang An', 'Ziyong Feng', 'Junchi Yan', 'Xue Yang'], 'affiliations': ['Beijing Institute of Technology', 'Beijing University of Technology', 'DeepGlint', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18795.jpg', 'data': {'categories': ['#dataset', '#alignment', '#training', '#multimodal', '#long_context'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ CLIP Ñ LLM Ñ‡ĞµÑ€ĞµĞ· curriculum learning', 'desc': 'ProCLIP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ ĞµÑ‘ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² CLIP, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ curriculum learning. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° CLIP Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² LLM-ÑĞ¼Ğ±ĞµĞ´Ğ´ĞµÑ€, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· self-distillation. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ CLIP.'}, 'en': {'title': 'Enhancing CLIP with ProCLIP: Aligning Vision and Language for Better Understanding', 'desc': "ProCLIP is a framework designed to improve the text processing abilities of CLIP by aligning its image encoder with a large language model (LLM) embedder. It addresses the limitations of CLIP's original text encoder, which struggles with long texts and multilingual inputs. By using curriculum learning and contrastive tuning, ProCLIP preserves the pretrained knowledge of CLIP while enhancing its semantic understanding. The framework employs techniques like self-distillation and specific loss functions to ensure effective alignment and prevent overfitting during training."}, 'zh': {'title': 'ProCLIPï¼šæå‡ CLIP çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›', 'desc': 'ProCLIP æ˜¯ä¸€ç§å¢å¼º CLIP æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ å’Œå¯¹æ¯”è°ƒä¼˜å°†å…¶å›¾åƒç¼–ç å™¨ä¸åŸºäº LLM çš„åµŒå…¥å™¨å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™ CLIP çš„é¢„è®­ç»ƒçŸ¥è¯†ã€‚åŸå§‹ CLIP æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥é•¿åº¦é™åˆ¶ä¸º 77 ä¸ªæ ‡è®°ï¼Œå½±å“äº†å…¶å¤„ç†é•¿æ–‡æœ¬å’Œç»†ç²’åº¦è¯­ä¹‰ç†è§£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCLIP æ–‡æœ¬ç¼–ç å™¨ä¸æ”¯æŒå¤šè¯­è¨€è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ProCLIP é€šè¿‡çŸ¥è¯†è’¸é¦å’Œå¯¹æ¯”è°ƒä¼˜ï¼Œé€æ­¥å®ç° CLIP å›¾åƒç¼–ç å™¨ä¸ LLM åµŒå…¥å™¨çš„æœ‰æ•ˆå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17519', 'title': 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation\n  Models', 'url': 'https://huggingface.co/papers/2510.17519', 'abstract': 'A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, large-scale generative models for visual content (e.g., images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in https://github.com/Shopee-MUG/MUG-V{our webpage}.', 'score': 7, 'issue_id': 6545, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': '8d54b8afe2e3fec0', 'authors': ['Yongshun Zhang', 'Zhongyi Fan', 'Yonghang Zhang', 'Zhangzikang Li', 'Weifeng Chen', 'Zhongwei Feng', 'Chaoyue Wang', 'Peng Hou', 'Anxiang Zeng'], 'affiliations': ['LLM Team, Shopee Pte. Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2510.17519.jpg', 'data': {'categories': ['#video', '#architecture', '#data', '#training', '#optimization', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MUG-V 10B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ e-commerce Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Megatron-Core Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑƒĞ·Ğ»Ğ¾Ğ². ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ²Ñ‹Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ inference pipeline, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… video generation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing Video Generation: MUG-V 10B Unleashed!', 'desc': 'This paper presents a new training framework designed for large-scale video generation models, addressing the challenges of data processing, model architecture, training strategy, and infrastructure. The framework enhances efficiency and performance by optimizing data preprocessing, video compression, and training techniques, leading to the development of the MUG-V 10B model. This model not only matches state-of-the-art performance but also excels in specific tasks like e-commerce video generation, outperforming existing open-source models in human evaluations. Additionally, the authors have made their training code and model weights publicly available, promoting further research and development in the field.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆçš„è®­ç»ƒæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ•°æ®å¤„ç†ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’ŒåŸºç¡€è®¾æ–½ã€‚é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼Œæ¨¡å‹åœ¨æ•°æ®é¢„å¤„ç†ã€è§†é¢‘å‹ç¼©ã€å‚æ•°ç¼©æ”¾ã€åŸºäºè¯¾ç¨‹çš„é¢„è®­ç»ƒå’Œå¯¹é½åè®­ç»ƒç­‰å„ä¸ªé˜¶æ®µéƒ½å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡å’Œæ€§èƒ½æ”¹è¿›ã€‚æœ€ç»ˆç”Ÿæˆçš„æ¨¡å‹MUG-V 10Båœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨ç”µå•†å¯¼å‘çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºåŸºçº¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç ”ç©¶å›¢é˜Ÿå¼€æºäº†å®Œæ•´çš„è®­ç»ƒä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œæ¨åŠ¨äº†å¤§è§„æ¨¡è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18873', 'title': 'DSI-Bench: A Benchmark for Dynamic Spatial Intelligence', 'url': 'https://huggingface.co/papers/2510.18873', 'abstract': "DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.", 'score': 4, 'issue_id': 6547, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': '9706b668238005bf', 'authors': ['Ziang Zhang', 'Zehan Wang', 'Guanghao Zhang', 'Weilong Dai', 'Yan Xia', 'Ziang Yan', 'Minjie Hong', 'Zhou Zhao'], 'affiliations': ['Alibaba Group', 'Shanghai AI Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18873.jpg', 'data': {'categories': ['#cv', '#reasoning', '#benchmark', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° AI Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ñ‚ĞµÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DSI-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 1000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 1700 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ´ĞµĞ²ÑÑ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 14 vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ğ°ÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»Ñ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ VLM Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'Evaluating Dynamic Spatial Reasoning in AI Models', 'desc': 'The paper introduces DSI-Bench, a benchmark designed to assess the dynamic spatial reasoning abilities of vision-language models (VLMs) and visual expertise models. It highlights the challenges these models face in understanding complex scenarios where both observers and objects are in motion. The benchmark includes nearly 1,000 dynamic videos and over 1,700 annotated questions that focus on various motion patterns. The evaluation of 14 models reveals significant limitations, such as confusion between observer and object motion and difficulties in accurately interpreting relative relationships in dynamic environments.'}, 'zh': {'title': 'åŠ¨æ€ç©ºé—´æ™ºèƒ½çš„è¯„ä¼°ä¸æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†DSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†è§‰ä¸“å®¶æ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«è¿‘1000ä¸ªåŠ¨æ€è§†é¢‘å’Œ1700å¤šä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæ¶µç›–äº†è§‚å¯Ÿè€…å’Œç‰©ä½“çš„ä¹ç§è§£è€¦è¿åŠ¨æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£è‡ªæˆ‘è¿åŠ¨ã€ç‰©ä½“è¿åŠ¨å’Œç›¸å¯¹å…³ç³»æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¸¸å¸¸æ··æ·†è§‚å¯Ÿè€…å’Œç‰©ä½“çš„è¿åŠ¨ã€‚DSI-Benchä¸ºæœªæ¥åŠ¨æ€ç©ºé—´æ™ºèƒ½æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦çš„å‘ç°å’Œè§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18775', 'title': 'UltraGen: High-Resolution Video Generation with Hierarchical Attention', 'url': 'https://huggingface.co/papers/2510.18775', 'abstract': 'UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.', 'score': 4, 'issue_id': 6544, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': '04f7bc0adba95d76', 'authors': ['Teng Hu', 'Jiangning Zhang', 'Zihan Su', 'Ran Yi'], 'affiliations': ['Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18775.jpg', 'data': {'categories': ['#video', '#games', '#optimization', '#architecture', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 4K Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'UltraGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ (1080P/2K/4K) Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… diffusion transformer Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹. UltraGen Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ dual-branch Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ‚Ğ²ÑŒ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UltraGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ñ super-resolution Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'UltraGen: High-Resolution Video Generation Made Efficient', 'desc': 'UltraGen is a new framework for generating high-resolution videos efficiently. It uses a hierarchical dual-branch attention architecture that separates local and global attention, allowing for detailed regional content and overall coherence. This approach, combined with a spatially compressed global modeling strategy, reduces computational costs while maintaining high-quality outputs. As a result, UltraGen can produce videos at resolutions up to 4K, surpassing previous models in both quality and performance.'}, 'zh': {'title': 'UltraGenï¼šé«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘çš„æ–°æ¡†æ¶', 'desc': 'UltraGenæ˜¯ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åˆæˆé«˜åˆ†è¾¨ç‡è§†é¢‘ã€‚å®ƒé‡‡ç”¨äº†åˆ†å±‚åŒåˆ†æ”¯æ³¨æ„åŠ›æ¶æ„ï¼Œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚é€šè¿‡ç©ºé—´å‹ç¼©çš„å…¨å±€å»ºæ¨¡ç­–ç•¥ï¼ŒUltraGenèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUltraGenåœ¨ç”Ÿæˆ1080På’Œ4Kåˆ†è¾¨ç‡è§†é¢‘æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17045', 'title': 'Video Reasoning without Training', 'url': 'https://huggingface.co/papers/2510.17045', 'abstract': 'The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning using Large Multimodal Models (LMMs) relies on costly reinforcement learning (RL) and verbose chain-of-thought, resulting in substantial computational overhead during both training and inference. Moreover, the mechanisms that control the thinking process in these reasoning models are very limited. In this paper, using entropy of the model\'s output as a signal, we discover that the high-quality models go through a series of micro-explorations and micro-exploitations which keep the reasoning process grounded (i.e., avoid excessive randomness while the model is exploring or thinking through an answer). We further observe that once this "thinking" process is over, more accurate models demonstrate a better convergence by reducing the entropy significantly via a final exploitation phase (i.e., a more certain convergence towards a solution trajectory). We then use these novel, theoretically-grounded insights to tune the model\'s behavior directly at inference, without using any RL or supervised fine-tuning. Specifically, during inference, our proposed approach called V-Reason (Video-Reason) adapts the value cache of the LMM via a few optimization steps on a small, trainable controller using an entropy-based objective, i.e., no supervision from any dataset or RL is necessary. This tuning improves the model\'s micro-exploration and exploitation behavior during inference. Our experiments show that our proposed method achieves significant improvements over the base instruction-tuned models across several video reasoning datasets, narrowing the gap with RL-trained models to within 0.6% average accuracy without any training, while offering massive efficiency benefits: output tokens are reduced by 58.6% compared to the RL model.', 'score': 3, 'issue_id': 6545, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 19', 'zh': '10æœˆ19æ—¥'}, 'hash': '568871e71cf8c80e', 'authors': ['Deepak Sridhar', 'Kartikeya Bhardwaj', 'Jeya Pradha Jeyaraj', 'Nuno Vasconcelos', 'Ankita Nayak', 'Harris Teague'], 'affiliations': ['Qualcomm AI Research', 'UCSD'], 'pdf_title_img': 'assets/pdf/title_img/2510.17045.jpg', 'data': {'categories': ['#reasoning', '#video', '#training', '#multimodal', '#inference', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ V-Reason â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Large Multimodal Models Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‚ Ñ„Ğ°Ğ·Ñ‹ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾-ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ value cache Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ° Ğ±ĞµĞ· reinforcement learning Ğ¸Ğ»Ğ¸ supervised fine-tuning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº RL-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² 0.6%), Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 58.6%.'}, 'en': {'title': 'Optimizing Video Reasoning with V-Reason: Efficiency Meets Accuracy', 'desc': "The paper introduces V-Reason, a novel method that enhances the performance of Large Multimodal Models (LMMs) in video reasoning tasks by optimizing their inference behavior using entropy-based techniques. Unlike traditional approaches that rely on reinforcement learning or extensive supervised fine-tuning, V-Reason directly tunes the model's output during inference, leading to improved accuracy and efficiency. The method leverages insights from the model's output entropy to balance exploration and exploitation, ensuring that the reasoning process remains grounded and converges effectively towards accurate solutions. Experimental results demonstrate that V-Reason significantly narrows the performance gap with RL-trained models while drastically reducing computational costs."}, 'zh': {'title': 'V-Reasonï¼šé«˜æ•ˆè§†é¢‘æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºV-Reasonçš„æ–¹æ³•ï¼Œé€šè¿‡åŸºäºç†µçš„ä¼˜åŒ–æ¥è°ƒæ•´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¡Œä¸ºï¼Œä»è€Œæé«˜è§†é¢‘æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œè€Œæ— éœ€ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æˆ–ç›‘ç£å¾®è°ƒã€‚ç ”ç©¶å‘ç°ï¼Œé«˜è´¨é‡æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šç»å†ä¸€ç³»åˆ—å¾®è§‚æ¢ç´¢å’Œå¾®è§‚åˆ©ç”¨ï¼Œä¿æŒæ¨ç†è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œæ¨ç†ç»“æŸåï¼Œæ›´å‡†ç¡®çš„æ¨¡å‹é€šè¿‡æ˜¾è‘—é™ä½ç†µå€¼æ¥å®ç°æ›´å¥½çš„æ”¶æ•›ã€‚V-Reasonæ–¹æ³•é€šè¿‡å°‘é‡ä¼˜åŒ–æ­¥éª¤ç›´æ¥åœ¨æ¨ç†é˜¶æ®µè°ƒæ•´æ¨¡å‹è¡Œä¸ºï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18489', 'title': 'Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from\n  Alternating-exposure Monocular Videos', 'url': 'https://huggingface.co/papers/2510.18489', 'abstract': 'A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.', 'score': 2, 'issue_id': 6548, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'd788eababe82cd47', 'authors': ['Jinfeng Liu', 'Lingtong Kong', 'Mi Zhou', 'Jinwen Chen', 'Dan Xu'], 'affiliations': ['The Hong Kong University of Science and Technology (HKUST)', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2510.18489.jpg', 'data': {'categories': ['#3d', '#benchmark', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 4D HDR ÑÑ†ĞµĞ½ Ğ¸Ğ· LDR Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ· ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Mono4DGS-HDR Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D HDR ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ½ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… LDR Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ÑÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ· ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ HDR Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ² Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ€ĞºĞ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ HDR Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Revolutionizing HDR Scene Reconstruction from LDR Videos', 'desc': 'This paper presents Mono4DGS-HDR, a novel system designed to reconstruct 4D high dynamic range (HDR) scenes from unposed low dynamic range (LDR) videos. The approach utilizes Gaussian Splatting within a two-stage optimization framework, allowing for effective HDR video reconstruction without requiring camera pose information. The first stage focuses on creating a Gaussian representation of the HDR video, while the second stage refines this representation in world space, incorporating camera poses. Additionally, a temporal luminance regularization technique is introduced to ensure consistent HDR appearance across frames, leading to superior rendering quality and speed compared to existing methods.'}, 'zh': {'title': 'ä»LDRè§†é¢‘é‡å»º4D HDRåœºæ™¯çš„åˆ›æ–°ç³»ç»Ÿ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Mono4DGS-HDRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»æœªæ ‡å®šçš„å•ç›®ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰è§†é¢‘ä¸­é‡å»ºå¯æ¸²æŸ“çš„4Dé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰åœºæ™¯çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŸºäºé«˜æ–¯ç‚¹äº‘çš„ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ³•ï¼Œç¬¬ä¸€é˜¶æ®µåœ¨æ­£äº¤ç›¸æœºåæ ‡ç©ºé—´ä¸­å­¦ä¹ è§†é¢‘HDRé«˜æ–¯è¡¨ç¤ºï¼Œæ¶ˆé™¤äº†å¯¹ç›¸æœºå§¿æ€çš„éœ€æ±‚ã€‚ç¬¬äºŒé˜¶æ®µå°†è§†é¢‘é«˜æ–¯è½¬æ¢ä¸ºä¸–ç•Œç©ºé—´ï¼Œå¹¶è”åˆä¼˜åŒ–ä¸–ç•Œé«˜æ–¯ä¸ç›¸æœºå§¿æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´äº®åº¦æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥å¢å¼ºHDRå¤–è§‚çš„æ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.16505', 'title': 'PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies', 'url': 'https://huggingface.co/papers/2510.16505', 'abstract': "PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants.", 'score': 2, 'issue_id': 6549, 'pub_date': '2025-10-18', 'pub_date_card': {'ru': '18 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 18', 'zh': '10æœˆ18æ—¥'}, 'hash': '98b154fc7678bf38', 'authors': ['Lukas Selch', 'Yufang Hou', 'M. Jehanzeb Mirza', 'Sivan Doveh', 'James Glass', 'Rogerio Feris', 'Wei Lin'], 'affiliations': ['Interdisciplinary Transformation University Austria', 'Johannes Kepler University Linz', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2510.16505.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PRISMM-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 262 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚, Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹, Ğ¸Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 21 Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ LMM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (26-54%), Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Evaluating Multimodal Models for Scientific Consistency', 'desc': 'PRISMM-Bench is a benchmark designed to evaluate how well large multimodal models (LMMs) can identify, correct, and reason about inconsistencies in scientific papers. It highlights the difficulties these models face when dealing with complex information presented in various formats, such as text, figures, and tables. The benchmark is based on real inconsistencies flagged by reviewers, making it more relevant than previous tests that used synthetic errors. The results show that current LMMs struggle significantly with these tasks, indicating a need for improvement in their ability to support scientific research effectively.'}, 'zh': {'title': 'å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'PRISMM-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦è®ºæ–‡ä¸­æ£€æµ‹ã€çº æ­£å’Œæ¨ç†ä¸ä¸€è‡´æ€§çš„èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†ä¸­çš„é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬ã€å›¾å½¢ã€è¡¨æ ¼å’Œæ–¹ç¨‹ä¹‹é—´çš„ç»†å¾®ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡ä»çœŸå®çš„å®¡ç¨¿äººæ ‡è®°çš„ä¸ä¸€è‡´æ€§ä¸­æ„å»ºåŸºå‡†ï¼ŒPRISMM-Bench è®¾è®¡äº†ä¸‰ä¸ªä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†è¿™äº›ä¸ä¸€è‡´æ€§æ—¶è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†åœ¨ç§‘å­¦ç ”ç©¶ä¸­å»ºç«‹å¯ä¿¡èµ–åŠ©æ‰‹çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.15600', 'title': 'Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism', 'url': 'https://huggingface.co/papers/2510.15600', 'abstract': 'Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the "Sketch-and-Fill" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.', 'score': 2, 'issue_id': 6547, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}, 'hash': '604d7aca35063798', 'authors': ['Haoran Sun', 'Yankai Jiang', 'Zhenyu Tang', 'Yaning Pan', 'Shuang Gu', 'Zekai Lin', 'Lilong Wang', 'Wenjie Lou', 'Lei Liu', 'Lei Bai', 'Xiaosong Wang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.15600.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#data', '#science', '#benchmark', '#agents'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ‚ Ğ¸Ğ´ĞµĞ¸ Ğ´Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Thoth â€” Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Â«Sketch-and-FillÂ» (Ğ½Ğ°Ğ±Ñ€Ğ¾ÑĞ¾Ğº Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SciRecipe Ñ 12 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² Ğ¸Ğ· 27 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Thoth Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ LLM Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'Thoth: Revolutionizing Scientific Protocol Generation with Precision and Reliability', 'desc': 'Thoth is a large language model designed to generate reliable scientific protocols using the Sketch-and-Fill paradigm and a structured reward mechanism. This model addresses the common issues of incomplete and inconsistent protocol generation found in existing models by utilizing a comprehensive dataset called SciRecipe, which includes over 12,000 structured protocols. The Sketch-and-Fill approach breaks down the protocol generation process into clear steps, ensuring that each part is explicit and verifiable. As a result, Thoth demonstrates superior performance in generating executable protocols, making it a valuable tool for enhancing reproducibility in scientific research.'}, 'zh': {'title': 'Thothï¼šæå‡ç§‘å­¦åè®®ç”Ÿæˆçš„å¯é æ€§ä¸æ‰§è¡ŒåŠ›', 'desc': 'Thothæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨Sketch-and-FillèŒƒå¼å’Œç»“æ„åŒ–ç»„ä»¶å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¯é å’Œå¯æ‰§è¡Œçš„ç§‘å­¦åè®®ã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è‡ªåŠ¨ç”Ÿæˆç²¾ç¡®ã€æœ‰åºçš„åè®®ï¼Œä»è€Œæé«˜ç§‘å­¦é‡ç°çš„æ•ˆç‡ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹ç”Ÿæˆä¸å®Œæ•´æˆ–ä¸ä¸€è‡´åè®®çš„é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†SciRecipeæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡12000ä¸ªç»“æ„åŒ–åè®®ã€‚Thothåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼Œæ¨åŠ¨äº†ç§‘å­¦åŠ©æ‰‹çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.14264', 'title': 'AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement\n  Learning Framework for Stock Trading', 'url': 'https://huggingface.co/papers/2510.14264', 'abstract': 'AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter', 'score': 2, 'issue_id': 6549, 'pub_date': '2025-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '6dd32561751df799', 'authors': ['Zheye Deng', 'Jiashu Wang'], 'affiliations': ['HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2510.14264.jpg', 'data': {'categories': ['#games', '#optimization', '#open_source', '#rl', '#training', '#agents', '#interpretability'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞĞ´Ğ¸Ğ½ ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ…Ğ°Ğ¾ÑĞ°: RL-Ñ‚Ñ€ĞµĞ¹Ğ´ĞµÑ€ Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹', 'desc': 'AlphaQuanter â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², AlphaQuanter Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€Ñ‹Ğ½ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼. Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ AlphaQuanter Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€ĞµĞ¹Ğ´ĞµÑ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹.'}, 'en': {'title': "Revolutionizing Automated Trading with AlphaQuanter's Reinforcement Learning", 'desc': "AlphaQuanter is a single-agent framework that utilizes reinforcement learning to enhance automated trading strategies. It learns dynamic policies that allow it to adapt to changing market conditions while proactively gathering relevant information. Unlike multi-agent systems, AlphaQuanter provides a transparent decision-making process, enabling better end-to-end optimization and consistent trading signals. The framework's performance on financial metrics is state-of-the-art, and its interpretable strategies offer valuable insights for human traders."}, 'zh': {'title': 'AlphaQuanterï¼šè‡ªåŠ¨åŒ–äº¤æ˜“çš„æ–°çºªå…ƒ', 'desc': 'AlphaQuanteræ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å•ä»£ç†æ¡†æ¶ï¼Œä¸“æ³¨äºè‡ªåŠ¨åŒ–äº¤æ˜“ã€‚å®ƒé€šè¿‡å­¦ä¹ åŠ¨æ€ç­–ç•¥å’Œä¸»åŠ¨è·å–ä¿¡æ¯ï¼Œè¾¾åˆ°äº†å“è¶Šçš„äº¤æ˜“è¡¨ç°ã€‚ä¸å¤šä»£ç†æ¡†æ¶ç›¸æ¯”ï¼ŒAlphaQuanteråœ¨æ•ˆç‡å’Œä¸€è‡´æ€§ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»å¸‚åœºåé¦ˆä¸­å­¦ä¹ å‡ºè¿è´¯çš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaQuanteråœ¨å…³é”®é‡‘èæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸ºäººç±»äº¤æ˜“è€…æä¾›äº†æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.18554', 'title': 'Extracting alignment data in open models', 'url': 'https://huggingface.co/papers/2510.18554', 'abstract': "Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of 10times) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset.", 'score': 1, 'issue_id': 6549, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': '01465adab5a272af', 'authors': ['Federico Barbero', 'Xiangming Gu', 'Christopher A. Choquette-Choo', 'Chawin Sitawarin', 'Matthew Jagielski', 'Itay Yona', 'Petar VeliÄkoviÄ‡', 'Ilia Shumailov', 'Jamie Hayes'], 'affiliations': ['AI Sequrity Company', 'Anthropic', 'Google DeepMind', 'MentaLeap', 'National University of Singapore', 'OpenAI', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.18554.jpg', 'data': {'categories': ['#hallucinations', '#rlhf', '#long_context', '#training', '#alignment', '#data'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ· Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ alignment-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ (ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ¾Ğº Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² SFT Ğ¸ RL, Ğ¸ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ¸ÑĞºĞ¸ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ alignment-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Hidden Data: Risks in Model Distillation Practices', 'desc': 'This paper discusses how to extract alignment training data from models that have already been trained, using embedding models to find semantic similarities. The authors argue that traditional methods like string matching are less effective than embedding models for this purpose. They demonstrate that using embedding models can reveal much more data than previously thought, highlighting a significant risk in current distillation practices. The findings suggest that distillation may inadvertently lead to models reusing parts of their original training data, raising important questions about the implications of this process.'}, 'zh': {'title': 'ä»åè®­ç»ƒæ¨¡å‹ä¸­æå–å¯¹é½æ•°æ®çš„æ½œåœ¨é£é™©ä¸æœºé‡', 'desc': 'æœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ä»åè®­ç»ƒæ¨¡å‹ä¸­æå–å¤§é‡å¯¹é½è®­ç»ƒæ•°æ®ï¼Œè¿™äº›æ•°æ®å¯ä»¥ç”¨äºæå‡æ¨¡å‹åœ¨é•¿æ–‡æœ¬æ¨ç†ã€å®‰å…¨æ€§ã€éµå¾ªæŒ‡ä»¤å’Œæ•°å­¦ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒåµŒå…¥æ¨¡å‹æ¯”ä¼ ç»Ÿçš„å­—ç¬¦ä¸²åŒ¹é…æ–¹æ³•æ›´é€‚åˆæˆ‘ä»¬çš„ç›®æ ‡ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å­—ç¬¦ä¸²ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œä½¿ç”¨è¿‘ä¼¼å­—ç¬¦ä¸²åŒ¹é…çš„æ–¹æ³•ä¼šä¸¥é‡ä½ä¼°å¯æå–æ•°æ®çš„æ•°é‡ï¼Œå¯èƒ½ä½ä¼°äº†åå€ä»¥ä¸Šã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æå–å¯¹é½æ•°æ®çš„æ½œåœ¨é£é™©ï¼Œå¹¶å¼•å‘äº†å…³äºè’¸é¦å®è·µä¸‹æ¸¸å½±å“çš„æœ‰è¶£è®¨è®ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17928', 'title': 'EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable\n  Learning', 'url': 'https://huggingface.co/papers/2510.17928', 'abstract': 'An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.', 'score': 0, 'issue_id': 6548, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': 'cc690abb2b8c7630', 'authors': ['He Du', 'Bowen Li', 'Aijun Yang', 'Siyang He', 'Qipeng Guo', 'Dacheng Tao'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2510.17928.jpg', 'data': {'categories': ['#hallucinations', '#synthetic', '#training', '#reinforcement_learning', '#agents', '#data', '#dataset', '#rl'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¾Ğº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ â€” Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LiveCodeBench Ğ¸ AgentBench-OS.'}, 'en': {'title': 'Revolutionizing Data Synthesis for Language Models', 'desc': 'This paper presents a new framework for creating reliable synthetic data for training language models, which is crucial for improving their performance in various tasks. The proposed method uses an evolutionary approach to generate problems, solutions, and verification artifacts that can be checked for accuracy. By focusing on consistency between human evaluations and automated checks, the framework enhances the quality of training data without relying on specific task rules. Experiments show that this approach leads to better results in reinforcement learning and model distillation, demonstrating its effectiveness across different applications.'}, 'zh': {'title': 'è¿›åŒ–æ¡†æ¶ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¯éªŒè¯æ€§ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºåˆæˆå¯éªŒè¯çš„æ•°æ®ï¼Œä»¥æå‡è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å’Œè’¸é¦è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»æœ€å°çš„ç§å­ç›‘ç£ä¸­ï¼Œè”åˆåˆæˆé—®é¢˜ã€å¤šæ ·çš„å€™é€‰è§£å†³æ–¹æ¡ˆå’ŒéªŒè¯å·¥ä»¶ï¼Œå¹¶é€šè¿‡ä¸€è‡´æ€§è¯„ä¼°å™¨è¿­ä»£å‘ç°ç­–ç•¥ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„å¯å‘å¼è§„åˆ™ï¼Œè€Œæ˜¯æä¾›äº†ä¸€ç§é€šç”¨çš„å¯éªŒè¯æ€§è¯„ä¼°æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi (1)', '#alignment (3)', '#architecture (6)', '#audio', '#benchmark (11)', '#cv (3)', '#data (6)', '#dataset (5)', '#diffusion (2)', '#ethics', '#games (4)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (4)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (7)', '#open_source (6)', '#optimization (10)', '#plp', '#rag', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (5)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (13)', '#transfer_learning', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-22 07:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-22 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-22 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    