{
    "date": {
        "ru": "25 марта",
        "en": "March 25",
        "zh": "3月25日"
    },
    "time_utc": "2025-03-25 03:26",
    "weekday": 1,
    "issue_id": 2875,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.17359",
            "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
            "url": "https://huggingface.co/papers/2503.17359",
            "abstract": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.",
            "score": 24,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "0046c940a41d8637",
            "authors": [
                "Jiwen Yu",
                "Yiran Qin",
                "Haoxuan Che",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Kuaishou",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17359.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в разработке игр: ИИ-генерируемые миры будущего",
                    "desc": "Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GGE позволяет создавать неограниченный новый контент для игр следующего поколения, используя преимущества IGV в синтезе высококачественного контента, моделировании физики мира и интерактивности. Авторы представляют комплексную структуру основных модулей GGE и иерархическую дорожную карту зрелости (L0-L4) для его развития. Это исследование открывает новые перспективы для разработки игр в эпоху искусственного интеллекта."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Generative Engines",
                    "desc": "This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies."
                },
                "zh": {
                    "title": "AI驱动的游戏创作新纪元",
                    "desc": "现代游戏开发面临着创造力和成本的重大挑战，传统游戏引擎的内容预设限制了创新。最近，视频生成模型的突破使得合成逼真且互动的虚拟环境成为可能，这为游戏创作带来了革命性的机会。我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，能够在下一代游戏中实现无限的新内容生成。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制互动、长期记忆能力和因果推理等方面的独特优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18940",
            "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
            "url": "https://huggingface.co/papers/2503.18940",
            "abstract": "Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling",
            "score": 5,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "83ffcf1c20f5d4db",
            "authors": [
                "Ye Tian",
                "Xin Xia",
                "Yuxi Ren",
                "Shanchuan Lin",
                "Xing Wang",
                "Xuefeng Xiao",
                "Yunhai Tong",
                "Ling Yang",
                "Bin Cui"
            ],
            "affiliations": [
                "Bytedance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18940.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Ускорение диффузионных моделей без потери качества",
                    "desc": "Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует предобученные низкоразрешающие модели для уменьшения вычислительных затрат без потери качества выходных данных. Метод следует схеме высокое-низкое-высокое разрешение при денойзинге, что позволяет ускорить вывод в 2.5-3 раза для задач генерации изображений и видео. Bottleneck Sampling не требует переобучения модели и сохраняет качество результатов на уровне стандартного полноразрешающего семплирования."
                },
                "en": {
                    "title": "Speeding Up Diffusion Models with Bottleneck Sampling",
                    "desc": "This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvements—up to 3 times faster for images and 2.5 times for videos—while still producing high-quality outputs."
                },
                "zh": {
                    "title": "瓶颈采样：高效的扩散模型推理",
                    "desc": "扩散模型在视觉内容生成方面表现出色，但在推理时由于计算成本高而难以部署。主要的计算负担来自于自注意力机制在图像或视频分辨率上的二次复杂性。我们提出了一种名为瓶颈采样的框架，利用低分辨率的先验知识来减少计算开销，同时保持输出质量。通过在高分辨率和低分辨率之间进行高低高的去噪工作流程，我们的实验表明，该方法在图像生成中加速推理速度可达3倍，在视频生成中可达2.5倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17489",
            "title": "Judge Anything: MLLM as a Judge Across Any Modality",
            "url": "https://huggingface.co/papers/2503.17489",
            "abstract": "Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.",
            "score": 5,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "bb040618997e1b0a",
            "authors": [
                "Shu Pu",
                "Yaochen Wang",
                "Dongping Chen",
                "Yuhang Chen",
                "Guohao Wang",
                "Qi Qin",
                "Zhongyi Zhang",
                "Zhiyuan Zhang",
                "Zetong Zhou",
                "Shuang Gong",
                "Yi Gui",
                "Yao Wan",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17489.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации",
                    "desc": "Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальных языковых моделей (MLLM) в задачах понимания и генерации контента различных модальностей. TaskAnything оценивает способности MLLM в 15 категориях задач с различными комбинациями модальностей, используя 1500 запросов. JudgeAnything оценивает способности MLLM выступать в роли судей, сравнивая их оценки с человеческими по методикам попарного сравнения и балльной оценки. Результаты показывают, что MLLM лучше справляются с задачами понимания, чем с задачами генерации, выявляя проблемы межмодальных предубеждений и галлюцинаций."
                },
                "en": {
                    "title": "Enhancing Multimodal Evaluation with MLLMs",
                    "desc": "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."
                },
                "zh": {
                    "title": "多模态评估的新视角",
                    "desc": "本论文探讨了在多模态理解（MMU）和生成（MMG）任务中评估生成基础模型的挑战，尤其是跨模态交互的复杂性。我们提出了使用多模态大语言模型（MLLMs）作为自动评估者的想法，并引入了两个基准：TaskAnything和JudgeAnything，分别用于评估MLLMs在任何模态任务中的整体性能和判断能力。实验结果显示，尽管MLLMs在MMU任务中表现出一定的潜力，但在MMG任务中面临显著挑战，暴露了跨模态偏见和幻觉问题。为了解决这些问题，我们提出了OmniArena，一个用于评估多模态模型和奖励模型的自动化平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17439",
            "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
            "url": "https://huggingface.co/papers/2503.17439",
            "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.",
            "score": 2,
            "issue_id": 2875,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "946d486485fedb03",
            "authors": [
                "Zhuoshi Pan",
                "Yu Li",
                "Honglin Lin",
                "Qizhi Pei",
                "Zinan Tang",
                "Wei Wu",
                "Chenlin Ming",
                "H. Vicky Zhao",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17439.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Учимся на ошибках: новый подход к улучшению математических способностей ИИ",
                    "desc": "Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математические задачи путем обучения на ошибках. LEMMA создает набор данных, состоящий из неправильных решений с ошибочными шагами и связями с правильными решениями для дообучения модели. Авторы вводят метод аугментации ошибок на основе типов ошибок для сбора разнообразных и репрезентативных ошибок. Эксперименты показывают, что LEMMA значительно улучшает производительность по сравнению с другими сильными базовыми моделями."
                },
                "en": {
                    "title": "Empowering LLMs: Learning from Errors to Enhance Reasoning",
                    "desc": "This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods."
                },
                "zh": {
                    "title": "从错误中学习，提升数学推理能力",
                    "desc": "大型语言模型（LLMs）在解决数学问题时展现了出色的推理能力。现有的方法主要关注提高正确训练数据的质量，而忽视了错误数据的价值，这可能会妨碍模型的反思能力。我们提出了一种通过学习错误来提升数学推理能力的方法，称为LEMMA。该方法通过构建包含错误步骤的错误解和与正确解的反思连接的数据集，来进行模型的微调，从而使模型能够在生成过程中自主纠正错误。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18769",
            "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
            "url": "https://huggingface.co/papers/2503.18769",
            "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "e92ee9df78b66019",
            "authors": [
                "Alan Dao",
                "Dinh Bach Vu",
                "Bui Quang Huy"
            ],
            "affiliations": [
                "Menlo Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18769.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#reasoning",
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AlphaSpace: Прорыв в пространственном мышлении ИИ",
                    "desc": "AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D декартовому пространству. Она использует токенизацию на основе семантики, кодируя информацию о высоте через специальные семантические токены, и интегрирует преимущественно символические синтетические данные для рассуждений. Этот подход позволяет LLM точно манипулировать объектами, позиционируя их по конкретным координатам [x, y, z]. Экспериментальные результаты показывают, что AlphaSpace значительно превосходит существующие модели в подзадачах манипулирования, достигая общей точности 66,67%."
                },
                "en": {
                    "title": "Enhancing 3D Navigation in Language Models with AlphaSpace",
                    "desc": "This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet."
                },
                "zh": {
                    "title": "AlphaSpace：提升语言模型的空间推理能力",
                    "desc": "本文介绍了一种新方法AlphaSpace，旨在提升大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记编码高度信息，并主要整合符号合成推理数据。该方法使得LLMs能够准确地通过特定的[x, y, z]坐标来操作物体。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14428",
            "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
            "url": "https://huggingface.co/papers/2503.14428",
            "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
            "score": 1,
            "issue_id": 2875,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "1cd532518024f266",
            "authors": [
                "Hongyu Zhang",
                "Yufan Deng",
                "Shenghai Yuan",
                "Peng Jin",
                "Zesen Cheng",
                "Yian Zhao",
                "Chang Liu",
                "Jie Chen"
            ],
            "affiliations": [
                "Peng Cheng Laboratory, Shenzhen, China",
                "School of Electronic and Computer Engineering, Peking University, Shenzhen, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14428.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения",
                    "desc": "MagicComp - это метод генерации видео по тексту, не требующий дополнительного обучения. Он использует двухфазовое уточнение для улучшения композиционной генерации: семантическое разрешение неоднозначности на этапе подготовки условий и динамическое слияние макетов на этапе шумоподавления. Метод решает проблемы связывания атрибутов, определения пространственных отношений и захвата сложных взаимодействий между несколькими объектами. MagicComp может быть интегрирован в существующие архитектуры генерации видео по тексту и превосходит современные методы в экспериментах."
                },
                "en": {
                    "title": "Enhancing Text-to-Video Generation with MagicComp",
                    "desc": "This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks."
                },
                "zh": {
                    "title": "MagicComp：提升文本到视频生成的创新方法",
                    "desc": "本文提出了一种名为MagicComp的文本到视频生成方法，旨在解决现有方法在属性绑定、空间关系确定和复杂动作交互方面的不足。该方法通过双阶段的精炼过程来增强组合式T2V生成，首先在条件阶段引入语义锚点消歧，以强化特定主题的语义并解决主题间的歧义。其次，在去噪阶段，提出动态布局融合注意力，通过掩蔽注意力调制灵活绑定主题与其时空区域。MagicComp是一种与模型无关的通用方法，可以无缝集成到现有的T2V架构中，并在多个基准测试中表现优异。"
                }
            }
        }
    ],
    "link_prev": "2025-03-24.html",
    "link_next": "2025-03-26.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "24.03",
        "en": "03/24",
        "zh": "3月24日"
    },
    "short_date_next": {
        "ru": "26.03",
        "en": "03/26",
        "zh": "3月26日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。MSPs需要整合多种模态，如文本和图表。目前，MSPs面临两大问题：多模态推理和缺乏反思能力。作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。",
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "pinyin": "这篇文章讨论了多模态科学问题（MSPs）及其挑战。\nZhè piān wénzhāng tǎolùn le duō móshuài kēxué wèntí (MSPs) jí qí tiǎozhàn.\n\nMSPs需要整合多种模态，如文本和图表。\nMSPs xūyào zhěnghé duō zhǒng móshuài, rú wénběn hé túbiǎo.\n\n目前，MSPs面临两大问题：多模态推理和缺乏反思能力。\nMùqián, MSPs miànlín liǎng dà wèntí: duō móshuài tuīlǐ hé quēfá fǎnsī nénglì.\n\n作者提出了基于大七人格和苏格拉底指导的多代理框架（MAPS）来解决这些问题。\nZuòzhě tíchū le jīyú dà qī réngé hé Sūgélādǐ zhǐdǎo de duō dàilǐ kuàngjià (MAPS) lái jiějué zhèxiē wèntí.\n\n实验结果显示，该框架在多个数据集上表现出色，超越了现有的最佳模型。\nShíyàn jiéguǒ xiǎnshì, gǎi kuàngjià zài duō gè shùjùjí shàng biǎoxiàn chūsè, chāoyuè le xiànyǒu de zuìjiā móxíng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"科学问题\", \"pinyin\": \"kē xué wèn tí\", \"trans\": \"scientific problems\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenges\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěng hé\", \"trans\": \"integrate\"},\n    {\"word\": \"模态\", \"pinyin\": \"mó tài\", \"trans\": \"modality\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图表\", \"pinyin\": \"tú biǎo\", \"trans\": \"charts\"},\n    {\"word\": \"面临\", \"pinyin\": \"miàn lín\", \"trans\": \"face\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"反思\", \"pinyin\": \"fǎn sī\", \"trans\": \"reflection\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"大七人格\", \"pinyin\": \"dà qī rén gé\", \"trans\": \"Big Five personality traits\"},\n    {\"word\": \"苏格拉底\", \"pinyin\": \"sū gé lā dǐ\", \"trans\": \"Socrates\"},\n    {\"word\": \"指导\", \"pinyin\": \"zhǐ dǎo\", \"trans\": \"guidance\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"结果\", \"pinyin\": \"jié guǒ\", \"trans\": \"results\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"最佳\", \"pinyin\": \"zuì jiā\", \"trans\": \"best\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses multimodal scientific problems (MSPs) and their challenges. MSPs require the integration of multiple modalities, such as text and graphs. Currently, MSPs face two major issues: multimodal reasoning and a lack of reflective capabilities. The authors propose a multi-agent framework (MAPS) based on the Big Five personality traits and Socratic guidance to address these issues. Experimental results show that this framework performs excellently on multiple datasets, surpassing existing state-of-the-art models.",
        "update_ts": "2025-03-24 09:12"
    }
}