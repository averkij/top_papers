{
    "date": {
        "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 28",
        "zh": "1æœˆ28æ—¥"
    },
    "time_utc": "2025-01-28 20:10",
    "weekday": 1,
    "issue_id": 1912,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.15368",
            "title": "Baichuan-Omni-1.5 Technical Report",
            "url": "https://huggingface.co/papers/2501.15368",
            "abstract": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
            "score": 34,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "f40b7f7b108c1500",
            "authors": [
                "Yadong Li",
                "Jun Liu",
                "Tao Zhang",
                "Tao Zhang",
                "Song Chen",
                "Tianpeng Li",
                "Zehuan Li",
                "Lijun Liu",
                "Lingfeng Ming",
                "Guosheng Dong",
                "Da Pan",
                "Chong Li",
                "Yuanbo Fang",
                "Dongdong Kuang",
                "Mingrui Wang",
                "Chenglin Zhu",
                "Youwei Zhang",
                "Hongyu Guo",
                "Fengyu Zhang",
                "Yuran Wang",
                "Bowen Ding",
                "Wei Song",
                "Xu Li",
                "Yuqi Huo",
                "Zheng Liang",
                "Shusen Zhang",
                "Xin Wu",
                "Shuai Zhao",
                "Linchu Xiong",
                "Yozhen Wu",
                "Jiahui Ye",
                "Wenhao Lu",
                "Bowen Li",
                "Yan Zhang",
                "Yaqi Zhou",
                "Xin Chen",
                "Lei Su",
                "Hongda Zhang",
                "Fuzhong Chen",
                "Xuezhen Dong",
                "Na Nie",
                "Zhiying Wu",
                "Bin Xiao",
                "Ting Li",
                "Shunya Dang",
                "Ping Zhang",
                "Yijia Sun",
                "Jincheng Wu",
                "Jinjie Yang",
                "Xionghai Lin",
                "Zhi Ma",
                "Kegeng Wu",
                "Jia li",
                "Aiyuan Yang",
                "Hui Liu",
                "Jianqiang Zhang",
                "Xiaoxi Chen",
                "Guangwei Ai",
                "Wentao Zhang",
                "Yicong Chen",
                "Xiaoqin Huang",
                "Kun Li",
                "Wenjing Luo",
                "Yifei Duan",
                "Lingling Zhu",
                "Ran Xiao",
                "Zhe Su",
                "Jiani Pu",
                "Dian Wang",
                "Xu Jia",
                "Tianyu Zhang",
                "Mengyu Ai",
                "Mang Wang",
                "Yujing Qiao",
                "Lei Zhang",
                "Yanjun Shen",
                "Fan Yang",
                "Miao Zhen",
                "Yijie Zhou",
                "Mingyang Chen",
                "Fei Li",
                "Chenzheng Zhu",
                "Keer Lu",
                "Yaqi Zhao",
                "Hao Liang",
                "Youquan Li",
                "Yanzhao Qin",
                "Linzhuang Sun",
                "Jianhua Xu",
                "Haoze Sun",
                "Mingan Lin",
                "Zenan Zhou",
                "Weipeng Chen"
            ],
            "affiliations": [
                "Baichuan Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15368.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Baichuan-Omni-1.5: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Baichuan-Omni-1.5 - ÑÑ‚Ğ¾ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‰Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Interaction with Baichuan-Omni-1.5",
                    "desc": "Baichuan-Omni-1.5 is a cutting-edge omni-modal model designed for seamless interaction across text, audio, and visual data. It utilizes a robust data cleaning and synthesis pipeline to process approximately 500 billion high-quality multimodal data points. The model features a specialized audio-tokenizer that captures both semantic and acoustic elements, enhancing its compatibility with multi-layered language models (MLLM). Through a multi-stage training approach, it effectively aligns and fine-tunes across modalities, outperforming existing models in various multimodal tasks, particularly in medical benchmarks."
                },
                "zh": {
                    "title": "å…¨æ¨¡æ€äº¤äº’çš„æ–°çºªå…ƒ",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Baichuan-Omni-1.5ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ¨¡æ€æ¨¡å‹ï¼Œå…·å¤‡å…¨æ¨¡æ€ç†è§£å’Œç«¯åˆ°ç«¯éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†å®ç°ä¸åŒæ¨¡æ€ä¹‹é—´æµç•…ä¸”é«˜è´¨é‡çš„äº¤äº’ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸‰ä¸ªå…³é”®æ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ¸…æ´—å’Œåˆæˆç®¡é“ï¼Œè·å¾—äº†çº¦5000äº¿æ¡é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªéŸ³é¢‘æ ‡è®°å™¨ï¼ˆBaichuan-Audio-Tokenizerï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰éŸ³é¢‘çš„è¯­ä¹‰å’Œå£°å­¦ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…¼å®¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15383",
            "title": "Qwen2.5-1M Technical Report",
            "url": "https://huggingface.co/papers/2501.15383",
            "abstract": "We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data synthesis, progressive pre-training, and multi-stage supervised fine-tuning are employed to effectively enhance long-context performance while reducing training costs.   To promote the use of long-context models among a broader user base, we present and open-source our inference framework. This framework includes a length extrapolation method that can expand the model context lengths by at least four times, or even more, without additional training. To reduce inference costs, we implement a sparse attention method along with chunked prefill optimization for deployment scenarios and a sparsity refinement method to improve precision. Additionally, we detail our optimizations in the inference engine, including kernel optimization, pipeline parallelism, and scheduling optimization, which significantly enhance overall inference performance. By leveraging our inference framework, the Qwen2.5-1M models achieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million tokens of context. This framework provides an efficient and powerful solution for developing applications that require long-context processing using open-source models.   The Qwen2.5-1M series currently includes the open-source models Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed model Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly improved in long-context tasks without compromising performance in short-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model significantly outperforms GPT-4o-mini in long-context tasks and supports contexts eight times longer.",
            "score": 22,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "203817e55fc3eb45",
            "authors": [
                "An Yang",
                "Bowen Yu",
                "Chengyuan Li",
                "Dayiheng Liu",
                "Fei Huang",
                "Haoyan Huang",
                "Jiandong Jiang",
                "Jianhong Tu",
                "Jianwei Zhang",
                "Jingren Zhou",
                "Junyang Lin",
                "Kai Dang",
                "Kexin Yang",
                "Le Yu",
                "Mei Li",
                "Minmin Sun",
                "Qin Zhu",
                "Rui Men",
                "Tao He",
                "Weijia Xu",
                "Wenbiao Yin",
                "Wenyuan Yu",
                "Xiafei Qiu",
                "Xingzhang Ren",
                "Xinlong Yang",
                "Yong Li",
                "Zhiying Xu",
                "Zipeng Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15383.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#long_context",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2.5-1M Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ² 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-1M Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking the Power of 1 Million Tokens with Qwen2.5-1M",
                    "desc": "The Qwen2.5-1M models introduce a significant advancement in handling long-context inputs, extending the context length to 1 million tokens. This is achieved through innovative techniques like long data synthesis and multi-stage supervised fine-tuning, which enhance performance while minimizing training costs. The open-source inference framework allows users to expand context lengths without additional training and includes optimizations for efficient deployment. Overall, these models demonstrate superior performance in long-context tasks compared to existing models, making them a valuable resource for applications requiring extensive context processing."
                },
                "zh": {
                    "title": "Qwen2.5-1Mï¼šé•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Qwen2.5-1Mç³»åˆ—æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†é•¿è¾¾100ä¸‡æ ‡è®°çš„ä¸Šä¸‹æ–‡ã€‚ä¸ä¹‹å‰çš„128Kç‰ˆæœ¬ç›¸æ¯”ï¼ŒQwen2.5-1Måœ¨é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œé‡‡ç”¨äº†é•¿æ•°æ®åˆæˆã€æ¸è¿›å¼é¢„è®­ç»ƒå’Œå¤šé˜¶æ®µç›‘ç£å¾®è°ƒç­‰å…³é”®æŠ€æœ¯ã€‚ä¸ºäº†é™ä½æ¨ç†æˆæœ¬ï¼Œæˆ‘ä»¬å®ç°äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ†å—é¢„å¡«å……ä¼˜åŒ–ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨ç†å¼•æ“çš„æ€§èƒ½ã€‚Qwen2.5-1Mæ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨çŸ­ä¸Šä¸‹æ–‡åœºæ™¯ä¸­æ€§èƒ½æ²¡æœ‰ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16142",
            "title": "Towards General-Purpose Model-Free Reinforcement Learning",
            "url": "https://huggingface.co/papers/2501.16142",
            "abstract": "Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.",
            "score": 13,
            "issue_id": 1898,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "0cf7cd0c9c1f5964",
            "authors": [
                "Scott Fujimoto",
                "Pierluca D'Oro",
                "Amy Zhang",
                "Yuandong Tian",
                "Michael Rabbat"
            ],
            "affiliations": [
                "Meta FAIR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16142.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MR.Q: ĞĞ° Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MR.Q. Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ±ĞµĞ·Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. MR.Q Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ·Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Towards Universal Problem-Solving with MR.Q in Reinforcement Learning",
                    "desc": "This paper presents a new model-free deep reinforcement learning algorithm called MR.Q, which aims to solve a wide range of problems without needing extensive tuning of hyperparameters. The authors utilize model-based representations to simplify the value function, allowing the algorithm to benefit from the advantages of model-based RL while avoiding the complexities of planning. MR.Q is evaluated across various standard RL benchmarks using a single set of hyperparameters, demonstrating competitive performance against both specialized and general algorithms. This work represents a significant advancement towards creating versatile and efficient model-free deep RL solutions."
                },
                "zh": {
                    "title": "æ„å»ºé€šç”¨çš„æ— æ¨¡å‹æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ç§é€šç”¨é—®é¢˜è§£å†³æ¡†æ¶ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒRLç®—æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šåŸºå‡†è¿›è¡Œè°ƒæ•´ï¼Œä¾èµ–äºç²¾å¿ƒè°ƒèŠ‚çš„è¶…å‚æ•°å’Œç®—æ³•é€‰æ‹©ã€‚æœ€è¿‘ï¼Œå¼ºå¤§çš„åŸºäºæ¨¡å‹çš„RLæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¤æ‚æ€§å’Œè¾ƒæ…¢çš„è¿è¡Œæ—¶é—´é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ— æ¨¡å‹æ·±åº¦RLç®—æ³•MR.Qï¼Œæ—¨åœ¨è§£å†³å¤šæ ·åŒ–çš„é¢†åŸŸå’Œé—®é¢˜è®¾ç½®ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºæ¨¡å‹çš„è¡¨ç¤ºæ–¹æ³•ï¼Œè¿‘ä¼¼çº¿æ€§åŒ–ä»·å€¼å‡½æ•°ï¼Œä»è€Œåœ¨é¿å…è§„åˆ’æˆ–æ¨¡æ‹Ÿè½¨è¿¹ç›¸å…³æˆæœ¬çš„åŒæ—¶ï¼Œåˆ©ç”¨åŸºäºæ¨¡å‹çš„RLæ‰€ä½¿ç”¨çš„æ›´å¯†é›†çš„ä»»åŠ¡ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15907",
            "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
            "url": "https://huggingface.co/papers/2501.15907",
            "abstract": "Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles. To bridge this gap, we introduce Emilia-Pipe, an open-source preprocessing pipeline to extract high-quality training data from valuable yet underexplored in-the-wild data that capture spontaneous human speech in real-world contexts. By leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech generation dataset derived from in-the-wild speech data. This dataset comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it the largest open-source speech generation dataset available. Extensive experiments demonstrate that Emilia significantly outperforms traditional audiobook datasets in generating spontaneous and human-like speech, showcasing superior performance in capturing diverse speaker timbre and speaking styles of real-world human speech. Furthermore, this work underscores the importance of scaling dataset size to advance speech generation research and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation.",
            "score": 10,
            "issue_id": 1903,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "bd221795c86585eb",
            "authors": [
                "Haorui He",
                "Zengqiang Shang",
                "Chaoren Wang",
                "Xuyuan Li",
                "Yicheng Gu",
                "Hua Hua",
                "Liwei Liu",
                "Chen Yang",
                "Jiaqi Li",
                "Peiyang Shi",
                "Yuancheng Wang",
                "Kai Chen",
                "Pengyuan Zhang",
                "Zhizheng Wu"
            ],
            "affiliations": [
                "Chinese University of Hong Kong, Shenzhen, China",
                "Laboratory of Speech and Intelligent Information Processing, Institute of Acoustics, CAS, Beijing, China",
                "Shanghai AI Laboratory, Shanghai, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15907.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#audio",
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#low_resource"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Emilia: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Emilia-Pipe - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¿Ğ¾Ğ½Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ° ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Emilia, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 101 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 6 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Emilia-Large Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 216 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Emilia Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ½Ğ¸Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿Ğ¾Ğ½Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸."
                },
                "en": {
                    "title": "Unlocking Spontaneous Speech with Emilia-Pipe",
                    "desc": "This paper presents Emilia-Pipe, a preprocessing tool designed to extract high-quality training data from spontaneous human speech in real-world settings. The authors introduce Emilia, a multilingual speech generation dataset that includes over 101k hours of diverse speech data across six languages. They further expand this dataset to Emilia-Large, which contains more than 216k hours, making it the largest open-source resource for speech generation. The results show that models trained on Emilia outperform those trained on traditional audiobook datasets, effectively capturing the variability and naturalness of human speech."
                },
                "zh": {
                    "title": "æ‰“ç ´ä¼ ç»Ÿï¼Œæ•æ‰çœŸå®è¯­éŸ³çš„å¤šæ ·æ€§",
                    "desc": "è¿‘å¹´æ¥ï¼Œè¯­éŸ³ç”Ÿæˆçš„è¿›å±•ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ¨¡å‹åœ¨æ•æ‰çœŸå®äººç±»è¯­éŸ³çš„è‡ªå‘æ€§å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºä»…é™äºæ­£å¼æœ—è¯»é£æ ¼çš„æœ‰å£°ä¹¦æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Emilia-Pipeï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„é¢„å¤„ç†ç®¡é“ï¼Œç”¨äºä»æœ‰ä»·å€¼ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„çœŸå®ç¯å¢ƒæ•°æ®ä¸­æå–é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡åˆ©ç”¨Emilia-Pipeï¼Œæˆ‘ä»¬æ„å»ºäº†Emiliaï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºçœŸå®ç¯å¢ƒè¯­éŸ³æ•°æ®çš„å¤šè¯­è¨€è¯­éŸ³ç”Ÿæˆæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡101kå°æ—¶çš„è¯­éŸ³ï¼Œæ¶µç›–å…­ç§è¯­è¨€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15570",
            "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
            "url": "https://huggingface.co/papers/2501.15570",
            "abstract": "As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at https://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside}, https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
            "score": 10,
            "issue_id": 1900,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "063647dfe2bd7b63",
            "authors": [
                "Lin Yueyu",
                "Li Zhiyuan",
                "Peter Yue",
                "Liu Xiao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.15570.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#architecture",
                    "#small_models",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ RNN Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ RWKV-7, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Qwen 2.5. Ğ¦ĞµĞ»ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RNN Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ QRWK 32B Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ RWKV-6, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ 8 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° 16 GPU AMD MI300X. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing RNN Expressiveness with RWKV Attention",
                    "desc": "This paper presents a new series of models derived from Qwen 2.5, focusing on enhancing the expressiveness of RNNs through a native RWKV-7 attention mechanism. The authors demonstrate that their hybrid quadratic and subquadratic attention models outperform traditional Transformer and Linear RNN architectures by significantly reducing key-value (KV) complexity. They introduce the QRWK 32B model, which achieves impressive efficiency by processing knowledge in just 8 hours using 16 AMD MI300X GPUs while retaining the performance of Qwen 2.5. Additionally, the distillation process allows for knowledge transfer from larger language models (LLMs) to smaller ones, making it a versatile approach for building more powerful foundation models."
                },
                "zh": {
                    "title": "æå‡RNNè¡¨è¾¾èƒ½åŠ›çš„æ–°æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ··åˆäºŒæ¬¡å’ŒäºšäºŒæ¬¡æ³¨æ„åŠ›æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜RNNçš„è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºRWKV-7æ³¨æ„åŠ›æ¶æ„ï¼Œæå‡ºäº†ä¸€ç³»åˆ—ä»Qwen 2.5ä¸­æç‚¼çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†è¶…è¶ŠTransformerçš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨16ä¸ªAMD MI300X GPUï¼Œæˆ‘ä»¬çš„QRWK 32Bæ¨¡å‹å°†çŸ¥è¯†å¤„ç†æ—¶é—´ç¼©çŸ­è‡³ä»…8å°æ—¶ï¼ŒåŒæ—¶ä¿æŒäº†Qwen 2.5çš„æ€§èƒ½ã€‚è¯¥æç‚¼è¿‡ç¨‹å¯ä»¥åˆ©ç”¨ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°ä»æ›´å¤§æ¨¡å‹åˆ°æ›´å°æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15369",
            "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
            "url": "https://huggingface.co/papers/2501.15369",
            "abstract": "We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, i.e., ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios.",
            "score": 7,
            "issue_id": 1898,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "50e030854cdc071f",
            "authors": [
                "Chuanyang Zheng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.15369.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "iFormer: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "iFormer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. iFormer Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ ConvNeXt Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ iFormer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "iFormer: Optimizing Mobile Vision with Speed and Accuracy",
                    "desc": "The paper introduces iFormer, a new type of mobile hybrid vision network designed to enhance both speed and accuracy for mobile applications. It combines the quick local processing of convolutional networks with the effective global understanding of self-attention mechanisms. By modifying a standard convolutional architecture, ConvNeXt, iFormer creates a lightweight model that reduces memory usage while improving performance. Experimental results show that iFormer achieves high accuracy on ImageNet-1k and excels in various downstream tasks, all while maintaining low latency on mobile devices."
                },
                "zh": {
                    "title": "iFormerï¼šç§»åŠ¨åº”ç”¨ä¸­çš„é«˜æ•ˆè§†è§‰ç½‘ç»œ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç§»åŠ¨æ··åˆè§†è§‰ç½‘ç»œå®¶æ—ï¼Œç§°ä¸ºiFormerï¼Œæ—¨åœ¨ä¼˜åŒ–ç§»åŠ¨åº”ç”¨çš„å»¶è¿Ÿå’Œå‡†ç¡®æ€§ã€‚iFormeræœ‰æ•ˆåœ°ç»“åˆäº†å·ç§¯çš„å¿«é€Ÿå±€éƒ¨è¡¨ç¤ºèƒ½åŠ›å’Œè‡ªæ³¨æ„åŠ›çš„é«˜æ•ˆå…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡å°†æ ‡å‡†å·ç§¯ç½‘ç»œConvNeXtè½¬åŒ–ä¸ºæ›´è½»é‡çº§çš„ç§»åŠ¨ç½‘ç»œï¼ŒiFormerå®ç°äº†å±€éƒ¨äº¤äº’çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç§»åŠ¨è°ƒåˆ¶æ³¨æ„åŠ›æœºåˆ¶å»é™¤äº†å¤šå¤´è‡ªæ³¨æ„åŠ›ä¸­çš„å†…å­˜å¯†é›†å‹æ“ä½œï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„è°ƒåˆ¶æœºåˆ¶æ¥å¢å¼ºåŠ¨æ€å…¨å±€è¡¨ç¤ºèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16295",
            "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
            "url": "https://huggingface.co/papers/2501.16295",
            "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining. Our code can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba",
            "score": 4,
            "issue_id": 1898,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "011d06607305f0f8",
            "authors": [
                "Weixin Liang",
                "Junhong Shen",
                "Genghan Zhang",
                "Ning Dong",
                "Luke Zettlemoyer",
                "Lili Yu"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "FAIR at Meta",
                "Machine Learning Department, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16295.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Mixture-of-Mamba: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mixture-of-Mamba, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Mixture-of-Mamba Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° SSM, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ² Ñ‚Ñ€ĞµÑ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Transfusion, Chameleon Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµÑ…Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ñ€ĞµÑ‡ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mixture-of-Mamba Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Multi-Modal Learning with Efficient SSMs",
                    "desc": "This paper introduces Mixture-of-Mamba, a new State Space Model (SSM) that enhances multi-modal pretraining by incorporating modality-aware sparsity. By parameterizing the Mamba block specifically for different modalities, the model efficiently utilizes features from various data types like text, images, and speech. The results show that Mixture-of-Mamba achieves comparable performance to existing models while significantly reducing computational costs, using fewer floating point operations (FLOPs). This work demonstrates the effectiveness of modality-aware sparsity in improving SSMs, setting new benchmarks in the field of multi-modal learning."
                },
                "zh": {
                    "title": "æ¨¡æ€æ„ŸçŸ¥ç¨€ç–æ€§ï¼šæå‡SSMçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ•ˆç‡",
                    "desc": "çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºåºåˆ—å»ºæ¨¡çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œé¢ä¸´æ— æ³•åˆ©ç”¨ç‰¹å®šæ¨¡æ€ç‰¹å¾çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„SSMæ¶æ„â€”â€”Mixture-of-Mambaï¼Œé€šè¿‡å¯¹Mambaæ¨¡å—è¿›è¡Œæ¨¡æ€ç‰¹å®šå‚æ•°åŒ–ï¼Œå¼•å…¥äº†æ¨¡æ€æ„ŸçŸ¥ç¨€ç–æ€§ã€‚è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨è¾ƒæ—©çš„è®­ç»ƒæ­¥éª¤ä¸­è¾¾åˆ°ç›¸åŒçš„æŸå¤±å€¼ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡æ€æ„ŸçŸ¥ç¨€ç–æ€§æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„è®¾è®¡åŸåˆ™ï¼Œèƒ½å¤Ÿå°†å…¶å½±å“ä»å˜æ¢å™¨æ‰©å±•åˆ°SSMsï¼Œå¹¶åœ¨å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­è®¾å®šæ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2403.09193",
            "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
            "url": "https://huggingface.co/papers/2403.09193",
            "abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.",
            "score": 3,
            "issue_id": 1911,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "e5fc94d983fca41c",
            "authors": [
                "Paul Gavrikov",
                "Jovita Lukasik",
                "Steffen Jung",
                "Robert Geirhos",
                "Bianca Lamm",
                "Muhammad Jehanzeb Mirza",
                "Margret Keuper",
                "Janis Keuper"
            ],
            "affiliations": [
                "Google DeepMind",
                "ICG, Graz University of Technology",
                "IMLA, Offenburg University",
                "Max Planck Institute for Informatics, Saarland Informatics Campus",
                "University of Mannheim",
                "University of Siegen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2403.09193.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#ethics",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ¢ĞµĞºÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ·Ğ³Ğ»ÑĞ´: ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡ĞµĞ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñƒ Ğ² VLM. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, VLM Ğ²ÑĞµ ĞµÑ‰Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Steering Visual Biases with Language in Vision Language Models",
                    "desc": "This paper investigates how vision language models (VLMs) incorporate human visual biases, particularly the texture vs. shape bias, which refers to the preference for local versus global information in images. The authors find that VLMs tend to be more shape-biased than traditional vision models, suggesting that language prompts can influence visual processing. Through experiments, they demonstrate that the shape bias can be adjusted significantly by changing the text prompts used with the models. However, despite these adjustments, the VLMs still do not fully match the strong human bias towards shape recognition."
                },
                "zh": {
                    "title": "é€šè¿‡è¯­è¨€å¼•å¯¼è§†è§‰åå·®çš„å¯èƒ½æ€§",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—å˜åŒ–ï¼Œæ”¯æŒä»é›¶æ ·æœ¬å›¾åƒåˆ†ç±»åˆ°å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰å¤šç§åº”ç”¨ã€‚è¿™äº›æ¨¡å‹é€šè¿‡è¯­è¨€æç¤ºæä¾›äº†ä¸€ç§ç›´è§‚çš„æ–¹å¼æ¥è®¿é—®è§†è§‰å†…å®¹ã€‚æˆ‘ä»¬ç ”ç©¶äº†VLMsä¸­å­˜åœ¨çš„è§†è§‰åå·®ï¼Œç‰¹åˆ«æ˜¯çº¹ç†ä¸å½¢çŠ¶åå·®ï¼Œå‘ç°VLMsåœ¨å½¢çŠ¶åå·®ä¸Šå¾€å¾€æ¯”çº¯è§†è§‰æ¨¡å‹æ›´å¼ºã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡æ–‡æœ¬çš„å¤šæ¨¡æ€èåˆï¼Œè§†è§‰åå·®å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¢«è°ƒèŠ‚ï¼Œä¸”æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯­è¨€æ¥å¼•å¯¼è§†è§‰åå·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15427",
            "title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas",
            "url": "https://huggingface.co/papers/2501.15427",
            "abstract": "Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release our synthetic characters and instruction-tuning dialogues to support public research.",
            "score": 3,
            "issue_id": 1910,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "fa7a70d2c9f398b9",
            "authors": [
                "Xiaoyang Wang",
                "Hongming Zhang",
                "Tao Ge",
                "Wenhao Yu",
                "Dian Yu",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15427.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#agents",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ supervised fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA-3 8B. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ GPT-4 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs with Character Generalization for Role-Playing",
                    "desc": "This paper discusses how to improve large language models (LLMs) for role-playing tasks by enabling them to adopt different character personas. The authors create a large dataset of character profiles and use two methodsâ€”response rewriting and response generationâ€”to produce responses that match these characters. They then fine-tune the LLaMA-3 8B model with this synthetic data to enhance its ability to generate character-aligned dialogues. The results show that their improved model performs similarly to advanced models like GPT-4o in role-playing scenarios, and they provide their resources for further research."
                },
                "zh": {
                    "title": "å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•é€šè¿‡å¤§è§„æ¨¡æ•°æ®åˆæˆæ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨Persona Hubåˆæˆå¤§é‡è§’è‰²æ¡£æ¡ˆï¼Œç„¶åæ¢ç´¢äº†ä¸¤ç§ç­–ç•¥ï¼šå“åº”é‡å†™å’Œå“åº”ç”Ÿæˆï¼Œä»¥åˆ›å»ºä¸è§’è‰²å¯¹é½çš„æŒ‡ä»¤å“åº”ã€‚é€šè¿‡å¯¹LLaMA-3 8Bæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæˆ‘ä»¬éªŒè¯äº†åˆæˆæŒ‡ä»¤è°ƒä¼˜æ•°æ®åœ¨è§’è‰²æ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨è§’è‰²æ‰®æ¼”å¯¹è¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸GPT-4oæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†åˆæˆè§’è‰²å’ŒæŒ‡ä»¤è°ƒä¼˜å¯¹è¯ä»¥æ”¯æŒå…¬å…±ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12370",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "url": "https://huggingface.co/papers/2501.12370",
            "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.",
            "score": 3,
            "issue_id": 1905,
            "pub_date": "2025-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "bffcdc51c572d8f2",
            "authors": [
                "Samira Abnar",
                "Harshay Shah",
                "Dan Busbridge",
                "Alaaeldin Mohamed Elnouby Ali",
                "Josh Susskind",
                "Vimal Thilak"
            ],
            "affiliations": [
                "Apple",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12370.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ few-shot. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ insights Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of Sparsity in Language Models",
                    "desc": "This paper investigates how to improve language models by scaling their capacity, focusing on two main factors: the number of parameters and the compute required for each example. It specifically looks at sparse Mixture-of-Experts (MoEs), which allow for a larger number of parameters without a corresponding increase in computational load. The authors explore how different levels of sparsity, or the proportion of inactive parameters, affect the model's performance during training and evaluation. Their findings suggest that there is an optimal level of sparsity that enhances both efficiency and performance, providing valuable insights for developing more effective machine learning architectures."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ç¨€ç–æ€§ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹å®¹é‡çš„æ‰©å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–æ··åˆä¸“å®¶ï¼ˆMoEsï¼‰æ¡†æ¶ä¸‹ã€‚å®¹é‡ä¸»è¦ç”±æ¨¡å‹å‚æ•°æ•°é‡å’Œæ¯ä¸ªæ ·æœ¬çš„è®¡ç®—é‡å†³å®šã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸åŒçš„çº¦æŸæ¡ä»¶ä¸‹ï¼Œå­˜åœ¨ä¸€ä¸ªæœ€ä½³çš„ç¨€ç–æ°´å¹³ï¼Œå¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚æ­¤ç ”ç©¶ä¸ºç†è§£ç¨€ç–æ€§åœ¨MoEsæ‰©å±•æ³•åˆ™ä¸­çš„å½±å“æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„æ¶æ„æä¾›äº†è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.16273",
            "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
            "url": "https://huggingface.co/papers/2501.16273",
            "abstract": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.",
            "score": 2,
            "issue_id": 1910,
            "pub_date": "2025-01-27",
            "pub_date_card": {
                "ru": "27 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 27",
                "zh": "1æœˆ27æ—¥"
            },
            "hash": "bd97733bda9e3557",
            "authors": [
                "Mohamed Elfeki",
                "Rui Liu",
                "Chad Voegele"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.16273.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ğ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ğ½Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ° 47% Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ² 4,7 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ğ½Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ½Ğ´ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¾Ğ½Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of Encoder-Decoder Models in Small Language Tasks",
                    "desc": "This paper highlights the advantages of encoder-decoder architectures over large decoder-only language models, especially for small language models (SLMs) with 1 billion parameters or fewer. The authors demonstrate that encoder-decoder models can achieve significantly lower latency and higher throughput on edge devices due to their efficient processing of input and separation of understanding and generation phases. They introduce a new knowledge distillation framework that allows these models to benefit from the capabilities of larger decoder-only models while maintaining their efficiency. The study concludes that as parameter budgets decrease, the choice of architecture becomes critical for effective deployment in resource-constrained environments."
                },
                "zh": {
                    "title": "ç¼–ç -è§£ç æ¶æ„çš„ä¼˜åŠ¿ä¸åº”ç”¨",
                    "desc": "æœ¬è®ºæ–‡åˆ†æäº†ç¼–ç -è§£ç æ¶æ„åœ¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸­çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼–ç -è§£ç æ¨¡å‹åœ¨é¦–æ¬¡ä»¤ç‰Œå»¶è¿Ÿä¸Šæ¯”ä»…è§£ç æ¨¡å‹ä½47%ï¼Œå¹¶ä¸”ååé‡æé«˜äº†4.7å€ã€‚è¿™ç§ä¼˜åŠ¿æºäºç¼–ç -è§£ç æ¶æ„çš„ä¸€æ¬¡æ€§è¾“å…¥å¤„ç†å’Œç†è§£ä¸ç”Ÿæˆé˜¶æ®µçš„é«˜æ•ˆåˆ†ç¦»ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œä½¿ç¼–ç -è§£ç æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è§£ç æ•™å¸ˆçš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶æ¶æ„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14912",
            "title": "Feasible Learning",
            "url": "https://huggingface.co/papers/2501.14912",
            "abstract": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL demands satisfactory performance on every individual data point. Since any model that meets the prescribed performance threshold is a valid FL solution, the choice of optimization algorithm and its dynamics play a crucial role in shaping the properties of the resulting solutions. In particular, we study a primal-dual approach which dynamically re-weights the importance of each sample during training. To address the challenge of setting a meaningful threshold in practice, we introduce a relaxation of FL that incorporates slack variables of minimal norm. Our empirical analysis, spanning image classification, age regression, and preference optimization in large language models, demonstrates that models trained via FL can learn from data while displaying improved tail behavior compared to ERM, with only a marginal impact on average performance.",
            "score": 2,
            "issue_id": 1898,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 24",
                "zh": "1æœˆ24æ—¥"
            },
            "hash": "7ded44debecf7694",
            "authors": [
                "Juan Ramirez",
                "Ignacio Hounie",
                "Juan Elenter",
                "Jose Gallego-Posada",
                "Meraj Hashemizadeh",
                "Alejandro Ribeiro",
                "Simon Lacoste-Julien"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Mila & UniversitÃ© de MontrÃ©al",
                "Spotify",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14912.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ˜Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Feasible Learning (FL). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑĞºĞ° (ERM), FL ÑÑ‚Ñ€ĞµĞ¼Ğ¸Ñ‚ÑÑ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ FL, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ERM."
                },
                "en": {
                    "title": "Ensuring Individual Sample Success with Feasible Learning",
                    "desc": "Feasible Learning (FL) is a new approach in machine learning that focuses on ensuring each training sample meets a specific performance standard, rather than just optimizing for overall average performance like traditional methods. This paradigm treats the training process as a feasibility problem, where any model that satisfies the performance criteria for all samples is considered valid. The paper explores a primal-dual optimization technique that adjusts the importance of each sample during training, enhancing the model's ability to learn effectively. Through various applications, including image classification and language model optimization, FL shows improved performance on challenging cases while maintaining similar average results compared to conventional methods."
                },
                "zh": {
                    "title": "å¯è¡Œå­¦ä¹ ï¼šæ¯ä¸ªæ ·æœ¬éƒ½è¦ä¼˜ç§€ï¼",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºå¯è¡Œå­¦ä¹ ï¼ˆFeasible Learningï¼ŒFLï¼‰ï¼Œå®ƒé€šè¿‡è§£å†³ä¸€ä¸ªå¯è¡Œæ€§é—®é¢˜æ¥è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œé™åˆ¶æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æŸå¤±ã€‚ä¸ä¼ ç»Ÿçš„ç»éªŒé£é™©æœ€å°åŒ–ï¼ˆEmpirical Risk Minimizationï¼ŒERMï¼‰æ¡†æ¶ä¸åŒï¼ŒFLè¦æ±‚æ¯ä¸ªæ•°æ®ç‚¹éƒ½èƒ½è¾¾åˆ°æ»¡æ„çš„æ€§èƒ½ã€‚FLçš„æœ‰æ•ˆæ€§ä¾èµ–äºä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©åŠå…¶åŠ¨æ€è°ƒæ•´æ ·æœ¬é‡è¦æ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼Œä½¿ç”¨FLè®­ç»ƒçš„æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€å¹´é¾„å›å½’å’Œå¤§è¯­è¨€æ¨¡å‹çš„åå¥½ä¼˜åŒ–ä¸­ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¹³å‡æ€§èƒ½çš„åŒæ—¶ï¼Œæ”¹å–„æ¨¡å‹åœ¨æç«¯æƒ…å†µä¸‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.14723",
            "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
            "url": "https://huggingface.co/papers/2501.14723",
            "abstract": "Scaling test-time compute is a promising axis for improving LLM capabilities. However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research. Here, we explore this problem in the context of solving real-world GitHub issues from the SWE-bench dataset. Our system, named CodeMonkeys, allows models to iteratively edit a codebase by jointly generating and running a testing script alongside their draft edit. We sample many of these multi-turn trajectories for every issue to generate a collection of candidate edits. This approach lets us scale \"serial\" test-time compute by increasing the number of iterations per trajectory and \"parallel\" test-time compute by increasing the number of trajectories per problem. With parallel scaling, we can amortize up-front costs across multiple downstream samples, allowing us to identify relevant codebase context using the simple method of letting an LLM read every file. In order to select between candidate edits, we combine voting using model-generated tests with a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys resolves 57.4% of issues from SWE-bench Verified using a budget of approximately 2300 USD. Our selection method can also be used to combine candidates from different sources. Selecting over an ensemble of edits from existing top SWE-bench Verified submissions obtains a score of 66.2% and outperforms the best member of the ensemble on its own. We fully release our code and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
            "score": 1,
            "issue_id": 1912,
            "pub_date": "2025-01-24",
            "pub_date_card": {
                "ru": "24 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 24",
                "zh": "1æœˆ24æ—¥"
            },
            "hash": "0aee5401febd2bf6",
            "authors": [
                "Ryan Ehrlich",
                "Bradley Brown",
                "Jordan Juravsky",
                "Ronald Clark",
                "Christopher RÃ©",
                "Azalia Mirhoseini"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.14723.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#plp",
                    "#open_source"
                ],
                "emoji": "ğŸ’",
                "ru": {
                    "title": "CodeMonkeys: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ CodeMonkeys Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ GitHub Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²ĞºĞ°Ğ¼Ğ¸. CodeMonkeys Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ 57.4% Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SWE-bench Verified."
                },
                "en": {
                    "title": "Enhancing LLMs with Scalable Test-Time Compute for Code Editing",
                    "desc": "This paper presents CodeMonkeys, a system designed to enhance the capabilities of large language models (LLMs) by scaling test-time compute during code editing tasks. It combines iterative code generation with testing script execution, allowing models to refine their edits through multiple iterations and trajectories. By leveraging both serial and parallel scaling, CodeMonkeys efficiently identifies relevant code context and selects the best candidate edits through a voting mechanism. The system demonstrates effectiveness by resolving over 57% of real-world GitHub issues while optimizing resource usage, and it shows improved performance when combining edits from various sources."
                },
                "zh": {
                    "title": "é€šè¿‡CodeMonkeysæå‡ä»£ç ç¼–è¾‘èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºCodeMonkeysçš„ç³»ç»Ÿï¼Œå®ƒå¯ä»¥é€šè¿‡ç”Ÿæˆå’Œè¿è¡Œæµ‹è¯•è„šæœ¬æ¥è¿­ä»£ç¼–è¾‘ä»£ç åº“ï¼Œä»è€Œè§£å†³å®é™…çš„GitHubé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¢åŠ æ¯ä¸ªé—®é¢˜çš„è¿­ä»£æ¬¡æ•°å’Œè½¨è¿¹æ•°é‡ï¼Œå®ç°äº†ä¸²è¡Œå’Œå¹¶è¡Œçš„æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ã€‚æœ€ç»ˆï¼ŒCodeMonkeysæˆåŠŸè§£å†³äº†57.4%çš„é—®é¢˜ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„é€‰æ‹©æ–¹æ³•ä¹Ÿèƒ½æœ‰æ•ˆç»“åˆæ¥è‡ªä¸åŒæ¥æºçš„å€™é€‰ç¼–è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.15420",
            "title": "Visual Generation Without Guidance",
            "url": "https://huggingface.co/papers/2501.15420",
            "abstract": "Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT.",
            "score": 1,
            "issue_id": 1912,
            "pub_date": "2025-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "d7e67912a685cbf9",
            "authors": [
                "Huayu Chen",
                "Kai Jiang",
                "Kaiwen Zheng",
                "Jianfei Chen",
                "Hang Su",
                "Jun Zhu"
            ],
            "affiliations": [
                "Department of Computer Science & Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.15420.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "GFT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Guidance-Free Training (GFT). GFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Classifier-Free Guidance (CFG), Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ÑƒĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ GFT Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Guidance-Free Training: Simplifying Visual Generative Models",
                    "desc": "This paper introduces Guidance-Free Training (GFT), a new approach for visual generative models that eliminates the need for classifier-free guidance during sampling. GFT achieves similar performance to traditional Classifier-Free Guidance (CFG) while only requiring a single model for inference, thus reducing computational costs by half. The method allows for training from scratch, avoiding reliance on pre-trained CFG networks, and retains the same maximum likelihood objective as CFG with minimal changes to existing implementations. Extensive experiments show that GFT performs comparably or better than CFG across various visual modeling domains, maintaining a good balance between diversity and fidelity."
                },
                "zh": {
                    "title": "æ— å¼•å¯¼è®­ç»ƒï¼šé™ä½è®¡ç®—æˆæœ¬çš„è§†è§‰ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æ— å¼•å¯¼é‡‡æ ·çš„è§†è§‰æ¨¡å‹æ˜¯æœ¬ç ”ç©¶çš„æ ¸å¿ƒã€‚æˆ‘ä»¬æå‡ºçš„æ— å¼•å¯¼è®­ç»ƒï¼ˆGFTï¼‰ç®—æ³•åœ¨æ€§èƒ½ä¸Šä¸ä¼ ç»Ÿçš„åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç›¸å½“ï¼Œä½†åªéœ€ä½¿ç”¨å•ä¸€æ¨¡å‹è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚GFTå¯ä»¥ç›´æ¥ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè€Œä¸ä¾èµ–äºé¢„è®­ç»ƒçš„CFGç½‘ç»œï¼Œä¸”å®ç°ç®€å•ã€‚é€šè¿‡åœ¨äº”ç§ä¸åŒçš„è§†è§‰æ¨¡å‹ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†GFTçš„æœ‰æ•ˆæ€§å’Œå¤šæ ·æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-27.html",
    "link_next": "2025-01-29.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 4,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 11,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† Baichuan-Omni-1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å…¨æ¨¡æ€ç†è§£å’Œç«¯åˆ°ç«¯éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°æµç•…çš„é«˜è´¨é‡è·¨æ¨¡æ€äº¤äº’ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸‰ä¸ªå…³é”®æ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ¸…æ´—å’Œåˆæˆç®¡é“ï¼Œè·å¾—äº†å¤§çº¦ 500B çš„é«˜è´¨é‡æ•°æ®ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªéŸ³é¢‘åˆ†è¯å™¨ï¼Œæ•æ‰éŸ³é¢‘çš„è¯­ä¹‰å’Œå£°å­¦ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡æ€çš„æœ‰æ•ˆååŒã€‚Baichuan-Omni-1.5 åœ¨å…¨æ¨¡æ€èƒ½åŠ›æ–¹é¢é¢†å…ˆäºå½“å‰æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªå¤šæ¨¡æ€åŒ»ç–—åŸºå‡†ä¸Šå–å¾—äº†å¯æ¯”çš„ç»“æœã€‚",
        "title": "Baichuan-Omni-1.5 Technical Report",
        "pinyin": "WÇ’men jiÃ¨shÃ o le Baichuan-Omni-1.5, zhÃ¨ shÃ¬ yÄ«gÃ¨ jÃ¹yÇ’u quÃ¡n mÃ³shÃ¬ lÇjiÄ› hÃ© duÄn dÃ o duÄn yÄ«npiÃ n shÄ“ngchÃ©ng nÃ©nglÃ¬ de mÃ³xÃ­ng. WÃ¨ile shÃ­xiÃ n liÃºchÃ ng de gÄo zhÃ¬liÃ ng kuÃ  mÃ³shÃ¬ jiÄohÃ¹, wÇ’men yÅuhuÃ  le sÄn gÃ¨ guÇnjiÃ n fÄngcÃ¨. ShÇ’uxiÄn, wÇ’men jiÃ nlÃ¬ le yÄ«gÃ¨ quÃ¡nmiÃ n de shÃ¹jÃ¹ qÄ«ngxÄ« hÃ© hÃ©chÃ©ng guÇndÇo, huÃ²dÃ© le dÃ yuÄ“ 500B de gÄo zhÃ¬liÃ ng shÃ¹jÃ¹ (wÃ©nbÄ›n, yÄ«npiÃ n hÃ© shÃ¬juÃ©). QÃ­cÃ¬, wÇ’men shÃ¨jÃ¬ le yÄ«gÃ¨ yÄ«npiÃ n fÄ“ncÃ­qÃ¬, bÄ«ngzhuÅ yÄ«npiÃ n de yÇ”yÃ¬ hÃ© shÄ“ngxuÃ© xÃ¬nxÄ«. ZuÃ¬hÃ²u, wÇ’men shÃ¨jÃ¬ le yÄ«gÃ¨ duÅ jiÄ“duÃ n xÃ¹nliÃ n cÃ¨lÃ¼Ã¨, quÃ¨bÇo suÇ’yÇ’u mÃ³shÃ¬ de yÇ’uxiÃ o xiÃ©tÃ³ng. Baichuan-Omni-1.5 zÃ i quÃ¡n mÃ³shÃ¬ nÃ©nglÃ¬ fÄngmiÃ n lÇngxiÄn yÃº dÄngqiÃ¡n mÃ³xÃ­ng, bÃ¬ng zÃ i duÅ gÃ¨ duÅ mÃ³shÃ¬ yÄ«liÃ¡o jÄ«zhÇ”n shÃ ng qudÃ© le kÄ›bÇ de jiÃ©guÇ’.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'},\n{'word': 'Baichuan-Omni-1.5', 'pinyin': 'BÃ i chuÄn-ÅŒu mÃ­-1.5', 'trans': 'Baichuan-Omni-1.5'},\n{'word': 'å…·æœ‰', 'pinyin': 'jÃ¹ yÇ’u', 'trans': 'have'},\n{'word': 'å…¨æ¨¡æ€', 'pinyin': 'quÃ¡n mÃ³ shÃ¬', 'trans': 'full modality'},\n{'word': 'ç†è§£', 'pinyin': 'lÇ jiÄ›', 'trans': 'understanding'},\n{'word': 'ç«¯åˆ°ç«¯', 'pinyin': 'duÄn dÃ o duÄn', 'trans': 'end-to-end'},\n{'word': 'éŸ³é¢‘', 'pinyin': 'yÄ«n pÃ­n', 'trans': 'audio'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'æµç•…', 'pinyin': 'liÃº chÃ ng', 'trans': 'smooth'},\n{'word': 'é«˜è´¨é‡', 'pinyin': 'gÄo zhÃ¬ liÃ ng', 'trans': 'high quality'},\n{'word': 'è·¨æ¨¡æ€', 'pinyin': 'kuÃ  mÃ³ shÃ¬', 'trans': 'cross-modality'},\n{'word': 'äº¤äº’', 'pinyin': 'jiÄo hÃ¹', 'trans': 'interaction'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimize'},\n{'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'},\n{'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'},\n{'word': 'å»ºç«‹', 'pinyin': 'jiÃ n lÃ¬', 'trans': 'establish'},\n{'word': 'å…¨é¢', 'pinyin': 'quÃ¡n miÃ n', 'trans': 'comprehensive'},\n{'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'},\n{'word': 'æ¸…æ´—', 'pinyin': 'qÄ«ng xÇ', 'trans': 'clean'},\n{'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthesize'},\n{'word': 'ç®¡é“', 'pinyin': 'guÇn dÃ o', 'trans': 'pipeline'},\n{'word': 'è·å¾—', 'pinyin': 'huÃ² dÃ©', 'trans': 'obtain'},\n{'word': 'å¤§çº¦', 'pinyin': 'dÃ  yuÄ“', 'trans': 'about'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨ jÃ¬', 'trans': 'design'},\n{'word': 'åˆ†è¯å™¨', 'pinyin': 'fÄ“n cÃ­ qÃ¬', 'trans': 'tokenizer'},\n{'word': 'æ•æ‰', 'pinyin': 'bÇ” zhuÅ', 'trans': 'capture'},\n{'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantics'},\n{'word': 'å£°å­¦', 'pinyin': 'shÄ“ng xuÃ©', 'trans': 'acoustics'},\n{'word': 'ä¿¡æ¯', 'pinyin': 'xÃ¬n xÄ«', 'trans': 'information'},\n{'word': 'å¤šé˜¶æ®µ', 'pinyin': 'duÅ jiÄ“ duÃ n', 'trans': 'multi-stage'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'training'},\n{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},\n{'word': 'ç¡®ä¿', 'pinyin': 'quÃ¨ bÇo', 'trans': 'ensure'},\n{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'},\n{'word': 'ååŒ', 'pinyin': 'xiÃ© tÃ³ng', 'trans': 'coordination'},\n{'word': 'é¢†å…ˆ', 'pinyin': 'lÇng xiÄn', 'trans': 'lead'},\n{'word': 'å½“å‰', 'pinyin': 'dÄng qiÃ¡n', 'trans': 'current'},\n{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},\n{'word': 'å–å¾—', 'pinyin': 'qÇ” dÃ©', 'trans': 'achieve'},\n{'word': 'å¯æ¯”', 'pinyin': 'kÄ› bÇ', 'trans': 'comparable'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}]",
        "trans": "We introduce Baichuan-Omni-1.5, a model with full-modal understanding and end-to-end audio generation capabilities. To achieve smooth, high-quality cross-modal interaction, we optimized three key aspects. First, we established a comprehensive data cleaning and synthesis pipeline, obtaining approximately 500B high-quality data (text, audio, and visual). Second, we designed an audio tokenizer to capture the semantic and acoustic information of audio. Lastly, we designed a multi-stage training strategy to ensure effective collaboration across all modalities. Baichuan-Omni-1.5 leads in full-modal capabilities and achieves comparable results on multiple multimodal medical benchmarks.",
        "update_ts": "2025-01-28 09:10"
    }
}