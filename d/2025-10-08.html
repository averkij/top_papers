
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. October 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 октября</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-07.html">⬅️ <span id="prev-date">07.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-09.html">➡️ <span id="next-date">09.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'};
        let feedDateNext = {'ru': '09.10', 'en': '10/09', 'zh': '10月9日'};
        let feedDatePrev = {'ru': '07.10', 'en': '10/07', 'zh': '10月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.06217', 'title': 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.06217', 'abstract': 'TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.', 'score': 32, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'bdefddb943fa9266', 'authors': ['Jiaru Zou', 'Soumya Roy', 'Vinay Kumar Verma', 'Ziyi Wang', 'David Wipf', 'Pan Lu', 'Sumit Negi', 'James Zou', 'Jingrui He'], 'affiliations': ['Amazon', 'Purdue University', 'Stanford University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.06217.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': '📊', 'ru': {'title': 'TaTToo: Process Reward Model с инструментами для работы с таблицами', 'desc': 'Исследователи представили TaTToo — новую Process Reward Model для улучшения рассуждений над табличными данными в LLM. Существующие PRM плохо справляются с табличными операциями вроде извлечения подтаблиц и работы со схемой данных. TaTToo решает эту проблему через явное моделирование табличных операций и интеграцию инструментов для верификации шагов рассуждений. Модель с 8 миллиардами параметров превосходит базовые PRM с 72B параметрами и улучшает точность на 30.9% в задачах численного анализа, fact-checking и работы с данными.'}, 'en': {'title': 'Revolutionizing Tabular Reasoning with TaTToo!', 'desc': 'TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters.'}, 'zh': {'title': 'TaTToo：提升表格推理的新方法', 'desc': 'TaTToo是一种新颖的基于表格的过程奖励模型，旨在提升表格推理能力。它通过明确处理表格特定操作和整合工具验证，显著改善了现有过程奖励模型的性能。研究表明，现有的过程奖励模型在处理表格推理时存在瓶颈，而TaTToo通过设计可扩展的数据整理管道和双阶段训练方法，克服了这些限制。经过评估，TaTToo在多个表格推理基准测试中表现优异，提升了下游大规模推理模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.26328', 'title': 'Fast-dLLM v2: Efficient Block-Diffusion LLM', 'url': 'https://huggingface.co/papers/2509.26328', 'abstract': "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.", 'score': 19, 'issue_id': 6298, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'd003eca6c18f4d37', 'authors': ['Chengyue Wu', 'Hao Zhang', 'Shuchen Xue', 'Shizhe Diao', 'Yonggan Fu', 'Zhijian Liu', 'Pavlo Molchanov', 'Ping Luo', 'Song Han', 'Enze Xie'], 'affiliations': ['MIT', 'NVIDIA', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26328.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#diffusion', '#optimization', '#architecture'], 'emoji': '⚡', 'ru': {'title': 'Быстрая параллельная генерация текста через блочную диффузию', 'desc': 'Статья представляет Fast-dLLM v2 — блочную диффузионную языковую модель, которая эффективно преобразует предобученные авторегрессионные LLM для параллельной генерации текста. Ключевое преимущество подхода в том, что требуется всего 1 миллиард токенов для дообучения (в 500 раз меньше, чем у полноценных диффузионных моделей). Модель использует блочный механизм диффузии с иерархической системой кэширования, что позволяет сохранять контекст и генерировать текст параллельно внутри блоков. В результате достигается ускорение в 2.5 раза по сравнению со стандартной авторегрессионной генерацией без потери качества.'}, 'en': {'title': 'Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation', 'desc': "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."}, 'zh': {'title': '快速高效的块扩散语言模型', 'desc': 'Fast-dLLM v2是一种块扩散语言模型，能够高效地将预训练的自回归模型转换为并行文本生成模型。该模型仅需约10亿个标记进行微调，相比于全注意力扩散模型，训练数据减少了500倍，同时保持了原始模型的性能。通过引入块扩散机制和互补注意力掩码，Fast-dLLM v2实现了块级双向上下文建模，并设计了分层缓存机制以加速解码。实验结果表明，Fast-dLLM v2在准确性上与自回归基线相当或更优，同时在效率上达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2510.03270', 'title': 'CoDA: Coding LM via Diffusion Adaptation', 'url': 'https://huggingface.co/papers/2510.03270', 'abstract': 'CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.', 'score': 16, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '49ba9b2131a56dc8', 'authors': ['Haolin Chen', 'Shiyu Wang', 'Can Qin', 'Bo Pang', 'Zuxin Liu', 'Jielin Qiu', 'Jianguo Zhang', 'Yingbo Zhou', 'Zeyuan Chen', 'Ran Xu', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang', 'Weiran Yao'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03270.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#diffusion', '#small_models', '#dataset'], 'emoji': '🌊', 'ru': {'title': 'Легковесный диффузионный кодер с направленной генерацией', 'desc': 'CoDA — это языковая модель на основе диффузии с 1.7 миллиардами параметров, специально обученная для генерации кода. В отличие от авторегрессивных моделей, диффузионные модели используют двунаправленный контекст и лучше справляются с заполнением пропусков в коде. CoDA обучалась на TPU с использованием открытого пайплайна и применяет confidence-guided sampling для ускорения инференса. На бенчмарках HumanEval и MBPP модель показывает результаты, сопоставимые с диффузионными моделями размером до 7B параметров, при этом оставаясь компактной.'}, 'en': {'title': 'CoDA: Lightweight Diffusion Coding with Competitive Performance', 'desc': 'CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants.'}, 'zh': {'title': 'CoDA：轻量级扩散编码的未来', 'desc': 'CoDA是一种具有17亿参数的扩散编码器，通过信心引导采样实现了与更小模型的竞争性能。它结合了大规模的扩散预训练和以代码为中心的中期训练，以及指令调优，从而保持了推理延迟的竞争力。CoDA在Humaneval、MBPP和EvalPlus等基准测试中，表现与高达70亿参数的扩散模型相当或更好。我们发布了模型检查点、评估工具和TPU训练管道，以加速轻量级扩散编码助手的研究。'}}}, {'id': 'https://huggingface.co/papers/2509.24107', 'title': 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and\n  Synthesis for SLMs', 'url': 'https://huggingface.co/papers/2509.24107', 'abstract': 'Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.', 'score': 11, 'issue_id': 6301, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '17db556aca22a151', 'authors': ['Shreyas Singh', 'Kunal Singh', 'Pradeep Moturi'], 'affiliations': ['Fractal AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.24107.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agents', '#rl', '#dataset', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Глубокий веб-поиск с помощью специализированных агентов', 'desc': 'Представлена система Fathom-DeepResearch, состоящая из двух специализированных моделей по 4 миллиарда параметров для сложных информационных задач. Первая модель оптимизирована для веб-поиска и запросов к страницам, обучена с помощью мультиагентного подхода и может делать более 20 последовательных вызовов инструментов. Вторая модель преобразует результаты поиска в структурированные отчёты с цитатами. Система достигает state-of-the-art результатов среди open-weights моделей на бенчмарках для поиска информации и различных задачах рассуждений.'}, 'en': {'title': 'Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch', 'desc': 'Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.'}, 'zh': {'title': '智能搜索与报告合成的未来', 'desc': 'Fathom-DeepResearch 是一个智能系统，专门用于网络搜索和报告合成，能够在开放式信息检索任务和多样化推理任务中表现出色。该系统由两个专门模型组成：Fathom-Search-4B 和 Fathom-Synthesizer-4B。Fathom-Search-4B 通过实时网络搜索和针对网页查询进行证据基础调查，采用了多项先进技术来优化其性能。Fathom-Synthesizer-4B 则将多轮 DeepSearch 追踪转换为结构化的、引用密集的 DeepResearch 报告，确保信息的全面合成。'}}}, {'id': 'https://huggingface.co/papers/2510.06062', 'title': 'ASPO: Asymmetric Importance Sampling Policy Optimization', 'url': 'https://huggingface.co/papers/2510.06062', 'abstract': 'ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.', 'score': 7, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '3090778074ce994d', 'authors': ['Jiakang Wang', 'Runze Liu', 'Lei Lin', 'Wenping Hu', 'Xiu Li', 'Fuzheng Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06062.jpg', 'data': {'categories': ['#training', '#rl', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Асимметричная важность токенов для стабильного обучения LLM', 'desc': 'Исследователи обнаружили фундаментальную проблему в методах обучения с подкреплением для больших языковых моделей: несбалансированное взвешивание токенов с положительными и отрицательными преимуществами. Традиционные подходы подавляют обновление низковероятностных токенов и чрезмерно усиливают высоковероятностные. Предложенный метод ASPO решает эту проблему путем «переворачивания» коэффициентов Importance Sampling для токенов с положительным преимуществом и использования механизма мягкого двойного клиппинга. Эксперименты показали значительное улучшение стабильности обучения и итоговой производительности на задачах программирования и математических рассуждений.'}, 'en': {'title': 'Balancing Token Weighting for Better LLM Training', 'desc': 'The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.'}, 'zh': {'title': '优化令牌加权，提升训练稳定性', 'desc': 'ASPO（不对称重要性采样策略优化）解决了在结果监督强化学习（OSRL）中令牌加权不平衡的问题。通过翻转正优势令牌的重要性采样比率，ASPO使得正负令牌的更新方向一致，从而提高了训练的稳定性和性能。此外，ASPO还引入了一种软双剪切机制，以稳定极端更新并保持梯度流动。实验结果表明，ASPO在编码和数学推理基准测试中显著改善了训练效果，减少了过早收敛现象。'}}}, {'id': 'https://huggingface.co/papers/2510.06036', 'title': 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?', 'url': 'https://huggingface.co/papers/2510.06036', 'abstract': "Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", 'score': 5, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '0d9aa29b1d58dbd6', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Linyi Yang', 'Wenxuan Huang', 'Wenjie Li', 'Xiting Wang', 'Jaehong Yoon', 'YunXing', 'XingYu', 'Jinjin Gu'], 'affiliations': ['East China Normal University', 'Hong Kong Polytechnic University', 'INSAIT', 'Nanyang Technological University', 'Renmin University', 'Southern University of Science and Technology', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06036.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#data', '#alignment', '#interpretability', '#training'], 'emoji': '🧗', 'ru': {'title': 'Обрыв отказа: почему безопасные модели внезапно становятся опасными', 'desc': 'Исследователи обнаружили феномен «обрыва отказа» в больших reasoning-моделях: модели правильно распознают вредоносные запросы в процессе рассуждения, но резко теряют намерение отказать прямо перед генерацией ответа. Используя методы механистической интерпретируемости, учёные выявили небольшой набор attention heads, которые подавляют безопасное поведение модели. На основе этих находок был разработан метод Cliff-as-a-Judge, который позволяет восстановить безопасность модели, используя всего 1.7% обучающих данных по сравнению с обычным подходом. Абляция всего 3% проблемных attention heads снижает успешность атак до уровня ниже 10%.'}, 'en': {'title': 'Understanding and Mitigating the Refusal Cliff in Reasoning Models', 'desc': "This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data."}, 'zh': {'title': '揭示推理模型的拒绝悬崖机制', 'desc': '本研究探讨了大型推理模型中的拒绝悬崖机制，发现这些模型在生成输出前拒绝意图会急剧下降。通过线性探测方法，我们追踪了拒绝意图在标记位置的变化，发现许多模型在思考过程中能够识别有害提示，但在输出前的最后几个标记处拒绝意图却显著降低。我们通过因果干预分析，识别出少量对拒绝行为产生负面影响的注意力头，去除这些头可以显著降低攻击成功率。基于这些发现，我们提出了一种新的数据选择方法，利用拒绝悬崖的特征来高效修复推理模型的安全对齐。'}}}, {'id': 'https://huggingface.co/papers/2510.05432', 'title': 'AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems', 'url': 'https://huggingface.co/papers/2510.05432', 'abstract': 'AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.', 'score': 5, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'effef15175939fca', 'authors': ['Shambhavi Mishra', 'Gaurav Sahu', 'Marco Pedersoli', 'Laurent Charlin', 'Jose Dolz', 'Christopher Pal'], 'affiliations': ['Canada CIFAR AI Chair', 'HEC Montreal', 'International Laboratory on Learning Systems (ILLS)', 'LIVIA, ETS Montreal', 'Mila Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.05432.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#rlhf', '#agents'], 'emoji': '🧪', 'ru': {'title': 'Может ли AI стать самостоятельным исследователем в машинном обучении?', 'desc': 'Исследование представляет AInstein — фреймворк для оценки способности больших языковых моделей (LLM) решать исследовательские задачи в области AI, используя только предобученные знания без файнтюнинга или внешних источников. Модели анализируют реальные задачи из статей ICLR 2025 и предлагают решения через итеративные циклы генерации и критики, имитируя научный процесс. Оценка проводится по трём метрикам: успешность решения, способность переоткрыть существующие методы и генерация новых валидных подходов. Результаты показывают, что LLM могут находить осмысленные решения и иногда предлагать креативные альтернативы, но их способности остаются хрупкими и сильно зависят от формулировки задачи.'}, 'en': {'title': 'AInstein: Unveiling the Problem-Solving Power of LLMs', 'desc': "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."}, 'zh': {'title': '评估大型语言模型的科学问题解决能力', 'desc': 'AInstein是一个评估大型语言模型（LLMs）解决问题能力的框架。它测试这些模型在没有领域特定微调或外部帮助的情况下，是否能够生成有效的AI研究问题解决方案。通过提取高质量ICLR 2025提交的精炼问题陈述，AInstein模拟科学研究中的提案、审查和修订循环。研究结果表明，尽管LLMs能够重新发现可行的解决方案并偶尔提出创造性的替代方案，但它们的解决问题能力仍然脆弱，且对问题的表述非常敏感。'}}}, {'id': 'https://huggingface.co/papers/2510.04081', 'title': 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.04081', 'abstract': "Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.", 'score': 5, 'issue_id': 6299, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '4f2356e6d1057b79', 'authors': ['Honglin Lin', 'Qizhi Pei', 'Xin Gao', 'Zhuoshi Pan', 'Yu Li', 'Juntao Li', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.04081.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#math', '#data', '#training'], 'emoji': '🔢', 'ru': {'title': 'Код как основа для автоматической генерации качественных рассуждений', 'desc': 'Caco — это фреймворк для автоматической генерации высококачественных данных для обучения математическим рассуждениям в LLM с использованием кода. Система создает цепочки рассуждений (chain-of-thought) в виде исполняемого кода, автоматически проверяет их корректность через выполнение, а затем преобразует обратно в естественный язык. Благодаря этому подходу был создан датасет Caco-1.3M, который обеспечивает разнообразие и верифицируемость обучающих примеров без участия человека. Эксперименты показали, что модели, обученные на этих данных, превосходят существующие решения на математических бенчмарках и лучше генерализуются на новые задачи.'}, 'en': {'title': 'Caco: Automating High-Quality Reasoning for LLMs', 'desc': 'Caco is a new framework designed to improve the reasoning abilities of large language models (LLMs) in solving mathematical problems. It automates the creation of high-quality reasoning data by using code to generate diverse and verifiable reasoning paths. This approach addresses the limitations of traditional Chain-of-Thought (CoT) methods, which often struggle with quality and scalability. By incorporating automated validation and a closed-loop synthesis process, Caco ensures that the generated reasoning data is both executable and adaptable to various tasks, leading to better performance on mathematical reasoning benchmarks.'}, 'zh': {'title': 'Caco：自动化高质量推理数据生成的创新框架', 'desc': 'Caco是一个代码辅助的思维链框架，旨在自动生成高质量、可验证和多样化的推理数据，从而提升大型语言模型在数学推理任务上的表现。该框架通过代码驱动的增强方法，解决了现有思维链方法在生成控制、质量不足和推理路径有限等问题。Caco首先在统一的代码格式上微调代码基础的思维链生成器，然后扩展数据生成以获得大量多样化的推理轨迹。通过代码执行和基于规则的过滤，Caco确保了逻辑正确性和结构多样性，从而实现了完全自动化和可扩展的推理数据合成。'}}}, {'id': 'https://huggingface.co/papers/2510.06182', 'title': 'Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context', 'url': 'https://huggingface.co/papers/2510.06182', 'abstract': 'Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  \t\t\t\t\tAI-generated summary \t\t\t\t A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.', 'score': 4, 'issue_id': 6300, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'ebd0b1b2a51e6c0a', 'authors': ['Yoav Gur-Arieh', 'Mor Geva', 'Atticus Geiger'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Goodfire', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.06182.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#data', '#multimodal', '#interpretability', '#architecture'], 'emoji': '🔗', 'ru': {'title': 'Три механизма связывания сущностей в языковых моделях', 'desc': 'Исследование показывает, как языковые модели связывают и извлекают сущности в контексте, используя три различных механизма. Позиционный механизм работает хорошо для коротких списков, но становится ненадёжным при увеличении количества связанных сущностей. LLM компенсируют это лексическим механизмом (поиск через связанные слова) и рефлексивным механизмом (прямые указатели). Разработанная каузальная модель, объединяющая все три механизма, предсказывает следующий токен с точностью 95% и работает даже на длинных текстах.'}, 'en': {'title': 'Unraveling Entity Binding in Language Models', 'desc': 'This paper explores how language models (LMs) bind and retrieve entities during in-context reasoning. It identifies three mechanisms used by LMs: positional, lexical, and reflexive, which help in accurately predicting the next tokens. The study reveals that while the positional mechanism works well for short lists of entities, it struggles with longer contexts, leading to the use of lexical and reflexive mechanisms for better retrieval. By developing a causal model that integrates these mechanisms, the authors achieve high accuracy in predicting token distributions across various tasks and input lengths.'}, 'zh': {'title': '语言模型的实体绑定与检索机制', 'desc': '本研究探讨了语言模型如何在上下文中绑定和检索实体。我们发现，传统的基于位置的机制在复杂情况下表现不佳，因此语言模型还使用了词汇机制和反射机制来提高检索的准确性。通过对九种模型和十个绑定任务的广泛实验，我们揭示了语言模型如何混合使用这些机制来驱动模型行为。最终，我们开发了一个结合三种机制的因果模型，能够在更长的输入文本中有效地预测下一个标记。'}}}, {'id': 'https://huggingface.co/papers/2510.06131', 'title': 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation', 'url': 'https://huggingface.co/papers/2510.06131', 'abstract': 'MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.', 'score': 4, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '0641a3ba669f441d', 'authors': ['Jiawei Mao', 'Yuhan Wang', 'Lifeng Chen', 'Can Zhao', 'Yucheng Tang', 'Dong Yang', 'Liangqiong Qu', 'Daguang Xu', 'Yuyin Zhou'], 'affiliations': ['NVIDIA', 'UC Santa Cruz', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.06131.jpg', 'data': {'categories': ['#diffusion', '#science', '#healthcare', '#multimodal'], 'emoji': '🏥', 'ru': {'title': 'Единая диффузионная модель для всех медицинских модальностей', 'desc': 'MeDiM — это первая медицинская модель дискретной диффузии, которая объединяет разные типы биомедицинских данных (изображения, текст и клинические записи) в едином вероятностном пространстве без модально-специфичных компонентов. В основе модели лежит мультимодальная LLM с модифицированной архитектурой: убрана каузальная маска внимания для двунаправленного контекста и добавлены непрерывные timestep embeddings для диффузионного процесса. Модель способна переводить между изображениями и текстом, а также генерировать согласованные пары медицинских изображений и отчётов по текстовым запросам. Эксперименты показывают высокое качество генерации и улучшение результатов на downstream-задачах при использовании совместно сгенерированных пар изображение-отчёт.'}, 'en': {'title': 'Unifying Biomedical Data with MeDiM: A Multimodal Diffusion Revolution', 'desc': "MeDiM is a novel medical discrete diffusion model designed to integrate various types of biomedical data, such as images, text, and clinical notes. It overcomes the limitations of traditional models that operate within specific modalities by learning shared distributions across all data types. By utilizing a multimodal large language model as its backbone, MeDiM can generate high-quality medical outputs and translate between different modalities effectively. The model's innovative design features, like bidirectional context and continuous timestep embeddings, contribute to its superior performance in generating coherent and clinically relevant multimodal outputs."}, 'zh': {'title': 'MeDiM：医学多模态生成的创新桥梁', 'desc': 'MeDiM是一种医学离散扩散模型，能够通过学习图像、文本和临床记录之间的共享分布来整合多模态生物医学数据。该模型克服了传统生成医学模型在特定模态场景下的局限性，实现了高保真度的生成和增强的下游性能。MeDiM统一了多种生成任务，包括图像与文本之间的翻译，以及根据提示共同生成跨领域的图像-报告对。通过使用多模态大型语言模型作为扩散骨干，MeDiM在一个共享的概率空间中桥接了视觉和语言表示。'}}}, {'id': 'https://huggingface.co/papers/2510.05560', 'title': 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video', 'url': 'https://huggingface.co/papers/2510.05560', 'abstract': "HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.", 'score': 4, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'fcf790fe9803a797', 'authors': ['Hongchi Xia', 'Chih-Hao Lin', 'Hao-Yu Hsu', 'Quentin Leboutet', 'Katelyn Gao', 'Michael Paulitsch', 'Benjamin Ummenhofer', 'Shenlong Wang'], 'affiliations': ['Intel', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.05560.jpg', 'data': {'categories': ['#3d', '#optimization', '#games', '#benchmark'], 'emoji': '🏗️', 'ru': {'title': 'Цифровые двойники с физикой и фотореализмом', 'desc': "HoloScene — это фреймворк для интерактивной 3D-реконструкции физического мира в виртуальные среды, готовые к симуляции. Система использует граф сцены, который кодирует геометрию объектов, их внешний вид, физические свойства и взаимосвязи между ними. Реконструкция формулируется как задача энергетической оптимизации, объединяющая данные наблюдений, физические ограничения и генеративные prior'ы через гибридный подход с sampling и градиентными методами. Результат — полные цифровые двойники с точной геометрией, физической стабильностью и фотореалистичным рендерингом для AR/VR, игр и робототехники."}, 'en': {'title': 'Revolutionizing 3D Reconstruction with HoloScene', 'desc': 'HoloScene is a cutting-edge framework for creating interactive 3D reconstructions that meet essential criteria for realistic simulations. It addresses common shortcomings in existing methods by ensuring geometry completeness, object interactivity, physical plausibility, and photorealistic rendering. The framework utilizes an energy-based optimization approach that combines observational data and physical constraints to produce accurate digital twins. Its effectiveness is demonstrated through superior performance on benchmark datasets and practical applications in gaming and digital-twin manipulation.'}, 'zh': {'title': 'HoloScene：实现真实感的交互式3D重建', 'desc': 'HoloScene是一个交互式3D重建框架，旨在实现几何完整性、物体交互性、物理合理性、照片级渲染和真实的物理属性。该框架通过能量优化问题来整合观察数据、物理约束和生成先验，形成一个统一的目标。HoloScene利用全面的交互场景图表示，编码物体的几何形状、外观和物理属性，同时考虑层次和物体间的关系。通过结合基于采样的探索和基于梯度的细化，优化过程高效进行，最终生成的数字双胞胎在新视角下展现出完整精确的几何形状和真实的渲染效果。'}}}, {'id': 'https://huggingface.co/papers/2510.05137', 'title': 'Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics', 'url': 'https://huggingface.co/papers/2510.05137', 'abstract': "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.", 'score': 4, 'issue_id': 6298, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'd780a77899923141', 'authors': ['Maojia Song', 'Renhang Liu', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Soujanya Poria', 'Jingren Zhou'], 'affiliations': ['Nanyang Technological University (NTU)', 'Singapore University of Technology and Design (SUTD)', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.05137.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#benchmark', '#leakage', '#architecture', '#agents'], 'emoji': '🔍', 'ru': {'title': 'WebDetective: Как научить AI-агентов думать самостоятельно, а не следовать подсказкам', 'desc': 'WebDetective — это новый бенчмарк для оценки многошаговых рассуждений в RAG-системах и веб-агентах. Существующие тесты содержат «утечку» пути рассуждений прямо в вопросе, позволяя моделям просто следовать поверхностным подсказкам вместо самостоятельного построения цепочки мыслей. Авторы создали контролируемую среду на основе Wikipedia и детальную систему оценки, которая раздельно измеряет качество поиска информации, использование знаний и способность отказаться от ответа при недостатке данных. Тестирование 25 современных моделей выявило критическую проблему: системы хорошо выполняют заданные пути рассуждений, но проваливаются, когда нужно самостоятельно их обнаружить.'}, 'en': {'title': 'WebDetective: Enhancing Multi-Hop Reasoning in AI Systems', 'desc': 'WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities.'}, 'zh': {'title': 'WebDetective：提升多跳推理的评估标准', 'desc': 'WebDetective是一个用于评估RAG系统和网络代理的多跳推理基准，旨在解决推理路径泄漏和单次评估的问题。该基准提供无提示的多跳问题，并配备一个受控的维基百科沙箱，以确保模型行为的可追溯性。通过对25个最先进模型的评估，我们发现这些模型在知识利用方面存在系统性弱点，尽管有足够的证据，但在缺乏证据时几乎没有适当的拒绝行为。我们开发了EvidenceLoop工作流程，专门针对基准识别的挑战，改进了搜索和综合能力。'}}}, {'id': 'https://huggingface.co/papers/2510.05342', 'title': 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization', 'url': 'https://huggingface.co/papers/2510.05342', 'abstract': 'MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.', 'score': 3, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '1a257d25fefce283', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.05342.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Адаптивные веса для каждого примера делают обучение по предпочтениям эффективнее', 'desc': 'MADPO — это новый метод выравнивания больших языковых моделей по предпочтениям, который решает проблему DPO с фиксированной температурой. Метод сначала обучает reward model для оценки сложности каждой пары примеров, а затем применяет индивидуальные адаптивные веса к loss-функции DPO для каждого sample. Это позволяет модели больше учиться на сложных примерах и меньше переобучаться на простых, в отличие от предыдущих методов с батч-уровневой или uniformной регуляризацией. Эксперименты показывают улучшение до +33.3% на качественных данных по сравнению с baseline методами.'}, 'en': {'title': 'Enhancing Preference Alignment with Adaptive Weighting', 'desc': "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."}, 'zh': {'title': '边际自适应优化，提升模型偏好对齐', 'desc': 'MADPO是一种边际自适应方法，通过为DPO损失提供实例级自适应加权，增强了大型语言模型的偏好对齐能力。该方法首先训练一个奖励模型来估计偏好边际，然后根据这些边际为每个训练样本应用连续的自适应权重。MADPO的重加权方案对困难样本增强信号，对简单样本减弱信号，从而实现了对学习信号的细致控制。实验结果表明，MADPO在情感生成任务中显著优于其他强基线，证明了其在偏好对齐方面的稳健性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.05156', 'title': 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation', 'url': 'https://huggingface.co/papers/2510.05156', 'abstract': "VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", 'score': 3, 'issue_id': 6300, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'cfcbeb2c67a2faa9', 'authors': ['Lesly Miculicich', 'Mihir Parmar', 'Hamid Palangi', 'Krishnamurthy Dj Dvijotham', 'Mirko Montanari', 'Tomas Pfister', 'Long T. Le'], 'affiliations': ['Google Cloud AI', 'Google Cloud AI Research', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.05156.jpg', 'data': {'categories': ['#inference', '#alignment', '#agents', '#security', '#healthcare'], 'emoji': '🛡️', 'ru': {'title': 'Формальные гарантии безопасности для LLM-агентов', 'desc': 'VeriGuard — это фреймворк для обеспечения формальных гарантий безопасности AI-агентов на основе LLM в критических областях вроде здравоохранения. Система работает в два этапа: офлайн-валидация создаёт и формально верифицирует поведенческую политику агента на соответствие требованиям безопасности, а онлайн-мониторинг проверяет каждое действие агента перед его выполнением. Такое разделение позволяет проводить тщательную проверку заранее, а во время работы использовать лёгкий мониторинг. Подход защищает от отклонений агента от целей пользователя, нарушений политик обработки данных и adversarial-атак.'}, 'en': {'title': 'Ensuring Safety in AI Agents with VeriGuard', 'desc': "VeriGuard is a framework designed to ensure the safety of large language model (LLM)-based agents by providing formal guarantees through a two-stage process. The first stage involves offline validation, where user intent is clarified to create specific safety specifications, and a behavioral policy is synthesized and rigorously tested for compliance. The second stage focuses on online monitoring, where the agent's actions are continuously validated against the pre-verified policy before they are executed. This approach enhances the reliability of AI agents in sensitive areas by ensuring they adhere to safety constraints and reducing risks associated with their deployment."}, 'zh': {'title': 'VeriGuard：确保智能体安全的双阶段框架', 'desc': 'VeriGuard是一个框架，旨在为基于大型语言模型（LLM）的智能体提供正式的安全保障。它通过离线验证和在线监控的双阶段架构，确保智能体的行为符合预定义的安全约束。首先，在离线阶段，VeriGuard通过明确用户意图来建立安全规范，并合成行为策略，经过测试和正式验证以确保合规。然后，在在线阶段，VeriGuard作为运行时监控器，验证每个提议的智能体动作，以确保其符合预先验证的政策，从而提高LLM智能体的可信度。'}}}, {'id': 'https://huggingface.co/papers/2510.05122', 'title': 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support\n  Conversation', 'url': 'https://huggingface.co/papers/2510.05122', 'abstract': 'CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.', 'score': 3, 'issue_id': 6301, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'a08f3bb063148c1c', 'authors': ['Jie Zhu', 'Yuanchen Zhou', 'Shuo Jiang', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05122.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning'], 'emoji': '🤗', 'ru': {'title': 'Усиление когнитивного мышления для эмоциональной поддержки', 'desc': 'Статья представляет CARE — фреймворк для улучшения диалоговых систем эмоциональной поддержки. В отличие от существующих подходов, которые фокусируются на синтетических данных, CARE развивает когнитивное мышление модели, используя оригинальный набор данных для обучения. Система применяет reinforcement learning для генерации логически связных и поддерживающих ответов. Эксперименты показывают значительное улучшение качества эмпатии и логической последовательности ответов без необходимости в масштабных синтетических корпусах.'}, 'en': {'title': 'Enhancing Emotional Support with Cognitive Reasoning', 'desc': 'CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.'}, 'zh': {'title': 'CARE：提升情感支持对话的认知推理能力', 'desc': 'CARE是一个框架，通过强化学习增强情感支持对话中的认知推理，提升响应质量和同理心，而不依赖于大规模的合成数据。情感支持对话在缓解心理压力和提供情感价值方面起着重要作用。以往的研究主要集中在数据增强和合成语料库的构建上，忽视了有效情感支持背后的深层认知推理过程。CARE利用原始的情感支持对话训练集，引导模型生成逻辑连贯和支持性的响应，从而显著提升认知推理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.05367', 'title': 'LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation', 'url': 'https://huggingface.co/papers/2510.05367', 'abstract': 'The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .', 'score': 2, 'issue_id': 6299, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'da5c54f2d1fce894', 'authors': ['Yang Xiao', 'Gen Li', 'Kaiyuan Deng', 'Yushu Wu', 'Zheng Zhan', 'Yanzhi Wang', 'Xiaolong Ma', 'Bo Hui'], 'affiliations': ['Clemson University', 'Microsoft Research', 'Northeastern University', 'The University of Arizona', 'University of Tulsa'], 'pdf_title_img': 'assets/pdf/title_img/2510.05367.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#video', '#optimization', '#inference'], 'emoji': '🎬', 'ru': {'title': 'Ускорение видеогенерации через управление памятью на разных стадиях', 'desc': 'Исследователи предложили метод ускорения генерации видео с помощью диффузионных моделей без дополнительного обучения. Они разделили процесс инференса на три стадии (кодирование, шумоподавление и декодирование) и обнаружили проблему резкого роста потребления памяти. Для каждой стадии разработаны специфические стратегии: асинхронный обмен кэша, разбиение признаков на части и нарезка латентных представлений при декодировании. В результате достигнуто ускорение работы модели при снижении потребления памяти с сохранением приемлемого качества генерируемого видео.'}, 'en': {'title': 'Accelerating Video Generation with Stage-Specific Strategies', 'desc': 'This paper focuses on improving the efficiency of video generation using diffusion models by introducing stage-specific strategies. It identifies that the inference process can be broken down into three stages: encoding, denoising, and decoding, and highlights the memory issues that arise during the latter two stages. The authors propose methods such as Asynchronous Cache Swapping, Feature Chunking, and Slicing Latents to optimize memory usage without significantly increasing processing time. Overall, their approach results in faster inference speeds and reduced memory consumption while keeping quality loss minimal.'}, 'zh': {'title': '阶段特定策略加速视频生成推理', 'desc': '本文提出了针对扩散模型在视频生成中的推理加速的阶段特定策略，旨在减少内存使用并保持生成质量。我们将推理过程分解为编码、去噪和解码三个阶段，并发现基于缓存的加速方法在后两个阶段常常导致内存激增。为了解决这个问题，我们分析了不同阶段推理的特征，并提出了三种减少内存消耗的策略：异步缓存交换、特征块和切片潜变量解码。同时，我们确保这三种策略引入的时间开销低于加速带来的收益。与基线相比，我们的方法实现了更快的推理速度和更低的内存使用，同时保持了可接受范围内的质量下降。'}}}, {'id': 'https://huggingface.co/papers/2510.03978', 'title': 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2510.03978', 'abstract': 'Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.', 'score': 2, 'issue_id': 6301, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'bb2f9d23dbe6e4fa', 'authors': ['Min Woo Sun', 'Alejandro Lozano', 'Javier Gamazo Tejero', 'Vishwesh Nath', 'Xiao Xiao Sun', 'James Burgess', 'Yuhui Zhang', 'Kun Yuan', 'Robert Tibshirani', 'Sean Huver', 'Serena Yeung-Levy'], 'affiliations': ['NVIDIA, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.03978.jpg', 'data': {'categories': ['#data', '#benchmark', '#long_context', '#healthcare', '#dataset', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Длинный контекст для биомедицинских изображений: больше слов — лучше результат', 'desc': 'Исследователи обнаружили, что стандартные vision-language модели с коротким контекстом (до 77 токенов) теряют важную информацию при обработке биомедицинских описаний изображений, которые часто содержат гораздо больше текста. Они создали датасет BIOMEDICA-LongCAP из 1 миллиона пар изображение-текст с расширенными описаниями из научных статей и обучили модель BMC-LongCLIP, поддерживающую до 512 токенов. Новая модель увеличила контекстное окно в 6.6 раз и сократила потери токенов с 55% до 2.2%, что привело к улучшению точности поиска изображений на 30% и классификации на 2%. Результаты показывают, что использование длинного контекста является перспективным направлением для развития мультимодальных моделей в биомедицине.'}, 'en': {'title': 'Unlocking the Power of Long Contexts in Biomedical Captioning', 'desc': 'This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.'}, 'zh': {'title': '扩展上下文，提升生物医学模型性能', 'desc': '本文探讨了在视觉语言模型中扩展文本编码器的上下文长度对生物医学描述任务的影响。传统的视觉语言模型通常使用较短的文本窗口，这导致长格式的描述被截断。研究发现，使用更长的上下文可以提高检索和分类的性能，因为它允许模型利用更详细的描述。为此，作者引入了一个新的数据集BIOMEDICA-LongCAP，并训练了支持长上下文的生物医学视觉语言模型BMC-LongCLIP，显著提升了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.06218', 'title': 'EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark', 'url': 'https://huggingface.co/papers/2510.06218', 'abstract': 'EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'a863fc0993d7f2f6', 'authors': ['Deheng Zhang', 'Yuqian Fu', 'Runyi Yang', 'Yang Miao', 'Tianwen Qian', 'Xu Zheng', 'Guolei Sun', 'Ajad Chhatkuli', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Luc Van Gool', 'Danda Pani Paudel'], 'affiliations': ['East China Normal University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06218.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#benchmark', '#games', '#transfer_learning', '#synthetic', '#cv'], 'emoji': '🌙', 'ru': {'title': 'Проверка AI-зрения в темноте от первого лица', 'desc': 'EgoNight — это первый комплексный бенчмарк для оценки egocentric-видения в ночных условиях с фокусом на visual question answering. Датасет включает 3658 пар вопрос-ответ на 90 видео, записанных как в дневное, так и в ночное время с выровненными сценами и действиями. Исследование показало значительное падение производительности современных multimodal LLM при переходе от дневных к ночным условиям. Помимо VQA, бенчмарк включает дополнительные задачи: поиск соответствий день-ночь и оценку глубины в ночных условиях для более полного тестирования моделей.'}, 'en': {'title': 'Bridging the Gap: Nighttime Vision for AI', 'desc': 'EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field.'}, 'zh': {'title': '夜间视觉问答的新基准：EgoNight', 'desc': 'EgoNight是一个全面的基准测试，专注于夜间自我中心视觉，特别是视觉问答（VQA）任务。现有的自我中心视觉基准主要集中在白天场景，忽视了低光照条件下的应用需求。EgoNight通过引入日夜对齐的视频，提升了夜间标注的质量，并揭示了不同光照条件下的性能差距。该基准包含3658个问答对，支持多种任务，旨在推动自我中心视觉研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2510.06139', 'title': 'Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.06139', 'abstract': "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.", 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '27bd729b0bb095f1', 'authors': ['Zanyi Wang', 'Dengyang Jiang', 'Liuzhuozheng Li', 'Sizhe Dang', 'Chengzu Li', 'Harry Yang', 'Guang Dai', 'Mengmeng Wang', 'Jingdong Wang'], 'affiliations': ['Baidu', 'SGIT AI Lab, State Grid Corporation of China', 'The Hong Kong University of Science and Technology', 'The University of Tokyo', 'University of California, San Diego', 'University of Cambridge', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.06139.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#games', '#alignment'], 'emoji': '🌊', 'ru': {'title': 'Сегментация видео через непрерывную деформацию под управлением текста', 'desc': 'FlowRVS решает задачу сегментации объектов в видео по текстовому описанию (RVOS), переформулируя её как проблему непрерывного потока. Вместо традиционного подхода «найти-затем-сегментировать», метод использует pretrained text-to-video модели для прямой деформации видео-представления в целевую маску под управлением языкового описания. Это позволяет обеспечить точный контроль на уровне пикселей, семантическое выравнивание текста и видео, а также временную согласованность. Подход достигает state-of-the-art результатов на всех основных бенчмарках RVOS, включая MeViS и Ref-DAVIS17.'}, 'en': {'title': 'Revolutionizing Video Segmentation with Continuous Flow', 'desc': 'FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding.'}, 'zh': {'title': 'FlowRVS：视频物体分割的新思路', 'desc': 'FlowRVS提出了一种新方法来解决视频物体分割中的引用问题，将其重新定义为一个连续流动问题。该方法利用预训练的文本到视频模型，实现了对视频中目标的精细像素控制和语义对齐。与传统的“定位-再分割”流程不同，FlowRVS通过直接学习语言引导的变形，从视频的整体表示到目标掩膜，保持了时间一致性。该框架在主要的引用视频物体分割基准测试中取得了新的最先进结果，展示了将视频理解任务建模为连续变形过程的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.04087', 'title': 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling', 'url': 'https://huggingface.co/papers/2510.04087', 'abstract': 'A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '77a7f937000c3b18', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.04087.jpg', 'data': {'categories': ['#inference', '#training', '#data', '#rlhf', '#alignment'], 'emoji': '🚦', 'ru': {'title': 'Научить AI понимать, что хорошо, а не только что лучше', 'desc': 'Исследователи предлагают новый подход к обучению моделей предпочтений, добавляя «внешнюю опцию» в данные сравнений. Традиционные reward models умеют определять, какой ответ лучше, но не могут понять, является ли ответ вообще приемлемым. Новый метод позволяет модели различать не только относительное качество, но и абсолютную приемлемость ответов, что особенно важно для сложных запросов. Адаптивная стратегия генерации с ранним выходом снижает количество неприемлемых ответов на 70% и ускоряет inference на 22%.'}, 'en': {'title': 'Enhancing Preference Alignment with Outside Options', 'desc': 'This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%.'}, 'zh': {'title': '提升偏好对齐的可靠性与效率', 'desc': '本文提出了一种新的框架，通过在偏好数据收集和建模中引入外部选项，提升了偏好对齐技术的可靠性和效率。现代的偏好对齐技术，如最佳N（BoN）采样，依赖于通过成对比较数据训练的奖励模型，虽然能有效学习相对偏好，但未能捕捉响应可接受性的信号。我们通过引入外部选项，训练出能够区分不仅是更好而是足够好的奖励模型，从而解决了可靠性缺口。实验结果表明，该框架在对齐和推理加速方面均显著提高了性能，提供了一个灵活的管理可靠性与计算效率之间权衡的工具。'}}}, {'id': 'https://huggingface.co/papers/2510.02341', 'title': 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning', 'url': 'https://huggingface.co/papers/2510.02341', 'abstract': 'DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '1bde94710320cd61', 'authors': ['Yifan Wang', 'Bolian Li', 'Junlin Wu', 'Zhaoxuan Tan', 'Zheli Liu', 'Ruqi Zhang', 'Ananth Grama', 'Qingkai Zeng'], 'affiliations': ['College of Computer Science, Nankai University', 'Department of Computer Science and Engineering, University of Notre Dame', 'Department of Computer Science, Purdue University', 'Department of Computer Science, Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2510.02341.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '🔄', 'ru': {'title': 'Обучение LLM на недовольстве пользователей: превращаем негатив в качество', 'desc': 'В статье представлен метод DRIFT для обучения больших языковых моделей на основе сигналов недовольства пользователей. В реальных системах пользователи часто итеративно улучшают ответы через уточнения и исправления, создавая неявные сигналы неудовлетворённости, в то время как явная положительная обратная связь встречается редко. DRIFT использует эти сигналы недовольства как якорь для обучения и динамически генерирует положительные примеры из развивающейся политики модели. На датасетах WildFeedback и UltraFeedback метод показывает улучшение до +12.29% по win rate на AlpacaEval2, превосходя базовые методы как DPO и SPIN, при этом сохраняя разнообразие генерируемых решений.'}, 'en': {'title': 'Harnessing User Dissatisfaction for Better Language Models', 'desc': 'The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs.'}, 'zh': {'title': '利用用户不满信号提升语言模型性能', 'desc': 'DRIFT是一种基于用户不满信号的迭代偏好训练方法，旨在提升大型语言模型的性能。该方法利用真实世界中的用户不满信号，动态采样正反馈，从而更好地适应用户的需求。实验结果表明，使用DRIFT训练的模型在多个任务上显著超越了传统方法，尤其是在大规模模型上表现尤为突出。DRIFT不仅提高了模型的性能，还保持了探索能力，能够生成更多样化的高奖励解决方案。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi', '#alignment (6)', '#architecture (4)', '#audio', '#benchmark (9)', '#cv (1)', '#data (6)', '#dataset (5)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare (3)', '#inference (5)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (6)', '#open_source (3)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (4)', '#rlhf (5)', '#robotics', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-08 06:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-08 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-08 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    