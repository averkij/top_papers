{
    "date": {
        "ru": "6 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 6",
        "zh": "2æœˆ6æ—¥"
    },
    "time_utc": "2026-02-06 01:17",
    "weekday": 4,
    "issue_id": 932,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.04705",
            "title": "ERNIE 5.0 Technical Report",
            "url": "https://huggingface.co/papers/2602.04705",
            "abstract": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.",
            "score": 200,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "2765e822381335a3",
            "authors": [
                "Haifeng Wang",
                "Hua Wu",
                "Tian Wu",
                "Yu Sun",
                "Jing Liu",
                "Dianhai Yu",
                "Yanjun Ma",
                "Jingzhou He",
                "Zhongjun He",
                "Dou Hong",
                "Qiwen Liu",
                "Shuohuan Wang",
                "Junyuan Shang",
                "Zhenyu Zhang",
                "Yuchen Ding",
                "Jinle Zeng",
                "Jiabin Yang",
                "Liang Shen",
                "Ruibiao Chen",
                "Weichong Yin",
                "Siyu Ding",
                "Dai Dai",
                "Shikun Feng",
                "Siqi Bao",
                "Bolei He",
                "Yan Chen",
                "Zhenyu Jiao",
                "Ruiqing Zhang",
                "Zeyu Chen",
                "Qingqing Dang",
                "Kaipeng Deng",
                "Jiajun Jiang",
                "Enlei Gong",
                "Guoxia Wang",
                "Yanlin Sha",
                "Yi Liu",
                "Yehan Zheng",
                "Weijian Xu",
                "Jiaxiang Liu",
                "Zengfeng Zeng",
                "Yingqi Qu",
                "Zhongli Li",
                "Zhengkun Zhang",
                "Xiyang Wang",
                "Zixiang Xu",
                "Xinchao Xu",
                "Zhengjie Huang",
                "Dong Wang",
                "Bingjin Chen",
                "Yue Chang",
                "Xing Yuan",
                "Shiwei Huang",
                "Qiao Zhao",
                "Xinzhe Ding",
                "Shuangshuang Qiao",
                "Baoshan Yang",
                "Bihong Tang",
                "Bin Li",
                "Bingquan Wang",
                "Binhan Tang",
                "Binxiong Zheng",
                "Bo Cui",
                "Bo Ke",
                "Bo Zhang",
                "Bowen Zhang",
                "Boyan Zhang",
                "Boyang Liu",
                "Caiji Zhang",
                "Can Li",
                "Chang Xu",
                "Chao Pang",
                "Chao Zhang",
                "Chaoyi Yuan",
                "Chen Chen",
                "Cheng Cui",
                "Chenlin Yin",
                "Chun Gan",
                "Chunguang Chai",
                "Chuyu Fang",
                "Cuiyun Han",
                "Dan Zhang",
                "Danlei Feng",
                "Danxiang Zhu",
                "Dong Sun",
                "Dongbo Li",
                "Dongdong Li",
                "Dongdong Liu",
                "Dongxue Liu",
                "Fan Ding",
                "Fan Hu",
                "Fan Li",
                "Fan Mo",
                "Feisheng Wu",
                "Fengwei Liu",
                "Gangqiang Hu",
                "Gaofeng Lu",
                "Gaopeng Yong",
                "Gexiao Tian",
                "Guan Wang",
                "Guangchen Ni",
                "Guangshuo Wu",
                "Guanzhong Wang",
                "Guihua Liu",
                "Guishun Li",
                "Haibin Li",
                "Haijian Liang",
                "Haipeng Ming",
                "Haisu Wang",
                "Haiyang Lu",
                "Haiye Lin",
                "Han Zhou",
                "Hangting Lou",
                "Hanwen Du",
                "Hanzhi Zhang",
                "Hao Chen",
                "Hao Du",
                "Hao Liu",
                "Hao Zhou",
                "Haochen Jiang",
                "Haodong Tian",
                "Haoshuang Wang",
                "Haozhe Geng",
                "Heju Yin",
                "Hong Chen",
                "Hongchen Xue",
                "Hongen Liu",
                "Honggeng Zhang",
                "Hongji Xu",
                "Hongwei Chen",
                "Hongyang Zhang",
                "Hongyuan Zhang",
                "Hua Lu",
                "Huan Chen",
                "Huan Wang",
                "Huang He",
                "Hui Liu",
                "Hui Zhong",
                "Huibin Ruan",
                "Jiafeng Lu",
                "Jiage Liang",
                "Jiahao Hu",
                "Jiahao Hu",
                "Jiajie Yang",
                "Jialin Li",
                "Jian Chen",
                "Jian Wu",
                "Jianfeng Yang",
                "Jianguang Jiang",
                "Jianhua Wang",
                "Jianye Chen",
                "Jiaodi Liu",
                "Jiarui Zhou",
                "Jiawei Lv",
                "Jiaxin Zhou",
                "Jiaxuan Liu",
                "Jie Han",
                "Jie Sun",
                "Jiefan Fang",
                "Jihan Liu",
                "Jihua Liu",
                "Jing Hu",
                "Jing Qian",
                "Jing Yan",
                "Jingdong Du",
                "Jingdong Wang",
                "Jingjing Wu",
                "Jingyong Li",
                "Jinheng Wang",
                "Jinjin Li",
                "Jinliang Lu",
                "Jinlin Yu",
                "Jinnan Liu",
                "Jixiang Feng",
                "Jiyi Huang",
                "Jiyuan Zhang",
                "Jun Liang",
                "Jun Xia",
                "Jun Yu",
                "Junda Chen",
                "Junhao Feng",
                "Junhong Xiang",
                "Junliang Li",
                "Kai Liu",
                "Kailun Chen",
                "Kairan Su",
                "Kang Hu",
                "Kangkang Zhou",
                "Ke Chen",
                "Ke Wei",
                "Kui Huang",
                "Kun Wu",
                "Kunbin Chen",
                "Lei Han",
                "Lei Sun",
                "Lei Wen",
                "Linghui Meng",
                "Linhao Yu",
                "Liping Ouyang",
                "Liwen Zhang",
                "Longbin Ji",
                "Longzhi Wang",
                "Meng Sun",
                "Meng Tian",
                "Mengfei Li",
                "Mengqi Zeng",
                "Mengyu Zhang",
                "Ming Hong",
                "Mingcheng Zhou",
                "Mingming Huang",
                "Mingxin Chen",
                "Mingzhu Cai",
                "Naibin Gu",
                "Nemin Qiu",
                "Nian Wang",
                "Peng Qiu",
                "Peng Zhao",
                "Pengyu Zou",
                "Qi Wang",
                "Qi Xin",
                "Qian Wang",
                "Qiang Zhu",
                "Qianhui Luo",
                "Qianwei Yang",
                "Qianyue He",
                "Qifei Wu",
                "Qinrui Li",
                "Qiwen Bao",
                "Quan Zhang",
                "Quanxiang Liu",
                "Qunyi Xie",
                "Rongrui Zhan",
                "Rufeng Dai",
                "Rui Peng",
                "Ruian Liu",
                "Ruihao Xu",
                "Ruijie Wang",
                "Ruixi Zhang",
                "Ruixuan Liu",
                "Runsheng Shi",
                "Ruting Wang",
                "Senbo Kang",
                "Shan Lu",
                "Shaofei Yu",
                "Shaotian Gong",
                "Shenwei Hu",
                "Shifeng Zheng",
                "Shihao Guo",
                "Shilong Fan",
                "Shiqin Liu",
                "Shiwei Gu",
                "Shixi Zhang",
                "Shuai Yao",
                "Shuang Zhang",
                "Shuangqiao Liu",
                "Shuhao Liang",
                "Shuwei He",
                "Shuwen Yang",
                "Sijun He",
                "Siming Dai",
                "Siming Wu",
                "Siyi Long",
                "Songhe Deng",
                "Suhui Dong",
                "Suyin Liang",
                "Teng Hu",
                "Tianchan Xu",
                "Tianliang Lv",
                "Tianmeng Yang",
                "Tianyi Wei",
                "Tiezhu Gao",
                "Ting Sun",
                "Ting Zhang",
                "Tingdan Luo",
                "Wei He",
                "Wei Luan",
                "Wei Yin",
                "Wei Zhang",
                "Wei Zhou",
                "Weibao Gong",
                "Weibin Li",
                "Weicheng Huang",
                "Weichong Dang",
                "Weiguo Zhu",
                "Weilong Zhang",
                "Weiqi Tan",
                "Wen Huang",
                "Wenbin Chang",
                "Wenjing Du",
                "Wenlong Miao",
                "Wenpei Luo",
                "Wenquan Wu",
                "Xi Shi",
                "Xi Zhao",
                "Xiang Gao",
                "Xiangguo Zhang",
                "Xiangrui Yu",
                "Xiangsen Wang",
                "Xiangzhe Wang",
                "Xianlong Luo",
                "Xianying Ma",
                "Xiao Tan",
                "Xiaocong Lin",
                "Xiaofei Wang",
                "Xiaofeng Peng",
                "Xiaofeng Wu",
                "Xiaojian Xu",
                "Xiaolan Yuan",
                "Xiaopeng Cui",
                "Xiaotian Han",
                "Xiaoxiong Liu",
                "Xiaoxu Fei",
                "Xiaoxuan Wu",
                "Xiaoyu Wang",
                "Xiaoyu Zhang",
                "Xin Sun",
                "Xin Wang",
                "Xinhui Huang",
                "Xinming Zhu",
                "Xintong Yu",
                "Xinyi Xu",
                "Xinyu Wang",
                "Xiuxian Li",
                "XuanShi Zhu",
                "Xue Xu",
                "Xueying Lv",
                "Xuhong Li",
                "Xulong Wei",
                "Xuyi Chen",
                "Yabing Shi",
                "Yafeng Wang",
                "Yamei Li",
                "Yan Liu",
                "Yanfu Cheng",
                "Yang Gao",
                "Yang Liang",
                "Yang Wang",
                "Yang Wang",
                "Yang Yang",
                "Yanlong Liu",
                "Yannian Fu",
                "Yanpeng Wang",
                "Yanzheng Lin",
                "Yao Chen",
                "Yaozong Shen",
                "Yaqian Han",
                "Yehua Yang",
                "Yekun Chai",
                "Yesong Wang",
                "Yi Song",
                "Yichen Zhang",
                "Yifei Wang",
                "Yifeng Guo",
                "Yifeng Kou",
                "Yilong Chen",
                "Yilong Guo",
                "Yiming Wang",
                "Ying Chen",
                "Ying Wang",
                "Yingsheng Wu",
                "Yingzhan Lin",
                "Yinqi Yang",
                "Yiran Xing",
                "Yishu Lei",
                "Yixiang Tu",
                "Yiyan Chen",
                "Yong Zhang",
                "Yonghua Li",
                "Yongqiang Ma",
                "Yongxing Dai",
                "Yongyue Zhang",
                "Yu Ran",
                "Yu Sun",
                "Yu-Wen Michael Zhang",
                "Yuang Liu",
                "Yuanle Liu",
                "Yuanyuan Zhou",
                "Yubo Zhang",
                "Yuchen Han",
                "Yucheng Wang",
                "Yude Gao",
                "Yuedong Luo",
                "Yuehu Dong",
                "Yufeng Hu",
                "Yuhui Cao",
                "Yuhui Yun",
                "Yukun Chen",
                "Yukun Gao",
                "Yukun Li",
                "Yumeng Zhang",
                "Yun Fan",
                "Yun Ma",
                "Yunfei Zhang",
                "Yunshen Xie",
                "Yuping Xu",
                "Yuqin Zhang",
                "Yuqing Liu",
                "Yurui Li",
                "Yuwen Wang",
                "Yuxiang Lu",
                "Zefeng Cai",
                "Zelin Zhao",
                "Zelun Zhang",
                "Zenan Lin",
                "Zezhao Dong",
                "Zhaowu Pan",
                "Zhaoyu Liu",
                "Zhe Dong",
                "Zhe Zhang",
                "Zhen Zhang",
                "Zhengfan Wu",
                "Zhengrui Wei",
                "Zhengsheng Ning",
                "Zhenxing Li",
                "Zhenyu Li",
                "Zhenyu Qian",
                "Zhenyun Li",
                "Zhi Li",
                "Zhichao Chen",
                "Zhicheng Dong",
                "Zhida Feng",
                "Zhifan Feng",
                "Zhihao Deng",
                "Zhijin Yu",
                "Zhiyang Chen",
                "Zhonghui Zheng",
                "Zhuangzhuang Guo",
                "Zhujun Zhang",
                "Zhuo Sun",
                "Zichang Liu",
                "Zihan Lin",
                "Zihao Huang",
                "Zihe Zhu",
                "Ziheng Zhao",
                "Ziping Chen",
                "Zixuan Zhu",
                "Ziyang Xu",
                "Ziyi Liang",
                "Ziyuan Gao"
            ],
            "affiliations": [
                "Baidu"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04705.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "ERNIE 5.0 â€” ÑÑ‚Ğ¾ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚ĞµÑ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ½ÑƒĞ»Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ MoE."
                },
                "en": {
                    "title": "ERNIE 5.0: Unifying Multimodal AI with Elastic Training",
                    "desc": "ERNIE 5.0 is a groundbreaking autoregressive model that integrates understanding and generation of multiple data types, including text, images, videos, and audio. It utilizes a sparse mixture-of-experts (MoE) architecture, allowing for efficient routing of experts based on the modality of the input. The model employs an elastic training approach, which enables it to adaptively learn various sub-models with different capacities and depths, optimizing performance while managing resource constraints. Extensive testing shows that ERNIE 5.0 excels in delivering balanced performance across all modalities, marking a significant advancement in the field of multimodal machine learning."
                },
                "zh": {
                    "title": "ERNIE 5.0ï¼šä¸‡äº¿å‚æ•°çš„å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹",
                    "desc": "ERNIE 5.0 æ˜¯ä¸€ä¸ªå…·æœ‰ä¸‡äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è¶…ç¨€ç–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡å¼¹æ€§è®­ç»ƒæ–¹æ³•è§£å†³å¤§è§„æ¨¡éƒ¨ç½²ä¸­çš„èµ„æºé™åˆ¶é—®é¢˜ã€‚ERNIE 5.0 åœ¨å•æ¬¡é¢„è®­ç»ƒä¸­å­¦ä¹ å¤šä¸ªå­æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ€§èƒ½ã€æ¨¡å‹å¤§å°å’Œæ¨ç†å»¶è¿Ÿä¹‹é—´çµæ´»æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒERNIE 5.0 åœ¨å¤šç§æ¨¡æ€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ ‡å¿—ç€è‡ªå›å½’æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„é¦–æ¬¡å¤§è§„æ¨¡åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03152",
            "title": "FASA: Frequency-aware Sparse Attention",
            "url": "https://huggingface.co/papers/2602.03152",
            "abstract": "FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.",
            "score": 103,
            "issue_id": 920,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "930179326b649b72",
            "authors": [
                "Yifei Wang",
                "Yueqi Wang",
                "Zhenrui Yue",
                "Huimin Zeng",
                "Yong Wang",
                "Ismini Lourentzou",
                "Zhengzhong Tu",
                "Xiangxiang Chu",
                "Julian McAuley"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "Texas A&M University",
                "UCSD",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03152.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#training",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞµÑˆĞ° Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ RoPE",
                    "desc": "FASA â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ KV-ĞºĞµÑˆĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Â«Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…Â» Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FASA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ñƒ, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "FASA: Efficient Memory Management for Long-Context LLMs",
                    "desc": "FASA is a new framework designed to optimize memory usage in Large Language Models (LLMs) by implementing query-aware token eviction and leveraging functional sparsity in the RoPE method. It addresses the challenge of high memory consumption in Key Value (KV) caches when processing long inputs by dynamically predicting the importance of tokens based on the current query. Unlike previous methods that either risk losing important information or rely on insufficient heuristics, FASA identifies a small set of 'dominant' frequency chunks that correlate well with the full attention head. This allows FASA to significantly reduce memory bandwidth and computational costs while maintaining high performance across various long-context tasks, achieving near-optimal accuracy with minimal token retention."
                },
                "zh": {
                    "title": "FASAï¼šé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å¤„ç†æ¡†æ¶",
                    "desc": "FASAæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥çš„ä»¤ç‰Œé©±é€å’ŒRoPEä¸­çš„åŠŸèƒ½ç¨€ç–æ€§æ¥å‡å°‘KVç¼“å­˜çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡çš„LLMä»»åŠ¡ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€é¢„æµ‹ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥æ—¶çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚FASAçš„å…³é”®å‘ç°æ˜¯ï¼ŒæŸäº›â€œä¸»å¯¼â€é¢‘ç‡å—åœ¨ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«é‡è¦ä»¤ç‰Œã€‚é€šè¿‡ä»…å¯¹è¿™äº›ç»è¿‡ä¿®å‰ªçš„ä»¤ç‰Œè¿›è¡Œé›†ä¸­æ³¨æ„åŠ›è®¡ç®—ï¼ŒFASAæ˜¾è‘—é™ä½äº†å†…å­˜å¸¦å®½éœ€æ±‚å’Œè®¡ç®—æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04634",
            "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.04634",
            "abstract": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
            "score": 72,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "dad5d6d10152daac",
            "authors": [
                "Zelai Xu",
                "Zhexuan Xu",
                "Ruize Zhang",
                "Chunyang Zhu",
                "Shi Yu",
                "Weilin Liu",
                "Quanlu Zhang",
                "Wenbo Ding",
                "Chao Yu",
                "Yu Wang"
            ],
            "affiliations": [
                "EE, Tsinghua University",
                "IIIS, Tsinghua University",
                "Infinigence AI",
                "SIGS, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04634.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#small_models",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° WideSeek-R1, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 20 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ WideSeek-R1-4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ DeepSeek-R1-671B, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Scaling Information Seeking with Multi-Agent Reinforcement Learning",
                    "desc": "This paper introduces WideSeek-R1, a multi-agent system designed to enhance information seeking through parallel execution using reinforcement learning. Unlike traditional systems that depend on fixed workflows, WideSeek-R1 employs a lead-agent-subagent framework that allows for scalable orchestration of tasks. By leveraging a shared Large Language Model (LLM) and specialized tools, the system optimizes the collaboration between the lead agent and its subagents. The results demonstrate that WideSeek-R1 can achieve performance levels similar to larger single-agent systems while benefiting from increased efficiency as more subagents are added."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼šä¿¡æ¯è·å–çš„æ–°ç»´åº¦",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºWideSeek-R1çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å®ç°ä¿¡æ¯çš„å¹¶è¡Œè·å–ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€æ™ºèƒ½ä½“æ–¹æ³•ç›¸æ¯”ï¼ŒWideSeek-R1é€šè¿‡å¼•å…¥ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œä¼˜åŒ–äº†ä»»åŠ¡çš„ç»„ç»‡èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨å…±äº«çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ç”¨å·¥å…·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¹¿æ³›çš„ä¿¡æ¯æœç´¢ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWideSeek-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å®½åº¦æ‰©å±•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04145",
            "title": "Training Data Efficiency in Multimodal Process Reward Models",
            "url": "https://huggingface.co/papers/2602.04145",
            "abstract": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.",
            "score": 70,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "04258fd37246fe76",
            "authors": [
                "Jinyuan Li",
                "Chengsong Huang",
                "Langlin Huang",
                "Shaoyang Xu",
                "Haolin Liu",
                "Wenxuan Zhang",
                "Jiaxin Huang"
            ],
            "affiliations": [
                "Singapore University of Technology and Design",
                "University of Virginia",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04145.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (MPRM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‰Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ÑĞ¼ĞµÑĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Balanced-Information Score (BIS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ BIS, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 10% Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficient MPRM Training: 10% Data, Full Performance!",
                    "desc": "This paper introduces a method for training Multimodal Process Reward Models (MPRMs) more efficiently by using a Balanced-Information Score (BIS). The BIS focuses on optimizing the mixture of positive and negative labels and their reliability, which helps in reducing the amount of training data needed. The authors demonstrate that their approach can achieve the same performance as using the full dataset by only utilizing 10% of the training data. This method not only saves resources but also enhances the training process by addressing redundancy in existing Monte Carlo-annotated corpora."
                },
                "zh": {
                    "title": "é«˜æ•ˆè®­ç»ƒå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„å¹³è¡¡ä¿¡æ¯è¯„åˆ†",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆMPRMï¼‰çš„é«˜æ•ˆè®­ç»ƒæ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•é€šè¿‡å¹³è¡¡ä¿¡æ¯è¯„åˆ†æ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒMPRMçš„è®­ç»ƒåœ¨éšæœºæŠ½æ ·æ—¶ä¼šè¿…é€Ÿé¥±å’Œï¼Œè¡¨æ˜ç°æœ‰çš„è’™ç‰¹å¡æ´›æ ‡æ³¨æ•°æ®å­˜åœ¨å†—ä½™ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œå¼ºè°ƒæ ‡ç­¾æ··åˆå’Œæ ‡ç­¾å¯é æ€§å¯¹ä¿¡æ¯æ¢¯åº¦æ›´æ–°çš„é‡è¦æ€§ã€‚é€šè¿‡å¼•å…¥å¹³è¡¡ä¿¡æ¯è¯„åˆ†ï¼ˆBISï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œä¼˜å…ˆè€ƒè™‘æ ‡ç­¾çš„æ··åˆæ€§å’Œå¯é æ€§ï¼Œä»è€Œåœ¨ä»…ä½¿ç”¨10%çš„è®­ç»ƒæ•°æ®æ—¶å®ç°ä¸å…¨æ•°æ®ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04804",
            "title": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models",
            "url": "https://huggingface.co/papers/2602.04804",
            "abstract": "OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.",
            "score": 41,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "72a18b31bbdad9fb",
            "authors": [
                "Yue Ding",
                "Yiyan Ji",
                "Jungang Li",
                "Xuyang Liu",
                "Xinlong Chen",
                "Junfei Wu",
                "Bozhou Li",
                "Bohan Zeng",
                "Yang Shi",
                "Yushuo Guan",
                "Yuanxing Zhang",
                "Jiaheng Liu",
                "Qiang Liu",
                "Pengfei Wan",
                "Liang Wang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Nanjing University",
                "New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
                "Peking University",
                "Sichuan University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04804.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#video",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "OmniSIFT â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Omni-LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. Ğ’ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OmniSIFT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 25% Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Token Compression for Omni-LLMs with OmniSIFT",
                    "desc": "OmniSIFT is a new framework designed to compress tokens in Omni-modal Large Language Models (Omni-LLMs) while keeping their performance high. It uses a two-step approach: first, it prunes unnecessary video data to reduce redundancy, and second, it selects relevant audio tokens based on visual information. This method significantly lowers the computational load by using only a fraction of the original tokens, yet it still achieves better results than existing compression techniques. The framework is optimized to work efficiently, making it a valuable tool for improving the efficiency of multimodal AI systems."
                },
                "zh": {
                    "title": "OmniSIFTï¼šé«˜æ•ˆçš„å…¨æ¨¡æ€ä»¤ç‰Œå‹ç¼©æ¡†æ¶",
                    "desc": "OmniSIFTæ˜¯ä¸€ç§é’ˆå¯¹å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆOmni-LLMsï¼‰çš„éå¯¹ç§°ä»¤ç‰Œå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ—¶ç©ºè§†é¢‘å‰ªæå’Œè§†è§‰å¼•å¯¼éŸ³é¢‘é€‰æ‹©æ¥å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒå“è¶Šçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µå‹ç¼©ç­–ç•¥ï¼šé¦–å…ˆï¼Œé€šè¿‡æ—¶ç©ºè§†é¢‘å‰ªææ¨¡å—å»é™¤è§†é¢‘ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œå…¶æ¬¡ï¼Œé€šè¿‡è§†è§‰å¼•å¯¼éŸ³é¢‘é€‰æ‹©æ¨¡å—è¿‡æ»¤éŸ³é¢‘ä»¤ç‰Œã€‚OmniSIFTé€šè¿‡å¯å¾®åˆ†çš„ç›´é€šä¼°è®¡å™¨è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œç¡®ä¿äº†é«˜æ•ˆæ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniSIFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å‚æ•°é‡å’Œå»¶è¿Ÿä¸Šå‡ä¼˜äºç°æœ‰çš„å‹ç¼©åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03560",
            "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
            "url": "https://huggingface.co/papers/2602.03560",
            "abstract": "Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.",
            "score": 36,
            "issue_id": 914,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "d66619c54b016c43",
            "authors": [
                "Yizhao Gao",
                "Jianyu Wei",
                "Qihao Zhang",
                "Yu Cheng",
                "Shimao Chen",
                "Zhengju Tang",
                "Zihan Jiang",
                "Yifan Song",
                "Hailin Zhang",
                "Liang Zhao",
                "Bo Yang",
                "Gang Wang",
                "Shijie Cao",
                "Fuli Luo"
            ],
            "affiliations": [
                "Xiaomi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03560.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Hybrid Sparse Attention (HySparse), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ KV-ĞºĞµÑˆĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ…. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ KV-ĞºĞµÑˆĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ¸ 80B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ HySparse Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ²."
                },
                "en": {
                    "title": "Efficient Attention with Hybrid Sparse Architecture",
                    "desc": "The Hybrid Sparse Attention (HySparse) architecture combines full and sparse attention layers to enhance efficiency and performance in machine learning models. By using the output from full attention layers to guide the selection of tokens in sparse layers, HySparse eliminates the need for additional proxies that complicate token importance prediction. This method not only improves the accuracy of token selection but also allows for the reuse of key-value (KV) caches, significantly reducing both computational load and memory usage. Evaluations show that HySparse outperforms traditional attention methods, achieving better results with fewer full attention layers while minimizing KV cache storage requirements."
                },
                "zh": {
                    "title": "æ··åˆç¨€ç–æ³¨æ„åŠ›ï¼šé«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ··åˆç¨€ç–æ³¨æ„åŠ›æ¶æ„ï¼ˆHySparseï¼‰ï¼Œå®ƒå°†å…¨æ³¨æ„åŠ›å±‚ä¸å¤šä¸ªç¨€ç–æ³¨æ„åŠ›å±‚äº¤æ›¿ä½¿ç”¨ã€‚HySparseé€šè¿‡å‰é¢çš„å…¨æ³¨æ„åŠ›å±‚æ¥æŒ‡å¯¼ç¨€ç–å±‚çš„æ ‡è®°é€‰æ‹©å’ŒKVç¼“å­˜çš„é‡ç”¨ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ç›¸æ¯”ï¼ŒHySparseé¿å…äº†ä½¿ç”¨é¢å¤–çš„ä»£ç†æ¥é¢„æµ‹æ ‡è®°é‡è¦æ€§ï¼Œç›´æ¥åˆ©ç”¨å…¨æ³¨æ„åŠ›å±‚ä½œä¸ºå‡†ç¡®çš„å‚è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHySparseåœ¨å¤šä¸ªæ¨¡å‹è®¾ç½®ä¸­å‡ä¼˜äºå…¨æ³¨æ„åŠ›å’Œæ··åˆSWAåŸºçº¿ï¼Œæ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å­˜å‚¨éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04515",
            "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
            "url": "https://huggingface.co/papers/2602.04515",
            "abstract": "EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.",
            "score": 31,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "bea7b12e4a91de7b",
            "authors": [
                "Yu Bai",
                "MingMing Yu",
                "Chaojie Li",
                "Ziyi Bai",
                "Xinlong Wang",
                "BÃ¶rje F. Karlsson"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04515.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼: ÑĞ·Ñ‹Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "EgoActor â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ RGB-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ»Ğ¾ĞºĞ¾Ğ¼Ğ¾Ñ†Ğ¸Ñ, Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ ÑÑ€ĞµĞ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ 4B Ğ¸ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "EgoActor: Bridging Instructions to Humanoid Actions",
                    "desc": "EgoActor is a vision-language model designed to convert high-level instructions into specific actions for humanoid robots. It addresses the challenges of integrating perception, movement, and manipulation in dynamic environments with limited information. The model can predict various actions, such as walking and turning, while coordinating real-time perception and execution. Extensive testing shows that EgoActor can effectively link abstract planning with practical execution across different tasks and environments."
                },
                "zh": {
                    "title": "EgoActorï¼šå°†é«˜å±‚æŒ‡ä»¤è½¬åŒ–ä¸ºäººå½¢æœºå™¨äººåŠ¨ä½œçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "EgoActor æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†é«˜å±‚æŒ‡ä»¤è½¬åŒ–ä¸ºç²¾ç¡®çš„äººå½¢æœºå™¨äººåŠ¨ä½œã€‚å®ƒé€šè¿‡é›†æˆæ„ŸçŸ¥å’Œæ‰§è¡Œï¼Œè§£å†³äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè¿åŠ¨å’Œæ“ä½œçš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å„ç§è¿åŠ¨åŸè¯­å’Œäººæœºäº¤äº’ï¼Œå®æ—¶åè°ƒæ„ŸçŸ¥ä¸æ‰§è¡Œã€‚é€šè¿‡å¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„å¹¿æ³›ç›‘ç£ï¼ŒEgoActor èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡å’ŒæœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„å†³ç­–å’Œæµç•…çš„åŠ¨ä½œæ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02958",
            "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
            "url": "https://huggingface.co/papers/2602.02958",
            "abstract": "Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
            "score": 31,
            "issue_id": 913,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "26882eb2c6324f51",
            "authors": [
                "Haocheng Xi",
                "Shuo Yang",
                "Yilong Zhao",
                "Muyang Li",
                "Han Cai",
                "Xingyang Li",
                "Yujun Lin",
                "Zhuoyang Zhang",
                "Jintao Zhang",
                "Xiuyu Li",
                "Zhiying Xu",
                "Jun Wu",
                "Chenfeng Xu",
                "Ion Stoica",
                "Song Han",
                "Kurt Keutzer"
            ],
            "affiliations": [
                "MIT",
                "MIT-IBM Watson AI Lab",
                "Stanford University",
                "Tsinghua University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02958.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Quant VideoGen Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ KV ĞºĞµÑˆĞ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ KV ĞºĞµÑˆĞ° Ğ² 7 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ñ… Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ (Ğ¼ĞµĞ½ĞµĞµ 4%), Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Efficient Memory Management",
                    "desc": "Quant VideoGen (QVG) is a novel framework designed to overcome the limitations of KV cache memory in autoregressive video diffusion models. It introduces Semantic Aware Smoothing to effectively manage video spatiotemporal redundancy, resulting in lower magnitude residuals that are easier to quantize. Additionally, QVG employs Progressive Residual Quantization, which systematically reduces quantization errors while balancing memory usage and video quality. This approach significantly decreases KV cache memory requirements by up to 7 times with minimal impact on latency, while enhancing the overall quality of video generation compared to existing methods."
                },
                "zh": {
                    "title": "é‡åŒ–è§†é¢‘ç”Ÿæˆï¼šå†…å­˜ä¸è´¨é‡çš„å®Œç¾å¹³è¡¡",
                    "desc": "Quant VideoGenï¼ˆQVGï¼‰æ˜¯ä¸€ç§é’ˆå¯¹è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹çš„KVç¼“å­˜é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³KVç¼“å­˜å†…å­˜é™åˆ¶çš„é—®é¢˜ã€‚é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥å¹³æ»‘æŠ€æœ¯ï¼ŒQVGèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºå†—ä½™ï¼Œç”Ÿæˆä½å¹…åº¦ã€é€‚åˆé‡åŒ–çš„æ®‹å·®ã€‚å®ƒè¿˜å¼•å…¥äº†æ¸è¿›å¼æ®‹å·®é‡åŒ–æ–¹æ¡ˆï¼Œå‡å°‘é‡åŒ–è¯¯å·®ï¼ŒåŒæ—¶å®ç°è´¨é‡ä¸å†…å­˜çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQVGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†KVç¼“å­˜å†…å­˜ï¼Œä¸”ç”Ÿæˆè´¨é‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02402",
            "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
            "url": "https://huggingface.co/papers/2602.02402",
            "abstract": "SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
            "score": 29,
            "issue_id": 913,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "f63bd3f7711df065",
            "authors": [
                "Mu Huang",
                "Hui Wang",
                "Kerui Ren",
                "Linning Xu",
                "Yunsong Zhou",
                "Mulin Yu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University, China",
                "Shanghai Artificial Intelligence Laboratory, China",
                "Shanghai Jiao Tong University, China",
                "The Chinese University of Hong Kong, China",
                "The University of Hong Kong, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02402.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SoMA â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Gaussian Splat, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ², Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½ĞµÑĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ½Ğ° 20% Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². SoMA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞºĞ°Ğ½Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "SoMA: Revolutionizing Soft Body Manipulation with Neural Simulation",
                    "desc": "SoMA is a novel simulator designed for manipulating soft bodies in three dimensions. It integrates deformable dynamics, environmental forces, and robot actions into a single neural framework, allowing for more accurate and stable simulations. By using learned Gaussian splats, SoMA can model complex interactions without relying on predefined physics, enhancing its ability to generalize beyond previously observed scenarios. This results in improved performance in real-world tasks, such as cloth folding, with a significant increase in accuracy and stability during long-horizon manipulations."
                },
                "zh": {
                    "title": "SoMAï¼šç¨³å®šçš„è½¯ä½“æ“æ§æ–°æ–¹æ³•",
                    "desc": "SoMAæ˜¯ä¸€ç§3Dé«˜æ–¯ç‚¹æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨å®ç°è½¯ä½“ç‰©ä½“çš„ç¨³å®šå’Œé•¿æ—¶é—´æ“æ§ã€‚å®ƒå°†å¯å˜å½¢åŠ¨åŠ›å­¦ã€ç¯å¢ƒåŠ›å’Œæœºå™¨äººåŠ¨ä½œç»“åˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç¥ç»ç©ºé—´ä¸­ã€‚ä¸ä¼ ç»Ÿæ¨¡æ‹Ÿå™¨ä¸åŒï¼ŒSoMAä¸ä¾èµ–äºé¢„å®šä¹‰çš„ç‰©ç†æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ çš„é«˜æ–¯ç‚¹æ¥å»ºæ¨¡äº¤äº’ï¼Œä»è€Œæé«˜äº†é‡æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“æ§ä¸­æé«˜äº†20%çš„å‡†ç¡®æ€§ï¼Œä½¿å¾—å¤æ‚ä»»åŠ¡å¦‚é•¿æ—¶é—´çš„å¸ƒæ–™æŠ˜å å˜å¾—å¯è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02196",
            "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
            "url": "https://huggingface.co/papers/2602.02196",
            "abstract": "Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.",
            "score": 29,
            "issue_id": 914,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "b5317920aaabb251",
            "authors": [
                "Hang Yan",
                "Xinyu Che",
                "Fangzhi Xu",
                "Qiushi Sun",
                "Zichen Ding",
                "Kanzhi Cheng",
                "Jian Zhang",
                "Tao Qin",
                "Jun Liu",
                "Qika Lin"
            ],
            "affiliations": [
                "Nanjing University",
                "National University of Singapore",
                "Shanghai AI Laboratory",
                "The University of Hong Kong",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02196.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Test-Time Improvement (TTI) â€” Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ TIDE â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ TTI Ğ½Ğ° Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing LLM Performance Through Test-Time Improvement",
                    "desc": "This paper introduces Test-Time Improvement (TTI) for autonomous LLM agents, which enhances their performance through iterative interactions with their environment. It identifies shortcomings in current evaluation methods that fail to adequately measure task optimization efficiency and memory usage. To address these issues, the authors propose the Test-time Improvement Diagnostic Evaluation (TIDE) framework, which analyzes TTI through three key dimensions: task completion dynamics, recursive looping behaviors, and memory constraints. The findings suggest that optimizing agent performance involves improving the interaction dynamics rather than just increasing internal reasoning capabilities."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ä»£ç†ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæå‡æ€§èƒ½ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æµ‹è¯•æ—¶æ”¹è¿›ï¼ˆTTIï¼‰ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒäº†é€šè¿‡ä¸ç¯å¢ƒçš„è¿­ä»£äº’åŠ¨æ¥æå‡æ€§èƒ½çš„é‡è¦æ€§ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰ä»»åŠ¡ä¼˜åŒ–æ•ˆç‡å’Œå†…å­˜åˆ©ç”¨æƒ…å†µï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†æµ‹è¯•æ—¶æ”¹è¿›è¯Šæ–­è¯„ä¼°ï¼ˆTIDEï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†TTIåˆ†è§£ä¸ºä¸‰ä¸ªç›¸äº’å…³è”çš„ç»´åº¦ï¼Œè¯„ä¼°ä»»åŠ¡å®Œæˆçš„æ—¶é—´åŠ¨æ€ä»¥åŠæ€§èƒ½å—é™çš„åŸå› ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒTIDEè¡¨æ˜ï¼Œæå‡ä»£ç†æ€§èƒ½ä¸ä»…éœ€è¦æ‰©å±•å†…éƒ¨æ¨ç†èƒ½åŠ›ï¼Œè¿˜éœ€è¦ä¼˜åŒ–ä»£ç†ä¸ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨åŠ¨æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22954",
            "title": "Residual Context Diffusion Language Models",
            "url": "https://huggingface.co/papers/2601.22954",
            "abstract": "Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a \"remasking\" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.",
            "score": 28,
            "issue_id": 913,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "a191f56a527a7d38",
            "authors": [
                "Yuezhou Hu",
                "Harman Singh",
                "Monishwaran Maheswaran",
                "Haocheng Xi",
                "Coleman Hooper",
                "Jintao Zhang",
                "Aditya Tomar",
                "Michael W. Mahoney",
                "Sewon Min",
                "Mehrdad Farajtabar",
                "Kurt Keutzer",
                "Amir Gholami",
                "Chenfeng Xu"
            ],
            "affiliations": [
                "Apple",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22954.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "â™»ï¸",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ·Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ¾ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Residual Context Diffusion (RCD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€Ğµmasking Ğ² Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ñ…Ğ¾Ñ‚Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 5-10 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Recycling Discarded Tokens for Enhanced Language Model Performance",
                    "desc": "Residual Context Diffusion (RCD) is a novel approach that enhances diffusion large language models (dLLMs) by reusing information from discarded tokens during the decoding process. Traditional dLLMs often discard less confident tokens, which leads to wasted computational resources. RCD addresses this by converting these discarded tokens into contextual residuals, which are then reintegrated into the decoding steps, improving overall model accuracy. This method not only boosts performance significantly but also maintains low computational costs, making it an efficient upgrade for existing dLLMs."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹çš„æ®‹å·®ä¸Šä¸‹æ–‡åˆ©ç”¨",
                    "desc": "æ®‹å·®ä¸Šä¸‹æ–‡æ‰©æ•£ï¼ˆRCDï¼‰é€šè¿‡å›æ”¶è¢«ä¸¢å¼ƒçš„æ ‡è®°ä¿¡æ¯ï¼Œå¢å¼ºäº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡æ®‹å·®ï¼Œå°†ä¸¢å¼ƒçš„æ ‡è®°è¡¨ç¤ºè½¬åŒ–ä¸ºæœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ³¨å…¥åˆ°åç»­çš„å»å™ªæ­¥éª¤ä¸­ï¼Œä»è€Œæé«˜è§£ç çš„å‡†ç¡®æ€§ã€‚RCDé‡‡ç”¨è§£è€¦çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¿å…äº†åå‘ä¼ æ’­å¸¦æ¥çš„å†…å­˜ç“¶é¢ˆã€‚å®éªŒè¡¨æ˜ï¼ŒRCDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—å¼€é”€æå°çš„æƒ…å†µä¸‹ï¼Œæå‡å‰æ²¿æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04879",
            "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.04879",
            "abstract": "DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
            "score": 25,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "fe26031fa8cb13a1",
            "authors": [
                "Penghui Qi",
                "Xiangxin Zhou",
                "Zichen Liu",
                "Tianyu Pang",
                "Chao Du",
                "Min Lin",
                "Wee Sun Lee"
            ],
            "affiliations": [
                "School of Computing, National University of Singapore",
                "Sea AI Lab, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04879.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° LLM Ñ‡ĞµÑ€ĞµĞ· DPPO",
                    "desc": "DPPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ PPO Ğ¿Ñ€Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¾Ğµ ĞºĞ°Ğº Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°Ğ»Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Binary Ğ¸ Top-K, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "DPPO: A Smarter Way to Fine-Tune Language Models",
                    "desc": "This paper introduces Divergence Proximal Policy Optimization (DPPO), a new method for fine-tuning Large Language Models (LLMs) that improves upon the traditional Proximal Policy Optimization (PPO) algorithm. The authors argue that the ratio clipping mechanism in PPO is not effective for LLMs due to their large vocabularies, leading to inefficient and unstable training. DPPO replaces this clipping with direct policy divergence constraints, which provide a more accurate measure of policy changes. The paper also presents efficient approximations to manage memory usage, demonstrating that DPPO significantly enhances training stability and efficiency in reinforcement learning applications for LLMs."
                },
                "zh": {
                    "title": "DPPOï¼šæå‡å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„ç¨³å®šæ€§ä¸æ•ˆç‡",
                    "desc": "DPPOï¼ˆDivergence Proximal Policy Optimizationï¼‰é€šè¿‡ç”¨ç›´æ¥çš„ç­–ç•¥å‘æ•£çº¦æŸæ›¿ä»£PPOä¸­çš„æ¯”ç‡è£å‰ªï¼Œè§£å†³äº†PPOåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒä¸­çš„å±€é™æ€§ã€‚è¿™ç§æ–¹æ³•æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è¯æ±‡é‡æ—¶ã€‚ä¼ ç»Ÿçš„PPOæ–¹æ³•åœ¨æ›´æ–°ä½æ¦‚ç‡tokenæ—¶è¿‡åº¦æƒ©ç½šï¼Œè€Œå¯¹é«˜æ¦‚ç‡tokençš„å˜åŒ–åˆ™çº¦æŸä¸è¶³ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚DPPOé€šè¿‡å¼•å…¥é«˜æ•ˆçš„äºŒè¿›åˆ¶å’ŒTop-Kè¿‘ä¼¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒä½å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œå‡†ç¡®æ•æ‰ç­–ç•¥å‘æ•£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02990",
            "title": "Learning to Repair Lean Proofs from Compiler Feedback",
            "url": "https://huggingface.co/papers/2602.02990",
            "abstract": "Neural theorem provers can be improved through supervised learning on proof repair data that includes compiler feedback and diagnostic explanations, enabling better failure interpretation and correction.  \t\t\t\t\tAI-generated summary \t\t\t\t As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at https://huggingface.co/datasets/uw-math-ai/APRIL{this link}.",
            "score": 24,
            "issue_id": 932,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "52d39f9e7dcc3978",
            "authors": [
                "Evan Wang",
                "Simon Chess",
                "Daniel Lee",
                "Siyuan Ge",
                "Ajit Mallavarapu",
                "Vasily Ilin"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, University of Washington, Seattle, United States",
                "Department of Mathematics, University of Washington, Seattle, United States",
                "Math AI Lab, University of Washington, Seattle, United States"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02990.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#plp"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ APRIL Ñ 260 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing Neural Theorem Provers with Feedback-Driven Learning",
                    "desc": "This paper discusses how to enhance neural theorem provers by using supervised learning on proof repair data that includes feedback from compilers and explanations for errors. The authors identify a gap in existing datasets, which mostly contain correct proofs, limiting the ability to learn from mistakes. They introduce a new dataset called APRIL, which consists of 260,000 examples of faulty proofs paired with compiler diagnostics and suggested corrections. By training language models on this dataset, they demonstrate significant improvements in the accuracy of proof repairs and the ability to reason based on feedback."
                },
                "zh": {
                    "title": "é€šè¿‡ç›‘ç£å­¦ä¹ æå‡ç¥ç»å®šç†è¯æ˜å™¨çš„ä¿®å¤èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç›‘ç£å­¦ä¹ æ¥æ”¹è¿›ç¥ç»å®šç†è¯æ˜å™¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯æ˜ä¿®å¤æ•°æ®æ—¶ã€‚æˆ‘ä»¬å¼•å…¥äº†APRILæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«260,000ä¸ªç³»ç»Ÿç”Ÿæˆçš„è¯æ˜å¤±è´¥ä¸ç¼–è¯‘å™¨è¯Šæ–­çš„é…å¯¹ã€‚é€šè¿‡åœ¨APRILä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤å‡†ç¡®æ€§å’ŒåŸºäºåé¦ˆçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒåŸºäºè¯Šæ–­çš„ç›‘ç£ä¿¡å·å¯ä»¥ä½œä¸ºä½¿ç”¨åé¦ˆçš„è¯æ˜å™¨çš„è¡¥å……è®­ç»ƒä¿¡å·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03510",
            "title": "Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2602.03510",
            "abstract": "Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.",
            "score": 23,
            "issue_id": 915,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "56a039064235d5b2",
            "authors": [
                "Bozhou Li",
                "Yushuo Guan",
                "Haolin Li",
                "Bohan Zeng",
                "Yiyan Ji",
                "Yue Ding",
                "Pengfei Wan",
                "Kun Gai",
                "Yuanxing Zhang",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Kling Team, Kuaishou Technology",
                "Nanjing University",
                "Peking University",
                "School of Artificial Intelligence, University of"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03510.jpg",
            "data": {
                "categories": [
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² DiT-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ depth-wise semantic routing, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞµÑ‘ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ classifier-free guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Text-Image Alignment with Depth-wise Semantic Routing",
                    "desc": "This paper presents a new framework for improving text conditioning in DiT-based text-to-image models by using a method called Depth-wise Semantic Routing. The framework optimizes the hidden states of large language models (LLMs) across multiple layers, allowing for better alignment between text and images during the generation process. By employing lightweight gates for organizing these hidden states, the model enhances its ability to generate coherent and contextually relevant images. The findings indicate that this depth-wise approach significantly outperforms traditional methods, particularly in tasks requiring compositional understanding."
                },
                "zh": {
                    "title": "æ·±åº¦è¯­ä¹‰è·¯ç”±ï¼šæå‡æ–‡æœ¬ä¸å›¾åƒç”Ÿæˆçš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å½’ä¸€åŒ–å‡¸èåˆæ¡†æ¶ï¼Œä»¥å¢å¼ºåŸºäºDiTçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­çš„æ–‡æœ¬æ¡ä»¶åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡æ·±åº¦è¯­ä¹‰è·¯ç”±ä¼˜åŒ–å¤šå±‚å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€ï¼Œä»è€Œæ”¹å–„æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½å’Œç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ·±åº¦è¯­ä¹‰è·¯ç”±æ˜¯ä¼˜è¶Šçš„æ¡ä»¶ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æœ¬ä¸å›¾åƒçš„åŒ¹é…åº¦ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå•çº¯çš„æ—¶é—´èåˆå¯èƒ½ä¼šé™ä½è§†è§‰ç”Ÿæˆçš„è´¨é‡ï¼Œå› æ­¤éœ€è¦å…³æ³¨æ—¶é—´ä¾èµ–çš„ä¿¡å·ä»¥å®ç°æ›´ç¨³å¥çš„æ¡ä»¶åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03907",
            "title": "HY3D-Bench: Generation of 3D Assets",
            "url": "https://huggingface.co/papers/2602.03907",
            "abstract": "HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.",
            "score": 22,
            "issue_id": 914,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "8be299843765de41",
            "authors": [
                "Team Hunyuan3D",
                ":",
                "Bowen Zhang",
                "Chunchao Guo",
                "Dongyuan Guo",
                "Haolin Liu",
                "Hongyu Yan",
                "Huiwen Shi",
                "Jiaao Yu",
                "Jiachen Xu",
                "Jingwei Huang",
                "Kunhong Li",
                "Lifu Wang",
                "Linus",
                "Penghao Wang",
                "Qingxiang Lin",
                "Ruining Tang",
                "Xianghui Yang",
                "Yang Li",
                "Yirui Guan",
                "Yunfei Zhao",
                "Yunhan Yang",
                "Zeqiang Lai",
                "Zhihao Liang",
                "Zibo Zhao"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03907.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#dataset",
                    "#data",
                    "#synthetic",
                    "#open_source",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "HY3D-Bench â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ 250 Ñ‚Ñ‹ÑÑÑ‡ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ â€” ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ 125 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering 3D Creation with HY3D-Bench",
                    "desc": "HY3D-Bench is an open-source platform that enhances 3D content creation by providing a large library of high-quality 3D objects and synthetic assets. It addresses data processing challenges in the field by offering 250,000 meticulously curated 3D objects, which are ready for training with features like watertight meshes and multi-view renderings. The platform also introduces structured part-level decomposition, allowing for detailed perception and editing of 3D models. Additionally, it includes a scalable AIGC synthesis pipeline that generates 125,000 synthetic assets, improving diversity in 3D categories and supporting advancements in robotics and digital content creation."
                },
                "zh": {
                    "title": "HY3D-Benchï¼šæ¨åŠ¨3Dåˆ›ä½œçš„å¼€æºç”Ÿæ€ç³»ç»Ÿ",
                    "desc": "HY3D-Benchæ˜¯ä¸€ä¸ªå¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨åŠ¨3Då†…å®¹åˆ›ä½œçš„èƒ½åŠ›ã€‚å®ƒæä¾›äº†25ä¸‡ä¸ªé«˜ä¿çœŸ3Då¯¹è±¡å’Œåˆæˆèµ„äº§ï¼Œè§£å†³äº†æ•°æ®å¤„ç†ç“¶é¢ˆçš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç»“æ„åŒ–çš„éƒ¨ä»¶çº§åˆ†è§£ï¼Œæ”¯æŒç»†ç²’åº¦çš„æ„ŸçŸ¥å’Œå¯æ§ç¼–è¾‘ã€‚HY3D-Benchè¿˜é€šè¿‡å¯æ‰©å±•çš„AIGCåˆæˆç®¡é“ï¼Œå¢åŠ äº†12.5ä¸‡ä¸ªåˆæˆèµ„äº§ï¼Œæå‡äº†é•¿å°¾ç±»åˆ«çš„å¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03828",
            "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
            "url": "https://huggingface.co/papers/2602.03828",
            "abstract": "",
            "score": 19,
            "issue_id": 921,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "6493ae5590ba73a3",
            "authors": [
                "Minjun Zhu",
                "Zhen Lin",
                "Yixuan Weng",
                "Panzhong Lu",
                "Qiujie Xie",
                "Yifan Wei",
                "Sifan Liu",
                "Qiyao Sun",
                "Yue Zhang"
            ],
            "affiliations": [
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03828.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Hybrid Models: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the training process and the importance of regularization techniques to prevent overfitting."
                },
                "zh": {
                    "title": "æ·±åº¦å­¦ä¹ æ–°æ¨¡å‹ï¼Œæå‡å›¾åƒåˆ†ç±»ç²¾åº¦ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å›¾åƒä¸­çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚æ­¤ç ”ç©¶ä¸ºå›¾åƒå¤„ç†é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03143",
            "title": "Self-Hinting Language Models Enhance Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.03143",
            "abstract": "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution Ï„ conditioned on (x,h). Crucially, the task reward R(x,Ï„) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.",
            "score": 19,
            "issue_id": 913,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "2755aeac8258da3c",
            "authors": [
                "Baohao Liao",
                "Hanze Dong",
                "Xinxing Xu",
                "Christof Monz",
                "Jiang Bian"
            ],
            "affiliations": [
                "Language Technology Lab, University of Amsterdam",
                "Microsoft",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03143.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#open_source",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ GRPO ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SAGE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ñ‚ĞºĞ°Ñ‚Ñ‹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ, Ğ½Ğ¾ Ğ½Ğµ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ, Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "SAGE: Enhancing Learning Diversity with Self-Hints",
                    "desc": "SAGE is a new reinforcement learning framework that builds on Group Relative Policy Optimization (GRPO) by introducing self-hints during training. These self-hints help to diversify the outcomes when rewards are sparse, which prevents the model from getting stuck due to identical rewards in a group. By sampling hints that guide the model's learning process, SAGE reshapes the rollout distribution while keeping the task reward unchanged. Experiments show that SAGE outperforms GRPO across multiple benchmarks, demonstrating its effectiveness in improving the alignment of large language models."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æç¤ºæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "SAGEæ˜¯ä¸€ç§åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥è‡ªæˆ‘æç¤ºæ¥å¢å¼ºGRPOçš„æ•ˆæœï¼Œä»è€Œæé«˜åœ¨ç¨€ç–å¥–åŠ±ä¸‹çš„ç»“æœå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆç´§å‡‘çš„æç¤ºï¼Œå¸®åŠ©æ¨¡å‹åœ¨ç›¸åŒçš„ç»ˆç«¯éªŒè¯å¥–åŠ±ä¸‹é‡å¡‘å›æ»šåˆ†å¸ƒã€‚SAGEçš„å…³é”®åœ¨äºï¼Œå®ƒåœ¨æµ‹è¯•æ—¶ä¸ä½¿ç”¨ä»»ä½•æç¤ºï¼Œç¡®ä¿æ¨¡å‹åœ¨æ²¡æœ‰ç‰¹æƒä¿¡æ¯çš„æƒ…å†µä¸‹ä»èƒ½è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºGRPOï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03587",
            "title": "CL-bench: A Benchmark for Context Learning",
            "url": "https://huggingface.co/papers/2602.03587",
            "abstract": "Language models struggle with context learning, requiring new knowledge and reasoning beyond pre-training, as demonstrated by a comprehensive benchmark revealing poor performance on real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.",
            "score": 18,
            "issue_id": 920,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "66980819a7bff281",
            "authors": [
                "Shihan Dou",
                "Ming Zhang",
                "Zhangyue Yin",
                "Chenhao Huang",
                "Yujiong Shen",
                "Junzhe Wang",
                "Jiayi Chen",
                "Yuchen Ni",
                "Junjie Ye",
                "Cheng Zhang",
                "Huaibing Xie",
                "Jianglu Hu",
                "Shaolei Wang",
                "Weichao Wang",
                "Yanling Xiao",
                "Yiting Liu",
                "Zenan Xu",
                "Zhen Guo",
                "Pluto Zhou",
                "Tao Gui",
                "Zuxuan Wu",
                "Xipeng Qiu",
                "Qi Zhang",
                "Xuanjing Huang",
                "Yu-Gang Jiang",
                "Di Wang",
                "Shunyu Yao"
            ],
            "affiliations": [
                "Hunyuan Team, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03587.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑƒĞ¼ĞµÑÑ‚ Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ CL-bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 500 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ 1899 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 23,7% Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Unlocking Context Learning for Real-World AI",
                    "desc": "This paper addresses the limitations of current language models (LMs) in context learning, which is essential for understanding and solving complex real-world tasks. It introduces CL-bench, a new benchmark designed to evaluate LMs on their ability to learn from specific contexts and apply new knowledge beyond their pre-training. The benchmark includes 500 complex contexts and nearly 2,000 tasks, highlighting the need for models to integrate domain-specific knowledge and reasoning skills. Evaluations show that even the best models struggle with context learning, achieving only 23.7% task success, indicating a significant gap in their capabilities for real-world applications."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›",
                    "desc": "å½“å‰çš„è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡Œæ¨ç†æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„ç°å®ä»»åŠ¡æ—¶å´é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›ä»»åŠ¡éœ€è¦æ¨¡å‹ä»ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ æ–°çŸ¥è¯†ï¼Œå¹¶è¿›è¡Œæ¨ç†ä»¥è§£å†³é—®é¢˜ï¼Œè¿™ç§èƒ½åŠ›è¢«ç§°ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†CL-benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«500ä¸ªå¤æ‚ä¸Šä¸‹æ–‡å’Œ1899ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å¹³å‡è§£å†³ç‡ä»…ä¸º17.2%ï¼Œè¡¨æ˜ä¸Šä¸‹æ–‡å­¦ä¹ ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç“¶é¢ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04575",
            "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
            "url": "https://huggingface.co/papers/2602.04575",
            "abstract": "Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution.  \t\t\t\t\tAI-generated summary \t\t\t\t For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the Vibe AIGC, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.   Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.",
            "score": 17,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "78e831000eb7b72d",
            "authors": [
                "Jiaheng Liu",
                "Yuanxing Zhang",
                "Shihao Li",
                "Xinping Lei"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "NJU-LINK Team, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04575.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ",
                    "desc": "Vibe AIGC Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ AI, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Meta-Planner, Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑIntent-Execution Gap â€” Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¼Ñ‹ÑĞ»Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ AI Ğ¸Ğ· Ñ…Ñ€ÑƒĞ¿ĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging Human Intent and Machine Execution with Vibe AIGC",
                    "desc": "Vibe AIGC presents a new approach to generative AI that allows users to express their aesthetic and functional preferences more effectively. Instead of relying solely on traditional models, it utilizes multi-agent workflows to better align human intent with machine output. Users act as Commanders, providing a high-level 'Vibe' that guides the content generation process. This method aims to overcome the Intent-Execution Gap by transforming AI into a reliable partner in creating complex digital assets."
                },
                "zh": {
                    "title": "Vibe AIGCï¼šé‡å¡‘äººæœºåä½œçš„ç”Ÿæˆæ€§AIæ–°èŒƒå¼",
                    "desc": "Vibe AIGCæå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½èŒƒå¼ï¼Œç”¨æˆ·å¯ä»¥æä¾›é«˜å±‚æ¬¡çš„ç¾å­¦å’ŒåŠŸèƒ½åå¥½ã€‚è¿™äº›åå¥½é€šè¿‡å¤šä»£ç†å·¥ä½œæµè¿›è¡Œåè°ƒï¼Œæ—¨åœ¨ç¼©å°äººç±»æ„å›¾ä¸æœºå™¨æ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚ç”¨æˆ·çš„è§’è‰²ä»ä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹å¸ˆè½¬å˜ä¸ºæŒ‡æŒ¥å®˜ï¼Œæä¾›ä¸€ä¸ªåŒ…å«ç¾å­¦å’ŒåŠŸèƒ½é€»è¾‘çš„é«˜å±‚æ¬¡è¡¨ç¤ºã€‚é€šè¿‡é€»è¾‘ç¼–æ’çš„è½¬å˜ï¼ŒVibe AIGCå°†äººç±»æƒ³è±¡ä¸æœºå™¨æ‰§è¡Œè¿æ¥èµ·æ¥ï¼Œé‡æ–°å®šä¹‰äººæœºåä½œç»æµã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03973",
            "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
            "url": "https://huggingface.co/papers/2602.03973",
            "abstract": "Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering.  \t\t\t\t\tAI-generated summary \t\t\t\t Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/",
            "score": 17,
            "issue_id": 914,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "23559ab6c18e6f3d",
            "authors": [
                "Shuo Liu",
                "Ishneet Sukhvinder Singh",
                "Yiqing Xu",
                "Jiafei Duan",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "National University of Singapore",
                "University of Oxford",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03973.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#inference"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ flow-matching Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·-Ğ·Ğ° Ñ‚ĞµÑĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Vision-Language Steering (VLS) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 31% Ğ² CALVIN Ğ¸ 13% Ğ² LIBERO-PRO, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Franka."
                },
                "en": {
                    "title": "Adapt and Overcome: Vision-Language Steering for Robust Robotics",
                    "desc": "This paper addresses the limitations of pretrained diffusion and flow-matching policies in robotics when faced with changes in the environment during testing. These policies often fail due to their reliance on specific training conditions, which do not generalize well to new situations. To overcome this, the authors introduce Vision-Language Steering (VLS), a method that allows for real-time adaptation of robot actions without retraining. VLS utilizes vision-language models to adjust the robot's trajectory based on new observations, significantly improving performance in varied environments."
                },
                "zh": {
                    "title": "è§†è§‰-è¯­è¨€å¼•å¯¼ï¼šæ— è®­ç»ƒçš„æ¨ç†æ—¶é€‚åº”æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰-è¯­è¨€å¼•å¯¼ï¼ˆVLSï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨ç†æ—¶é€‚åº”å†»ç»“çš„ç”Ÿæˆæœºå™¨äººç­–ç•¥ã€‚ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ‰©æ•£æˆ–æµåŒ¹é…ç­–ç•¥åœ¨æµ‹è¯•æ—¶é‡åˆ°ç¯å¢ƒå˜åŒ–æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¸è®­ç»ƒé…ç½®ç´§å¯†è€¦åˆã€‚VLSé€šè¿‡åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°ï¼ŒæŒ‡å¯¼å»å™ªè¿‡ç¨‹ï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ç­–ç•¥å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ–°çš„ç©ºé—´å’Œä»»åŠ¡è¦æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLSåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œè¯„ä¼°ä¸­å‡ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†é€‚åº”èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03442",
            "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
            "url": "https://huggingface.co/papers/2602.03442",
            "abstract": "Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
            "score": 17,
            "issue_id": 913,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "317cbc0d4fd16984",
            "authors": [
                "Mingxuan Du",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Shaohan Wang",
                "Pengyu Wang",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology, Beijing, China",
                "University of Science and Technology of China, Hefei, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03442.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rag",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° A-RAG â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»Ğ¸Ğ±Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾, Ğ»Ğ¸Ğ±Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, A-RAG Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ A-RAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Models with Dynamic Retrieval Decisions",
                    "desc": "The paper introduces the Agentic RAG (A-RAG) framework, which allows language models to make dynamic retrieval decisions at different levels of detail. Unlike traditional retrieval-augmented generation (RAG) systems that use fixed algorithms or predefined workflows, A-RAG empowers models to adaptively utilize retrieval tools such as keyword search, semantic search, and chunk reading. This flexibility enables the model to efficiently scale its performance as it improves, leading to better results in open-domain question answering tasks. Experiments show that A-RAG outperforms existing methods while using fewer tokens, highlighting its effectiveness in leveraging advanced model capabilities."
                },
                "zh": {
                    "title": "åŠ¨æ€é€‚åº”çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºA-RAGçš„ä»£ç†æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€é€‚åº”å¤šå±‚æ¬¡çš„æ£€ç´¢å†³ç­–ã€‚ä¼ ç»Ÿçš„RAGç³»ç»Ÿæ— æ³•å……åˆ†åˆ©ç”¨å‰æ²¿è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œé€šå¸¸ä¾èµ–äºå•ä¸€æ£€ç´¢æˆ–é¢„å®šä¹‰å·¥ä½œæµã€‚A-RAGé€šè¿‡æä¾›å…³é”®å­—æœç´¢ã€è¯­ä¹‰æœç´¢å’Œå—è¯»å–ç­‰ä¸‰ç§æ£€ç´¢å·¥å…·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªç²’åº¦ä¸Šè‡ªé€‚åº”åœ°æœç´¢å’Œæ£€ç´¢ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒA-RAGåœ¨å¤šä¸ªå¼€æ”¾é¢†åŸŸé—®ç­”åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ£€ç´¢çš„ä»¤ç‰Œæ•°é‡ç›¸å½“æˆ–æ›´å°‘ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹èƒ½åŠ›çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.18207",
            "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
            "url": "https://huggingface.co/papers/2601.18207",
            "abstract": "Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.",
            "score": 16,
            "issue_id": 913,
            "pub_date": "2026-01-26",
            "pub_date_card": {
                "ru": "26 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 26",
                "zh": "1æœˆ26æ—¥"
            },
            "hash": "bd0f45c9106cc44d",
            "authors": [
                "James Burgess",
                "Jan N. Hansen",
                "Duo Peng",
                "Yuhui Zhang",
                "Alejandro Lozano",
                "Min Woo Sun",
                "Emma Lundberg",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Chan Zuckerberg Biohub Network",
                "KTH Royal Institute of Technology",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.18207.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rag",
                    "#science",
                    "#dataset",
                    "#agents",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¸Ğ½Ñƒ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑŒÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 16 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PaperSearchQA Ñ 60 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°."
                },
                "en": {
                    "title": "Empowering AI with Advanced Reasoning for Scientific Question-Answering",
                    "desc": "This paper presents a novel approach to training search agents that utilize reinforcement learning with verifiable rewards (RLVR) to improve technical question-answering in scientific domains. By focusing on a corpus of 16 million biomedical paper abstracts, the authors create a dataset called PaperSearchQA, which includes 60,000 factoid questions relevant to the scientific literature. The trained agents demonstrate advanced reasoning capabilities, outperforming traditional retrieval methods and showcasing behaviors such as planning and self-verification. This work not only enhances the relevance of AI in scientific research but also provides scalable methods for future applications in various scientific fields."
                },
                "zh": {
                    "title": "ç§‘å­¦è®ºæ–‡é—®ç­”çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœç´¢ä»£ç†ï¼Œä¸“é—¨ç”¨äºç§‘å­¦è®ºæ–‡çš„æŠ€æœ¯é—®ç­”ä»»åŠ¡ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±çš„æ–¹æ³•ï¼Œè¿™äº›ä»£ç†åœ¨å›ç­”æŠ€æœ¯æ€§é—®é¢˜æ—¶è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢æ–¹æ³•ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«1600ä¸‡ç¯‡ç”Ÿç‰©åŒ»å­¦è®ºæ–‡æ‘˜è¦çš„æœç´¢è¯­æ–™åº“ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªåä¸ºPaperSearchQAçš„é—®ç­”æ•°æ®é›†ï¼ŒåŒ…å«6ä¸‡ä¸ªå¯å›ç­”çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æœç´¢ä»£ç†èƒ½å¤Ÿè¿›è¡Œè§„åˆ’ã€æ¨ç†å’Œè‡ªæˆ‘éªŒè¯ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04816",
            "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
            "url": "https://huggingface.co/papers/2602.04816",
            "abstract": "Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
            "score": 15,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "5562f2949c282a62",
            "authors": [
                "Zhengqing Yuan",
                "Lichao Sun",
                "Yanfang",
                "Ye"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04816.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ¾Ñ‚ Ñ€Ğ¾Ğ»ĞµĞ¹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ñ‡ĞµÑ€ĞµĞ· Ñ…Ğ¾ÑÑ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ",
                    "desc": "Horizon-LM â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€Ğ¾Ğ»Ğ¸ CPU Ğ¸ GPU, ÑĞ´ĞµĞ»Ğ°Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ñ…Ğ¾ÑÑ‚-Ğ¼Ğ°ÑˆĞ¸Ğ½Ñƒ, Ğ° GPU â€” Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° GPU. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 120 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ H200 GPU Ñ 1.5 TB Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Horizon-LM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ñ…Ğ¾ÑÑ‚-Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ° Ğ½Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒĞ·Ğ»Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Large-Model Training with Horizon-LM",
                    "desc": "Horizon-LM is a novel training system designed to optimize large language models (LLMs) on single GPUs by changing how CPUs and GPUs interact. It shifts the primary role of memory management from GPUs to CPUs, allowing GPUs to function only as temporary computation units. This approach eliminates the need for persistent GPU memory, enabling the training of models with up to 120 billion parameters on a single GPU. By using techniques like explicit recomputation and pipelined execution, Horizon-LM significantly increases training efficiency and reduces memory constraints, making large-model training more accessible."
                },
                "zh": {
                    "title": "Horizon-LMï¼šå• GPU ä¸Šçš„å¤§æ¨¡å‹è®­ç»ƒæ–°çªç ´",
                    "desc": "Horizon-LM æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§æ¨¡å‹è®­ç»ƒä¸­ GPU å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å®ƒé€šè¿‡é‡æ–°å®šä¹‰ CPU å’Œ GPU çš„è§’è‰²ï¼Œå°†ä¸»å­˜å‚¨å™¨è§†ä¸ºä¸»è¦å‚æ•°å­˜å‚¨ï¼Œå¹¶å°† GPU ä»…ç”¨ä½œä¸´æ—¶è®¡ç®—å¼•æ“ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æ˜¾å¼é‡è®¡ç®—å’Œæµæ°´çº¿æ‰§è¡Œï¼Œæ¶ˆé™¤äº† GPU ä¸Šçš„æŒä¹…æ¨¡å—ï¼Œä»è€Œä½¿æ¨¡å‹è§„æ¨¡ä¸ GPU æ•°é‡è§£è€¦ã€‚Horizon-LM åœ¨å•ä¸ª GPU ä¸Šèƒ½å¤Ÿé«˜æ•ˆè®­ç»ƒé«˜è¾¾ 120B å‚æ•°çš„æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒååé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04735",
            "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
            "url": "https://huggingface.co/papers/2602.04735",
            "abstract": "Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.",
            "score": 13,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "71bcc623c3779bfc",
            "authors": [
                "Mengru Wang",
                "Zhenqian Xu",
                "Junfeng Fang",
                "Yunzhi Yao",
                "Shumin Deng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04735.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ° Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Data2Behavior â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Manipulating Data Features (MDF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ… ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MDF Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ 20% GPU-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Predicting Biases Before Training with Data2Behavior",
                    "desc": "Data2Behavior is a novel approach that predicts unintended behaviors in machine learning models before they are trained. It utilizes a method called Manipulating Data Features (MDF), which analyzes the features of training data to identify potential biases without needing to adjust model parameters. This proactive analysis helps in revealing latent statistical signals that could lead to safety risks. By using MDF, researchers can efficiently assess model vulnerabilities while significantly reducing computational costs compared to traditional fine-tuning methods."
                },
                "zh": {
                    "title": "åœ¨è®­ç»ƒå‰é¢„æµ‹æ¨¡å‹åè§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "Data2Behavior æ˜¯ä¸€ç§åœ¨è®­ç»ƒä¹‹å‰é¢„æµ‹æ¨¡å‹æ„å¤–è¡Œä¸ºçš„æ–¹æ³•ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œç§°ä¸º Manipulating Data Features (MDF)ï¼Œé€šè¿‡åˆ†ææ•°æ®ç‰¹å¾æ¥æ­ç¤ºæ½œåœ¨çš„åè§ï¼Œè€Œæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ã€‚MDF é€šè¿‡å°†å€™é€‰æ•°æ®çš„å‡å€¼è¡¨ç¤ºæ³¨å…¥åˆ°åŸºç¡€æ¨¡å‹çš„å‰å‘ä¼ æ’­ä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é¢„æµ‹æ¨¡å‹çš„æ½œåœ¨é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMDF åœ¨æ¶ˆè€—çº¦ 20% çš„ GPU èµ„æºçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå¯é åœ°é¢„æµ‹æ„å¤–è¡Œä¸ºå¹¶æä¾›é¢„è®­ç»ƒçš„è„†å¼±æ€§æ´å¯Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22859",
            "title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering",
            "url": "https://huggingface.co/papers/2601.22859",
            "abstract": "MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset of verifiable Docker environments.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.",
            "score": 13,
            "issue_id": 915,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "2d065538f3f284b2",
            "authors": [
                "Chuanzhe Guo",
                "Jingjing Wu",
                "Sijun He",
                "Yang Chen",
                "Zhaoqi Kuang",
                "Shilong Fan",
                "Bingjin Chen",
                "Siqi Bao",
                "Jing Liu",
                "Hua Wu",
                "Qingfu Zhu",
                "Wanxiang Che",
                "Haifeng Wang"
            ],
            "affiliations": [
                "Baidu Inc., Shenzhen, China",
                "Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology, Harbin, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22859.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#plp",
                    "#agents",
                    "#low_resource"
                ],
                "emoji": "ğŸ³",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MEnvAgent â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ€ĞµĞ´. ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MEnvBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 1000 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, MEnvAgent Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Fail-to-Pass Ğ½Ğ° 8,6% Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° 43%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ MEnvData-SWE â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Docker Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ."
                },
                "en": {
                    "title": "Automating Multi-Language Environment Construction for Software Engineering",
                    "desc": "MEnvAgent is a framework designed to automate the creation of software engineering environments across multiple programming languages. It uses a Planning-Execution-Verification architecture to efficiently handle the construction of these environments, addressing common failures autonomously. The framework also incorporates an Environment Reuse Mechanism, which minimizes computational costs by reusing previously built environments. Evaluations show that MEnvAgent significantly improves performance on a new benchmark, while also providing the largest open-source dataset of verifiable Docker environments for software engineering tasks."
                },
                "zh": {
                    "title": "MEnvAgentï¼šè‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ç¯å¢ƒæ„å»ºçš„å¤šè¯­è¨€æ¡†æ¶",
                    "desc": "MEnvAgentæ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„ç¯å¢ƒæ„å»ºã€‚å®ƒé‡‡ç”¨è§„åˆ’-æ‰§è¡Œ-éªŒè¯æ¶æ„å’Œç¯å¢ƒé‡ç”¨æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ„å»ºå¤±è´¥çš„é—®é¢˜ã€‚é€šè¿‡åœ¨MEnvBenchåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒMEnvAgentåœ¨å¤šä¸ªè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒFail-to-Passç‡æé«˜äº†8.6%ï¼Œæ—¶é—´æˆæœ¬é™ä½äº†43%ã€‚æ­¤å¤–ï¼ŒMEnvAgentè¿˜åˆ›å»ºäº†MEnvData-SWEï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¼€æºå¯éªŒè¯Dockerç¯å¢ƒæ•°æ®é›†ï¼Œä¿ƒè¿›äº†è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æŒç»­æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04284",
            "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.04284",
            "abstract": "Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.",
            "score": 12,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "cd96815789a76d79",
            "authors": [
                "Yansong Ning",
                "Jun Fang",
                "Naiqiang Tan",
                "Hao Liu"
            ],
            "affiliations": [
                "AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)",
                "CSE, The Hong Kong University of Science and Technology",
                "Didichuxing Co. Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04284.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#training",
                    "#rl"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent-Omit, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ¿Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºÑƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Agent-Omit-8B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimize Agent Interactions with Adaptive Omission",
                    "desc": "Agent-Omit is a novel training framework designed for large language model (LLM) agents to improve their efficiency during multi-turn interactions by selectively omitting unnecessary thoughts and observations. The framework addresses the issue that previous methods treated all interaction data equally, failing to recognize that the importance of thoughts and observations can vary from one turn to another. By conducting quantitative analyses, the authors demonstrate how these factors influence both the effectiveness and efficiency of agents. The proposed method includes a unique reinforcement learning approach that encourages agents to adaptively omit redundant information, leading to better performance compared to existing LLM agent strategies."
                },
                "zh": {
                    "title": "Agent-Omitï¼šæå‡å¤šè½®äº¤äº’æ•ˆç‡çš„æ™ºèƒ½çœç•¥æ¡†æ¶",
                    "desc": "Agent-Omitæ˜¯ä¸€ç§è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šè½®äº¤äº’ä¸­è‡ªé€‚åº”åœ°çœç•¥å†—ä½™çš„æ€ç»´å’Œè§‚å¯Ÿï¼Œä»è€Œåœ¨æ•ˆæœå’Œæ•ˆç‡ä¹‹é—´å®ç°æ›´å¥½çš„å¹³è¡¡ã€‚ç°æœ‰ç ”ç©¶æœªèƒ½è€ƒè™‘ä¸åŒè½®æ¬¡ä¸­æ€ç»´çš„å¿…è¦æ€§å’Œè§‚å¯Ÿçš„å®ç”¨æ€§ï¼Œå› æ­¤Agent-Omité€šè¿‡å®šé‡ç ”ç©¶è¿™äº›å› ç´ å¯¹ä»£ç†æ•ˆæœå’Œæ•ˆç‡çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆæˆå°‘é‡å†·å¯åŠ¨æ•°æ®æ¥å¾®è°ƒä»£ç†çš„çœç•¥è¡Œä¸ºï¼Œå¹¶å¼•å…¥äº†ä¸€ç§çœç•¥æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æ¿€åŠ±ä»£ç†çš„è‡ªé€‚åº”çœç•¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgent-Omitåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸å‰æ²¿LLMä»£ç†ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02160",
            "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use",
            "url": "https://huggingface.co/papers/2602.02160",
            "abstract": "",
            "score": 11,
            "issue_id": 921,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "a9b20d119694b3bd",
            "authors": [
                "Bowen Xu",
                "Shaoyu Wu",
                "Hao Jiang",
                "Kai Liu",
                "Xin Chen",
                "Lulu Hu",
                "Bin Yang"
            ],
            "affiliations": [
                "Alibaba Cloud Computing, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02160.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Hybrid Models: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ï¼Œæå‡æ¨¡å‹æ•ˆç‡ï¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–ç‰¹å¾é€‰æ‹©æ¥å‡å°‘è®¡ç®—å¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œè¿™é¡¹ç ”ç©¶ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03916",
            "title": "SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?",
            "url": "https://huggingface.co/papers/2602.03916",
            "abstract": "SpatiaLab presents a comprehensive benchmark for evaluating vision-language models' spatial reasoning capabilities across realistic, diverse scenarios, revealing significant gaps compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.",
            "score": 9,
            "issue_id": 918,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "1db96f60865a4e5a",
            "authors": [
                "Azmine Toushik Wasi",
                "Wahid Faisal",
                "Abdur Rahman",
                "Mahfuz Ahmed Anik",
                "Munem Shahriar",
                "Mohsin Mahmud Topu",
                "Sadia Tasnim Meem",
                "Rahatun Nesa Priti",
                "Sabrina Afroz Mitu",
                "Md. Iqramul Hoque",
                "Shahriyar Zaman Ridoy",
                "Mohammed Eunus Ali",
                "Majd Hawasly",
                "Mohammad Raza",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "BRAC University",
                "Computational Intelligence and Operations Laboratory (CIOL)",
                "Monash University",
                "North South University (NSU)",
                "Qatar Computing Research Institute (QCRI)",
                "Shahjalal University of Science and Technology (SUST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03916.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ SpatiaLab â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1400 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° â€” Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 54.93% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 87.57% Ñƒ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap in Spatial Reasoning for Vision-Language Models",
                    "desc": "SpatiaLab is a new benchmark designed to test how well vision-language models (VLMs) can understand spatial reasoning in real-world scenarios. Unlike previous studies that used simple or artificial environments, SpatiaLab includes 1,400 visual question-answer pairs across various categories like positioning, depth, and navigation. The results show that current VLMs significantly underperform compared to humans, with accuracy rates revealing a gap in understanding complex spatial relationships. This benchmark aims to highlight the limitations of VLMs and guide future research to improve their spatial reasoning abilities."
                },
                "zh": {
                    "title": "SpatiaLabï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "SpatiaLabæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†åŒ…å«1400ä¸ªè§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ç±»åˆ«ï¼Œæ—¨åœ¨åæ˜ ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ç©ºé—´å…³ç³»å’Œæ·±åº¦æ„ŸçŸ¥æ–¹é¢ã€‚SpatiaLabä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„è¯„ä¼°æ¡†æ¶ï¼Œå¸®åŠ©æ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02140",
            "title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2602.02140",
            "abstract": "Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.",
            "score": 9,
            "issue_id": 914,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "3baaec9a0a6e1234",
            "authors": [
                "Chenlong Wang",
                "Yuhang Chen",
                "Zhihan Hu",
                "Dongping Chen",
                "Wenhu Chen",
                "Sarah Wiegreffe",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "MBZUAI",
                "University of Maryland",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02140.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GapEval Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾."
                },
                "en": {
                    "title": "Bridging the Gap: Understanding vs. Generating in Unified Multimodal Models",
                    "desc": "This paper discusses the limitations of unified multimodal models (UMMs) in integrating understanding and generation tasks. It introduces GapEval, a benchmark that measures the coherence between these two capabilities in a model. The study finds that current UMMs only achieve superficial integration, with a significant gap between how they understand and generate information. Additionally, the research highlights that knowledge across different modalities often remains disconnected, indicating a need for deeper cognitive convergence in future models."
                },
                "zh": {
                    "title": "ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›çš„ç»Ÿä¸€ä¹‹è·¯ä»éœ€æ¢ç´¢",
                    "desc": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œè¡¨æ˜å®ƒä»¬çš„æ•´åˆä»…åœç•™åœ¨è¡¨é¢ï¼Œè€Œéæ·±å±‚æ¬¡çš„è®¤çŸ¥èåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GapEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒå‘åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é‡åŒ–ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„å·®è·ï¼Œå¹¶æµ‹é‡è¿™ä¸¤ç§â€œç»Ÿä¸€â€æ–¹å‘çš„è®¤çŸ¥ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¶æ„çš„UMMåœ¨è¿™ä¸¤ä¸ªæ–¹å‘ä¸Šå§‹ç»ˆå­˜åœ¨å·®è·ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹ä»…å®ç°äº†è¡¨å±‚ç»Ÿä¸€ï¼Œè€Œæœªèƒ½è¾¾åˆ°æ·±å±‚æ¬¡çš„è®¤çŸ¥èåˆã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼ŒUMMä¸­çš„çŸ¥è¯†å¾€å¾€æ˜¯åˆ†ç¦»çš„ï¼Œè·¨æ¨¡æ€çš„èƒ½åŠ›å’ŒçŸ¥è¯†å¹¶ä¸åŒæ­¥ï¼Œè¿™ä¸ºè¿›ä¸€æ­¥æ¢ç´¢æä¾›äº†æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02554",
            "title": "BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation",
            "url": "https://huggingface.co/papers/2602.02554",
            "abstract": "BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.",
            "score": 8,
            "issue_id": 913,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "b594dc20d8c25615",
            "authors": [
                "Jingwen Xu",
                "Yiyang Lu",
                "Zisu Huang",
                "Changze Lv",
                "Xiaohua Wang",
                "Shizheng Li",
                "Zhibo Xu",
                "Zhengkang Guo",
                "Zhengyuan Wang",
                "Muzhao Tian",
                "Xuanjing Huang",
                "Xiaoqing Zheng"
            ],
            "affiliations": [
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02554.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#low_resource",
                    "#plp",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "BatCoder Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· back-translation. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ LLM Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanEval Ğ¸ MBPP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 83.5% Ğ¸ 81.0% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ pass@1, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ baseline Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Code and Documentation Generation with BatCoder",
                    "desc": "BatCoder is a novel self-supervised reinforcement learning framework that enhances the generation of code and its corresponding documentation. It utilizes a back-translation method where documentation is created from code, and then this documentation is used to regenerate the original code, allowing the model to learn from the semantic similarities. This process provides an implicit reward signal for reinforcement learning, improving the model's ability to generate accurate code and documentation. By leveraging only code for training, BatCoder significantly increases the amount of available training data, leading to impressive performance on code-related benchmarks."
                },
                "zh": {
                    "title": "BatCoderï¼šä¼˜åŒ–ä»£ç ä¸æ–‡æ¡£ç”Ÿæˆçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶",
                    "desc": "BatCoderæ˜¯ä¸€ç§è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åå‘ç¿»è¯‘å…±åŒä¼˜åŒ–ä»£ç ç”Ÿæˆå’Œæ–‡æ¡£ç”Ÿæˆã€‚è¯¥æ–¹æ³•é¦–å…ˆä»ä»£ç ç”Ÿæˆæ–‡æ¡£ï¼Œç„¶ååˆ©ç”¨ç”Ÿæˆçš„æ–‡æ¡£é‡æ„åŸå§‹ä»£ç ï¼ŒäºŒè€…ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ä½œä¸ºéšå¼å¥–åŠ±ï¼Œä¿ƒè¿›æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚é€šè¿‡ä»…ä½¿ç”¨ä»£ç è¿›è¡Œè®­ç»ƒï¼ŒBatCoderæ˜¾è‘—å¢åŠ äº†å¯ç”¨çš„è®­ç»ƒæ ·æœ¬ï¼Œè§£å†³äº†é«˜è´¨é‡ä»£ç -æ–‡æ¡£å¯¹ç¨€ç¼ºçš„é—®é¢˜ã€‚åœ¨HumanEvalå’ŒMBPPåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBatCoderçš„è¡¨ç°ä¼˜äºè®¸å¤šå¼ºå¤§çš„å¼€æºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03979",
            "title": "Likelihood-Based Reward Designs for General LLM Reasoning",
            "url": "https://huggingface.co/papers/2602.03979",
            "abstract": "Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.",
            "score": 7,
            "issue_id": 914,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "a6bd9b45a789ae4d",
            "authors": [
                "Ariel Kwiatkowski",
                "Natasha Butt",
                "Ismail Labiad",
                "Julia Kempe",
                "Yann Ollivier"
            ],
            "affiliations": [
                "Meta FAIR",
                "New York University",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03979.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Log-Probability Rewards: A Game Changer for Fine-Tuning Reasoning in LLMs",
                    "desc": "This paper explores the effectiveness of using log-probability rewards for fine-tuning large language models (LLMs) in reasoning tasks. Unlike traditional binary rewards, log-probability rewards are derived from the likelihood of generating the correct answer, which allows for more nuanced feedback and is scalable. The authors demonstrate that log-probability rewards outperform binary rewards in both verifiable and non-verifiable reasoning benchmarks, leading to better performance in chain-of-thought (CoT) learning. This approach aligns with the next-token log-likelihood loss used during pretraining, making it a promising method for enhancing LLMs' reasoning capabilities."
                },
                "zh": {
                    "title": "å¯¹æ•°æ¦‚ç‡å¥–åŠ±ï¼šé“¾å¼æ€ç»´å¾®è°ƒçš„æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨æ¨ç†åŸºå‡†ä¸Šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ—¶ï¼Œä½¿ç”¨åŸºäºå¯¹æ•°æ¦‚ç‡çš„å¥–åŠ±å‡½æ•°çš„ä¼˜åŠ¿ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒå¥–åŠ±ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šçš„éªŒè¯å™¨ï¼Œå¹¶ä¸”å¯ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å‚è€ƒç­”æ¡ˆçš„å¯¹æ•°æ¦‚ç‡ä½œä¸ºé“¾å¼æ€ç»´å­¦ä¹ çš„å¥–åŠ±ï¼Œåœ¨å„ç§è®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¯éªŒè¯å’Œä¸å¯éªŒè¯çš„æ¨ç†åŸºå‡†ä¸Šã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™ä¸ºé“¾å¼æ€ç»´å¾®è°ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å¥–åŠ±æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿æ¥çŸ­æœŸå¯éªŒè¯å’Œé•¿æœŸä¸å¯éªŒè¯çš„ç­”æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01640",
            "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain",
            "url": "https://huggingface.co/papers/2602.01640",
            "abstract": "Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.",
            "score": 7,
            "issue_id": 913,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "968f7428d684ed07",
            "authors": [
                "Shuai Zhang",
                "Jiayu Hu",
                "Zijie Chen",
                "Zeyuan Ding",
                "Yi Zhang",
                "Yingji Zhang",
                "Ziyi Zhou",
                "Junwei Liao",
                "Shengjie Zhou",
                "Yong Dai",
                "Zhenzhong Lan",
                "Xiaozhu Ju"
            ],
            "affiliations": [
                "Beijing Innovatio",
                "Westlake University, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01640.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Agentic Automatic Evaluation (A2Eval) â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²ÑƒÑ… ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Data Agent ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ° Eval Agent ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° 85%, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 77% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ² 4.6 Ñ€Ğ°Ğ·Ğ°, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ½Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing VLM Assessment with Automated Evaluation",
                    "desc": "The paper introduces the Agentic Automatic Evaluation (A2Eval) framework, which automates the assessment of embodied vision-language models (VLMs) using collaborative agents. It addresses the limitations of traditional evaluation methods that rely on static benchmarks, which are often redundant and imbalanced, leading to high costs and skewed model rankings. A2Eval features a Data Agent that creates a balanced evaluation suite and an Eval Agent that validates evaluation processes, significantly reducing evaluation suite size and computational costs. The framework demonstrates improved ranking accuracy and efficiency, setting a new standard for evaluating embodied VLMs."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæå‡æ¨¡å‹è¯„ä¼°æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgentic Automatic Evaluation (A2Eval) çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„ç°æœ‰çš„å…·èº«è§†è§‰-è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚ä¼ ç»Ÿè¯„ä¼°ä¾èµ–äºé™æ€çš„ã€ä¸“å®¶å®šä¹‰çš„æ‰‹åŠ¨æ ‡æ³¨åŸºå‡†ï¼Œå­˜åœ¨å†—ä½™å’Œè¦†ç›–ä¸å‡çš„é—®é¢˜ï¼Œå¯¼è‡´é«˜æ˜‚çš„æˆæœ¬å’Œæ¨¡å‹æ’åå¤±çœŸã€‚A2Evalé€šè¿‡ä¸¤ä¸ªåä½œä»£ç†è‡ªåŠ¨åŒ–åŸºå‡†åˆ›å»ºå’Œè¯„ä¼°ï¼Œæ˜¾è‘—é™ä½äº†è¯„ä¼°å¥—ä»¶çš„è§„æ¨¡å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚ç»è¿‡10ä¸ªåŸºå‡†å’Œ13ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼ŒA2Evalå®ç°äº†85%çš„è¯„ä¼°å¥—ä»¶å‹ç¼©å’Œ77%çš„è®¡ç®—æˆæœ¬é™ä½ï¼Œç¡®ç«‹äº†é«˜ä¿çœŸã€ä½æˆæœ¬çš„è¯„ä¼°æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04486",
            "title": "Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition",
            "url": "https://huggingface.co/papers/2602.04486",
            "abstract": "MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit modality bias, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.",
            "score": 6,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "4754e7698d48a793",
            "authors": [
                "Jinlong Ma",
                "Yu Zhang",
                "Xuefeng Bai",
                "Kehai Chen",
                "Yuwei Wang",
                "Zeming Liu",
                "Jun Yu",
                "Min Zhang"
            ],
            "affiliations": [
                "Beijing University of Aeronautics and Astronautics",
                "Harbin Institute of Technology, Shenzhen, China",
                "Institute of Computing Technology Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04486.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#rlhf"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞœLLM Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ MCR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ñ… ÑÑ…ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing Cross-Modal Reasoning to Combat Modality Bias in GMNER",
                    "desc": "This paper addresses the issue of modality bias in Grounded Multimodal Named Entity Recognition (GMNER) tasks when using Multimodal Large Language Models (MLLMs). The authors propose a novel approach called Modality-aware Consistency Reasoning (MCR), which enhances cross-modal reasoning through techniques like Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI helps convert abstract constraints into actionable reasoning processes, while CVO allows the model to adjust its reasoning paths effectively. Experimental results show that MCR significantly reduces modality bias and outperforms existing methods in GMNER and visual grounding tasks."
                },
                "zh": {
                    "title": "æ¶ˆé™¤æ¨¡æ€åå·®ï¼Œæå‡å¤šæ¨¡æ€å®ä½“è¯†åˆ«æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŸºäºæ–‡æœ¬çš„å®ä½“è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ¨¡æ€åå·®çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ¨¡æ€æ„ŸçŸ¥ä¸€è‡´æ€§æ¨ç†ï¼ˆMCRï¼‰ï¼Œé€šè¿‡å¤šæ ·å¼æ¨ç†æ¨¡å¼æ³¨å…¥ï¼ˆMRSIï¼‰å’Œçº¦æŸå¼•å¯¼çš„å¯éªŒè¯ä¼˜åŒ–ï¼ˆCVOï¼‰æ¥å¢å¼ºè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚MRSIå°†æŠ½è±¡çº¦æŸè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æ¨ç†é“¾ï¼Œè€ŒCVOåˆ™ä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCRæœ‰æ•ˆå‡è½»äº†æ¨¡æ€åå·®ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.20499",
            "title": "Efficient Autoregressive Video Diffusion with Dummy Head",
            "url": "https://huggingface.co/papers/2601.20499",
            "abstract": "Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.",
            "score": 5,
            "issue_id": 916,
            "pub_date": "2026-01-28",
            "pub_date_card": {
                "ru": "28 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 28",
                "zh": "1æœˆ28æ—¥"
            },
            "hash": "ec5420af41fc1df3",
            "authors": [
                "Hang Guo",
                "Zhaoyang Jia",
                "Jiahao Li",
                "Bin Li",
                "Yuanhao Cai",
                "Jiangshan Wang",
                "Yawei Li",
                "Yan Lu"
            ],
            "affiliations": [
                "ETH Zurich",
                "Johns Hopkins University",
                "Microsoft Research Asia",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.20499.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#video",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Dummy Forcing: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ multi-head self-attention, Ğ³Ğ´Ğµ Ğ¾ĞºĞ¾Ğ»Ğ¾ 25% Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ ĞºĞ°Ğ´Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dummy Forcing, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ°ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° KV Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¼ĞµĞ½ĞµĞµ 0.5%) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ 24.3 FPS."
                },
                "en": {
                    "title": "Enhancing Video Diffusion Efficiency with Dummy Forcing",
                    "desc": "This paper addresses the inefficiencies in autoregressive video diffusion models caused by their attention mechanisms, which often do not fully utilize historical frames. The authors introduce a method called Dummy Forcing, which enhances efficiency by implementing heterogeneous memory allocation and dynamic head programming. This approach reduces redundancy in head-wise context and allows for better management of attention across different frames. As a result, Dummy Forcing achieves significant speed improvements in video generation while maintaining high quality, demonstrating a 2.0x speedup with minimal quality loss."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å†å²å¸§æ—¶æ•ˆç‡ä¸é«˜ï¼Œå¯¼è‡´æ³¨æ„åŠ›æœºåˆ¶çš„åˆ©ç”¨ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºDummy Forcingï¼Œé€šè¿‡å¼‚æ„å†…å­˜åˆ†é…å’ŒåŠ¨æ€å¤´ç¼–ç¨‹æ¥æé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•å‡å°‘äº†å¤´éƒ¨ä¸Šä¸‹æ–‡çš„å†—ä½™ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡æ‰“åŒ…æŠ€æœ¯å®ç°äº†æ›´å¼ºçš„ç¼“å­˜å‹ç¼©ã€‚Dummy Forcingåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå°†é€Ÿåº¦æå‡è‡³åŸºçº¿çš„2.0å€ï¼Œæ”¯æŒä»¥24.3å¸§æ¯ç§’çš„é€Ÿåº¦ç”Ÿæˆè§†é¢‘ï¼Œä¸”è´¨é‡ä¸‹é™ä¸åˆ°0.5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04442",
            "title": "No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data",
            "url": "https://huggingface.co/papers/2602.04442",
            "abstract": "Machine translation experiments for Turkic languages using nllb-200, LoRA fine-tuning, and prompt-based approaches achieved varying chrF++ scores across language pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.",
            "score": 4,
            "issue_id": 921,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "7dd630b48c5ca316",
            "authors": [
                "Dmitry Karpov"
            ],
            "affiliations": [
                "PAO Severstal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04442.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#synthetic",
                    "#training",
                    "#open_source",
                    "#rag",
                    "#machine_translation",
                    "#dataset",
                    "#small_models",
                    "#optimization"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‚ÑÑ€ĞºÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ÑƒÑÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ Ñ‚ÑÑ€ĞºÑĞºĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ°ÑˆĞºĞ¸Ñ€ÑĞºĞ¸Ğ¹, ĞºĞ°Ğ·Ğ°Ñ…ÑĞºĞ¸Ğ¹, ĞºĞ¸Ñ€Ğ³Ğ¸Ğ·ÑĞºĞ¸Ğ¹, Ñ‚Ğ°Ñ‚Ğ°Ñ€ÑĞºĞ¸Ğ¹ Ğ¸ Ñ‡ÑƒĞ²Ğ°ÑˆÑĞºĞ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²: Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ NLLB-200 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° LoRA Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ prompt-based Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ DeepSeek. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ chrF++ 49.71 Ğ´Ğ»Ñ ĞºĞ°Ğ·Ğ°Ñ…ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ° prompt-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Turkic Language Translation with Advanced Techniques",
                    "desc": "This paper investigates machine translation techniques for five Turkic language pairs, focusing on the effectiveness of the nllb-200 model. By applying LoRA fine-tuning on synthetic data, the authors achieved notable chrF++ scores, particularly 49.71 for Kazakh and 46.94 for Bashkir. Additionally, they explored prompt-based methods, which yielded a score of 39.47 for Chuvash using DeepSeek-V3.2. The study also highlights the performance of zero-shot and retrieval-based approaches for Tatar and Kyrgyz, and the authors provide the dataset and model weights for further research."
                },
                "zh": {
                    "title": "çªå¥è¯­è¨€æœºå™¨ç¿»è¯‘çš„æ–°æ¢ç´¢",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†äº”ç§çªå¥è¯­è¨€å¯¹çš„æœºå™¨ç¿»è¯‘ï¼ŒåŒ…æ‹¬ä¿„è¯­-å·´ä»€åŸºå°”è¯­ã€ä¿„è¯­-å“ˆè¨å…‹è¯­ã€ä¿„è¯­-å‰å°”å‰æ–¯è¯­ã€è‹±è¯­-å¡”å¡”å°”è¯­å’Œè‹±è¯­-æ¥šç“¦ä»€è¯­ã€‚é€šè¿‡å¯¹nllb-200-distilled-600Mæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œä½¿ç”¨åˆæˆæ•°æ®åœ¨å“ˆè¨å…‹è¯­å’Œå·´ä»€åŸºå°”è¯­ä¸Šåˆ†åˆ«è¾¾åˆ°äº†chrF++ 49.71å’Œ46.94çš„å¾—åˆ†ã€‚ä½¿ç”¨DeepSeek-V3.2è¿›è¡Œæç¤ºï¼Œç»“åˆæ£€ç´¢åˆ°çš„ç›¸ä¼¼ç¤ºä¾‹ï¼Œæ¥šç“¦ä»€è¯­çš„å¾—åˆ†ä¸º39.47ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ•°æ®é›†å’Œè·å¾—çš„æ¨¡å‹æƒé‡ï¼Œä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02350",
            "title": "Context Learning for Multi-Agent Discussion",
            "url": "https://huggingface.co/papers/2602.02350",
            "abstract": "Multi-Agent Discussion methods suffer from inconsistency due to individual context misalignment, which is addressed through a context learning approach that dynamically generates context instructions for each agent to improve consensus reaching and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.",
            "score": 4,
            "issue_id": 913,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "b15d12664950527b",
            "authors": [
                "Xingyuan Hua",
                "Sheng Yue",
                "Xinyi Li",
                "Yizhe Zhao",
                "Jinrui Zhang",
                "Ju Ren"
            ],
            "affiliations": [
                "College of Computer Science, Northwest University",
                "Department of Computer Science and Technology, Tsinghua University",
                "School of Cyber Science and Technology, Sun Yat-sen University",
                "Zhongguancun Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02350.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ M2CL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ€Ğ°ÑƒĞ½Ğ´Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº ÑˆÑƒĞ¼Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ M2CL Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 20-50% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Consensus in Multi-Agent Discussions with Dynamic Context Learning",
                    "desc": "This paper addresses the problem of inconsistency in Multi-Agent Discussion (MAD) methods caused by misaligned individual contexts among agents. It introduces a novel approach called Multi-LLM Context Learning (M2CL), which dynamically generates context instructions for each agent during discussions. By employing a self-adaptive mechanism, M2CL enhances context coherence and reduces discrepancies, allowing agents to reach a more accurate consensus. The results demonstrate that M2CL outperforms existing methods by a significant margin, improving performance on various complex tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ï¼Œæå‡å¤šæ™ºèƒ½ä½“è®¨è®ºä¸€è‡´æ€§",
                    "desc": "å¤šæ™ºèƒ½ä½“è®¨è®ºæ–¹æ³•ï¼ˆMADï¼‰åœ¨ä¸ªä½“ä¸Šä¸‹æ–‡ä¸ä¸€è‡´çš„æƒ…å†µä¸‹å®¹æ˜“å‡ºç°è®¨è®ºä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šLLMä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼ˆM2CLï¼‰ï¼Œè¯¥æ–¹æ³•ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å­¦ä¹ ä¸€ä¸ªä¸Šä¸‹æ–‡ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨æ¯è½®è®¨è®ºä¸­åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡æŒ‡ä»¤ã€‚M2CLé€šè¿‡è‡ªåŠ¨ä¿¡æ¯ç»„ç»‡å’Œç²¾ç‚¼ï¼Œæ§åˆ¶ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§å’Œè¾“å‡ºå·®å¼‚ï¼Œä»è€Œå¸®åŠ©æ™ºèƒ½ä½“é€æ­¥è¾¾æˆæ­£ç¡®çš„å…±è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM2CLåœ¨å­¦æœ¯æ¨ç†ã€å…·èº«ä»»åŠ¡å’Œç§»åŠ¨æ§åˆ¶ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°20%è‡³50%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04805",
            "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
            "url": "https://huggingface.co/papers/2602.04805",
            "abstract": "Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokensâ€”a learned discrete representation for skinning weightsâ€”and TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.",
            "score": 3,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "c8d5aa1f9ec328d2",
            "authors": [
                "Jia-peng Zhang",
                "Cheng-Feng Pu",
                "Meng-Hao Guo",
                "Yan-Pei Cao",
                "Shi-Min Hu"
            ],
            "affiliations": [
                "BNRist, Department of Computer Science and Technology, Tsinghua University, China",
                "VAST, China",
                "Zhili College, Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04805.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#3d",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ğ½Ğ³Ğ° 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ² SkinTokens â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² ÑĞºĞ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ TokenRig â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚ Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¶Ğ¸ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ SkinTokens Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing 3D Rigging with SkinTokens and TokenRig",
                    "desc": "This paper addresses the challenges of animation rigging in generative 3D models by introducing SkinTokens, a learned discrete representation for skinning weights. It reframes the skinning task from a complex regression problem to a more manageable token sequence prediction problem using a framework called TokenRig. This unified autoregressive model simultaneously learns the relationships between skeletons and skin deformations, enhancing rigging accuracy. The approach is further improved through reinforcement learning, resulting in significant gains in skinning accuracy and robustness for 3D content creation."
                },
                "zh": {
                    "title": "ç”Ÿæˆ3Dæ¨¡å‹çš„é«˜æ•ˆåŠ¨ç”»ç»‘å®šè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶é’ˆå¯¹ç”Ÿæˆ3Dæ¨¡å‹åœ¨åŠ¨ç”»ç»‘å®šä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SkinTokensï¼Œè¿™æ˜¯ä¸€ç§å­¦ä¹ çš„ç¦»æ•£è¡¨ç¤ºï¼Œç”¨äºçš®è‚¤æƒé‡çš„å»ºæ¨¡ã€‚é€šè¿‡å°†çš®è‚¤ç»‘å®šä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºæ›´æ˜“å¤„ç†çš„ä»¤ç‰Œåºåˆ—é¢„æµ‹é—®é¢˜ï¼Œç ”ç©¶è€…åˆ©ç”¨FSQ-CVAEæ•æ‰çš®è‚¤ç»‘å®šçš„å†…åœ¨ç¨€ç–æ€§ã€‚TokenRigæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå®ƒå°†éª¨éª¼å‚æ•°å’ŒSkinTokenså»ºæ¨¡ä¸ºä¸€ä¸ªå•ä¸€åºåˆ—ï¼Œä»è€Œå­¦ä¹ éª¨éª¼ä¸çš®è‚¤å˜å½¢ä¹‹é—´çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤æ‚èµ„äº§ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†æå‡ï¼ŒSkinTokensçš„è¡¨ç¤ºåœ¨çš®è‚¤ç»‘å®šç²¾åº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†98%-133%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01849",
            "title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models",
            "url": "https://huggingface.co/papers/2602.01849",
            "abstract": "Self-rewarding sequential Monte Carlo enables effective sampling of masked diffusion language models by using parallel diffusion processes and trajectory-level confidence signals to improve generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.",
            "score": 3,
            "issue_id": 917,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "c3ff136c0b47f18f",
            "authors": [
                "Ziwei Luo",
                "Ziqi Jin",
                "Lei Wang",
                "Lidong Bing",
                "Thomas B. SchÃ¶n"
            ],
            "affiliations": [
                "MiroMind AI, Singapore",
                "Nanyang Technological University, Singapore",
                "Uppsala University, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01849.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (SMC) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†Ğ°Ğ¼, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Diversity in Language Models with Self-Rewarding SMC",
                    "desc": "This paper introduces a new algorithm called self-rewarding sequential Monte Carlo (SMC) for improving the sampling process of masked diffusion language models (MDLMs). The authors highlight that traditional methods often use a confidence-based approach that limits diversity by only selecting the most confident tokens, leading to poor generation quality. To overcome this, the proposed SMC method utilizes multiple parallel diffusion processes, or particles, to explore different generation paths. By incorporating trajectory-level confidence as a self-rewarding signal, the algorithm effectively enhances the quality of generated samples without requiring additional training or external rewards."
                },
                "zh": {
                    "title": "è‡ªå¥–åŠ±åºåˆ—è’™ç‰¹å¡æ´›ï¼šæå‡æ©è”½æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå¥–åŠ±çš„åºåˆ—è’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰ç®—æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆé‡‡æ ·æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆMDLMsï¼‰ã€‚è¯¥ç®—æ³•é€šè¿‡è§‚å¯Ÿç°æœ‰MDLMsä¾èµ–äºåŸºäºç½®ä¿¡åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œè§£å†³äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§ä¸‹é™é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å¹¶è¡Œå¯åŠ¨å¤šä¸ªäº¤äº’çš„æ‰©æ•£è¿‡ç¨‹ï¼ˆç§°ä¸ºç²’å­ï¼‰æ¥æ¢ç´¢è½¨è¿¹ï¼Œå¹¶å¼•å…¥è½¨è¿¹çº§åˆ«çš„ç½®ä¿¡åº¦ä½œä¸ºè‡ªå¥–åŠ±ä¿¡å·ï¼Œä»¥åˆ†é…ç²’å­çš„é‡è¦æ€§æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªMDLMså’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†é‡‡æ ·è´¨é‡ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¥–åŠ±æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04883",
            "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
            "url": "https://huggingface.co/papers/2602.04883",
            "abstract": "PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias.  \t\t\t\t\tAI-generated summary \t\t\t\t We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.",
            "score": 2,
            "issue_id": 914,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "f2aa1a9a84c88dae",
            "authors": [
                "Yanru Qu",
                "Cheng-Yen Hsieh",
                "Zaixiang Zheng",
                "Ge Liu",
                "Quanquan Gu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04883.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞºÑƒĞ»ÑŒĞ¿Ñ‚ÑƒÑ€Ğ° Ğ±ĞµĞ»ĞºĞ¾Ğ²",
                    "desc": "PAR â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ downsampling Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…, Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, Ğ¸ flow-based Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ğ¾ÑÑ‚Ğ¾Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ exposure bias â€” Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ â€” Ñ‡ĞµÑ€ĞµĞ· noisy context learning Ğ¸ scheduled sampling. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ»ĞºĞ¸ Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "PAR: Sculpting Protein Structures with Multi-Scale Autoregressive Modeling",
                    "desc": "The paper introduces PAR, a novel multi-scale autoregressive framework designed for generating protein backbones. It utilizes a hierarchical structure modeling approach, where protein structures are created in a coarse-to-fine manner, allowing for detailed refinement at each scale. Key components include multi-scale downsampling for training, an autoregressive transformer for encoding information, and a flow-based decoder for generating backbone atoms. PAR effectively addresses exposure bias through techniques like noisy context learning, enabling high-quality protein structure generation with strong generalization capabilities."
                },
                "zh": {
                    "title": "PARï¼šè›‹ç™½è´¨ç»“æ„ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "PARæ˜¯ä¸€ç§å¤šå°ºåº¦è‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºè›‹ç™½è´¨ä¸»é“¾çš„ç”Ÿæˆã€‚å®ƒé€šè¿‡å±‚æ¬¡ç»“æ„å»ºæ¨¡ã€è‡ªå›å½’å˜æ¢å™¨å’ŒåŸºäºæµçš„è§£ç ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è›‹ç™½è´¨ç»“æ„ã€‚PARçš„å…³é”®åœ¨äºå¤šå°ºåº¦ä¸‹é‡‡æ ·æ“ä½œã€è‡ªå›å½’å˜æ¢å™¨å’Œæµå¼ä¸»é“¾è§£ç å™¨ï¼Œè¿™äº›ç»„ä»¶å…±åŒä½œç”¨ä»¥æé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚æ­¤å¤–ï¼ŒPARæœ‰æ•ˆç¼“è§£äº†è‡ªå›å½’æ¨¡å‹çš„æ›å…‰åå·®é—®é¢˜ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04651",
            "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF",
            "url": "https://huggingface.co/papers/2602.04651",
            "abstract": "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE",
            "score": 1,
            "issue_id": 922,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "3dff57d5bb02d752",
            "authors": [
                "Dipan Maity"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.04651.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rl",
                    "#optimization",
                    "#rlhf",
                    "#open_source"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ SAFE Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° PPO. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¿ĞµÑÑĞ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KL-Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°Ğ¼Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ°Ğ¼Ğ¸ PID. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° 5.15%, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ ÑĞ±Ğ¾ĞµĞ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "SAFE: A Stable Approach to Language Model Alignment",
                    "desc": "This paper introduces SAFE, a new reinforcement learning algorithm designed for aligning language models more effectively than the traditional Proximal Policy Optimization (PPO). SAFE enhances stability and performance by implementing a Double Soft-Min Critic for better value estimation and a multi-layer stabilization framework that includes entropy-aware KL divergence control. Unlike PPO, which uses fixed penalties, SAFE dynamically adjusts penalties based on the reward's rate of change, helping to prevent issues like reward oscillations and policy divergence. Experimental results demonstrate that SAFE outperforms PPO in training-average reward while maintaining computational efficiency and stability, making it suitable for real-world applications."
                },
                "zh": {
                    "title": "ç¨³å®šå¯¹é½ï¼Œæå‡æ€§èƒ½çš„å¼ºåŒ–å­¦ä¹ æ–°ç®—æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æ”¹å–„è¯­è¨€æ¨¡å‹å¯¹é½çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç®—æ³•åä¸ºSAFEï¼ˆç¨³å®šå¯¹é½å¾®è°ƒä¸ç†µæ„ŸçŸ¥æ§åˆ¶ï¼‰ï¼Œé€šè¿‡å¢å¼ºçš„KLæ•£åº¦æ§åˆ¶å’Œè‡ªé€‚åº”å¥–åŠ±ç®¡ç†æ¥ä¼˜åŒ–ä¼ ç»Ÿçš„PPOæ–¹æ³•ã€‚ä¸æ ‡å‡†PPOçš„å¯¹ç§°KLæƒ©ç½šä¸åŒï¼ŒSAFEèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æƒ©ç½šï¼Œä»¥åŒºåˆ†é«˜ç†µæ¢ç´¢å’Œä½ç†µæ¨¡å¼å´©æºƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAFEåœ¨è®­ç»ƒå¹³å‡å¥–åŠ±ä¸Šæ¯”PPOæé«˜äº†5.15%ï¼Œå¹¶ä¸”åœ¨KLæ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04605",
            "title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce",
            "url": "https://huggingface.co/papers/2602.04605",
            "abstract": "RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data.  \t\t\t\t\tAI-generated summary \t\t\t\t Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.",
            "score": 1,
            "issue_id": 913,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "fb4dcd00a576c30e",
            "authors": [
                "Rahul Bajaj",
                "Anuj Garg"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.04605.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… â€” ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°",
                    "desc": "RexBERT â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ BERT-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ecom-niverse, ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 350 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ€Ğ¾Ğ·Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ RexBERT Ñ 17M Ğ´Ğ¾ 400M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ in-domain Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ğ¶Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "RexBERT: Tailored Transformers for E-Commerce Excellence",
                    "desc": "RexBERT is a new family of BERT-style encoders tailored for e-commerce tasks, achieving better results through specialized pretraining and high-quality data. It utilizes a massive dataset called Ecom-niverse, which consists of 350 billion tokens from various retail sources, ensuring comprehensive coverage of e-commerce semantics. The training process involves a three-phase approach that enhances the model's ability to understand context and specialize in domain-specific tasks. Remarkably, RexBERT models, despite having fewer parameters than traditional encoders, outperform them in tasks like token classification and semantic similarity, proving that targeted training is more effective than simply increasing model size."
                },
                "zh": {
                    "title": "RexBERTï¼šç”µå­å•†åŠ¡è¯­ä¹‰çš„å¼ºå¤§ç¼–ç å™¨",
                    "desc": "RexBERTæ˜¯ä¸€ç§ä¸“ä¸ºç”µå­å•†åŠ¡è¯­ä¹‰è®¾è®¡çš„BERTé£æ ¼ç¼–ç å™¨å®¶æ—ï¼Œé€šè¿‡ä¸“é—¨çš„é¢„è®­ç»ƒå’Œé«˜è´¨é‡çš„é¢†åŸŸå†…æ•°æ®ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å‘å¸ƒäº†Ecom-niverseï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«3500äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ï¼Œä¸“é—¨ä»å„ç§é›¶å”®å’Œè´­ç‰©æ¥æºä¸­æ•´ç†è€Œæˆã€‚RexBERTçš„é¢„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸€èˆ¬é¢„è®­ç»ƒã€ä¸Šä¸‹æ–‡æ‰©å±•å’Œé€æ­¥é¢†åŸŸä¸“ä¸šåŒ–ä¸‰ä¸ªé˜¶æ®µã€‚å°½ç®¡å‚æ•°æ•°é‡æ¯”ä¸€èˆ¬ç¼–ç å™¨å°‘2-3å€ï¼ŒRexBERTåœ¨ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†æ›´å¤§çš„é€šç”¨ç¼–ç å™¨ï¼Œè¯æ˜äº†é«˜è´¨é‡é¢†åŸŸæ•°æ®ä¸åˆç†è®­ç»ƒæ–¹æ³•çš„ç»“åˆåœ¨ç”µå­å•†åŠ¡åº”ç”¨ä¸­æ›´å…·ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04581",
            "title": "Trust The Typical",
            "url": "https://huggingface.co/papers/2602.04581",
            "abstract": "A novel framework for LLM safety that treats safety as an out-of-distribution detection problem, achieving state-of-the-art performance without harmful example training through semantic space analysis and efficient GPU implementation.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.",
            "score": 1,
            "issue_id": 932,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "7cdb54c391c1cdfb",
            "authors": [
                "Debargha Ganguly",
                "Sreehari Sankar",
                "Biyao Zhang",
                "Vikash Singh",
                "Kanan Gupta",
                "Harshini Kavuru",
                "Alan Luo",
                "Weicong Chen",
                "Warren Morningstar",
                "Raghu Machiraju",
                "Vipin Chaudhary"
            ],
            "affiliations": [
                "Case Western Reserve University",
                "Google Research",
                "The Ohio State University",
                "University of Pittsburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04581.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#security",
                    "#open_source",
                    "#inference",
                    "#benchmark",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹: OOD Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ÑƒĞ³Ñ€Ğ¾Ğ·",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ (OOD detection). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº T3 Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ ĞºĞ°Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 18 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¶ĞµĞ¹Ğ»Ğ±Ñ€ĞµĞ¹ĞºĞ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² 40 Ñ€Ğ°Ğ·. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ GPU-Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ vLLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing LLM Safety with Out-of-Distribution Detection",
                    "desc": "This paper presents a new framework called Trust The Typical (T3) for ensuring the safety of large language models (LLMs) by treating safety as an out-of-distribution (OOD) detection problem. Instead of relying on identifying harmful examples, T3 focuses on understanding and modeling the distribution of safe prompts in a semantic space. The framework achieves state-of-the-art performance across various benchmarks without needing to train on harmful examples, significantly reducing false positive rates. Additionally, T3 is implemented efficiently on GPUs, allowing for real-time safety checks during token generation with minimal performance overhead."
                },
                "zh": {
                    "title": "å®‰å…¨æ€§æ–°è§†è§’ï¼šä»ç†è§£å®‰å…¨åˆ°æ£€æµ‹å¨èƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨æ¡†æ¶ï¼Œç§°ä¸ºTrust The Typicalï¼ˆT3ï¼‰ï¼Œå°†å®‰å…¨æ€§è§†ä¸ºä¸€ç§åˆ†å¸ƒå¤–æ£€æµ‹é—®é¢˜ã€‚T3é€šè¿‡åˆ†æè¯­ä¹‰ç©ºé—´ï¼Œå­¦ä¹ å¯æ¥å—æç¤ºçš„åˆ†å¸ƒï¼Œå¹¶å°†æ˜¾è‘—åç¦»æ ‡è®°ä¸ºæ½œåœ¨å¨èƒã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒT3ä¸éœ€è¦åœ¨æœ‰å®³ç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå´åœ¨18ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†è¯¯æŠ¥ç‡ã€‚è¯¥æ¨¡å‹ä»…åœ¨å®‰å…¨çš„è‹±è¯­æ–‡æœ¬ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿ç§»åˆ°å¤šç§é¢†åŸŸå’Œ14ç§è¯­è¨€ï¼Œä¸”åœ¨å¤§è§„æ¨¡å·¥ä½œè´Ÿè½½ä¸‹é›†æˆäº†GPUä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒç»­å®‰å…¨ç›‘æ§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04547",
            "title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
            "url": "https://huggingface.co/papers/2602.04547",
            "abstract": "OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.",
            "score": 1,
            "issue_id": 916,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "f43f95e3f4610b81",
            "authors": [
                "Luca Zedda",
                "Andrea Loddo",
                "Cecilia Di Ruberto"
            ],
            "affiliations": [
                "Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04547.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#science",
                    "#transfer_learning",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "OmniRad â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ foundation model, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² OmniRad Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 2.05% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ F1 Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Dice Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "OmniRad: Revolutionizing Radiology with Self-Supervised Learning",
                    "desc": "OmniRad is a self-supervised foundation model specifically designed for radiology, trained on a vast dataset of 1.2 million medical images. It enhances performance in both classification and segmentation tasks by leveraging representation reuse and enabling cross-task transferability. The model is evaluated through various adaptation methods, including lightweight task-specific adapters and full fine-tuning, demonstrating its versatility and effectiveness. Results show that OmniRad outperforms existing models in classification and segmentation benchmarks, indicating its potential for improving radiological analysis."
                },
                "zh": {
                    "title": "OmniRadï¼šæ”¾å°„å­¦ä»»åŠ¡çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹",
                    "desc": "OmniRadæ˜¯ä¸€ç§è‡ªç›‘ç£çš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ï¼Œç»è¿‡120ä¸‡å¼ åŒ»å­¦å›¾åƒçš„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹å¼ºè°ƒè¡¨ç¤ºé‡ç”¨å’Œè·¨ä»»åŠ¡å¯è½¬ç§»æ€§ï¼Œé€‚ç”¨äºå¤šç§æˆåƒæ¨¡å¼çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸‹æ¸¸é€‚åº”æ–¹æ¡ˆä¸‹è¯„ä¼°äº†é¢„è®­ç»ƒçš„ç¼–ç å™¨ï¼ŒåŒ…æ‹¬ä½¿ç”¨å†»ç»“ä¸»å¹²çš„è½»é‡çº§ä»»åŠ¡ç‰¹å®šé€‚é…å™¨å’Œå…¨ç«¯åˆ°ç«¯å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniRadåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šå‡ä¼˜äºå…¶ä»–åŸºç¡€æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„ç‰¹å¾èšç±»å’Œæ¨¡æ€åˆ†ç¦»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04289",
            "title": "Proxy Compression for Language Modeling",
            "url": "https://huggingface.co/papers/2602.04289",
            "abstract": "Proxy compression trains language models on both raw byte sequences and compressed views, enabling efficient training with end-to-end raw-byte inference while maintaining model robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.",
            "score": 1,
            "issue_id": 919,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "8eb67e4751e7216d",
            "authors": [
                "Lin Zheng",
                "Xinyu Li",
                "Qian Liu",
                "Xiachong Feng",
                "Lingpeng Kong"
            ],
            "affiliations": [
                "TikTok",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04289.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "ĞŸÑ€ÑĞ¼Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ±Ğ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑÑ‹Ñ€Ñ‹Ğµ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ°Ñ…, Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ±Ğ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ´Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Proxy Compression: Efficient Training with Raw Bytes and Compressed Views",
                    "desc": "This paper presents a new training method called proxy compression for language models. It allows models to learn from both raw byte sequences and their compressed versions, improving training efficiency. By aligning these two formats, the model can perform well even when it primarily trains on compressed data. The results show that proxy compression outperforms traditional methods, especially as the model size increases, while still using raw bytes for inference."
                },
                "zh": {
                    "title": "ä»£ç†å‹ç¼©ï¼šæå‡è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºä»£ç†å‹ç¼©çš„è®­ç»ƒæ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ–¹æ³•åŒæ—¶ä½¿ç”¨åŸå§‹å­—èŠ‚åºåˆ—å’Œå¤–éƒ¨å‹ç¼©è§†å›¾è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½å¤Ÿé«˜æ•ˆå¤„ç†åŸå§‹å­—èŠ‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å‹ç¼©åºåˆ—å’ŒåŸå§‹å­—èŠ‚ä¹‹é—´å®ç°å†…éƒ¨å¯¹é½ï¼Œä»è€Œå¢å¼ºä¸¤è€…ä¹‹é—´çš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»£ç†å‹ç¼©åœ¨ä»£ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹è¶…è¶Šäº†çº¯å­—èŠ‚çº§åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.04271",
            "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
            "url": "https://huggingface.co/papers/2602.04271",
            "abstract": "",
            "score": 1,
            "issue_id": 921,
            "pub_date": "2026-02-04",
            "pub_date_card": {
                "ru": "4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 4",
                "zh": "2æœˆ4æ—¥"
            },
            "hash": "0f4ed884dd76bdbc",
            "authors": [
                "Lifan Wu",
                "Ruijie Zhu",
                "Yubo Ai",
                "Tianzhu Zhang"
            ],
            "affiliations": [
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.04271.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.03955",
            "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
            "url": "https://huggingface.co/papers/2602.03955",
            "abstract": "AgentArk distills multi-agent reasoning dynamics into a single model through hierarchical distillation strategies, enabling efficient yet powerful reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.",
            "score": 1,
            "issue_id": 930,
            "pub_date": "2026-02-03",
            "pub_date_card": {
                "ru": "3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 3",
                "zh": "2æœˆ3æ—¥"
            },
            "hash": "ce617a98a37f9013",
            "authors": [
                "Yinyi Luo",
                "Yiqiao Jin",
                "Weichen Yu",
                "Mengqi Zhang",
                "Srijan Kumar",
                "Xiaoxiao Li",
                "Weijie Xu",
                "Xin Chen",
                "Jindong Wang"
            ],
            "affiliations": [
                "Amazon",
                "Carnegie Mellon University",
                "Georgia Institute of Technology",
                "University of British Columbia",
                "William & Mary"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.03955.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ° Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "AgentArk â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ fine-tuning Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Multi-Agent Reasoning in a Single Model",
                    "desc": "AgentArk introduces a method to simplify multi-agent reasoning into a single model using hierarchical distillation techniques. This approach allows the model to maintain the reasoning power of multiple agents while being computationally efficient. By focusing on training rather than inference, the model achieves strong reasoning abilities and self-correction similar to that of multi-agent systems. The paper explores various strategies to enhance the model's robustness and generalization across different reasoning tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ™ºèƒ½ä½“ï¼Œå¼ºå¤§æ¨ç†èƒ½åŠ›",
                    "desc": "AgentArké€šè¿‡å±‚æ¬¡è’¸é¦ç­–ç•¥å°†å¤šæ™ºèƒ½ä½“æ¨ç†åŠ¨æ€æç‚¼ä¸ºå•ä¸€æ¨¡å‹ï¼Œä»è€Œå®ç°é«˜æ•ˆè€Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ™ºèƒ½è½¬åŒ–ä¸ºå•ä¸ªæ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œé”™è¯¯ä¼ æ’­ã€‚ç ”ç©¶ä¸­æ¢è®¨äº†ä¸‰ç§å±‚æ¬¡è’¸é¦ç­–ç•¥ï¼ŒåŒ…æ‹¬å¢å¼ºæ¨ç†çš„å¾®è°ƒã€åŸºäºè½¨è¿¹çš„å¢å¼ºå’Œè¿‡ç¨‹æ„ŸçŸ¥è’¸é¦ã€‚æœ€ç»ˆï¼Œè’¸é¦æ¨¡å‹åœ¨ä¿æŒå•ä¸ªæ™ºèƒ½ä½“æ•ˆç‡çš„åŒæ—¶ï¼Œå±•ç°å‡ºå¤šæ™ºèƒ½ä½“çš„å¼ºæ¨ç†å’Œè‡ªæˆ‘ä¿®æ­£æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02863",
            "title": "\"I May Not Have Articulated Myself Clearly\": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time",
            "url": "https://huggingface.co/papers/2602.02863",
            "abstract": "Analysis of reasoning failures in large language models reveals that instability signals derived from token log probabilities and entropy can predict incorrect answers and distinguish between corrective and destructive instability based on timing of distribution shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model \"loses the thread\" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (corrective instability), whereas late instability is more often followed by failure (destructive instability), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.",
            "score": 1,
            "issue_id": 925,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "ac79778620477e44",
            "authors": [
                "Jinkun Chen",
                "Fengxiang Cheng",
                "Sijia Han",
                "Vlado Keselj"
            ],
            "affiliations": [
                "Dalhousie University",
                "Meta",
                "Tsinghua University",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02863.jpg",
            "data": {
                "categories": [
                    "#inference"
                ],
                "emoji": "ğŸ§µ",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ½Ğ¸Ñ‚ÑŒ: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ™ĞµĞ½ÑĞµĞ½Ğ°-Ğ¨ĞµĞ½Ğ½Ğ¾Ğ½Ğ° Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ) Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ, Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… (Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ) Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ API Ğ±ĞµĞ· ĞºĞ°ĞºĞ¾Ğ³Ğ¾-Ğ»Ğ¸Ğ±Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Predicting Failures in Language Models Through Instability Signals",
                    "desc": "This paper investigates how reasoning failures in large language models (LLMs) can be detected during the generation process rather than just at the end. It introduces a method that uses token log probabilities and entropy to create instability signals, which can predict when a model is likely to produce incorrect answers. The study finds that the timing of distribution shifts is crucial, as early instability can lead to correct answers while late instability often results in failure. This approach is model-agnostic and does not require any training, making it a useful diagnostic tool for understanding LLM behavior."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å¤±è´¥çš„ç¨³å®šæ€§ä¿¡å·",
                    "desc": "æœ¬æ–‡åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¨ç†å¤±è´¥ï¼Œå‘ç°é€šè¿‡ä»¤ç‰Œæ—¥å¿—æ¦‚ç‡å’Œç†µçš„ç¨³å®šæ€§ä¿¡å·å¯ä»¥é¢„æµ‹é”™è¯¯ç­”æ¡ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§å¯ä»¥é€šè¿‡ç®€å•çš„ä¿¡å·æ¥æ£€æµ‹ï¼Œè¿™ä¸ªä¿¡å·ç»“åˆäº†è¿ç»­æ­¥éª¤çš„åˆ†å¸ƒå˜åŒ–å’Œä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸åŒæ—¶é—´ç‚¹çš„ä¸ç¨³å®šæ€§å¯¹æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆæœ‰ä¸åŒçš„å½±å“ï¼Œæ—©æœŸçš„ä¸ç¨³å®šæ€§å¯èƒ½å¯¼è‡´æ­£ç¡®ç­”æ¡ˆï¼Œè€Œæ™šæœŸçš„ä¸ç¨³å®šæ€§åˆ™æ›´å¯èƒ½å¯¼è‡´å¤±è´¥ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹ï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯Šæ–­è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02495",
            "title": "Reward-free Alignment for Conflicting Objectives",
            "url": "https://huggingface.co/papers/2602.02495",
            "abstract": "A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
            "score": 1,
            "issue_id": 923,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "635e37fa950f4745",
            "authors": [
                "Peter Chen",
                "Xiaopeng Li",
                "Xi Chen",
                "Tianyi Lin"
            ],
            "affiliations": [
                "CUHK SZ",
                "Columbia University",
                "NYU Stern"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02495.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´: Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ñ†ĞµĞ»ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ RACO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ñ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ²ĞµÑĞ°Ğ¼ Ñ†ĞµĞ»ĞµĞ¹, ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen, Llama Ğ¸ Gemma Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Language Models Without Rewards: A New Approach to Conflicting Objectives",
                    "desc": "This paper introduces a new framework called Reward-free Alignment for Conflicted Objectives (RACO) to tackle the challenges of aligning large language models (LLMs) with multiple conflicting goals. It uses a special technique called conflict-averse gradient descent with clipping to resolve issues that arise when trying to improve several objectives at once. The authors demonstrate that their method can lead to better trade-offs between these objectives without relying on complex reward models. Experiments show that RACO outperforms existing methods in achieving optimal performance across various LLM architectures."
                },
                "zh": {
                    "title": "æ— å¥–åŠ±å¯¹é½ï¼šè§£å†³è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å†²çª",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— å¥–åŠ±å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹ä¸­çš„å¤šç›®æ ‡å†²çªé—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å†²çªè§„é¿çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•å¹¶è¿›è¡Œè£å‰ªï¼Œæ˜¾è‘—æ”¹å–„äº†ä¸åŒæ¨¡å‹æ¶æ„ä¹‹é—´çš„å¸•ç´¯æ‰˜æƒè¡¡ã€‚è¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨æˆå¯¹åå¥½æ•°æ®ï¼Œé¿å…äº†ä¼ ç»ŸåŠ æƒæŸå¤±æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç›®æ ‡æ‘˜è¦å’Œå®‰å…¨å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¤šç›®æ ‡å¯¹é½åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.02341",
            "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
            "url": "https://huggingface.co/papers/2602.02341",
            "abstract": "LongVPO is a two-stage Direct Preference Optimization framework that enables short-context vision-language models to understand ultra-long videos through synthetic preference triples and recursive captioning, achieving state-of-the-art performance with minimal human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.",
            "score": 1,
            "issue_id": 917,
            "pub_date": "2026-02-02",
            "pub_date_card": {
                "ru": "2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 2",
                "zh": "2æœˆ2æ—¥"
            },
            "hash": "4e672120bde8e3da",
            "authors": [
                "Zhenpeng Huang",
                "Jiaqi Li",
                "Zihan Jia",
                "Xinhao Li",
                "Desen Meng",
                "Lingxue Song",
                "Xi Chen",
                "Liang Li",
                "Limin Wang"
            ],
            "affiliations": [
                "JIUTIAN Research",
                "Shanghai AI Laboratory",
                "State Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.02341.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#video",
                    "#synthetic",
                    "#benchmark",
                    "#rlhf",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "LongVPO â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 16 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Long-Video Understanding with Minimal Annotations",
                    "desc": "LongVPO is a two-stage framework designed to enhance short-context vision-language models for understanding ultra-long videos without requiring extensive human annotations. In the first stage, it generates synthetic preference triples by linking questions to short video clips and filtering out irrelevant information to ensure clear guidance for the model. The second stage involves a recursive captioning process that creates detailed metadata for long videos, allowing the model to perform complex reasoning tasks using a large language model. This approach achieves state-of-the-art results on long-video benchmarks with minimal synthetic examples, demonstrating an efficient method for video comprehension."
                },
                "zh": {
                    "title": "é«˜æ•ˆç†è§£è¶…é•¿è§†é¢‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "LongVPOæ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„ç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©çŸ­æ—¶ä¸Šä¸‹æ–‡çš„è§†è§‰-è¯­è¨€æ¨¡å‹ç†è§£è¶…é•¿è§†é¢‘ï¼Œè€Œæ— éœ€é•¿è§†é¢‘æ³¨é‡Šã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å°†é—®é¢˜é”šå®šåˆ°å•ä¸ªçŸ­ç‰‡æ®µï¼Œåˆæˆåå¥½ä¸‰å…ƒç»„ï¼Œå¹¶ä½¿ç”¨è§†è§‰ç›¸ä¼¼æ€§å’Œé—®é¢˜ç‰¹å¼‚æ€§è¿‡æ»¤æ¥å‡å°‘ä½ç½®åå·®ã€‚ç¬¬äºŒé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬åœ¨é•¿è§†é¢‘ä¸Šé‡‡ç”¨é€’å½’å­—å¹•ç”Ÿæˆåœºæ™¯çº§å…ƒæ•°æ®ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ®µæ¨ç†æŸ¥è¯¢ï¼Œä»è€Œå¯¹æ¨¡å‹çš„åå¥½è¿›è¡Œå¯¹é½ã€‚LongVPOä»…ä½¿ç”¨16Kåˆæˆç¤ºä¾‹ï¼Œä¸”æ— éœ€æ˜‚è´µçš„äººç±»æ ‡ç­¾ï¼Œä¾¿åœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.22596",
            "title": "FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data",
            "url": "https://huggingface.co/papers/2601.22596",
            "abstract": "A large-scale building change detection dataset named FOTBCD is introduced, covering 28 French departments with high-resolution imagery and comprehensive annotations for both binary and instance-level change detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.",
            "score": 1,
            "issue_id": 923,
            "pub_date": "2026-01-30",
            "pub_date_card": {
                "ru": "30 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 30",
                "zh": "1æœˆ30æ—¥"
            },
            "hash": "fb49687b96207f19",
            "authors": [
                "Abdelrrahman Moubane"
            ],
            "affiliations": [
                "Retgen AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.22596.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FOTBCD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 28 Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ñ… Ğ´ĞµĞ¿Ğ°Ñ€Ñ‚Ğ°Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ (~28,000 Ğ¿Ğ°Ñ€ ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²) Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½ÑƒÑ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ´Ğ²Ğ¸Ğ³Ğ°."
                },
                "en": {
                    "title": "FOTBCD: Enhancing Building Change Detection with Geographic Diversity",
                    "desc": "The paper presents FOTBCD, a comprehensive dataset for building change detection that includes high-resolution imagery and detailed annotations across 28 departments in France. It supports both binary and instance-level change detection tasks, making it versatile for various machine learning applications. The dataset is designed to facilitate large-scale benchmarking and is particularly valuable for evaluating models under geographic domain shifts. Empirical results demonstrate that the geographic diversity of FOTBCD enhances the generalization capabilities of change detection algorithms compared to existing datasets."
                },
                "zh": {
                    "title": "FOTBCDï¼šæ¨åŠ¨å»ºç­‘å˜åŒ–æ£€æµ‹çš„åœ°ç†å¤šæ ·æ€§",
                    "desc": "FOTBCDæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å»ºç­‘å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†æ³•å›½28ä¸ªçœä»½ï¼Œæä¾›é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå…¨é¢çš„æ³¨é‡Šã€‚è¯¥æ•°æ®é›†æ”¯æŒäºŒå…ƒå’Œå®ä¾‹çº§å˜åŒ–æ£€æµ‹ä»»åŠ¡ï¼ŒåŒ…å«çº¦28,000å¯¹å‰åå›¾åƒåŠå…¶åƒç´ çº§å˜åŒ–æ©è†œã€‚FOTBCDçš„è®¾è®¡æ—¨åœ¨è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨åœ°ç†é¢†åŸŸè½¬ç§»ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿æ ‡ç­¾è´¨é‡ã€‚é€šè¿‡ä¸å…¶ä»–æ•°æ®é›†çš„æ¯”è¾ƒï¼ŒFOTBCDå±•ç¤ºäº†åœ°ç†å¤šæ ·æ€§ä¸å»ºç­‘å˜åŒ–æ£€æµ‹çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„å…³è”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.01031",
            "title": "HalluHard: A Hard Multi-Turn Hallucination Benchmark",
            "url": "https://huggingface.co/papers/2602.01031",
            "abstract": "Large language models continue to generate plausible but ungrounded factual claims in multi-turn dialogue, with hallucinations remaining significant even when utilizing web search for verification across high-stakes domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce HalluHard, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search (approx 30% for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.",
            "score": 0,
            "issue_id": 930,
            "pub_date": "2026-02-01",
            "pub_date_card": {
                "ru": "1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 1",
                "zh": "2æœˆ1æ—¥"
            },
            "hash": "bf75b93352357023",
            "authors": [
                "Dongyang Fan",
                "Sebastien Delsad",
                "Nicolas Flammarion",
                "Maksym Andriushchenko"
            ],
            "affiliations": [
                "ELLIS Institute Tubingen",
                "EPFL",
                "Max Planck Institute for Intelligent Systems",
                "Tubingen AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.01031.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸš¨",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ°Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HalluHard Ñ 950 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… (Ğ¿Ñ€Ğ°Ğ²Ğ¾, Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ñ‰ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² 30% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Tackling Hallucinations in Multi-Turn Dialogue with HalluHard",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs), where they generate plausible but factually incorrect statements during multi-turn dialogues. The authors introduce HalluHard, a benchmark designed to evaluate hallucination in high-stakes domains like law, medicine, and coding, requiring models to provide inline citations for their claims. They propose a judging pipeline that uses web search to verify the accuracy of these citations, revealing that even the best models still produce significant hallucinations. The study finds that factors such as model capacity and the context of the dialogue influence the frequency and nature of these hallucinations."
                },
                "zh": {
                    "title": "åº”å¯¹å¤šè½®å¯¹è¯ä¸­çš„å¹»è§‰é—®é¢˜",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­ä»ç„¶ä¼šç”Ÿæˆçœ‹ä¼¼åˆç†ä½†ç¼ºä¹ä¾æ®çš„äº‹å®å£°æ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡å¢åŠ æ—¶ï¼Œæ—©æœŸé”™è¯¯ä¼šåŠ å‰§ã€‚æˆ‘ä»¬æå‡ºäº†HalluHardï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®å¹»è§‰åŸºå‡†ï¼Œæ¶µç›–æ³•å¾‹æ¡ˆä¾‹ã€ç ”ç©¶é—®é¢˜ã€åŒ»ç–—æŒ‡å—å’Œç¼–ç ç­‰å››ä¸ªé«˜é£é™©é¢†åŸŸï¼Œå…±æœ‰950ä¸ªç§å­é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è¦æ±‚å¯¹äº‹å®å£°æ˜è¿›è¡Œå†…è”å¼•ç”¨æ¥å®ç°äº‹å®ä¾æ®çš„æ“ä½œåŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨ç½‘ç»œæœç´¢ï¼Œå¹»è§‰ç°è±¡ä»ç„¶æ˜¾è‘—ï¼Œä¸”æ¨¡å‹çš„èƒ½åŠ›ã€è½®æ¬¡ä½ç½®ã€æœ‰æ•ˆæ¨ç†å’Œæ‰€éœ€çŸ¥è¯†ç±»å‹éƒ½ä¼šå½±å“å¹»è§‰è¡Œä¸ºã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-05.html",
    "link_next": "2026-02-09.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "05.02",
        "en": "02/05",
        "zh": "2æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "09.02",
        "en": "02/09",
        "zh": "2æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 2,
        "#benchmark": 16,
        "#agents": 7,
        "#cv": 5,
        "#rl": 9,
        "#rlhf": 6,
        "#rag": 3,
        "#plp": 3,
        "#inference": 10,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 11,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 19,
        "#robotics": 3,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 14,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 4,
        "#synthetic": 3,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 12,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 3
    }
}