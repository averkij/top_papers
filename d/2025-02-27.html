
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. February 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 февраля</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-26.html">⬅️ <span id="prev-date">26.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-28.html">➡️ <span id="next-date">28.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'};
        let feedDateNext = {'ru': '28.02', 'en': '02/28', 'zh': '2月28日'};
        let feedDatePrev = {'ru': '26.02', 'en': '02/26', 'zh': '2月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.18411', 'title': 'OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference', 'url': 'https://huggingface.co/papers/2502.18411', 'abstract': "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.", 'score': 54, 'issue_id': 2409, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'a37015745aae1e1d', 'authors': ['Xiangyu Zhao', 'Shengyuan Ding', 'Zicheng Zhang', 'Haian Huang', 'Maosong Cao', 'Weiyun Wang', 'Jiaqi Wang', 'Xinyu Fang', 'Wenhai Wang', 'Guangtao Zhai', 'Haodong Duan', 'Hua Yang', 'Kai Chen'], 'affiliations': ['Fudan University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18411.jpg', 'data': {'categories': ['#alignment', '#training', '#benchmark', '#multimodal', '#dataset', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Приведение мультимодальных ИИ-моделей в соответствие с человеческими ценностями', 'desc': 'Статья представляет OmniAlign-V - набор данных для улучшения мультимодальных языковых моделей (MLLM) в соответствии с человеческими предпочтениями. Авторы также разработали MM-AlignBench - бенчмарк для оценки соответствия MLLM человеческим ценностям. Эксперименты показывают, что дообучение на OmniAlign-V значительно улучшает согласованность моделей с предпочтениями людей. Датасеты, бенчмарк и код доступны в открытом доступе.'}, 'en': {'title': 'Aligning MLLMs with Human Preferences through OmniAlign-V', 'desc': 'This paper presents OmniAlign-V, a new dataset containing 200,000 high-quality training samples that include diverse images and complex questions to help multi-modal large language models (MLLMs) better align with human preferences. The authors also introduce MM-AlignBench, a benchmark for evaluating how well MLLMs reflect human values. By fine-tuning MLLMs with the OmniAlign-V dataset using techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), the models show improved alignment with human preferences while maintaining their performance on standard Visual Question Answering (VQA) tasks. The resources, including datasets and benchmarks, are made publicly available to support further research in this area.'}, 'zh': {'title': '提升多模态模型与人类偏好的对齐', 'desc': '本文介绍了OmniAlign-V，这是一个包含20万个高质量训练样本的综合数据集，旨在提高多模态大型语言模型（MLLMs）与人类偏好的对齐。数据集中包含多样的图像、复杂的问题和多种响应格式，以增强模型的适应性。我们还提出了MM-AlignBench，这是一个专门设计的人类标注基准，用于评估MLLMs与人类价值观的对齐程度。实验结果表明，使用OmniAlign-V进行微调显著提高了模型的人类偏好对齐，同时保持或提升了在标准视觉问答基准上的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.18137', 'title': 'SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference', 'url': 'https://huggingface.co/papers/2502.18137', 'abstract': 'An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.', 'score': 41, 'issue_id': 2409, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '1029ef0dffc41bba', 'authors': ['Jintao Zhang', 'Chendong Xiang', 'Haofeng Huang', 'Jia Wei', 'Haocheng Xi', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.18137.jpg', 'data': {'categories': ['#architecture', '#inference', '#video', '#optimization', '#cv'], 'emoji': '🚀', 'ru': {'title': 'Универсальное разреженное внимание для ускорения нейросетей', 'desc': 'В статье представлен новый метод SpargeAttn для оптимизации внимания в нейронных сетях. Он использует двухэтапный онлайн-фильтр для предсказания карты внимания и пропуска некоторых матричных умножений. SpargeAttn универсален и применим к различным моделям, включая генерацию текста, изображений и видео. Эксперименты показывают значительное ускорение работы моделей без потери качества.'}, 'en': {'title': 'Accelerating Attention with SpargeAttn: Speed Meets Efficiency', 'desc': 'This paper introduces SpargeAttn, a novel approach to implementing sparse attention in machine learning models. It addresses the challenge of quadratic time complexity in attention mechanisms by leveraging the inherent sparsity of attention maps. The proposed method employs a two-stage online filtering process to efficiently predict and optimize the attention map, allowing for the omission of unnecessary computations. Experimental results demonstrate that SpargeAttn accelerates various models across different domains, such as language and image processing, while maintaining high performance metrics.'}, 'zh': {'title': '通用稀疏注意力，提升模型计算效率！', 'desc': '在大型模型中，高效的注意力实现至关重要，因为其时间复杂度为平方级。幸运的是，注意力通常表现出稀疏性，即注意力图中的许多值接近于零，这使得可以省略相应的计算。本文提出了一种名为SpargeAttn的通用稀疏和量化注意力机制，能够加速各种模型的计算，同时保持端到端的性能。我们的方法通过两阶段的在线过滤器来实现，第一阶段快速准确地预测注意力图，第二阶段设计了一个在线的softmax感知过滤器，进一步提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2502.18449', 'title': 'SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution', 'url': 'https://huggingface.co/papers/2502.18449', 'abstract': "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.", 'score': 37, 'issue_id': 2409, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '938af9b1ea2398d8', 'authors': ['Yuxiang Wei', 'Olivier Duchenne', 'Jade Copet', 'Quentin Carbonneaux', 'Lingming Zhang', 'Daniel Fried', 'Gabriel Synnaeve', 'Rishabh Singh', 'Sida I. Wang'], 'affiliations': ['Carnegie Mellon University', 'FAIR at Meta', 'GenAI at Meta', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.18449.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#dataset', '#math', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SWE-RL: Революция в обучении языковых моделей для задач разработки ПО', 'desc': 'Данная статья представляет SWE-RL - первый подход к масштабированию обучения с подкреплением (RL) для улучшения рассуждений больших языковых моделей (LLM) в реальных задачах разработки программного обеспечения. Используя легковесную систему вознаграждений на основе правил, SWE-RL позволяет LLM автономно восстанавливать процессы рассуждений разработчиков, обучаясь на обширных данных эволюции открытого программного обеспечения. Модель Llama3-SWE-RL-70B, обученная на основе Llama 3, достигает впечатляющих результатов на бенчмарке SWE-bench Verified, превосходя другие LLM среднего размера. Интересно, что несмотря на обучение только на данных эволюции ПО, модель демонстрирует улучшенные навыки обобщенных рассуждений в различных областях.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Software Engineering Reasoning', 'desc': 'This paper presents SWE-RL, a novel approach that applies reinforcement learning (RL) to enhance the reasoning abilities of large language models (LLMs) specifically for software engineering tasks. By utilizing a lightweight rule-based reward system, SWE-RL allows LLMs to learn from extensive open-source software evolution data, which includes various stages of software development. The resulting model, Llama3-SWE-RL-70B, achieves impressive performance on real-world GitHub issues, outperforming other medium-sized LLMs and even rivaling larger proprietary models. Additionally, this approach not only improves software-related reasoning but also demonstrates generalized reasoning skills across various tasks, indicating its broad applicability.'}, 'zh': {'title': '通过强化学习提升大语言模型的推理能力', 'desc': '这篇论文介绍了SWE-RL，这是第一个将强化学习应用于真实软件工程的模型。通过使用轻量级的基于规则的奖励机制，SWE-RL能够从大量开源软件演变数据中学习，自动恢复开发者的推理过程和解决方案。训练后的模型Llama3-SWE-RL-70B在真实的GitHub问题上达到了41.0%的解决率，表现优于其他中型语言模型。尽管仅在软件演变数据上进行强化学习，Llama3-SWE-RL仍展现出广泛的推理能力，能够在多个领域任务中取得良好结果。'}}}, {'id': 'https://huggingface.co/papers/2502.17363', 'title': 'KV-Edit: Training-Free Image Editing for Precise Background Preservation', 'url': 'https://huggingface.co/papers/2502.17363', 'abstract': 'Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free approach that uses KV cache in DiTs to maintain background consistency, where background tokens are preserved rather than regenerated, eliminating the need for complex mechanisms or expensive training, ultimately generating new content that seamlessly integrates with the background within user-provided regions. We further explore the memory consumption of the KV cache during editing and optimize the space complexity to O(1) using an inversion-free method. Our approach is compatible with any DiT-based generative model without additional training. Experiments demonstrate that KV-Edit significantly outperforms existing approaches in terms of both background and image quality, even surpassing training-based methods. Project webpage is available at https://xilluill.github.io/projectpages/KV-Edit', 'score': 26, 'issue_id': 2409, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': 'd1e7a717a0d2e56e', 'authors': ['Tianrui Zhu', 'Shiyi Zhang', 'Jiawei Shao', 'Yansong Tang'], 'affiliations': ['Institute of Artificial Intelligence (TeleAI), China Telecom', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.17363.jpg', 'data': {'categories': ['#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'KV-Edit: Революция в редактировании изображений с сохранением согласованности фона', 'desc': 'KV-Edit - это новый подход к редактированию изображений, основанный на использовании KV-кэша в DiT-моделях для сохранения согласованности фона. Метод позволяет генерировать новый контент, который органично интегрируется с фоном в заданных пользователем областях, без необходимости сложных механизмов или дорогостоящего обучения. Авторы оптимизировали пространственную сложность до O(1), используя метод без инверсии. Эксперименты показали, что KV-Edit значительно превосходит существующие подходы по качеству фона и изображения в целом.'}, 'en': {'title': 'Seamless Image Editing with KV-Edit: No Training Needed!', 'desc': 'The paper introduces KV-Edit, a novel method for image editing that addresses the challenge of maintaining background consistency. Unlike traditional methods that require extensive training, KV-Edit utilizes a KV cache in Denoising Transformers (DiTs) to preserve background tokens, allowing for seamless integration of new content. This approach simplifies the editing process by avoiding complex mechanisms and optimizing memory usage to O(1) without sacrificing quality. Experimental results show that KV-Edit outperforms existing techniques, including those that rely on training, in both background consistency and overall image quality.'}, 'zh': {'title': 'KV-Edit：无训练的背景一致性图像编辑新方法', 'desc': '背景一致性在图像编辑任务中仍然是一个重要挑战。现有方法在保持与原始图像相似性和生成符合目标内容之间存在权衡。我们提出了KV-Edit，这是一种无训练的方法，利用KV缓存来保持背景一致性，保留背景标记而不是重新生成，从而简化了复杂机制和高成本训练的需求。实验表明，KV-Edit在背景和图像质量方面显著优于现有方法，甚至超越了基于训练的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.18364', 'title': 'ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation', 'url': 'https://huggingface.co/papers/2502.18364', 'abstract': 'Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.', 'score': 24, 'issue_id': 2409, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '23632a98b9252831', 'authors': ['Yifan Pu', 'Yiming Zhao', 'Zhicong Tang', 'Ruihong Yin', 'Haoxing Ye', 'Yuhui Yuan', 'Dong Chen', 'Jianmin Bao', 'Sirui Zhang', 'Yanbin Wang', 'Lin Liang', 'Lijuan Wang', 'Ji Li', 'Xiu Li', 'Zhouhui Lian', 'Gao Huang', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.18364.jpg', 'data': {'categories': ['#multimodal', '#cv'], 'emoji': '🎭', 'ru': {'title': 'ART: революция в генерации многослойных изображений', 'desc': 'В статье представлен метод Anonymous Region Transformer (ART) для генерации многослойных прозрачных изображений на основе текстового запроса и анонимной разметки областей. ART позволяет модели автономно определять соответствие между визуальными и текстовыми токенами, что отличает его от семантической разметки. Механизм послойной обрезки регионов значительно снижает вычислительные затраты и позволяет эффективно генерировать изображения с множеством слоев. Авторы также предлагают автоэнкодер для кодирования и декодирования прозрачности в многослойных изображениях.'}, 'en': {'title': 'Revolutionizing Multi-Layer Image Generation with ART', 'desc': 'This paper presents the Anonymous Region Transformer (ART), a novel approach for generating multi-layer transparent images using a global text prompt. ART allows the model to autonomously match visual tokens to text tokens through an anonymous region layout, improving upon traditional semantic layouts. The layer-wise region crop mechanism enhances efficiency by reducing attention computation costs, enabling the generation of images with many distinct layers quickly. Overall, ART introduces a new paradigm for interactive content creation, allowing for precise control and scalable image generation.'}, 'zh': {'title': '匿名区域变换器：多层图像生成的新方法', 'desc': '多层图像生成是一个重要任务，使用户能够隔离、选择和编辑特定的图像层，从而改变与生成模型的交互方式。本文介绍了一种名为匿名区域变换器（ART）的新方法，它可以根据全局文本提示和匿名区域布局直接生成可变的多层透明图像。该方法允许生成模型自主决定哪些视觉标记与哪些文本标记对齐，显著提高了生成效率，并减少了计算成本。通过引入高质量的多层透明图像自编码器，ART为交互式内容创作建立了新的范式。'}}}, {'id': 'https://huggingface.co/papers/2502.17262', 'title': 'Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective', 'url': 'https://huggingface.co/papers/2502.17262', 'abstract': 'The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the "emergence phenomenon", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.', 'score': 15, 'issue_id': 2410, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '246c4bb39b0f6996', 'authors': ['Chengyin Xu', 'Kaiyuan Chen', 'Xiao Li', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.17262.jpg', 'data': {'categories': ['#optimization', '#small_models', '#training', '#benchmark'], 'emoji': '🔮', 'ru': {'title': 'Предсказание будущего больших языковых моделей', 'desc': 'Статья представляет новый метод прогнозирования производительности больших языковых моделей (LLM) на различных задачах. Авторы предлагают подход Clustering-On-Difficulty (COD), который группирует задачи по сложности и выбирает предсказуемое подмножество для оценки. COD использует ансамбль небольших моделей для точного прогнозирования результатов 70-миллиардной LLM на восьми важных бенчмарках. Этот метод помогает эффективнее распределять вычислительные ресурсы при обучении LLM и контролировать процесс обучения.'}, 'en': {'title': 'Predicting LLM Performance with Clustering-On-Difficulty', 'desc': 'This paper addresses the challenges of predicting the performance of Large Language Models (LLMs) before they are fully trained. It introduces a new framework called Clustering-On-Difficulty (COD), which clusters tasks based on their difficulty to create a predictable subset of tasks. By focusing on this subset, the framework allows for more accurate predictions of how well the LLM will perform on a broader set of tasks. The method has shown significant accuracy improvements, achieving a mean deviation of just 1.36% when predicting the performance of a 70 billion parameter LLM.'}, 'zh': {'title': '基于难度聚类的下游性能预测', 'desc': '这篇论文讨论了如何在训练大型语言模型（LLM）之前准确预测其在下游任务上的表现。由于“涌现现象”和任务难度分布不均，现有的性能预测方法往往不够准确。为了解决这些问题，作者提出了一种基于难度聚类的下游性能预测框架（COD），通过聚类任务并排除不适合的集群来构建可预测的支持子集。该方法在70B LLM的性能预测中表现出色，提供了有效的资源分配建议。'}}}, {'id': 'https://huggingface.co/papers/2502.15499', 'title': 'Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models', 'url': 'https://huggingface.co/papers/2502.15499', 'abstract': 'Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing gradient explosion and dissipation. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.', 'score': 12, 'issue_id': 2410, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': '3fb4636aad6d75b4', 'authors': ['Ya Wang', 'Zhijian Zhuo', 'Yutao Zeng', 'Xun Zhou', 'Jian Yang', 'Xiaoqing Li'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'Seed-Foundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.15499.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🚀', 'ru': {'title': 'Стабильное обучение языковых моделей через разделение масштаба и распределения', 'desc': 'Статья представляет новый метод Scale-Distribution Decoupling (SDD) для стабилизации обучения больших языковых моделей (LLM). SDD разделяет масштаб и распределение весовой матрицы в полносвязных слоях, применяя нормализацию и обучаемый вектор масштабирования. Это предотвращает взрыв и затухание градиентов, особенно в глубоких сетях. Эксперименты показывают, что метод стабилизирует обучение различных архитектур LLM и превосходит существующие техники.'}, 'en': {'title': 'Stabilizing Large Language Model Training with SDD', 'desc': 'This paper addresses the issue of training stability in large language models, especially those using Post-Norm Transformers, which often face problems like gradient explosion and dissipation. The authors introduce a new method called Scale-Distribution Decoupling (SDD), which separates the scale and distribution of weight matrices in fully-connected layers to enhance training stability. By implementing a normalization mechanism and a learnable scaling vector, SDD ensures well-conditioned gradients and improves optimization efficiency in deep networks. Experimental results show that SDD not only stabilizes training across various architectures but also outperforms existing normalization techniques while being lightweight and compatible with current frameworks.'}, 'zh': {'title': '规模-分布解耦：稳定大型语言模型训练的创新方法', 'desc': '在大型语言模型（LLMs）的预训练中，训练稳定性一直是一个挑战，尤其是对于后归一化变换器架构，容易出现梯度爆炸和消散。本文提出了一种新方法，称为规模-分布解耦（SDD），通过明确解耦全连接层中权重矩阵的规模和分布来稳定训练。SDD采用归一化机制来调节激活值，并使用可学习的缩放向量来保持良好的梯度条件，有效防止梯度爆炸和消散。实验结果表明，我们的方法在各种LLM架构中稳定了训练，并在不同的归一化配置中优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2502.16069', 'title': 'Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents', 'url': 'https://huggingface.co/papers/2502.16069', 'abstract': 'Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4times improvement in correctly answering experimental questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.', 'score': 9, 'issue_id': 2424, 'pub_date': '2025-02-22', 'pub_date_card': {'ru': '22 февраля', 'en': 'February 22', 'zh': '2月22日'}, 'hash': 'fca1ff11dd417d30', 'authors': ['Patrick Tser Jern Kon', 'Jiachen Liu', 'Qiuyi Ding', 'Yiming Qiu', 'Zhenning Yang', 'Yibo Huang', 'Jayanth Srinivasa', 'Myungjin Lee', 'Mosharaf Chowdhury', 'Ang Chen'], 'affiliations': ['Cisco Systems', 'Department of Computer Science and Engineering, University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2502.16069.jpg', 'data': {'categories': ['#agents', '#training', '#science', '#interpretability', '#benchmark', '#open_source'], 'emoji': '🧪', 'ru': {'title': 'Curie: ИИ-агент для строгих научных экспериментов', 'desc': 'Статья представляет Curie - фреймворк ИИ-агента для автоматизации научных экспериментов. Curie состоит из трех ключевых компонентов: модуля внутриагентной строгости, межагентной строгости и модуля знаний об экспериментах. Фреймворк нацелен на повышение надежности, методического контроля и интерпретируемости автоматизированных экспериментов. Тестирование на новом эталонном наборе из 46 вопросов по информатике показало значительное улучшение по сравнению с базовыми методами.'}, 'en': {'title': 'Curie: Enhancing Rigor in Scientific Experimentation with AI', 'desc': 'This paper introduces Curie, an AI agent framework aimed at improving the rigor of scientific experimentation. Curie incorporates three main components: an intra-agent rigor module for enhancing reliability, an inter-agent rigor module for ensuring methodical control, and an experiment knowledge module for boosting interpretability. The framework was evaluated using a new benchmark consisting of 46 questions from various computer science domains, demonstrating a significant performance improvement over existing methods. The results indicate that Curie can effectively automate rigorous experimentation, making it a valuable tool for researchers.'}, 'zh': {'title': 'Curie：提升科学实验严谨性的AI框架', 'desc': '本论文提出了一种名为Curie的人工智能代理框架，旨在提高科学实验的严谨性。Curie通过三个关键模块实现这一目标：内部代理严谨模块增强可靠性，外部代理严谨模块保持方法控制，以及实验知识模块提升可解释性。我们设计了一个新的实验基准，包含46个问题，涵盖四个计算机科学领域，以评估Curie的性能。与最强基线相比，Curie在正确回答实验问题方面实现了3.4倍的提升。'}}}, {'id': 'https://huggingface.co/papers/2502.18461', 'title': 'K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs', 'url': 'https://huggingface.co/papers/2502.18461', 'abstract': 'Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties of LoRA can effectively guide diffusion models in merging learned subject and style. Building on this insight, we propose K-LoRA, a simple yet effective training-free LoRA fusion approach. In each attention layer, K-LoRA compares the Top-K elements in each LoRA to be fused, determining which LoRA to select for optimal fusion. This selection mechanism ensures that the most representative features of both subject and style are retained during the fusion process, effectively balancing their contributions. Experimental results demonstrate that the proposed method effectively integrates the subject and style information learned by the original LoRAs, outperforming state-of-the-art training-based approaches in both qualitative and quantitative results.', 'score': 9, 'issue_id': 2412, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'db8a7da9e8b3af17', 'authors': ['Ziheng Ouyang', 'Zhen Li', 'Qibin Hou'], 'affiliations': ['VCIP, School of Computer Science, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18461.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#training'], 'emoji': '🎨', 'ru': {'title': 'K-LoRA: Эффективное слияние стиля и содержания без дополнительного обучения', 'desc': 'Статья представляет новый метод K-LoRA для объединения различных LoRA (Low-Rank Adaptation) в генеративных диффузионных моделях. K-LoRA использует механизм выбора Top-K элементов в каждом слое внимания для оптимального слияния информации о стиле и содержании. Этот подход не требует дополнительного обучения и эффективно сохраняет характеристики как оригинального объекта, так и стиля. Экспериментальные результаты показывают, что K-LoRA превосходит современные методы, требующие обучения, как по качественным, так и по количественным показателям.'}, 'en': {'title': 'K-LoRA: Effortless Fusion of Style and Subject in LoRA Models', 'desc': 'This paper introduces K-LoRA, a novel approach for fusing different Low-Rank Adaptation (LoRA) models without the need for additional training. The method leverages the intrinsic properties of LoRA to guide diffusion models in effectively merging learned subject and style. By comparing the Top-K elements in each attention layer, K-LoRA selects the most representative features from each LoRA, ensuring that both subject and style are preserved during fusion. Experimental results show that K-LoRA outperforms existing training-based methods in integrating subject and style information, achieving superior qualitative and quantitative outcomes.'}, 'zh': {'title': 'K-LoRA：无训练的LoRA融合新方法', 'desc': '本论文探讨了如何有效结合不同的LoRA，以同时生成学习到的风格和内容。我们提出了一种名为K-LoRA的方法，它是一种简单而有效的无训练LoRA融合方法。K-LoRA在每个注意力层中比较要融合的LoRA中的Top-K元素，从而选择最优的LoRA进行融合。实验结果表明，该方法在保留主题和风格信息方面优于现有的基于训练的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.18356', 'title': 'WebGames: Challenging General-Purpose Web-Browsing AI Agents', 'url': 'https://huggingface.co/papers/2502.18356', 'abstract': "We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents.", 'score': 6, 'issue_id': 2410, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '54dae5eb2e25ce92', 'authors': ['George Thomas', 'Alex J. Chan', 'Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Fraser Greenlee', 'Andy Toulis', 'Marvin Purtorab'], 'affiliations': ['Clusterfudge Ltd.', 'Convergence Labs Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2502.18356.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#games', '#agents'], 'emoji': '🌐', 'ru': {'title': 'WebGames: тестирование AI-агентов в реальных веб-сценариях', 'desc': 'WebGames - это комплексный набор тестов для оценки AI-агентов общего назначения, предназначенных для веб-браузинга. Он включает более 50 интерактивных задач, разработанных для тестирования ограничений современных систем искусственного интеллекта в различных аспектах взаимодействия с веб-браузером. Результаты оценки ведущих моделей vision-language, таких как GPT-4o, Claude Computer-Use, Gemini-1.5-Pro и Qwen2-VL, показывают значительный разрыв в возможностях по сравнению с человеческой производительностью. Бенчмарк WebGames доступен публично и предоставляет надежную основу для измерения прогресса в разработке более способных веб-агентов.'}, 'en': {'title': 'Bridging the AI Performance Gap in Web Browsing', 'desc': 'WebGames is a new benchmark suite designed to test web-browsing AI agents with over 50 interactive challenges. These challenges are easy for humans but are meant to expose the weaknesses of AI in tasks like browser interactions and cognitive processing. The testing environment is self-contained, allowing for consistent evaluations with clear correct answers. When tested, top AI models showed a significant performance gap compared to humans, achieving only 43.1% success versus 95.7% for humans, indicating that current AI struggles with intuitive web tasks.'}, 'zh': {'title': 'WebGames：评估网页浏览AI的全新基准', 'desc': '我们介绍了WebGames，这是一个全面的基准测试套件，旨在通过50多个互动挑战评估通用网页浏览AI代理。这些挑战设计得对人类来说简单明了，但系统地测试当前AI系统在基本浏览器交互、先进输入处理、认知任务、工作流自动化和互动娱乐等方面的局限性。我们的框架通过一个封闭的测试环境消除了外部依赖，确保可重复的评估和可验证的真实解决方案。评估结果显示，最佳AI系统的成功率仅为43.1%，而人类的表现为95.7%，突显了当前AI系统在处理人类直观的常见网页交互模式方面的基本局限性。'}}}, {'id': 'https://huggingface.co/papers/2502.17425', 'title': 'Introducing Visual Perception Token into Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2502.17425', 'abstract': 'To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken', 'score': 5, 'issue_id': 2414, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '0e916367fd15b42f', 'authors': ['Runpeng Yu', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.17425.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#architecture', '#cv'], 'emoji': '👁️', 'ru': {'title': 'Токены визуального восприятия: новый шаг в развитии мультимодальных ИИ', 'desc': 'В этой статье представлена концепция Токенов Визуального Восприятия для мультимодальных больших языковых моделей (MLLM). Эти токены позволяют MLLM автономно контролировать процесс визуального восприятия, выборочно анализируя определенные области изображения или фокусируясь на информации, связанной с конкретными категориями объектов. Авторы предлагают два типа токенов: Токен Выбора Региона и Токен Перекодирования Зрения. Эксперименты показывают, что использование этих токенов значительно улучшает производительность MLLM в задачах пространственного мышления и детального понимания изображений.'}, 'en': {'title': 'Empowering MLLMs with Visual Perception Tokens for Enhanced Understanding', 'desc': 'This paper introduces the Visual Perception Token concept to enhance the capabilities of Multimodal Large Language Models (MLLMs) in visual perception. The proposed tokens allow MLLMs to autonomously control their visual perception processes, enabling them to focus on specific image regions or object categories. Two types of tokens are designed: the Region Selection Token for identifying areas needing further analysis, and the Vision Re-Encoding Token for guiding additional perception actions. Experimental results show that these tokens significantly improve spatial reasoning and fine-grained understanding, leading to a performance increase of 23.6% in a 2B model compared to previous methods.'}, 'zh': {'title': '赋能MLLM的视觉感知控制机制', 'desc': '多模态大型语言模型（MLLM）依赖视觉编码器的感知过程来利用视觉信息。视觉感知的完整性和准确性对空间推理和细粒度理解等任务的精度有重要影响。本文提出了视觉感知令牌的概念，以增强MLLM对其视觉感知过程的控制能力。我们设计了区域选择令牌和视觉重新编码令牌两种类型的视觉感知令牌，实验表明这些令牌在处理空间推理和提高细粒度理解方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2502.14855', 'title': 'Prompt-to-Leaderboard', 'url': 'https://huggingface.co/papers/2502.14855', 'abstract': "Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l.", 'score': 4, 'issue_id': 2422, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '1aff9de655abd821', 'authors': ['Evan Frick', 'Connor Chen', 'Joseph Tennyson', 'Tianle Li', 'Wei-Lin Chiang', 'Anastasios N. Angelopoulos', 'Ion Stoica'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2502.14855.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#open_source', '#interpretability', '#training'], 'emoji': '🏆', 'ru': {'title': 'Точная оценка языковых моделей с учетом специфики запросов', 'desc': 'Статья представляет метод Prompt-to-Leaderboard (P2L) для оценки языковых моделей с учетом конкретных запросов. P2L использует большую языковую модель для генерации коэффициентов Брэдли-Терри на основе естественно-языковых запросов, что позволяет создавать специфичные для запросов рейтинги моделей. Этот подход обеспечивает более точную оценку производительности моделей по сравнению с усредненными метриками. Исследование показывает, что P2L лучше отражает нюансы работы языковых моделей и демонстрирует масштабирование по степенному закону, подобное самим большим языковым моделям.'}, 'en': {'title': 'Personalized Evaluation with Prompt-to-Leaderboard (P2L)', 'desc': 'This paper introduces a new method called Prompt-to-Leaderboard (P2L) for evaluating large language models (LLMs) based on specific prompts rather than averaged metrics. P2L generates leaderboards that reflect how well models perform on individual prompts by using a trained LLM to output Bradley-Terry coefficients, which predict human preferences. This approach allows for more nuanced evaluations, optimal model routing, and insights into model strengths and weaknesses. The results indicate that P2L captures the complexities of model performance better than traditional methods, achieving notable success in the Chatbot Arena.'}, 'zh': {'title': '提示特定排行榜：提升语言模型评估的精准度', 'desc': '这篇论文提出了一种新的评估大型语言模型（LLM）的方法，称为Prompt-to-Leaderboard（P2L）。P2L通过生成特定于提示的排行榜，解决了传统评估中用户和提示特定性能变化被平均化的问题。该方法训练LLM以自然语言提示为输入，输出Bradley-Terry系数向量，从而预测人类偏好投票。研究表明，P2L能够更好地捕捉语言模型性能的细微差别，并在Chatbot Arena中取得了优异的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.16825', 'title': 'Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization', 'url': 'https://huggingface.co/papers/2502.16825', 'abstract': 'Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to scale up the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a decline in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 (C_7^2) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position mu - 2sigma rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.', 'score': 4, 'issue_id': 2421, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '174509953671dacd', 'authors': ['Yao Xiao', 'Hai Ye', 'Linyao Chen', 'Hwee Tou Ng', 'Lidong Bing', 'Xiaoli Li', 'Roy Ka-wei Lee'], 'affiliations': ['Institute for Infocomm Research, A*Star, Singapore', 'National University of Singapore', 'Shanda AI Research Institute', 'Singapore University of Technology and Design', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2502.16825.jpg', 'data': {'categories': ['#training', '#alignment', '#rlhf'], 'emoji': '📊', 'ru': {'title': 'Оптимизация выбора данных для выравнивания языковых моделей', 'desc': 'Статья посвящена улучшению процесса выравнивания больших языковых моделей (LLM) с помощью итеративной генерации данных и переобучения модели. Авторы предлагают новый метод конструирования данных предпочтений для Direct Preference Optimization (DPO), основанный на анализе нормального распределения выборочных наград. Они обнаружили, что выбор отвергнутого ответа на позиции mu - 2sigma вместо минимальной награды критически важен для оптимальной производительности. Предложенная стратегия позволяет последовательно улучшать производительность модели по мере увеличения масштаба выборки.'}, 'en': {'title': 'Enhancing LLM Alignment with Scalable Preference Data Construction', 'desc': 'This paper discusses improving the alignment of large language models (LLMs) through iterative data generation and model retraining. It introduces Direct Preference Optimization (DPO), which uses preference pairs of responses to enhance training data selection. The authors find that traditional methods of selecting responses based on extreme rewards can hinder performance as sample sizes grow. They propose a new strategy for constructing preference data that leverages the normal distribution of sample rewards, leading to better model performance with larger datasets.'}, 'zh': {'title': '优化偏好数据构建，提升模型对齐性能', 'desc': '本文探讨了如何通过迭代数据生成和模型重训练来提高大型语言模型（LLMs）的对齐性能。我们提出了一种新的偏好数据构建策略，通过对样本奖励的正态分布进行分析，优化了选择和拒绝响应的过程。实验表明，选择奖励位置在 mu - 2sigma 的拒绝响应，而不是最低奖励，可以显著提升模型性能。最终，我们的策略在样本规模增加时，能够持续改善模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.17535', 'title': 'The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?', 'url': 'https://huggingface.co/papers/2502.17535', 'abstract': 'Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods.', 'score': 4, 'issue_id': 2412, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '38076575c5631707', 'authors': ['Zhenheng Tang', 'Xiang Liu', 'Qian Wang', 'Peijie Dong', 'Bingsheng He', 'Xiaowen Chu', 'Bo Li'], 'affiliations': ['CSE, The Hong Kong University of Science and Technology', 'DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.17535.jpg', 'data': {'categories': ['#inference', '#optimization', '#reasoning', '#rag', '#training'], 'emoji': '🎟️', 'ru': {'title': 'Лотерейные языковые модели: путь к эффективному сжатию без потери функциональности', 'desc': "Статья рассматривает гипотезу лотерейной языковой модели (LLM) в контексте сжатия моделей и кэша KV. Авторы предполагают, что для заданной LLM и задачи существует меньшая 'лотерейная' LLM, способная достичь той же производительности с помощью многошагового рассуждения и внешних инструментов. В работе обсуждаются последние достижения в области LLM, включая генерацию с поиском, многошаговое рассуждение и вычислительную выразительность. Статья подчеркивает важность учета этих аспектов при разработке методов сжатия LLM и кэша KV."}, 'en': {'title': 'Unlocking Efficiency: The Lottery LLM Hypothesis', 'desc': "This paper discusses the challenges of reducing the computational and storage costs of large language models (LLMs) while maintaining their performance. It reviews recent advancements in techniques like retrieval-augmented generation and multi-step reasoning that improve LLM capabilities. The authors introduce the 'lottery LLM' hypothesis, suggesting that a smaller model can achieve similar performance as a larger one by leveraging external tools and reasoning strategies. They also highlight important features that current compression methods often neglect, which are crucial for the success of both lottery LLMs and KV cache compression."}, 'zh': {'title': '压缩模型，提升性能的关键！', 'desc': '本文探讨了大语言模型（LLM）在模型压缩和KV缓存压缩方面的最新进展。研究者们关注如何在降低计算和存储成本的同时，保持压缩后模型的性能。我们提出了一个“彩票LLM假设”，即对于特定的LLM和任务，存在一个更小的LLM能够通过多步推理和外部工具实现与原始模型相同的性能。最后，文章总结了彩票LLM和KV缓存压缩所需的关键能力，这些能力在现有方法中常常被忽视。'}}}, {'id': 'https://huggingface.co/papers/2502.16794', 'title': 'AAD-LLM: Neural Attention-Driven Auditory Scene Understanding', 'url': 'https://huggingface.co/papers/2502.16794', 'abstract': 'Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.', 'score': 4, 'issue_id': 2410, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '680b3da301440d2b', 'authors': ['Xilin Jiang', 'Sukru Samet Dindar', 'Vishal Choudhari', 'Stephan Bickel', 'Ashesh Mehta', 'Guy M McKhann', 'Adeen Flinker', 'Daniel Friedman', 'Nima Mesgarani'], 'affiliations': ['Department of Electrical Engineering, Columbia University, USA', 'Department of Neurological Surgery, Columbia University, USA', 'Hofstra Northwell School of Medicine, USA', 'Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, USA', 'Neurology Department, New York University, USA', 'The Feinstein Institutes for Medical Research, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.16794.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#alignment', '#audio'], 'emoji': '👂', 'ru': {'title': 'Слушать как человек: ИИ с избирательным вниманием', 'desc': 'Статья представляет новый подход к обработке аудиоданных с использованием больших языковых моделей (LLM), учитывающий избирательное внимание человека. Авторы предлагают систему AAD-LLM, которая интегрирует сигналы мозга для определения, на какого говорящего слушатель обращает внимание. Модель сначала предсказывает объект внимания на основе нейронной активности, а затем генерирует ответы с учетом этого состояния внимания. Система показала улучшенное соответствие намерениям слушателя в задачах описания говорящих, транскрипции речи и ответов на вопросы в ситуациях с несколькими говорящими.'}, 'en': {'title': 'Listening with Intention: Enhancing Machine Hearing through Attention', 'desc': "This paper introduces a new approach to auditory processing in machine learning called Intention-Informed Auditory Scene Understanding (II-ASU). It presents a prototype system, Auditory Attention-Driven LLM (AAD-LLM), which uses brain signals to determine which speaker a listener is focusing on in complex sound environments. By integrating intracranial electroencephalography (iEEG) data, the model can tailor its responses based on the listener's attention, enhancing the relevance of its outputs. The evaluation shows that AAD-LLM significantly improves performance in tasks like speaker description and question answering, aligning better with human auditory perception."}, 'zh': {'title': '意图驱动的听觉理解，提升机器听觉能力', 'desc': '这篇论文介绍了一种新的听觉场景理解模型，称为意图驱动的听觉场景理解（II-ASU）。该模型通过分析脑电图信号，识别听众关注的特定说话者，从而生成更符合听众意图的响应。与传统的听觉大语言模型不同，AAD-LLM能够根据听众的注意力状态调整其输出。研究表明，该模型在多说话者场景中的表现优于现有模型，能够更好地理解和响应听众的需求。'}}}, {'id': 'https://huggingface.co/papers/2502.17422', 'title': 'MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.17422', 'abstract': "Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.", 'score': 2, 'issue_id': 2426, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '3fd784a7d93488b3', 'authors': ['Jiarui Zhang', 'Mahyar Khayatkhoei', 'Prateek Chhikara', 'Filip Ilievski'], 'affiliations': ['University of Southern California, USA', 'Vrije Universiteit Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2502.17422.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#optimization', '#interpretability', '#training', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Улучшение восприятия мелких деталей в мультимодальных ИИ-моделях', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) менее эффективны в восприятии мелких визуальных деталей по сравнению с крупными при ответе на вопросы об изображениях. Авторы обнаружили, что модели знают, куда смотреть, даже когда дают неправильный ответ. На основе этих выводов предложены методы визуального вмешательства без обучения, использующие внутренние знания модели для улучшения восприятия мелких деталей. Эксперименты показали значительное повышение точности MLLM на различных тестах визуального ответа на вопросы без необходимости дополнительного обучения.'}, 'en': {'title': 'Enhancing Visual Detail Recognition in MLLMs', 'desc': "This paper investigates the visual perception capabilities of Multimodal Large Language Models (MLLMs), particularly focusing on their ability to recognize small visual details compared to larger ones. The authors find that MLLMs' performance is significantly affected by the size of the visual subject in questions, and they establish a causal relationship through an intervention study. They also analyze the attention patterns of MLLMs, revealing that these models can identify relevant areas in images even when they answer incorrectly. To address the limitations in recognizing small details, the paper proposes training-free visual intervention methods that utilize the models' internal attention and gradient maps, demonstrating improved accuracy on various benchmarks without additional training."}, 'zh': {'title': '提升视觉细节感知，突破MLLMs的限制', 'desc': '多模态大型语言模型（MLLMs）在视觉识别任务中取得了快速进展。本文研究了MLLMs在回答图像问题时，能否有效感知小的视觉细节。研究发现，模型的表现对视觉主题的大小非常敏感，并通过干预研究证明了这一因果关系。此外，我们提出了一种无训练的视觉干预方法，利用模型内部的注意力和梯度图来增强其对小视觉细节的感知。'}}}, {'id': 'https://huggingface.co/papers/2502.17814', 'title': 'An Overview of Large Language Models for Statisticians', 'url': 'https://huggingface.co/papers/2502.17814', 'abstract': 'Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.', 'score': 2, 'issue_id': 2424, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'bc5cbd3359161d3e', 'authors': ['Wenlong Ji', 'Weizhe Yuan', 'Emily Getzen', 'Kyunghyun Cho', 'Michael I. Jordan', 'Song Mei', 'Jason E Weston', 'Weijie J. Su', 'Jing Xu', 'Linjun Zhang'], 'affiliations': ['INRIA', 'Meta FAIR', 'New York University', 'Rutgers University', 'Stanford University', 'UC Berkeley', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2502.17814.jpg', 'data': {'categories': ['#training', '#math', '#ethics', '#interpretability', '#architecture', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Статистика на страже доверия к языковым моделям', 'desc': 'Большие языковые модели (LLM) стали трансформационным инструментом в искусственном интеллекте, демонстрируя впечатляющие возможности в различных задачах. Статья исследует области, где статистики могут внести важный вклад в развитие LLM, особенно для повышения их надежности и прозрачности. Рассматриваются такие вопросы как количественная оценка неопределенности, интерпретируемость, справедливость, приватность и адаптация моделей. Авторы стремятся способствовать более глубокому сотрудничеству между ИИ и статистикой для развития как теоретических основ, так и практических приложений LLM.'}, 'en': {'title': 'Bridging AI and Statistics for Trustworthy LLMs', 'desc': 'This paper discusses the intersection of Large Language Models (LLMs) and statistics, highlighting the need for statistical methods to enhance the reliability and transparency of LLMs. It identifies key areas where statisticians can contribute, such as uncertainty quantification, interpretability, and fairness, which are crucial for building trust in AI systems. The authors argue that addressing these statistical challenges can improve decision-making and causal inference in LLM applications. Ultimately, the paper advocates for a collaborative approach between AI and statistics to tackle complex societal issues effectively.'}, 'zh': {'title': '统计学与大型语言模型的深度融合', 'desc': '大型语言模型（LLMs）在人工智能领域展现了卓越的能力，能够进行文本生成、推理和决策等多种任务。尽管其成功主要依赖于计算能力和深度学习架构的进步，但在不确定性量化、决策制定、因果推断和分布变化等领域，仍需更深入的统计学参与。本文探讨了统计学家在LLMs发展中的重要贡献，特别是在提升模型的可信度和透明度方面。我们关注的不仅是不确定性量化、可解释性、公平性、隐私保护和模型适应性，还考虑了LLMs在统计分析中的潜在角色。'}}}, {'id': 'https://huggingface.co/papers/2502.15612', 'title': 'LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models', 'url': 'https://huggingface.co/papers/2502.15612', 'abstract': "State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns.", 'score': 2, 'issue_id': 2419, 'pub_date': '2025-02-21', 'pub_date_card': {'ru': '21 февраля', 'en': 'February 21', 'zh': '2月21日'}, 'hash': '04a878da4ff4475d', 'authors': ['Hugo Pitorro', 'Marcos Treviso'], 'affiliations': ['Instituto de Telecomunicações, Lisbon'], 'pdf_title_img': 'assets/pdf/title_img/2502.15612.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#long_context', '#multimodal', '#machine_translation'], 'emoji': '🔍', 'ru': {'title': 'Заглянуть внутрь Mamba: новый метод интерпретации SSM', 'desc': 'Статья представляет новый метод интерпретации моделей состояния (SSM), таких как Mamba, для обработки длинных последовательностей. Авторы предлагают LaTIM - метод декомпозиции на уровне токенов, который позволяет детально анализировать работу Mamba-1 и Mamba-2. Метод был протестирован на различных задачах, включая машинный перевод и генерацию на основе поиска. Результаты показывают эффективность LaTIM в выявлении паттернов взаимодействия токенов в моделях Mamba.'}, 'en': {'title': 'Unlocking Interpretability in State Space Models with LaTIM', 'desc': "This paper presents LaTIM, a new method designed to improve the interpretability of state space models (SSMs) like Mamba. While SSMs are efficient for handling long sequences, they lack tools to understand how individual tokens contribute to the model's decisions. LaTIM allows researchers to break down and analyze the interactions between tokens at a granular level, enhancing our understanding of Mamba's processing across different layers. The effectiveness of LaTIM is validated through various tasks, showcasing its ability to reveal intricate token relationships within the model."}, 'zh': {'title': '提升Mamba模型的可解释性', 'desc': '状态空间模型（SSMs），如Mamba，成为了长序列建模中比变压器更高效的替代方案。然而，尽管它们的应用越来越广泛，SSMs仍然缺乏理解和改进基于注意力架构所需的可解释性工具。最近的研究虽然提供了对Mamba内部机制的见解，但并没有明确分解每个token的贡献，导致对Mamba如何在各层中选择性处理序列的理解存在空白。我们提出了LaTIM，这是一种新颖的token级分解方法，能够为Mamba-1和Mamba-2提供细粒度的可解释性，并在多种任务中验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.17092', 'title': 'Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI', 'url': 'https://huggingface.co/papers/2502.17092', 'abstract': 'We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks.', 'score': 2, 'issue_id': 2412, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '5433ca3c66d5e184', 'authors': ['Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2502.17092.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#reasoning', '#architecture', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективные мультимодальные модели без избыточных данных', 'desc': 'Представлена линейка мультимодальных моделей Shakti VLM с 1B и 4B параметров, нацеленных на повышение эффективности обучения. Модели используют архитектурные инновации, включая QK-нормализацию для стабильности внимания и гибридные техники нормализации. Применяется трехэтапная стратегия обучения для оптимизации эффективности. Результаты показывают, что Shakti VLM превосходит аналоги в задачах понимания документов, визуального рассуждения и OCR при меньшем объеме данных.'}, 'en': {'title': 'Efficient Multimodal Learning with Shakti VLM', 'desc': "Shakti VLM is a new family of vision-language models that come in sizes of 1 billion and 4 billion parameters, aimed at improving data efficiency in multimodal learning. Unlike other models that rely heavily on large datasets, Shakti utilizes innovative architectural features to achieve strong performance with fewer training tokens. Key improvements include QK-Normalization for stable attention mechanisms, hybrid normalization methods, and better positional encoding. The model's three-stage training approach enhances learning efficiency, demonstrating that effective design and training can lead to high performance without needing vast amounts of data."}, 'zh': {'title': '高效多模态学习的新选择：Shakti VLM', 'desc': 'Shakti VLM是一种视觉-语言模型，具有10亿和40亿参数，旨在解决多模态学习中的数据效率问题。与需要大量训练数据的传统模型不同，Shakti通过架构创新，能够用更少的标记实现竞争力的结果。其关键进展包括QK归一化以提高注意力稳定性、混合归一化技术和增强的位置编码。此外，三阶段训练策略进一步优化了学习效率。'}}}, {'id': 'https://huggingface.co/papers/2502.17910', 'title': 'Scaling LLM Pre-training with Vocabulary Curriculum', 'url': 'https://huggingface.co/papers/2502.17910', 'abstract': 'Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains.', 'score': 1, 'issue_id': 2430, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'ec4e582f8f6789ce', 'authors': ['Fangyuan Yu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.17910.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#open_source', '#transfer_learning', '#architecture'], 'emoji': '📚', 'ru': {'title': 'Динамическое расширение словаря для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей - curriculum learning для словаря. Метод чередует расширение словаря на основе энтропии и оптимизацию модели, позволяя изучать представления для различных уровней токенизации. Это приводит к оптимальному распределению вычислений: длинные токены для предсказуемого контента, короткие - для сложных контекстов. Эксперименты на небольших моделях GPT показывают улучшение эффективности масштабирования.'}, 'en': {'title': 'Dynamic Vocabulary Learning for Efficient Language Models', 'desc': 'This paper presents a new method called vocabulary curriculum learning, which allows language models to adapt their vocabularies dynamically during training, similar to how humans learn language. The approach involves alternating between expanding the vocabulary based on entropy and optimizing the model, which helps in learning better representations of language. By using longer tokens for predictable content and shorter tokens for complex contexts, the model can allocate computational resources more effectively. Experiments show that this method improves the efficiency of smaller GPT models, and the authors plan to explore its application in larger models and various domains.'}, 'zh': {'title': '动态词汇学习，提升语言模型效率', 'desc': '现代语言模型依赖于在预训练之前固定的静态词汇表，而人类语言学习则表现出自适应的词汇获取能力。为了解决这个问题，我们提出了词汇课程学习的方法，通过相对于词汇大小的对数线性缩放增益来提高预训练效率。该方法在熵引导的词汇扩展和模型优化之间交替进行，使模型能够在不同的分词粒度上学习可迁移的表示。实验表明，这种动态分词方法在小规模GPT模型上提高了缩放效率，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.18302', 'title': 'LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation', 'url': 'https://huggingface.co/papers/2502.18302', 'abstract': 'In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: https://zrealli.github.io/LDGen.', 'score': 0, 'issue_id': 2428, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '6358bac58e7b708b', 'authors': ['Pengzhi Li', 'Pengfei Yu', 'Zide Liu', 'Wei He', 'Xuhao Pan', 'Xudong Rao', 'Tao Wei', 'Wei Chen'], 'affiliations': ['Li Auto Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2502.18302.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#low_resource', '#multilingual', '#training', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'LDGen: Многоязычная генерация изображений с помощью больших языковых моделей', 'desc': 'LDGen - это новый метод интеграции больших языковых моделей (LLM) в существующие модели диффузии текста в изображение. Он решает проблемы многоязычной обработки, используя передовые возможности LLM для получения точной семантической информации. LDGen применяет легковесный адаптер и кросс-модальный рефайнер для эффективного выравнивания признаков и взаимодействия между LLM и признаками изображений. Метод превосходит базовые модели в соответствии промпту и эстетическом качестве изображений, поддерживая несколько языков.'}, 'en': {'title': 'Efficient Multilingual Image Generation with LDGen', 'desc': 'This paper presents LDGen, a new method that combines large language models (LLMs) with text-to-image diffusion models to reduce computational costs. It addresses the limitations of traditional text encoders like CLIP and T5, which struggle with multilingual image generation. LDGen uses a hierarchical caption optimization strategy and human instruction techniques to extract accurate semantic information. The method includes a lightweight adapter and a cross-modal refiner to enhance feature alignment, allowing for efficient zero-shot multilingual image generation with improved prompt adherence and image quality.'}, 'zh': {'title': 'LDGen：高效的多语言图像生成方法', 'desc': '本文介绍了一种新方法LDGen，用于将大型语言模型（LLMs）集成到现有的文本到图像扩散模型中，同时减少计算需求。传统的文本编码器如CLIP和T5在多语言处理上存在局限性，影响了跨语言的图像生成。我们通过利用LLMs的先进能力，采用层次化的标题优化和人类指令技术，提取精确的语义信息。LDGen减少了训练时间，实现了零样本多语言图像生成，并在提示遵循性和图像美学质量上超越了基线模型。'}}}, {'id': 'https://huggingface.co/papers/2502.18316', 'title': 'WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging', 'url': 'https://huggingface.co/papers/2502.18316', 'abstract': 'We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.', 'score': 0, 'issue_id': 2422, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '875de4e64c13e56b', 'authors': ['Ahmed Elhady', 'Eneko Agirre', 'Mikel Artetxe'], 'affiliations': ['HiTZ Center, University of the Basque Country (UPV/EHU)', 'Reka AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.18316.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'WiCkeD: Повышение сложности языковых тестов для более точной оценки ИИ', 'desc': "Статья представляет метод WiCkeD, который повышает сложность существующих тестов с множественным выбором путем случайной замены одного из вариантов на 'Ни один из вышеперечисленных'. Авторы применили WiCkeD к 6 популярным бенчмаркам и оценили 18 языковых моделей с открытыми весами. Результаты показали среднее снижение производительности моделей на 12.1 пункта по сравнению с оригинальными версиями датасетов. Метод WiCkeD также оказался эффективным при использовании цепочки рассуждений (chain-of-thought) на трех наборах данных MMLU."}, 'en': {'title': 'WiCkeD: Elevating Benchmark Complexity for Enhanced Model Evaluation', 'desc': "WiCkeD is a novel method designed to enhance the difficulty of existing multiple-choice benchmarks by introducing a 'None of the above' option randomly. This approach can be applied automatically to any benchmark, increasing the challenge for machine learning models. In experiments with 18 open-weight large language models (LLMs) across 6 popular benchmarks, we observed an average performance drop of 12.1 points when using the WiCkeD variant. Additionally, the method reveals varying sensitivities among models to the increased reasoning demands, providing deeper insights into their capabilities."}, 'zh': {'title': 'WiCkeD：提升多项选择基准的挑战性', 'desc': '我们介绍了一种名为WiCkeD的方法，通过随机将选项替换为“以上皆非”来增加现有多项选择基准的复杂性。这种方法常用于教育测试，可以自动应用于任何现有基准，使其更具挑战性。我们将WiCkeD应用于六个流行的基准，并评估了18个开放权重的大型语言模型（LLMs）。结果显示，模型的性能平均下降了12.1个百分点，表明WiCkeD对增强推理能力的模型同样具有挑战性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi', '#alignment (3)', '#architecture (8)', '#audio (1)', '#benchmark (7)', '#cv (5)', '#data', '#dataset (3)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (6)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (8)', '#open_source (7)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (16)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-27 02:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-27 02:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-27 02:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    