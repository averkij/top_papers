
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. June 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 июня</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-13.html">⬅️ <span id="prev-date">13.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-17.html">➡️ <span id="next-date">17.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'};
        let feedDateNext = {'ru': '17.06', 'en': '06/17', 'zh': '6月17日'};
        let feedDatePrev = {'ru': '13.06', 'en': '06/13', 'zh': '6月13日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.09600', 'title': 'Effective Red-Teaming of Policy-Adherent Agents', 'url': 'https://huggingface.co/papers/2506.09600', 'abstract': "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks", 'score': 32, 'issue_id': 4307, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '3de0c796f8d5a171', 'authors': ['Itay Nakash', 'George Kour', 'Koren Lazar', 'Matan Vetzler', 'Guy Uziel', 'Ateret Anaby-Tavor'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2506.09600.jpg', 'data': {'categories': ['#security', '#benchmark', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление защиты ЛЛМ-агентов от манипуляций пользователей', 'desc': 'Статья представляет CRAFT - многоагентную систему, использующую стратегии убеждения с учетом политик для тестирования устойчивости ЛЛМ-агентов в сфере обслуживания клиентов. Авторы предлагают новую модель угроз, фокусирующуюся на злоумышленниках, пытающихся эксплуатировать агентов в личных целях. CRAFT превосходит традиционные методы взлома, такие как DAN-промпты и эмоциональные манипуляции. Исследователи также представляют бенчмарк tau-break для оценки устойчивости агентов к манипулятивному поведению пользователей.'}, 'en': {'title': 'Strengthening Policy-Adherent Agents Against Adversarial Manipulation', 'desc': 'The paper introduces CRAFT, a multi-agent system designed to test and enhance the resilience of policy-adherent language model (LLM) agents in customer service against adversarial attacks. It highlights the challenge of ensuring these agents follow strict policies while still providing helpful interactions. The authors propose a new threat model that focuses on adversarial users who attempt to exploit these agents for personal gain. Additionally, they present tau-break, a benchmark for evaluating agent robustness, and discuss various defense strategies, revealing the need for more robust protections against manipulation.'}, 'zh': {'title': 'CRAFT：提升政策遵循代理的鲁棒性', 'desc': 'CRAFT是一个多智能体系统，使用政策意识的劝说策略，旨在挑战遵循政策的基于大语言模型的客户服务代理，以评估和提高其对对抗性攻击的鲁棒性。随着任务导向的LLM代理在严格政策领域的应用增加，确保代理始终遵循这些规则并适当地拒绝违规请求变得至关重要。为此，本文提出了一种新颖的威胁模型，专注于利用遵循政策的代理进行个人利益的对抗性用户。CRAFT通过利用政策意识的劝说策略，在客户服务场景中有效地削弱了遵循政策的代理，超越了传统的越狱方法。'}}}, {'id': 'https://huggingface.co/papers/2506.11924', 'title': 'Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation', 'url': 'https://huggingface.co/papers/2506.11924', 'abstract': 'A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.', 'score': 27, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'bf8d340f29d7ad95', 'authors': ['Min-Seop Kwak', 'Junho Kim', 'Sangdoo Yun', 'Dongyoon Han', 'Taekyoung Kim', 'Seungryong Kim', 'Jin-Hwa Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'SNU AIIS'], 'pdf_title_img': 'assets/pdf/title_img/2506.11924.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Диффузионная модель для согласованной генерации изображений и геометрии с новых ракурсов', 'desc': 'Эта статья представляет новую систему генерации изображений и геометрии с новых ракурсов, основанную на диффузионных моделях. Метод использует искажение и заполнение пробелов, а также дистилляцию внимания между модальностями для точного выравнивания генерируемых изображений и геометрии. Система применяет условное моделирование на основе близости для интеграции информации о глубине и нормалях. Результаты демонстрируют высококачественный синтез с новых ракурсов и полную трехмерную реконструкцию сцен.'}, 'en': {'title': 'High-Fidelity 3D View Synthesis through Diffusion and Attention', 'desc': 'This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes.'}, 'zh': {'title': '基于扩散的高保真图像与几何体生成', 'desc': '本文提出了一种基于扩散的框架，通过扭曲和修复的方法生成对齐的新视图图像和几何体。与以往需要密集姿态图像或限制于特定领域视图的生成模型不同，我们的方法利用现成的几何预测器来预测参考图像的部分几何体，并将新视图合成视为图像和几何体的修复任务。为了确保生成的图像和几何体之间的准确对齐，我们提出了跨模态注意力蒸馏，将图像扩散分支的注意力图注入到并行的几何扩散分支中。通过这种多任务方法，我们实现了几何稳健的图像合成和清晰的几何预测，最终在未见场景中实现了高保真度的视图合成。'}}}, {'id': 'https://huggingface.co/papers/2506.10892', 'title': 'The Diffusion Duality', 'url': 'https://huggingface.co/papers/2506.10892', 'abstract': 'Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo', 'score': 23, 'issue_id': 4305, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '974b708b2e781af0', 'authors': ['Subham Sekhar Sahoo', 'Justin Deschenaux', 'Aaron Gokaslan', 'Guanghan Wang', 'Justin Chiu', 'Volodymyr Kuleshov'], 'affiliations': ['Computer and Information Science, Cornell Tech, NYC, USA', 'School of Computer and Communication Sciences, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2506.10892.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#optimization', '#open_source', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Duo: Ускорение диффузионных языковых моделей с помощью гауссовских техник', 'desc': 'Метод Duo улучшает дискретные диффузионные модели с равномерным состоянием, перенося техники из гауссовской диффузии. Он вводит стратегию курируемого обучения, управляемую гауссовским процессом, что удваивает скорость обучения за счет снижения дисперсии. Duo также представляет дискретную дистилляцию согласованности, адаптируя метод из непрерывной в дискретную среду. Это позволяет ускорить генерацию текста в диффузионных языковых моделях на два порядка.'}, 'en': {'title': 'Duo: Accelerating Diffusion Models for Fast Text Generation', 'desc': 'This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models.'}, 'zh': {'title': 'Duo：加速文本生成的创新方法', 'desc': '本文提出了一种名为Duo的方法，旨在通过将高斯扩散的技术转移到均匀状态离散扩散模型中，从而提高训练速度和快速文本生成能力。均匀状态离散扩散模型具有自我纠正的能力，但通常在性能上不及自回归模型和掩蔽扩散模型。Duo通过引入基于高斯过程的课程学习策略，显著提高了训练速度，并在多个基准测试中超越了自回归模型。该方法还采用了离散一致性蒸馏技术，使得扩散语言模型能够实现快速的少步生成。'}}}, {'id': 'https://huggingface.co/papers/2506.10128', 'title': 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs', 'url': 'https://huggingface.co/papers/2506.10128', 'abstract': 'ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.', 'score': 13, 'issue_id': 4309, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5045b62235ae5509', 'authors': ['Xiyao Wang', 'Zhengyuan Yang', 'Chao Feng', 'Yongyuan Liang', 'Yuhang Zhou', 'Xiaoyu Liu', 'Ziyi Zang', 'Ming Li', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Furong Huang', 'Lijuan Wang'], 'affiliations': ['Cardiff University', 'Microsoft', 'University of Maryland, College Park', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.10128.jpg', 'data': {'categories': ['#rl', '#benchmark', '#hallucinations', '#cv', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'ViCrit: обучение мультимодальных моделей критическому восприятию визуальной информации', 'desc': 'Статья представляет ViCrit - задачу обучения с подкреплением для улучшения визуального восприятия мультимодальных моделей. ViCrit обучает модели обнаруживать тонкие искажения в подписях к изображениям, что позволяет повысить точность восприятия визуальной информации. Модели, обученные с помощью ViCrit, демонстрируют значительные улучшения в различных задачах компьютерного зрения. Полученные улучшения переносятся на новые домены, включая абстрактные изображения и визуальные математические задачи.'}, 'en': {'title': 'Enhancing Visual Perception in VLMs with ViCrit', 'desc': "ViCrit is a reinforcement learning task designed to enhance the visual perception capabilities of vision-language models (VLMs) by training them to identify subtle hallucinations in image captions. The task involves injecting minor visual description errors into human-written captions and challenging the model to locate these errors based on the corresponding images. This approach not only maintains the complexity of visual perception but also provides a clear and straightforward reward system for the model's performance. The results show that models trained with ViCrit achieve significant improvements across various visual benchmarks, indicating that this method fosters a deeper understanding of visual content rather than mere memorization."}, 'zh': {'title': '通过ViCrit提升视觉语言模型的感知能力', 'desc': 'ViCrit是一种强化学习任务，旨在微调视觉语言模型（VLMs），通过训练模型检测图像标题中的细微幻觉来提高视觉感知能力。该方法通过在人工撰写的图像标题中注入轻微的视觉描述错误，要求模型识别这些错误，从而保持感知的难度。经过ViCrit任务训练的模型在各种视觉语言基准测试中表现出显著的提升，且这些改进不仅限于自然图像数据，还能迁移到抽象图像推理和视觉数学等领域。我们的研究表明，细致的幻觉批评是一种有效且可推广的目标，有助于增强VLMs的视觉感知能力。'}}}, {'id': 'https://huggingface.co/papers/2506.11928', 'title': 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?', 'url': 'https://huggingface.co/papers/2506.11928', 'abstract': 'LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.', 'score': 11, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '4d3f2213d58dd8dd', 'authors': ['Zihan Zheng', 'Zerui Cheng', 'Zeyu Shen', 'Shang Zhou', 'Kaiyuan Liu', 'Hansen He', 'Dongruixuan Li', 'Stanley Wei', 'Hangyi Hao', 'Jianzhu Yao', 'Peiyao Sheng', 'Zixuan Wang', 'Wenhao Chai', 'Aleksandra Korolova', 'Peter Henderson', 'Sanjeev Arora', 'Pramod Viswanath', 'Jingbo Shang', 'Saining Xie'], 'affiliations': ['Canyon Crest Academy', 'McGill University', 'New York University', 'Princeton University', 'Sentient Foundation', 'University of California San Diego', 'University of Washington', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.11928.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#games'], 'emoji': '🤖', 'ru': {'title': 'LLM в программировании: сила в реализации, слабость в алгоритмах', 'desc': 'Исследование показывает, что крупные языковые модели (LLM) хорошо справляются с задачами по программированию, требующими сложной реализации, но испытывают трудности с тонким алгоритмическим мышлением. Для оценки этого был создан бенчмарк LiveCodeBench Pro, включающий задачи из Codeforces, ICPC и IOI. Анализ выявил, что лучшая модель достигает только 53% pass@1 на задачах средней сложности и 0% на сложных задачах без внешних инструментов. Исследование подчеркивает значительный разрыв между возможностями LLM и уровнем человека-гроссмейстера в программировании.'}, 'en': {'title': 'Bridging the Gap: LLMs vs. Human Algorithmic Mastery', 'desc': 'This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions.'}, 'zh': {'title': '大型语言模型在算法推理中的局限性', 'desc': '这篇论文探讨了大型语言模型（LLMs）在竞争编程中的表现，尤其是在实现密集型问题上表现良好，但在复杂算法推理方面存在不足。研究引入了LiveCodeBench Pro，这是一个基于Codeforces、ICPC和IOI的问题基准，旨在减少数据污染的可能性。通过对模型生成的提交进行逐行分析，发现当前的前沿模型在中等难度问题上的通过率仅为53%，而在困难问题上则为0%。这表明，尽管LLMs在实现精度上表现出色，但在复杂的算法推理和案例分析中仍然存在显著的局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.08989', 'title': 'SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.08989', 'abstract': "A self-aware problem synthesis framework that leverages model weaknesses enhances reinforcement learning with verifiable rewards, improving large language model performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.", 'score': 8, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '332357cc97416117', 'authors': ['Xiao Liang', 'Zhong-Zhi Li', 'Yeyun Gong', 'Yang Wang', 'Hengyuan Zhang', 'Yelong Shen', 'Ying Nian Wu', 'Weizhu Chen'], 'affiliations': ['Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08989.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование ИИ через осознание собственных слабостей', 'desc': 'Статья представляет новый подход к обучению с подкреплением для больших языковых моделей, называемый Self-aware Weakness-driven problem Synthesis (SwS). Эта система способна выявлять слабые места модели и создавать новые задачи для их улучшения. SwS не требует внешних размеченных данных и позволяет модели самостоятельно определять и устранять свои недостатки. Результаты показывают значительное улучшение производительности на задачах рассуждения для моделей размером 7B и 32B параметров.'}, 'en': {'title': 'Empowering Models by Learning from Their Weaknesses', 'desc': 'This paper presents a Self-aware Weakness-driven problem Synthesis framework (SwS) that enhances reinforcement learning for large language models (LLMs) by focusing on their weaknesses. The framework identifies specific areas where the model struggles and generates new problems to help the model improve in those areas. By systematically augmenting the training set with these tailored problems, the model can better learn and generalize its reasoning capabilities. The results show significant performance improvements on reasoning tasks, demonstrating the effectiveness of leveraging model weaknesses for training.'}, 'zh': {'title': '自我意识驱动的强化学习问题合成', 'desc': '本文提出了一种自我意识的弱点驱动问题合成框架（SwS），旨在通过识别模型的不足来增强强化学习（RL）中的可验证奖励。该框架系统地分析模型在训练过程中反复失败的问题，并利用这些失败案例提炼核心概念，合成新的问题以加强模型的薄弱环节。通过这种方法，模型能够在后续的增强训练中集中精力克服自身的弱点，而无需依赖外部知识蒸馏。实验结果表明，该框架在多个推理基准测试中显著提高了模型的性能，平均提升达10.0%和7.7%。'}}}, {'id': 'https://huggingface.co/papers/2506.11930', 'title': 'Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback', 'url': 'https://huggingface.co/papers/2506.11930', 'abstract': "LLMs show resistance to feedback, termed feedback friction, even under ideal conditions, and sampling-based strategies only partially mitigate this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.", 'score': 7, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '0fc67d5ee77483c7', 'authors': ['Dongwei Jiang', 'Alvin Zhang', 'Andrew Wang', 'Nicholas Andrews', 'Daniel Khashabi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.11930.jpg', 'data': {'categories': ['#training', '#hallucinations', '#alignment', '#reasoning', '#rlhf'], 'emoji': '🧠', 'ru': {'title': "Преодоление барьеров обучения: борьба с 'трением обратной связи' в LLM", 'desc': "Исследование показывает, что большие языковые модели (LLM) обладают сопротивлением к обратной связи, названным 'трением обратной связи', даже в идеальных условиях. Эксперименты проводились на различных задачах с использованием современных LLM, включая Claude 3.7. Стратегии на основе сэмплирования, такие как прогрессивное увеличение температуры, лишь частично смягчают эту проблему. Авторы надеются, что выявление этой проблемы поможет будущим исследованиям в области самосовершенствования LLM."}, 'en': {'title': 'Unpacking Feedback Friction in LLMs', 'desc': "This paper investigates the phenomenon of feedback friction in large language models (LLMs), where these models struggle to effectively incorporate external feedback even in ideal conditions. The authors conduct controlled experiments where a solver model receives targeted feedback from a feedback generator based on near-complete ground-truth answers. Despite this optimal setup, the models consistently show resistance to changing their incorrect responses, indicating a significant limitation in their learning process. The study also explores various strategies to mitigate this issue, such as sampling-based methods, but finds that these approaches only partially improve performance, highlighting the need for further research into enhancing LLMs' self-improvement capabilities."}, 'zh': {'title': '揭示大型语言模型的反馈摩擦问题', 'desc': '本研究探讨了大型语言模型（LLMs）在接收外部反馈时的表现，发现它们存在一种称为反馈摩擦（feedback friction）的现象，即使在理想条件下也难以有效整合反馈。我们设计了一个控制实验环境，评估模型在数学推理、知识推理和科学推理等多种任务中的反馈整合能力。尽管提供了接近完美的反馈，模型仍然表现出对反馈的抵抗，未能显著改善其错误答案。通过实验不同的采样策略，我们发现这些策略虽然有所改善，但仍未能使模型达到预期的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.11886', 'title': 'Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache', 'url': 'https://huggingface.co/papers/2506.11886', 'abstract': 'FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.', 'score': 6, 'issue_id': 4310, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '6e5e8424bfbe53cc', 'authors': ['Xiaoran Liu', 'Siyang He', 'Qiqi Wang', 'Ruixiao Li', 'Yuerong Song', 'Zhigeng Liu', 'Linlin Li', 'Qun Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.11886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие памяти LLM без потери точности', 'desc': 'FourierAttention - это фреймворк для повышения эффективности использования памяти в больших языковых моделях (LLM). Он сжимает малочувствительные к длинному контексту размерности головок трансформера, используя ортогональные базисы Фурье, сохраняя при этом точность модели. FourierAttention не требует дополнительного обучения и эксплуатирует различные роли размерностей головок трансформера: нижние размерности фокусируются на локальном контексте, а верхние захватывают зависимости на большом расстоянии. Оценки на моделях LLaMA показывают, что FourierAttention достигает лучшей точности для длинного контекста на бенчмарках LongBench и Needle-In-A-Haystack.'}, 'en': {'title': 'Enhancing Memory Efficiency in LLMs with FourierAttention', 'desc': 'FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment.'}, 'zh': {'title': 'FourierAttention：提升大型语言模型的内存效率', 'desc': 'FourierAttention是一种无需训练的框架，旨在提高大型语言模型的内存效率。它通过使用正交傅里叶基来压缩长上下文无关的变换头维度，同时保持模型的准确性。该方法利用变换头维度的异质性，低维度优先处理局部上下文，而高维度则捕捉长程依赖。评估结果表明，FourierAttention在长上下文准确性方面表现优异，且通过定制的Triton内核优化内存使用，确保高效部署。'}}}, {'id': 'https://huggingface.co/papers/2506.07464', 'title': 'DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO', 'url': 'https://huggingface.co/papers/2506.07464', 'abstract': 'DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.', 'score': 5, 'issue_id': 4308, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'f8c207e9d26fe89e', 'authors': ['Jinyoung Park', 'Jeehye Na', 'Jinyoung Kim', 'Hyunwoo J. Kim'], 'affiliations': ['KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07464.jpg', 'data': {'categories': ['#rlhf', '#video', '#benchmark', '#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🎥', 'ru': {'title': 'DeepVideo-R1: Улучшение рассуждений на видео с помощью регрессионного GRPO', 'desc': 'DeepVideo-R1 - это модель для улучшения рассуждений на основе видео, использующая регрессионный подход GRPO (Reg-GRPO) и адаптивное увеличение данных. Модель решает проблемы применения GRPO к видео-ЯБМ, такие как зависимость от защитных механизмов и проблема исчезающего преимущества. Reg-GRPO переформулирует задачу GRPO как регрессионную, напрямую предсказывая преимущество. Стратегия увеличения данных с учетом сложности динамически добавляет обучающие примеры на решаемых уровнях сложности.'}, 'en': {'title': 'Enhancing Video Reasoning with Reg-GRPO and Smart Data Augmentation', 'desc': 'DeepVideo-R1 is a novel approach that enhances video reasoning in large language models by utilizing a regression-based method called Reg-GRPO. This method reformulates the Group Relative Policy Optimization (GRPO) objective into a regression task, allowing for more direct policy guidance without the need for complex safeguards. Additionally, the paper introduces a difficulty-aware data augmentation strategy that adjusts training samples based on their solvable difficulty, which helps in generating diverse and informative reward signals. The results demonstrate that DeepVideo-R1 significantly boosts performance on various video reasoning benchmarks, showcasing its effectiveness in the field.'}, 'zh': {'title': 'DeepVideo-R1：提升视频推理的新方法', 'desc': 'DeepVideo-R1 是一种增强视频推理性能的模型，采用了回归型的 GRPO 方法和难度感知的数据增强策略。该研究探讨了 GRPO 在视频大语言模型中的应用，并识别出影响有效学习的两个主要问题：依赖保护措施和优势消失问题。为了解决这些挑战，DeepVideo-R1 通过回归 GRPO 重新构建了 GRPO 目标，直接预测优势值，从而简化了政策指导。实验结果表明，DeepVideo-R1 在多个视频推理基准测试中显著提高了视频推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.11997', 'title': 'pLSTM: parallelizable Linear Source Transition Mark networks', 'url': 'https://huggingface.co/papers/2506.11997', 'abstract': 'pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: https://github.com/ml-jku/plstm_experiments.', 'score': 4, 'issue_id': 4308, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '6dff119551b986fc', 'authors': ['Korbinian Pöppel', 'Richard Freinschlag', 'Thomas Schmied', 'Wei Lin', 'Sepp Hochreiter'], 'affiliations': ['Johannes Kepler University Linz'], 'pdf_title_img': 'assets/pdf/title_img/2506.11997.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#optimization', '#graphs', '#architecture', '#cv', '#open_source', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'pLSTM: Параллельные линейные RNN для эффективной обработки многомерных данных', 'desc': 'В статье представлена новая архитектура нейронной сети pLSTM (parallelizable Linear Source Transition Mark), разработанная для обработки направленных ациклических графов (DAG). pLSTM способна эффективно обрабатывать многомерные структуры данных, такие как изображения или молекулярные графы, преодолевая ограничения современных рекуррентных архитектур. Модель демонстрирует превосходную производительность на задачах с длинными зависимостями и превосходит трансформеры на ряде бенчмарков. pLSTM решает проблему затухающих/взрывающихся градиентов для длинных последовательностей в DAG с помощью двух режимов: направленного распространения и диффузного распределения.'}, 'en': {'title': 'pLSTMs: Revolutionizing Long-Range Learning in DAGs', 'desc': 'The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs), a new type of linear recurrent neural network (RNN) designed for processing data structured as directed acyclic graphs (DAGs). Unlike traditional RNNs and Transformers, pLSTMs can efficiently handle multi-dimensional data without being limited to sequential processing. They address the vanishing and exploding gradient problems through two modes of operation, allowing for effective long-range dependencies in data. The authors demonstrate that pLSTMs outperform Transformers on various benchmarks, particularly in tasks requiring long-distance extrapolation, such as computer vision and molecular graph analysis.'}, 'zh': {'title': 'pLSTMs：超越变换器的长距离学习新方法', 'desc': 'pLSTMs是一种可并行化的线性递归神经网络，专为有向无环图（DAG）设计，能够在长距离任务和基准测试中表现优于变换器（Transformers）。与现代递归架构相比，pLSTMs通过源、转移和标记门的设计，解决了长距离传播中的消失和爆炸梯度问题。该方法适用于更高结构的数据，如二维网格和树形结构，能够有效处理图像等多维数据。通过引入箭头指向外推的合成计算机视觉任务，pLSTMs展示了其在长距离推断中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2506.09427', 'title': 'A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation', 'url': 'https://huggingface.co/papers/2506.09427', 'abstract': "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.", 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5c1dd5f02a121213', 'authors': ['Yukang Feng', 'Jianwen Sun', 'Chuanhao Li', 'Zizhen Li', 'Jiaxin Ai', 'Fanrui Zhang', 'Yifan Chang', 'Sizhuo Zhou', 'Shenglin Zhang', 'Yu Dai', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09427.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#optimization', '#multimodal', '#games'], 'emoji': '🔄', 'ru': {'title': 'InterSyn: новый уровень мультимодального обучения', 'desc': 'Статья представляет InterSyn - крупномасштабный мультимодальный датасет для обучения языковых моделей. InterSyn создан с использованием метода самооценки с итеративным уточнением (SEIR) и содержит диалоги с тесно переплетенными изображениями и текстом. Авторы также представляют SynJudge - инструмент для автоматической оценки качества мультимодальных выходных данных. Эксперименты показывают, что обучение на InterSyn улучшает производительность мультимодальных моделей по всем метрикам оценки.'}, 'en': {'title': 'Enhancing Multimodal AI with InterSyn and SEIR', 'desc': 'The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models.'}, 'zh': {'title': 'InterSyn：提升多模态理解与生成的关键数据集', 'desc': 'InterSyn是一个大规模的数据集，旨在提高多模态理解和生成能力。它通过自我评估与迭代精炼（SEIR）方法构建，包含多轮指令驱动的对话和紧密交织的图像-文本输出。为了评估这些输出的质量，文章还介绍了SynJudge，一个自动评估工具，可以从文本内容、图像内容、图像质量和图像-文本协同四个维度进行量化评估。实验结果表明，使用SEIR方法构建的数据集质量显著提高，训练在InterSyn上的大型多模态模型在所有评估指标上均表现出一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.09366', 'title': 'SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending', 'url': 'https://huggingface.co/papers/2506.09366', 'abstract': 'SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.', 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '411c39c85d7cabe0', 'authors': ['Yuxuan Kuang', 'Haoran Geng', 'Amine Elhafsi', 'Tan-Dzung Do', 'Pieter Abbeel', 'Jitendra Malik', 'Marco Pavone', 'Yue Wang'], 'affiliations': ['Peking University', 'Stanford University', 'University of California, Berkeley', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.09366.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#rl', '#open_source', '#games'], 'emoji': '🤖', 'ru': {'title': 'SkillBlender: умное сочетание навыков для универсальных гуманоидных роботов', 'desc': 'SkillBlender - это новая иерархическая система обучения с подкреплением для универсального управления гуманоидными роботами. Она предварительно обучает примитивные навыки, а затем динамически комбинирует их для выполнения сложных задач локомоции и манипуляции. Авторы также представляют SkillBench - разнообразный симулированный бенчмарк для оценки таких систем. Эксперименты показывают, что SkillBlender значительно превосходит базовые методы, обеспечивая более точные и реалистичные движения роботов в повседневных сценариях.'}, 'en': {'title': 'Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending', 'desc': 'SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field.'}, 'zh': {'title': 'SkillBlender：高效的人形机器人运动操控框架', 'desc': 'SkillBlender 是一个层次化的强化学习框架，利用预训练的基本技能高效解决人形机器人在多样化环境中的运动操控任务。该框架首先预训练与任务无关的目标导向基本技能，然后动态融合这些技能，以最小的任务特定奖励设计完成复杂的运动操控任务。通过引入 SkillBench，一个包含多种模拟环境和挑战性任务的基准测试，SkillBlender 提供了科学的评估指标，平衡了准确性和可行性。大量的模拟实验表明，SkillBlender 显著优于所有基线方法，能够自然地规范行为，避免奖励黑客行为，从而实现更准确和可行的运动。'}}}, {'id': 'https://huggingface.co/papers/2506.11474', 'title': 'Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards', 'url': 'https://huggingface.co/papers/2506.11474', 'abstract': 'Med-PRM enhances clinical decision making by verifying reasoning steps against medical knowledge bases, achieving state-of-the-art performance in medical QA benchmarks with improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/', 'score': 3, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '123eb749b81758d8', 'authors': ['Jaehoon Yun', 'Jiwoong Sohn', 'Jungwoo Park', 'Hyunjae Kim', 'Xiangru Tang', 'Yanjun Shao', 'Yonghoe Koo', 'Minhyeok Ko', 'Qingyu Chen', 'Mark Gerstein', 'Michael Moor', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'ETH Zürich', 'Hanyang University College of Medicine', 'Korea University', 'University of Ulsan College of Medicine', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.11474.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#training', '#rag', '#reasoning', '#benchmark', '#science', '#small_models'], 'emoji': '🩺', 'ru': {'title': 'Точная диагностика с искусственным интеллектом: Med-PRM верифицирует каждый шаг', 'desc': 'Med-PRM - это новая система для улучшения клинического принятия решений, использующая большие языковые модели. Она проверяет каждый шаг рассуждения врача, сравнивая его с медицинскими базами знаний. Med-PRM показала лучшие результаты на пяти тестах по медицинским вопросам и ответам, улучшив точность базовых моделей на 13.5%. Система может быть легко интегрирована с другими сильными моделями, например Meerkat, достигая точности более 80% на тесте MedQA.'}, 'en': {'title': 'Enhancing Medical Decision Making with Verified Reasoning Steps', 'desc': 'Med-PRM is a new framework designed to improve clinical decision making by verifying reasoning steps against established medical knowledge bases. It addresses the challenge of error localization in large language models, which is crucial for accurate medical diagnoses. By using retrieval-augmented generation, Med-PRM can assess the quality of reasoning in a detailed manner, leading to enhanced accuracy in medical question answering tasks. The framework has shown significant performance improvements, achieving state-of-the-art results on multiple benchmarks and demonstrating its versatility with various policy models.'}, 'zh': {'title': 'Med-PRM：提升医疗决策的智能助手', 'desc': 'Med-PRM是一种过程奖励建模框架，旨在通过对医疗知识库的验证来增强临床决策能力。该模型利用检索增强生成技术，逐步验证推理过程中的每一步，确保推理的准确性。通过对中间推理步骤进行验证，Med-PRM能够细致地评估推理质量，从而提高医疗问答基准的表现。实验结果显示，Med-PRM在多个医疗QA基准上达到了最先进的性能，显著提升了基础模型的准确率。'}}}, {'id': 'https://huggingface.co/papers/2506.11274', 'title': 'Learning a Continue-Thinking Token for Enhanced Test-Time Scaling', 'url': 'https://huggingface.co/papers/2506.11274', 'abstract': 'A continuous thinking token learned via reinforcement learning improves language model accuracy more effectively than a fixed token during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "</think>" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model\'s accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.', 'score': 3, 'issue_id': 4313, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '770628ec96646ceb', 'authors': ['Liran Ringel', 'Elad Tolochinsky', 'Yaniv Romano'], 'affiliations': ['Department of Computer Science, Technion Israel Institute of Technology', 'Department of Electrical and Computer Engineering, Technion Israel Institute of Technology', 'Independent Researcher'], 'pdf_title_img': 'assets/pdf/title_img/2506.11274.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#inference', '#training', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение токена продолжения мысли повышает точность языковых моделей', 'desc': 'Статья описывает метод улучшения языковых моделей с помощью обучения специального токена для продолжения рассуждений. Авторы используют обучение с подкреплением для оптимизации эмбеддинга этого токена, оставляя веса модели неизменными. Эксперименты показывают, что такой подход превосходит как базовую модель, так и метод с фиксированным токеном для продления вычислений. На бенчмарке GSM8K новый метод дает улучшение точности на 4.2% по сравнению с базовой моделью.'}, 'en': {'title': 'Learned Thinking Token Boosts Language Model Accuracy!', 'desc': 'This paper presents a novel approach to enhance language model performance by introducing a learned continuous thinking token through reinforcement learning. Unlike fixed tokens that merely extend reasoning time, the learned token adapts dynamically to improve accuracy during inference. The authors demonstrate that this method outperforms both the baseline model and traditional fixed-token strategies on standard math benchmarks. Notably, their approach achieves a significant accuracy boost, particularly on the GSM8K benchmark, showcasing the effectiveness of learning over static methods.'}, 'zh': {'title': '学习连续思考标记，提升语言模型准确性', 'desc': '本文探讨了一种通过强化学习学习的连续思考标记，如何在推理过程中比固定标记更有效地提高语言模型的准确性。研究表明，使用专门的连续思考标记可以触发更长的推理步骤，从而提升模型性能。我们在DeepSeek-R1的精简版本中加入了一个学习到的"<|continue-thinking|>"标记，并通过强化学习训练其嵌入，同时保持模型权重不变。实验结果显示，学习到的标记在标准数学基准测试中，相比于基线模型和使用固定标记的测试时间扩展方法，取得了显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.08592', 'title': 'Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings', 'url': 'https://huggingface.co/papers/2506.08592', 'abstract': 'A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.', 'score': 3, 'issue_id': 4307, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '64e56d52fd4bf03d', 'authors': ['Liyan Xu', 'Zhenlin Su', 'Mo Yu', 'Jiangnan Li', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08592.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#open_source', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Точность в деталях: новый взгляд на возможности текстовых энкодеров', 'desc': "Представлен новый датасет CapRetrieval для оценки способности текстовых энкодеров распознавать мелкие сущности и события. Исследование выявило ограничения плотного поиска даже в простых случаях. Авторы предложили стратегии дообучения энкодеров для улучшения результатов на CapRetrieval. Также была обнаружена проблема 'дилеммы гранулярности' - сложность одновременного выражения мелких деталей и общей семантики в эмбеддингах."}, 'en': {'title': 'Enhancing Fine-Grained Entity Recognition in Text Encoders', 'desc': 'This paper introduces a new dataset called CapRetrieval, designed to test how well text encoders can identify detailed entities and events in text. The authors highlight a common problem where these encoders struggle with fine-grained retrieval, even in straightforward scenarios. Through zero-shot evaluation, they demonstrate that existing models often fail to match fine details, regardless of their size or training data. To improve performance, they propose data generation strategies for fine-tuning encoders, addressing the challenge of balancing detailed recognition with overall semantic understanding.'}, 'zh': {'title': '提升文本编码器的细粒度识别能力', 'desc': '本文介绍了一个新的数据集CapRetrieval，用于评估文本编码器识别细粒度实体和事件的能力。研究发现，现有的文本编码器在密集检索任务中存在局限性，无法有效识别语义中的细粒度信息。通过零样本评估，发现无论模型大小或训练来源，编码器在细粒度匹配上都可能失败。为了解决这个问题，本文提出了数据生成策略来微调编码器，从而在CapRetrieval上获得最佳性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08477', 'title': 'Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.08477', 'abstract': "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", 'score': 3, 'issue_id': 4306, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '4b240f248019e671', 'authors': ['Fengjun Pan', 'Anh Tuan Luu', 'Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08477.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#ethics', '#interpretability', '#small_models', '#multimodal', '#benchmark'], 'emoji': '🕵️', 'ru': {'title': 'Умный детектив для вредных мемов', 'desc': 'U-CoT+ - это новый фреймворк для обнаружения вредоносных мемов, который преобразует их в текстовые описания. Он использует специально разработанные человеком инструкции и промптинг с нулевым выстрелом для достижения высокой гибкости и объяснимости с помощью малых языковых моделей. Фреймворк отделяет интерпретацию мемов от их классификации, что позволяет эффективно использовать ресурсы. Эксперименты на семи эталонных наборах данных подтверждают эффективность этого подхода для объяснимого обнаружения вредоносных мемов с ограниченными ресурсами.'}, 'en': {'title': 'Transforming Memes into Text for Smarter Detection', 'desc': 'U-CoT+ is a new framework designed to detect harmful memes by transforming them into textual descriptions. This approach uses a meme-to-text pipeline that preserves details, allowing for better interpretation without needing complex visual analysis. By applying human-crafted guidelines with zero-shot Chain of Thought (CoT) prompting, the framework enhances flexibility and explainability in the detection process. The effectiveness of U-CoT+ is demonstrated through extensive experiments on various benchmark datasets, showcasing its potential for efficient and interpretable meme moderation using small-scale large language models (LLMs).'}, 'zh': {'title': 'U-CoT+: 高效可解释的有害迷因检测框架', 'desc': 'U-CoT+是一个新颖的框架，用于检测有害的网络迷因。它通过将迷因转换为文本描述，并使用人类设计的指导原则，结合零-shot链式推理，来实现高灵活性和可解释性。该框架避免了对复杂视觉内容的直接推理，从而提高了资源效率。实验结果表明，U-CoT+在小规模大语言模型上实现了有效的有害迷因检测。'}}}, {'id': 'https://huggingface.co/papers/2506.11136', 'title': 'JAFAR: Jack up Any Feature at Any Resolution', 'url': 'https://huggingface.co/papers/2506.11136', 'abstract': 'JAFAR is a lightweight feature upsampler using an attention-based module with Spatial Feature Transform modulation, enabling high-resolution features from Foundation Vision Encoders without high-resolution supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io', 'score': 2, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'ba6fb6c9e607162e', 'authors': ['Paul Couairon', 'Loick Chambon', 'Louis Serrano', 'Jean-Emmanuel Haugeard', 'Matthieu Cord', 'Nicolas Thome'], 'affiliations': ['Sorbonne Université, CNRS, ISIR, F-75005 Paris, France', 'Thales, TSGF, cortAIx Labs, France', 'Valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.11136.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'JAFAR: Умное повышение разрешения без высокоразрешающего обучения', 'desc': 'JAFAR - это легковесный модуль для повышения разрешения признаков, использующий механизм внимания и модуляцию Spatial Feature Transform. Он позволяет получать высокоразрешающие признаки из базовых энкодеров компьютерного зрения без необходимости в высокоразрешающем обучении. JAFAR эффективно восстанавливает мелкие пространственные детали и превосходит существующие методы повышения разрешения признаков в различных задачах. Несмотря на обучение при низких коэффициентах увеличения, модель хорошо обобщается на значительно более высокие выходные масштабы.'}, 'en': {'title': 'JAFAR: Elevating Vision Features with Attention and Modulation', 'desc': 'JAFAR is a novel feature upsampler that enhances the spatial resolution of visual features from Foundation Vision Encoders without requiring high-resolution supervision. It utilizes an attention-based module with Spatial Feature Transform modulation to align high-resolution queries with low-resolution keys, improving semantic coherence. The model is lightweight and flexible, allowing it to upscale features to any desired resolution effectively. Experimental results indicate that JAFAR excels in recovering fine details and outperforms existing methods in various dense vision tasks.'}, 'zh': {'title': 'JAFAR：轻量级特征上采样的新选择', 'desc': 'JAFAR是一种轻量级的特征上采样器，利用基于注意力的模块和空间特征变换调制，能够从基础视觉编码器中生成高分辨率特征，而无需高分辨率的监督。该方法通过增强低级图像特征与语义丰富的低分辨率键之间的语义对齐，来提升视觉特征的空间分辨率。尽管没有高分辨率的监督，JAFAR在低上采样比率和分辨率下的学习表现出色，能够很好地推广到更高的输出尺度。实验结果表明，JAFAR在恢复细粒度空间细节方面表现优异，并在多种下游任务中超越了现有的特征上采样方法。'}}}, {'id': 'https://huggingface.co/papers/2506.11702', 'title': 'Configurable Preference Tuning with Rubric-Guided Synthetic Data', 'url': 'https://huggingface.co/papers/2506.11702', 'abstract': 'Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning', 'score': 1, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '7a7eb1af4ef17eef', 'authors': ['Víctor Gallego'], 'affiliations': ['Komorebi AI Technologies, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2506.11702.jpg', 'data': {'categories': ['#rlhf', '#synthetic', '#training', '#dataset', '#alignment', '#open_source'], 'emoji': '🎛️', 'ru': {'title': 'Гибкая настройка языковых моделей под меняющиеся предпочтения пользователей', 'desc': 'Эта статья представляет новый подход под названием Configurable Preference Tuning (CPT) для настройки языковых моделей. CPT позволяет динамически корректировать поведение моделей на основе явных, понятных человеку директив. Метод использует синтетически сгенерированные данные о предпочтениях, основанные на структурированных рубриках, определяющих желаемые атрибуты. Такой подход обеспечивает тонкую настройку и моделирование более нюансированной обратной связи от человека.'}, 'en': {'title': 'Dynamic Adaptation of Language Models with Configurable Preference Tuning', 'desc': "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."}, 'zh': {'title': '动态调整语言模型行为的可配置偏好调优', 'desc': '可配置偏好调优（CPT）是一种新框架，使语言模型能够根据人类可理解的指令动态调整其行为。与传统的直接偏好优化（DPO）方法不同，CPT允许模型使用合成生成的偏好数据进行微调，从而在推理时根据系统提示调节输出。通过这种方式，模型能够在不重新训练的情况下，响应不同的上下文和需求。该方法不仅提供了更细致的控制，还能更好地模拟复杂和依赖上下文的人类反馈。'}}}, {'id': 'https://huggingface.co/papers/2506.10082', 'title': 'LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.10082', 'abstract': 'A mask-based LoRA tuning method for video editing adapts pretrained Image-to-Video models for flexible and high-quality video editing, using spatial masks and reference images for context-specific adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '14126cd5898da5a7', 'authors': ['Chenjian Gao', 'Lihe Ding', 'Xin Cai', 'Zhanpeng Huang', 'Zibin Wang', 'Tianfan Xue'], 'affiliations': ['SenseTime Research', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.10082.jpg', 'data': {'categories': ['#multimodal', '#games', '#diffusion', '#video', '#optimization', '#training'], 'emoji': '🎬', 'ru': {'title': 'Гибкое редактирование видео с помощью масок и LoRA', 'desc': 'Предложен метод настройки LoRA на основе масок для редактирования видео, адаптирующий предобученные модели Image-to-Video. Подход сохраняет фоновые области, позволяя контролировать распространение изменений. Используются дополнительные референсы в виде альтернативных ракурсов или репрезентативных состояний сцены. Маска позволяет модели динамически фокусироваться на нужных областях, извлекая информацию из соответствующих источников.'}, 'en': {'title': 'Flexible Video Editing with Mask-Based LoRA Tuning', 'desc': 'This paper presents a novel mask-based Low-Rank Adaptation (LoRA) tuning method for enhancing video editing capabilities using pretrained Image-to-Video models. The approach allows for flexible and high-quality edits by utilizing spatial masks and reference images, which provide context-specific guidance during the editing process. By preserving background regions and enabling controlled propagation of edits, the method improves upon existing techniques that often lack adaptability for subsequent frames. Experimental results demonstrate that this method outperforms current state-of-the-art video editing approaches, making it a significant advancement in the field.'}, 'zh': {'title': '灵活高效的视频编辑新方法', 'desc': '本文提出了一种基于掩码的LoRA调优方法，用于视频编辑，旨在灵活地适应预训练的图像到视频模型。该方法通过空间掩码和参考图像进行上下文特定的调整，保持背景区域的同时实现可控的编辑传播。与传统方法相比，这种方法在不改变模型架构的情况下，提供了高效且适应性强的视频编辑能力。实验结果表明，该方法在视频编辑性能上优于现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2506.10056', 'title': 'Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput', 'url': 'https://huggingface.co/papers/2506.10056', 'abstract': 'The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses a verifier in the ranking process. The growing consensus is that a comprehensive verifier (e.g., a full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play a crucial role in scaling verification through trading accuracy for speed, even when a comprehensive verifier is available. Their value becomes especially apparent when used in a generate-prune-then-rank approach, where a faster but less accurate verifier removes incorrect solutions prior to ranking -- leading to a system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems.', 'score': 1, 'issue_id': 4318, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '1ee14b5fd87f1f86', 'authors': ['Gabriel Orlanski', 'Nicholas Roberts', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.10056.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Баланс скорости и точности: новый взгляд на верификацию кода с помощью LLM', 'desc': "Статья исследует компромисс между скоростью и точностью в парадигме генерации и ранжирования программ с помощью больших языковых моделей (LLM). Авторы оспаривают общепринятое мнение о приоритете полного набора тестов над моделью оценки результатов (ORM). Исследование показывает, что ORM играет ключевую роль в масштабировании верификации, позволяя значительно ускорить процесс при небольшой потере точности. Предложенный подход 'генерация-отсев-ранжирование' с использованием ORM оказывается в 11.65 раз быстрее при снижении точности всего на 8.33% по сравнению с полным набором тестов."}, 'en': {'title': 'Balancing Speed and Accuracy in Program Verification with ORMs', 'desc': 'This paper investigates the effectiveness of using outcome reward models (ORMs) in the coding task solution process with large language models (LLMs). It challenges the common belief that comprehensive verifiers should always be prioritized, highlighting the trade-off between speed and accuracy. The authors demonstrate that ORMs can significantly enhance the efficiency of program verification by allowing a faster, less accurate verifier to filter out incorrect solutions before ranking. Their findings suggest that a generate-prune-then-rank strategy can achieve a balance, resulting in a system that is much faster while maintaining acceptable accuracy levels.'}, 'zh': {'title': '速度与准确性的权衡：优化编码任务的验证方法', 'desc': '这篇论文探讨了使用大型语言模型（LLMs）解决编码任务的标准方法，即生成-然后-排名的程序。研究表明，全面的验证器（如完整的测试套件）通常被认为优于结果奖励模型（ORM），但作者挑战了这一假设。通过系统地研究速度与准确性之间的权衡，发现ORM在提高验证速度方面发挥了重要作用，即使在有全面验证器的情况下。最终，生成-修剪-然后-排名的方法使得系统速度提高了11.65倍，准确性仅下降了8.33%。'}}}, {'id': 'https://huggingface.co/papers/2506.11130', 'title': 'A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data', 'url': 'https://huggingface.co/papers/2506.11130', 'abstract': 'A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.', 'score': 1, 'issue_id': 4308, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '743ff411fbd34247', 'authors': ['Cheng Kang Chou', 'Chan-Jan Hsu', 'Ho-Lam Chung', 'Liang-Hsuan Tseng', 'Hsi-Chun Cheng', 'Yu-Kuan Fu', 'Kuan Po Huang', 'Hung-Yi Lee'], 'affiliations': ['MediaTek Research', 'National Taiwan University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2506.11130.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#transfer_learning', '#low_resource', '#audio', '#synthetic'], 'emoji': '🔁', 'ru': {'title': 'Самосовершенствующаяся система АСР: от псевдо-меток к улучшенному распознаванию', 'desc': 'Предложена самосовершенствующаяся система для улучшения распознавания речи с использованием только немаркированных данных. Процесс начинается с генерации псевдо-меток существующей моделью АСР, которые затем используются для обучения высококачественной системы синтеза речи. Затем синтезированные пары речь-текст используются для дообучения исходной модели АСР, замыкая цикл самосовершенствования. Эффективность подхода продемонстрирована на тайваньском мандаринском диалекте, где модель Twister, адаптированная из Whisper-large-v2, показала значительное снижение ошибок по сравнению с базовой моделью.'}, 'en': {'title': 'Enhancing ASR with Unlabeled Data: The Twister Framework', 'desc': 'This paper presents a self-refining framework designed to improve Automatic Speech Recognition (ASR) performance using only unlabeled datasets. The framework begins with an existing ASR model that generates pseudo-labels from unannotated speech data, which are then utilized to train a high-fidelity Text-to-Speech (TTS) system. Synthesized speech and text pairs are incorporated back into the original ASR model, creating a closed-loop system that enhances its accuracy. The proposed method, tested on Taiwanese Mandarin, shows significant error rate reductions, demonstrating its effectiveness in low-resource environments.'}, 'zh': {'title': '自我优化框架提升ASR性能的创新之路', 'desc': '本文提出了一种自我优化框架，通过使用未标注的数据集来提升自动语音识别（ASR）的性能。该框架首先利用现有的ASR模型为未标注的语音生成伪标签，然后训练一个高保真的文本到语音（TTS）系统。接着，将合成的语音文本对引入原始的ASR系统，形成一个闭环的自我改进循环。实验结果表明，该框架在台湾普通话语音上有效，能够显著降低错误率，尤其在低资源或特定领域的应用中具有实际意义。'}}}, {'id': 'https://huggingface.co/papers/2506.08915', 'title': 'Inherently Faithful Attention Maps for Vision Transformers', 'url': 'https://huggingface.co/papers/2506.08915', 'abstract': 'An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.', 'score': 1, 'issue_id': 4309, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'e080cfdc03932dd9', 'authors': ['Ananthu Aniraj', 'Cassio F. Dantas', 'Dino Ienco', 'Diego Marcos'], 'affiliations': ['Inrae', 'Inria', 'University of Montpellier'], 'pdf_title_img': 'assets/pdf/title_img/2506.08915.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#architecture', '#interpretability'], 'emoji': '👁️', 'ru': {'title': 'Фокусировка на главном: улучшение восприятия объектов с помощью масок внимания', 'desc': 'Данная статья представляет метод на основе внимания, использующий обученные бинарные маски для улучшения устойчивости восприятия объектов. Метод состоит из двух этапов: первый обрабатывает все изображение для обнаружения частей объекта, второй использует маскирование входных данных для фокусировки на релевантных областях. Оба этапа обучаются совместно, что позволяет второму этапу улучшать результаты первого. Эксперименты показывают, что данный подход значительно повышает устойчивость к ложным корреляциям и нетипичным фонам.'}, 'en': {'title': 'Focusing Attention for Robust Object Perception', 'desc': 'This paper presents an attention-based method that utilizes learned binary masks to enhance object perception in images. By focusing on relevant regions and filtering out irrelevant information, the method improves the robustness of predictions, especially in challenging contexts. The proposed two-stage framework first identifies object parts and task-relevant areas, then restricts analysis to these regions using attention masking. Experimental results show that this approach effectively mitigates the impact of spurious correlations and out-of-distribution backgrounds on object recognition tasks.'}, 'zh': {'title': '基于注意力的鲁棒物体感知方法', 'desc': '本文提出了一种基于注意力的算法，利用学习到的二进制注意力掩码来提高物体感知的鲁棒性。该方法通过关注相关的图像区域，过滤掉无关的信息，从而确保只有被关注的区域影响预测结果。我们设计了一个两阶段的框架，第一阶段处理完整图像以发现物体部分并识别任务相关区域，第二阶段则利用输入的注意力掩码限制感受野，从而进行更专注的分析。实验结果表明，该方法在应对虚假相关性和分布外背景方面显著提高了鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.11116', 'title': 'Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models', 'url': 'https://huggingface.co/papers/2506.11116', 'abstract': 'Infinity-Instruct, a comprehensive instruction dataset, enhances both foundational and chat capabilities of large language models through curation and synthesis, achieving superior performance compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our datasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and codeshttps://gitee.com/li-touch/infinity-instruct have been publicly released.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '4de7c4de7a621e11', 'authors': ['Jijie Li', 'Li Du', 'Hanyu Zhao', 'Bo-wen Zhang', 'Liangdong Wang', 'Boyan Gao', 'Guang Liu', 'Yonghua Lin'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.11116.jpg', 'data': {'categories': ['#open_source', '#data', '#transfer_learning', '#benchmark', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Infinity-Instruct: революция в обучении языковых моделей', 'desc': 'Infinity-Instruct - это новый набор данных для обучения больших языковых моделей (LLM), который улучшает как базовые, так и диалоговые возможности моделей. Он состоит из двух частей: 7,4 млн базовых инструкций и 1,5 млн инструкций для чат-ботов. Модели, обученные на Infinity-Instruct, показывают значительное улучшение производительности по сравнению с существующими аналогами. Например, InfInstruct-LLaMA3.1-70B превосходит GPT-4-0314 на 8,6% в задачах следования инструкциям.'}, 'en': {'title': 'Unlocking LLM Potential with Infinity-Instruct', 'desc': 'Infinity-Instruct is a new instruction dataset that improves the performance of large language models (LLMs) in both foundational and chat tasks. It consists of 7.4 million curated foundational instructions and 1.5 million synthesized chat instructions, created through advanced data selection and filtering techniques. The dataset has been tested on various open-source models, showing significant performance improvements over existing instruction datasets. This research highlights the importance of combining foundational and chat training to enhance the overall capabilities of LLMs.'}, 'zh': {'title': '提升大型语言模型的指令能力', 'desc': 'Infinity-Instruct是一个全面的指令数据集，旨在提升大型语言模型的基础能力和对话能力。通过两阶段的处理流程，我们从超过1亿个样本中筛选出740万条高质量的基础指令，并合成150万条高质量的对话指令。经过实证评估，使用Infinity-Instruct微调的多个开源模型在基础和指令跟随基准测试中均表现出显著的性能提升。我们的研究结果表明，基础训练和对话训练之间的协同作用对大型语言模型的发展具有重要意义。'}}}, {'id': 'https://huggingface.co/papers/2506.03857', 'title': 'Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation', 'url': 'https://huggingface.co/papers/2506.03857', 'abstract': 'A novel candidate annotation paradigm using a teacher-student framework improves data quality for下游 applications by encouraging large language models to output multiple labels when uncertain.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.', 'score': 1, 'issue_id': 4314, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9efc9f12a990b701', 'authors': ['Mingxuan Xia', 'Haobo Wang', 'Yixuan Li', 'Zewei Yu', 'Jindong Wang', 'Junbo Zhao', 'Runze Wu'], 'affiliations': ['NetEase Fuxi AI Lab', 'University of Wisconsin Madison', 'William & Mary', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03857.jpg', 'data': {'categories': ['#data', '#dataset', '#training', '#optimization', '#alignment'], 'emoji': '🏷️', 'ru': {'title': 'Улучшение качества аннотаций с помощью множественных меток от языковых моделей', 'desc': 'Статья представляет новый подход к аннотации данных с использованием больших языковых моделей (LLM). Авторы предлагают парадигму кандидатной аннотации, где LLM выдает несколько возможных меток при неопределенности. Для получения уникальных меток используется система учитель-ученик с дистилляцией знаний в малую языковую модель (SLM). Теоретически и экспериментально показано, что этот метод превосходит прямое использование одиночных аннотаций.'}, 'en': {'title': 'Empowering Uncertainty: Multi-Label Annotation for Better Data Quality', 'desc': 'This paper introduces a new approach to data annotation using a teacher-student framework that enhances the quality of labels produced by large language models (LLMs). Instead of forcing LLMs to choose a single label, the method encourages them to generate multiple potential labels when they are uncertain. This strategy helps to mitigate the risk of incorrect labeling, which can degrade the quality of data for subsequent applications. The proposed framework, called CanDist, uses a Small Language Model (SLM) to refine these candidate annotations, leading to better performance in text classification tasks.'}, 'zh': {'title': '利用教师-学生框架提升数据标注质量', 'desc': '本文提出了一种新的候选标注范式，利用教师-学生框架来提高数据质量。该方法鼓励大型语言模型在不确定时输出多个标签，从而减少错误标注的风险。通过这种方式，模型能够更好地处理复杂样本，确保下游应用的数据质量。实验结果表明，该方法在六个文本分类任务中表现出色，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.10387', 'title': 'Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills', 'url': 'https://huggingface.co/papers/2506.10387', 'abstract': 'Hierarchical Multimodal Skills and Skill-Augmented Monte Carlo Tree Search improve multimodal GUI agent performance in long-horizon tasks by abstracting knowledge and bridging the offline-online domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/', 'score': 0, 'issue_id': 4315, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '9c47a3d6d929dc16', 'authors': ['Yuquan Xie', 'Zaijing Li', 'Rui Shao', 'Gongwei Chen', 'Kaiwen Zhou', 'Yinchuan Li', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Huawei Noahs Ark Lab', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10387.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#benchmark', '#long_context', '#games', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Иерархические навыки и МКТП: прорыв в долгосрочном планировании ГПИ-агентов', 'desc': 'Статья представляет новый подход к улучшению производительности мультимодальных ГПИ-агентов в задачах с длительным горизонтом планирования. Авторы предлагают модуль Иерархических Мультимодальных Навыков (HMS) для абстрагирования знаний и алгоритм Дерева Поиска Монте-Карло с Дополнением Навыками (SA-MCTS) для преодоления разрыва между офлайн и онлайн доменами. На основе HMS разработан агент Mirage-1, который превосходит предыдущие модели на нескольких бенчмарках. Эксперименты показывают значительное улучшение производительности на различных наборах данных, включая новый бенчмарк AndroidLH.'}, 'en': {'title': 'Empowering GUI Agents with Hierarchical Skills for Long-Horizon Success', 'desc': 'This paper introduces a new approach to improve the performance of multimodal GUI agents in long-horizon tasks by using Hierarchical Multimodal Skills (HMS) and Skill-Augmented Monte Carlo Tree Search (SA-MCTS). HMS organizes knowledge into a hierarchy of execution skills, core skills, and meta-skills, allowing agents to better generalize and plan tasks. SA-MCTS enhances the search process by utilizing skills learned in offline settings to streamline decision-making in online environments. The proposed agent, Mirage-1, demonstrates significant performance improvements over existing agents in various benchmarks, showcasing its effectiveness in real-world applications.'}, 'zh': {'title': '层次化技能与增强搜索提升GUI代理表现', 'desc': '本文提出了一种层次化多模态技能模块（HMS），旨在解决多模态图形用户界面（GUI）代理在长时间任务中的知识不足问题。HMS通过将轨迹逐步抽象为执行技能、核心技能和元技能，构建了一个层次化的知识结构，以支持长时间任务的规划。为了弥补离线和在线领域之间的差距，本文还提出了技能增强的蒙特卡洛树搜索算法（SA-MCTS），该算法有效利用离线环境中获得的技能，减少在线树搜索中的动作搜索空间。实验结果表明，Mirage-1在多个基准测试中显著优于之前的代理，提升了性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi', '#alignment (3)', '#architecture (4)', '#audio (1)', '#benchmark (14)', '#cv (5)', '#data (4)', '#dataset (10)', '#diffusion (3)', '#ethics (1)', '#games (5)', '#graphs (1)', '#hallucinations (2)', '#healthcare (1)', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (2)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (6)', '#optimization (12)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (5)', '#rlhf (4)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (3)', '#training (12)', '#transfer_learning (5)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-16 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-16 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-16 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    