
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 27 papers. March 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 марта</span> | <span id="title-articles-count">27 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-26.html">⬅️ <span id="prev-date">26.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-28.html">➡️ <span id="next-date">28.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'};
        let feedDateNext = {'ru': '28.03', 'en': '03/28', 'zh': '3月28日'};
        let feedDatePrev = {'ru': '26.03', 'en': '03/26', 'zh': '3月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.20215', 'title': 'Qwen2.5-Omni Technical Report', 'url': 'https://huggingface.co/papers/2503.20215', 'abstract': "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", 'score': 55, 'issue_id': 2924, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'dd7a3c8e8564b973', 'authors': ['Jin Xu', 'Zhifang Guo', 'Jinzheng He', 'Hangrui Hu', 'Ting He', 'Shuai Bai', 'Keqin Chen', 'Jialin Wang', 'Yang Fan', 'Kai Dang', 'Bin Zhang', 'Xiong Wang', 'Yunfei Chu', 'Junyang Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20215.jpg', 'data': {'categories': ['#architecture', '#agi', '#benchmark', '#multimodal', '#video', '#games', '#audio'], 'emoji': '🤖', 'ru': {'title': 'Qwen2.5-Omni: Мультимодальный ИИ нового поколения', 'desc': 'Qwen2.5-Omni - это мультимодальная модель, способная воспринимать текст, изображения, аудио и видео, генерируя при этом текстовые и речевые ответы в потоковом режиме. Модель использует блочную обработку для аудио и видео энкодеров, а также новый подход к позиционному кодированию TMRoPE для синхронизации временных меток. Архитектура Thinker-Talker позволяет одновременно генерировать текст и речь без взаимных помех. Qwen2.5-Omni демонстрирует высокую производительность на различных мультимодальных бенчмарках, включая Omni-Bench и MMLU.'}, 'en': {'title': 'Streamlining Multimodal Interaction with Qwen2.5-Omni', 'desc': 'Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.'}, 'zh': {'title': '多模态流式生成的未来', 'desc': '本文介绍了Qwen2.5-Omni，这是一个端到端的多模态模型，能够处理文本、图像、音频和视频等多种输入，同时生成文本和自然语音响应。为了实现多模态信息的流式处理，音频和视觉编码器采用了块处理的方法，并通过一种新颖的位置嵌入方法TMRoPE来同步音频和视频的时间戳。该模型的Thinker-Talker架构使得文本生成和语音生成可以并行进行，避免了两者之间的干扰。Qwen2.5-Omni在多模态基准测试中表现出色，尤其在流式语音生成方面，其性能优于大多数现有的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2503.19757', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy', 'url': 'https://huggingface.co/papers/2503.19757', 'abstract': "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.", 'score': 39, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'f481410d892c371a', 'authors': ['Zhi Hou', 'Tianyi Zhang', 'Yuwen Xiong', 'Haonan Duan', 'Hengjun Pu', 'Ronglei Tong', 'Chengyang Zhao', 'Xizhou Zhu', 'Yu Qiao', 'Jifeng Dai', 'Yuntao Chen'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI, CAS', 'College of Computer Science and Technology, Zhejiang University', 'MMLab, The Chinese University of Hong Kong', 'Peking University', 'SenseTime Research', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19757.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#diffusion', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов с помощью диффузионных трансформеров', 'desc': 'Статья представляет Dita - масштабируемую модель для обучения роботов, использующую архитектуру трансформеров для денойзинга непрерывных последовательностей действий. В отличие от предыдущих подходов, Dita применяет контекстное обусловливание, что позволяет точнее согласовывать действия с визуальными данными. Модель эффективно интегрирует разнородные наборы данных, повышая устойчивость к различным вариациям среды. Эксперименты показывают высокую производительность Dita как в симуляции, так и в реальном мире при адаптации к сложным долгосрочным задачам.'}, 'en': {'title': 'Dita: Transforming Robot Action Learning with Multimodal Diffusion', 'desc': 'The paper introduces Dita, a new framework that improves how robots learn to perform actions by using advanced Transformer models. Unlike previous methods that used simple networks to predict actions, Dita employs a sophisticated approach called in-context conditioning, which helps align actions more closely with visual information from past experiences. This allows Dita to effectively handle a wide range of actions and environments, making it adaptable to different tasks and camera views. The results show that Dita not only performs well in simulations but also adapts successfully to real-world scenarios with minimal additional training.'}, 'zh': {'title': 'Dita：提升机器人适应能力的创新框架', 'desc': '本文介绍了一种名为Dita的框架，旨在提高机器人在多样化动作空间中的适应能力。Dita利用Transformer架构，通过统一的多模态扩散过程直接去噪连续动作序列，克服了传统方法的局限。该框架通过上下文条件化实现了去噪动作与历史观察的原始视觉标记之间的精细对齐，从而更好地建模动作变化和环境细节。Dita在多种基准测试中表现出色，能够有效适应真实世界的环境变化，并成功执行复杂的长期任务。'}}}, {'id': 'https://huggingface.co/papers/2503.20314', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'url': 'https://huggingface.co/papers/2503.20314', 'abstract': "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.", 'score': 28, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'e9770d8e9d313979', 'authors': ['WanTeam', ':', 'Ang Wang', 'Baole Ai', 'Bin Wen', 'Chaojie Mao', 'Chen-Wei Xie', 'Di Chen', 'Feiwu Yu', 'Haiming Zhao', 'Jianxiao Yang', 'Jianyuan Zeng', 'Jiayu Wang', 'Jingfeng Zhang', 'Jingren Zhou', 'Jinkai Wang', 'Jixuan Chen', 'Kai Zhu', 'Kang Zhao', 'Keyu Yan', 'Lianghua Huang', 'Mengyang Feng', 'Ningyi Zhang', 'Pandeng Li', 'Pingyu Wu', 'Ruihang Chu', 'Ruili Feng', 'Shiwei Zhang', 'Siyang Sun', 'Tao Fang', 'Tianxing Wang', 'Tianyi Gui', 'Tingyu Weng', 'Tong Shen', 'Wei Lin', 'Wei Wang', 'Wei Wang', 'Wenmeng Zhou', 'Wente Wang', 'Wenting Shen', 'Wenyuan Yu', 'Xianzhong Shi', 'Xiaoming Huang', 'Xin Xu', 'Yan Kou', 'Yangyu Lv', 'Yifei Li', 'Yijing Liu', 'Yiming Wang', 'Yingya Zhang', 'Yitong Huang', 'Yong Li', 'You Wu', 'Yu Liu', 'Yulin Pan', 'Yun Zheng', 'Yuntao Hong', 'Yupeng Shi', 'Yutong Feng', 'Zeyinzi Jiang', 'Zhen Han', 'Zhi-Fan Wu', 'Ziyu Liu'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.20314.jpg', 'data': {'categories': ['#video', '#open_source', '#multimodal', '#architecture', '#diffusion', '#benchmark', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Wan: Открытый набор передовых видео-моделей для революции в генерации видео', 'desc': 'Статья представляет Wan - комплексный набор видео-моделей, основанных на архитектуре диффузионного трансформера. Wan достигает значительных улучшений в генеративных возможностях благодаря инновациям, включая новый VAE, масштабируемые стратегии предобучения и автоматизированные метрики оценки. Модель Wan с 14 миллиардами параметров, обученная на огромном наборе данных, демонстрирует превосходную производительность по сравнению с существующими открытыми и коммерческими решениями. Wan предлагает эффективные модели для различных задач генерации видео и открыт для сообщества, что способствует развитию технологий в этой области.'}, 'en': {'title': 'Wan: Revolutionizing Video Generation with Open-Source Models', 'desc': 'This paper introduces Wan, a suite of video foundation models that enhances video generation using a diffusion transformer approach. Wan features a novel Variational Autoencoder (VAE) and scalable pre-training strategies, which improve its generative capabilities. The 14B model, trained on a massive dataset, showcases superior performance compared to existing models, while the 1.3B model offers efficiency for consumer-grade hardware. By open-sourcing the models and code, Wan aims to support the video generation community and expand creative opportunities in video production.'}, 'zh': {'title': '推动视频生成的开放模型——Wan', 'desc': '本报告介绍了Wan，这是一个全面且开放的视频基础模型套件，旨在推动视频生成的边界。Wan基于主流的扩散变换器范式，通过创新的变分自编码器（VAE）、可扩展的预训练策略、大规模数据整理和自动评估指标，显著提升了生成能力。Wan的14B模型在数十亿图像和视频的数据集上训练，展示了视频生成在数据和模型规模方面的扩展规律，超越了现有的开源模型和商业解决方案。该模型不仅高效且多功能，支持多种下游应用，且所有代码和模型均已开源，旨在促进视频生成社区的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.19990', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'url': 'https://huggingface.co/papers/2503.19990', 'abstract': "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90\\% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.", 'score': 26, 'issue_id': 2922, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'ec85f1936ae0edd9', 'authors': ['Kexian Tang', 'Junyao Gao', 'Yanhong Zeng', 'Haodong Duan', 'Yanan Sun', 'Zhening Xing', 'Wenran Liu', 'Kaifeng Lyu', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Simons Institute, UC Berkeley', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19990.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧩', 'ru': {'title': 'LEGO-Puzzles: выявление пробелов в пространственном мышлении ИИ', 'desc': 'Статья представляет LEGO-Puzzles - новый бенчмарк для оценки пространственного мышления и последовательного рассуждения у мультимодальных больших языковых моделей (MLLM). Бенчмарк состоит из 1100 задач визуальных вопросов и ответов на основе LEGO, охватывающих различные аспекты пространственного мышления. Результаты показывают, что даже самые мощные MLLM справляются лишь с половиной тестов, в то время как люди достигают более 90% точности. Исследование также выявило ограниченные возможности MLLM в генерации изображений LEGO по инструкциям сборки.'}, 'en': {'title': 'LEGO-Puzzles: Unveiling Spatial Reasoning Gaps in MLLMs', 'desc': "This paper introduces LEGO-Puzzles, a benchmark designed to evaluate the spatial reasoning and sequential understanding capabilities of Multimodal Large Language Models (MLLMs). The benchmark consists of 1,100 visual question-answering samples across 11 tasks, ranging from basic to complex reasoning. The evaluation reveals that current MLLMs struggle with spatial reasoning, achieving only about 50% accuracy compared to over 90% for humans. Additionally, the study assesses MLLMs' ability to generate images based on assembly instructions, finding that only a couple of models perform adequately, highlighting significant gaps in MLLMs' spatial understanding."}, 'zh': {'title': 'LEGO-Puzzles：评估多模态语言模型的空间推理能力', 'desc': '多步骤空间推理是理解和推理空间关系的重要能力，尤其在复杂的现实应用中，如机器人操作和自动导航。为评估当前多模态大型语言模型（MLLMs）在这一能力上的表现，我们引入了LEGO-Puzzles，这是一个可扩展的基准，旨在通过基于LEGO的任务评估空间理解和顺序推理。LEGO-Puzzles包含1100个精心策划的视觉问答样本，涵盖从基本空间理解到复杂多步骤推理的11个不同任务。我们的评估显示，现有的MLLMs在空间推理能力上存在显著不足，最强的模型仅能回答约一半的测试案例，而人类参与者的准确率超过90%。'}}}, {'id': 'https://huggingface.co/papers/2503.20201', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'url': 'https://huggingface.co/papers/2503.20201', 'abstract': "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.", 'score': 22, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'a9b1bed8d26f5055', 'authors': ['Salaheddin Alzubi', 'Creston Brooks', 'Purva Chiniya', 'Edoardo Contente', 'Chiara von Gerlach', 'Lucas Irwin', 'Yihan Jiang', 'Arda Kaz', 'Windsor Nguyen', 'Sewoong Oh', 'Himanshu Tyagi', 'Pramod Viswanath'], 'affiliations': ['Princeton University', 'Sentient', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.20201.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#agents', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'ODS: открытый ИИ-поиск на уровне проприетарных решений', 'desc': 'Open Deep Search (ODS) - это новая система, объединяющая возможности открытых языковых моделей с инструментами веб-поиска для ответов на запросы. ODS состоит из двух компонентов: Open Search Tool (инструмент поиска) и Open Reasoning Agent (агент рассуждений), которые работают с базовой языковой моделью по выбору пользователя. Система демонстрирует высокую эффективность на бенчмарках SimpleQA и FRAMES, превосходя существующие решения. ODS позволяет улучшить возможности любых языковых моделей, добавляя им функции поиска и рассуждений.'}, 'en': {'title': 'Empowering Open-Source LLMs with Advanced Search and Reasoning', 'desc': 'Open Deep Search (ODS) is a new framework designed to enhance the reasoning abilities of open-source large language models (LLMs) by integrating them with advanced web search tools. It features two main components: the Open Search Tool, which performs web searches, and the Open Reasoning Agent, which interprets tasks and coordinates actions, including using the search tool. ODS has shown to significantly improve performance on benchmarks like SimpleQA and FRAMES, even surpassing proprietary models like GPT-4o Search Preview in accuracy. This innovation allows users to leverage powerful open-source LLMs while achieving state-of-the-art results in query answering tasks.'}, 'zh': {'title': '开放深度搜索：提升开源LLM的推理与搜索能力', 'desc': '我们介绍了开放深度搜索（ODS），旨在缩小专有搜索人工智能解决方案与开源解决方案之间的差距。ODS的主要创新是通过推理代理增强最新开源大语言模型（LLM）的推理能力，使其能够明智地使用网络搜索工具来回答查询。ODS由两个组件组成：开放搜索工具和开放推理代理，后者负责解释任务并协调一系列操作，包括调用工具。通过与强大的开源推理LLM（如DeepSeek-R1）结合，ODS在SimpleQA和FRAMES两个基准测试中几乎达到了现有的最先进水平，甚至在FRAMES基准上提高了9.7%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.20240', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models', 'url': 'https://huggingface.co/papers/2503.20240', 'abstract': 'Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.', 'score': 19, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '4add99d8d7510263', 'authors': ['Prin Phunyaphibarn', 'Phillip Y. Lee', 'Jaihoon Kim', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20240.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#video'], 'emoji': '🔀', 'ru': {'title': 'Улучшение CFG: замена безусловного шума для качественной генерации', 'desc': 'Статья посвящена улучшению метода Classifier-Free Guidance (CFG) в обучении условных диффузионных моделей. Авторы обнаружили, что совместное обучение условного и безусловного предсказания шума приводит к ухудшению качества генерации. Они предлагают заменять безусловный шум в CFG на предсказания базовой модели, что значительно улучшает условную генерацию. Эксперименты подтверждают эффективность этого подхода для различных моделей генерации изображений и видео.'}, 'en': {'title': 'Enhancing Conditional Generation with Improved Unconditional Noise', 'desc': 'This paper discusses Classifier-Free Guidance (CFG), a technique used in training conditional diffusion models. The authors identify that using a single network for both conditional and unconditional noise prediction leads to poor performance in generating unconditional noise, which negatively impacts the quality of conditional outputs. They propose a solution where the unconditional noise predictions are replaced with those from a better-performing base model, resulting in improved conditional generation. The findings are validated through experiments on various CFG-based models for generating images and videos, demonstrating the effectiveness of their approach.'}, 'zh': {'title': '提升条件生成质量的无条件噪声替代', 'desc': '无分类器引导（CFG）是一种在训练条件扩散模型中使用的基本技术。传统的CFG训练方法是使用单一网络同时学习条件和无条件噪声预测，但这种联合学习会导致无条件噪声的先验质量较差。研究表明，使用更好的无条件生成模型的噪声替代可以显著提高条件生成的质量。我们通过实验验证了这一点，适用于多种基于CFG的条件模型，包括图像和视频生成。'}}}, {'id': 'https://huggingface.co/papers/2503.19480', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers', 'url': 'https://huggingface.co/papers/2503.19480', 'abstract': "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available.", 'score': 14, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '0dc97844010f5fb0', 'authors': ['Shijie Ma', 'Yuying Ge', 'Teng Wang', 'Yuxin Guo', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Institute of Automation, CAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.19480.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#optimization', '#benchmark', '#training'], 'emoji': '🔬', 'ru': {'title': 'GenHancer: Улучшение визуальных репрезентаций через синергию генеративных и дискриминативных моделей', 'desc': 'Статья исследует синергию между генеративными и дискриминативными моделями в контексте улучшения визуальных представлений. Авторы обнаружили, что визуально идеальные генерации не всегда оптимальны для улучшения репрезентаций и предложили метод GenHancer. Этот метод фокусируется на эффективном извлечении детальных знаний из генеративных моделей, используя глобальные визуальные токены и двухэтапную стратегию обучения. GenHancer превосходит предыдущие методы на бенчмарке MMVP-VLM, демонстрируя улучшение на 6.0% для OpenAICLIP.'}, 'en': {'title': 'Enhancing CLIP with GenHancer: Bridging Generative and Discriminative Models', 'desc': 'This paper investigates the relationship between generative and discriminative models in machine learning, specifically focusing on enhancing the performance of the Contrastive Language-Image Pre-Training (CLIP) model. The authors found that while generative models can improve representations, perfect visual generations do not always lead to optimal results. They propose a method called GenHancer, which effectively extracts fine-grained knowledge while reducing irrelevant information through careful conditioning and denoising strategies. Their approach outperforms existing methods on the MMVP-VLM benchmark, demonstrating its potential for improving vision-centric tasks in multimodal large language models.'}, 'zh': {'title': '生成与判别模型的完美结合', 'desc': '本文探讨了生成模型与判别模型之间的协同作用，特别是如何利用生成模型来增强判别模型CLIP的表示能力。研究发现，视觉上完美的生成并不总是最优的表示增强方式，关键在于有效提取细粒度知识并减少无关信息。我们提出了GenHancer方法，通过优化条件机制、去噪配置和生成范式，显著提升了CLIP在多模态任务中的表现。最终，GenHancer在MMVP-VLM基准测试中超越了之前的研究成果，展示了其在视觉中心性能上的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.20020', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'url': 'https://huggingface.co/papers/2503.20020', 'abstract': "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.", 'score': 13, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '5edeeaed81b90426', 'authors': ['Gemini Robotics Team', 'Saminda Abeyruwan', 'Joshua Ainslie', 'Jean-Baptiste Alayrac', 'Montserrat Gonzalez Arenas', 'Travis Armstrong', 'Ashwin Balakrishna', 'Robert Baruch', 'Maria Bauza', 'Michiel Blokzijl', 'Steven Bohez', 'Konstantinos Bousmalis', 'Anthony Brohan', 'Thomas Buschmann', 'Arunkumar Byravan', 'Serkan Cabi', 'Ken Caluwaerts', 'Federico Casarini', 'Oscar Chang', 'Jose Enrique Chen', 'Xi Chen', 'Hao-Tien Lewis Chiang', 'Krzysztof Choromanski', "David D'Ambrosio", 'Sudeep Dasari', 'Todor Davchev', 'Coline Devin', 'Norman Di Palo', 'Tianli Ding', 'Adil Dostmohamed', 'Danny Driess', 'Yilun Du', 'Debidatta Dwibedi', 'Michael Elabd', 'Claudio Fantacci', 'Cody Fong', 'Erik Frey', 'Chuyuan Fu', 'Marissa Giustina', 'Keerthana Gopalakrishnan', 'Laura Graesser', 'Leonard Hasenclever', 'Nicolas Heess', 'Brandon Hernaez', 'Alexander Herzog', 'R. Alex Hofer', 'Jan Humplik', 'Atil Iscen', 'Mithun George Jacob', 'Deepali Jain', 'Ryan Julian', 'Dmitry Kalashnikov', 'M. Emre Karagozler', 'Stefani Karp', 'Chase Kew', 'Jerad Kirkland', 'Sean Kirmani', 'Yuheng Kuang', 'Thomas Lampe', 'Antoine Laurens', 'Isabel Leal', 'Alex X. Lee', 'Tsang-Wei Edward Lee', 'Jacky Liang', 'Yixin Lin', 'Sharath Maddineni', 'Anirudha Majumdar', 'Assaf Hurwitz Michaely', 'Robert Moreno', 'Michael Neunert', 'Francesco Nori', 'Carolina Parada', 'Emilio Parisotto', 'Peter Pastor', 'Acorn Pooley', 'Kanishka Rao', 'Krista Reymann', 'Dorsa Sadigh', 'Stefano Saliceti', 'Pannag Sanketi', 'Pierre Sermanet', 'Dhruv Shah', 'Mohit Sharma', 'Kathryn Shea', 'Charles Shu', 'Vikas Sindhwani', 'Sumeet Singh', 'Radu Soricut', 'Jost Tobias Springenberg', 'Rachel Sterneck', 'Razvan Surdulescu', 'Jie Tan', 'Jonathan Tompson', 'Vincent Vanhoucke', 'Jake Varley', 'Grace Vesom', 'Giulia Vezzani', 'Oriol Vinyals', 'Ayzaan Wahid', 'Stefan Welker', 'Paul Wohlhart', 'Fei Xia', 'Ted Xiao', 'Annie Xie', 'Jinyu Xie', 'Peng Xu', 'Sichun Xu', 'Ying Xu', 'Zhuo Xu', 'Yuxiang Yang', 'Rui Yao', 'Sergey Yaroshenko', 'Wenhao Yu', 'Wentao Yuan', 'Jingwei Zhang', 'Tingnan Zhang', 'Allan Zhou', 'Yuxiang Zhou'], 'affiliations': ['Gemini Robotics Team, Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.20020.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#agents', '#agi', '#ethics', '#games', '#reasoning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Gemini Robotics: ИИ выходит в реальный мир', 'desc': 'Статья представляет семейство моделей Gemini Robotics, основанных на Gemini 2.0 и предназначенных для управления роботами. Gemini Robotics - это усовершенствованная мультимодальная модель типа Vision-Language-Action, способная напрямую контролировать роботов и выполнять сложные манипуляции. Модель Gemini Robotics-ER расширяет возможности рассуждений Gemini на физический мир, улучшая пространственное и временное понимание. Исследователи демонстрируют, как эти модели могут быть применены в различных робототехнических задачах, включая обнаружение объектов, прогнозирование траекторий и адаптацию к новым воплощениям роботов.'}, 'en': {'title': 'Empowering Robots with Gemini Robotics: A Leap in Multimodal AI', 'desc': "This paper presents Gemini Robotics, a new family of AI models designed specifically for robotic applications, building on the Gemini 2.0 framework. The model integrates Vision-Language-Action (VLA) capabilities, allowing robots to perform complex manipulation tasks with smooth and adaptive movements. It can learn from limited demonstrations and adapt to new tasks and robot designs, showcasing its versatility in various environments. Additionally, the paper introduces Gemini Robotics-ER, which enhances the model's reasoning abilities in physical contexts, addressing safety and practical applications in robotics."}, 'zh': {'title': 'Gemini Robotics：通用机器人的新纪元', 'desc': '最近大型多模态模型的进展使得数字领域的通用能力显著提升，但将其应用于机器人等物理代理仍然面临挑战。本文介绍了一种新型的人工智能模型，专为机器人设计，基于Gemini 2.0构建。Gemini Robotics是一个先进的视觉-语言-动作（VLA）通用模型，能够直接控制机器人，执行复杂的操作任务，并对物体类型和位置的变化具有鲁棒性。通过额外的微调，Gemini Robotics可以专门化为新的能力，包括解决长时间跨度的高灵巧任务，以及从少量示例中学习新任务。'}}}, {'id': 'https://huggingface.co/papers/2503.19786', 'title': 'Gemma 3 Technical Report', 'url': 'https://huggingface.co/papers/2503.19786', 'abstract': 'We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.', 'score': 12, 'issue_id': 2934, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'df574ff057c95baa', 'authors': ['Gemma Team', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre Ramé', 'Morgane Rivière', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Etienne Pot', 'Ivo Penchev', 'Gaël Liu', 'Francesco Visin', 'Kathleen Kenealy', 'Lucas Beyer', 'Xiaohai Zhai', 'Anton Tsitsulin', 'Robert Busa-Fekete', 'Alex Feng', 'Noveen Sachdeva', 'Benjamin Coleman', 'Yi Gao', 'Basil Mustafa', 'Iain Barr', 'Emilio Parisotto', 'David Tian', 'Matan Eyal', 'Colin Cherry', 'Jan-Thorsten Peter', 'Danila Sinopalnikov', 'Surya Bhupatiraju', 'Rishabh Agarwal', 'Mehran Kazemi', 'Dan Malkin', 'Ravin Kumar', 'David Vilar', 'Idan Brusilovsky', 'Jiaming Luo', 'Andreas Steiner', 'Abe Friesen', 'Abhanshu Sharma', 'Abheesht Sharma', 'Adi Mayrav Gilady', 'Adrian Goedeckemeyer', 'Alaa Saade', 'Alex Feng', 'Alexander Kolesnikov', 'Alexei Bendebury', 'Alvin Abdagic', 'Amit Vadi', 'András György', 'André Susano Pinto', 'Anil Das', 'Ankur Bapna', 'Antoine Miech', 'Antoine Yang', 'Antonia Paterson', 'Ashish Shenoy', 'Ayan Chakrabarti', 'Bilal Piot', 'Bo Wu', 'Bobak Shahriari', 'Bryce Petrini', 'Charlie Chen', 'Charline Le Lan', 'Christopher A. Choquette-Choo', 'CJ Carey', 'Cormac Brick', 'Daniel Deutsch', 'Danielle Eisenbud', 'Dee Cattle', 'Derek Cheng', 'Dimitris Paparas', 'Divyashree Shivakumar Sreepathihalli', 'Doug Reid', 'Dustin Tran', 'Dustin Zelle', 'Eric Noland', 'Erwin Huizenga', 'Eugene Kharitonov', 'Frederick Liu', 'Gagik Amirkhanyan', 'Glenn Cameron', 'Hadi Hashemi', 'Hanna Klimczak-Plucińska', 'Harman Singh', 'Harsh Mehta', 'Harshal Tushar Lehri', 'Hussein Hazimeh', 'Ian Ballantyne', 'Idan Szpektor', 'Ivan Nardini', 'Jean Pouget-Abadie', 'Jetha Chan', 'Joe Stanton', 'John Wieting', 'Jonathan Lai', 'Jordi Orbay', 'Joseph Fernandez', 'Josh Newlan', 'Ju-yeong Ji', 'Jyotinder Singh', 'Kat Black', 'Kathy Yu', 'Kevin Hui', 'Kiran Vodrahalli', 'Klaus Greff', 'Linhai Qiu', 'Marcella Valentine', 'Marina Coelho', 'Marvin Ritter', 'Matt Hoffman', 'Matthew Watson', 'Mayank Chaturvedi', 'Michael Moynihan', 'Min Ma', 'Nabila Babar', 'Natasha Noy', 'Nathan Byrd', 'Nick Roy', 'Nikola Momchev', 'Nilay Chauhan', 'Noveen Sachdeva', 'Oskar Bunyan', 'Pankil Botarda', 'Paul Caron', 'Paul Kishan Rubenstein', 'Phil Culliton', 'Philipp Schmid', 'Pier Giuseppe Sessa', 'Pingmei Xu', 'Piotr Stanczyk', 'Pouya Tafti', 'Rakesh Shivanna', 'Renjie Wu', 'Renke Pan', 'Reza Rokni', 'Rob Willoughby', 'Rohith Vallu', 'Ryan Mullins', 'Sammy Jerome', 'Sara Smoot', 'Sertan Girgin', 'Shariq Iqbal', 'Shashir Reddy', 'Shruti Sheth', 'Siim Põder', 'Sijal Bhatnagar', 'Sindhu Raghuram Panyam', 'Sivan Eiger', 'Susan Zhang', 'Tianqi Liu', 'Trevor Yacovone', 'Tyler Liechty', 'Uday Kalra', 'Utku Evci', 'Vedant Misra', 'Vincent Roseberry', 'Vlad Feinberg', 'Vlad Kolesnikov', 'Woohyun Han', 'Woosuk Kwon', 'Xi Chen', 'Yinlam Chow', 'Yuvein Zhu', 'Zichuan Wei', 'Zoltan Egyed', 'Victor Cotruta', 'Minh Giang', 'Phoebe Kirk', 'Anand Rao', 'Kat Black', 'Nabila Babar', 'Jessica Lo', 'Erica Moreira', 'Luiz Gustavo Martins', 'Omar Sanseviero', 'Lucas Gonzalez', 'Zach Gleicher', 'Tris Warkentin', 'Vahab Mirrokni', 'Evan Senter', 'Eli Collins', 'Joelle Barral', 'Zoubin Ghahramani', 'Raia Hadsell', 'Yossi Matias', 'D. Sculley', 'Slav Petrov', 'Noah Fiedel', 'Noam Shazeer', 'Oriol Vinyals', 'Jeff Dean', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Clement Farabet', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Rohan Anil', 'Dmitry', 'Lepikhin', 'Sebastian Borgeaud', 'Olivier Bachem', 'Armand Joulin', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'Léonard Hussenot'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.19786.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#open_source', '#multilingual', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Gemma 3: Мультимодальный ИИ с улучшенной эффективностью и расширенными возможностями', 'desc': 'Представлена Gemma 3 - мультимодальная версия семейства легковесных открытых моделей Gemma с диапазоном от 1 до 27 миллиардов параметров. Модель получила возможности понимания изображений, расширенную поддержку языков и увеличенный контекст до 128К токенов. Архитектура была изменена для уменьшения KV-кэша памяти путем увеличения соотношения локальных и глобальных слоев внимания. Модели Gemma 3 обучены с использованием дистилляции и показывают превосходную производительность по сравнению с Gemma 2 как в предобученных, так и в инструктированных версиях.'}, 'en': {'title': 'Gemma 3: Multimodal Mastery with Extended Context!', 'desc': 'Gemma 3 is a new version of the Gemma model family that enhances multimodal capabilities, allowing it to understand both text and images. It features a larger scale, with models ranging from 1 to 27 billion parameters, and supports longer context lengths of at least 128K tokens. The architecture has been optimized to manage memory usage better during long contexts by adjusting the balance of local and global attention layers. With improved training techniques, Gemma 3 outperforms its predecessor, Gemma 2, in various tasks including math, chat, and multilingual processing, making it a strong competitor in the field.'}, 'zh': {'title': 'Gemma 3：多模态轻量级模型的突破', 'desc': 'Gemma 3 是 Gemma 系列轻量级开放模型的多模态版本，参数规模从 1 到 270 亿不等。该版本引入了视觉理解能力，支持更多语言，并且能够处理更长的上下文，至少达到 128K 个标记。我们通过调整模型架构，增加局部注意力层与全局注意力层的比例，来减少长上下文下 KV-cache 内存的爆炸。Gemma 3 模型经过蒸馏训练，表现优于 Gemma 2，特别是在数学、对话、指令跟随和多语言能力方面有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.20672', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation', 'url': 'https://huggingface.co/papers/2503.20672', 'abstract': 'Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.', 'score': 11, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'b04cbfb976ce4e45', 'authors': ['Yuyang Peng', 'Shishi Xiao', 'Keming Wu', 'Qisheng Liao', 'Bohan Chen', 'Kevin Lin', 'Danqing Huang', 'Ji Li', 'Yuhui Yuan'], 'affiliations': ['Brown University', 'Microsoft', 'Microsoft Research Asia', 'Tsinghua University', 'University of Liverpool'], 'pdf_title_img': 'assets/pdf/title_img/2503.20672.jpg', 'data': {'categories': ['#long_context', '#cv', '#synthetic', '#dataset', '#rag'], 'emoji': '📊', 'ru': {'title': 'Революция в генерации бизнес-контента: от текста к инфографике', 'desc': 'Статья представляет новый подход к генерации бизнес-контента, включая инфографику и слайды, на основе пользовательских запросов уровня статьи и сверхплотных макетов. Авторы создали масштабируемый набор данных Infographics-650K с высококачественным бизнес-контентом, используя послойную схему генерации инфографики с помощью извлечения информации. Они также разработали схему кросс-внимания с учетом макета, которая внедряет десятки регион-специфичных подсказок в латентное пространство обрезанных регионов согласно сверхплотным макетам. Результаты показывают превосходство предложенной системы над современными аналогами, такими как Flux и SD3, на наборе запросов BizEval.'}, 'en': {'title': 'Revolutionizing Business Content Generation with Ultra-Dense Layouts', 'desc': 'This paper presents advancements in text-to-image generation, specifically targeting the creation of business content like infographics and slides from article-level prompts. The authors introduce a new dataset, Infographics-650K, which includes ultra-dense layouts and is designed to address the challenges of longer context lengths and limited high-quality data. They propose a layout-guided cross attention mechanism that allows for precise generation across multiple sub-regions, enhancing the fidelity of the output. The results show significant improvements over existing models, encouraging further research in business content generation.'}, 'zh': {'title': '推动商业内容生成的新突破', 'desc': '本文关注于文章级视觉文本渲染的挑战，特别是在生成高质量商业内容方面，如信息图和幻灯片。我们提出了一种新的任务，旨在根据用户提供的描述性提示和超密集布局生成这些内容。我们构建了一个可扩展的高质量商业内容数据集Infographics-650K，并实现了一种基于布局的交叉注意力机制，以处理复杂的区域提示。我们的系统在与现有最先进系统的比较中表现出色，并通过消融实验验证了各个组件的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.20757', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search', 'url': 'https://huggingface.co/papers/2503.20757', 'abstract': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.', 'score': 7, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'd4c3b116518a2b0f', 'authors': ['Yunhai Hu', 'Yilun Zhao', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20757.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#rag', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'MCTS-RAG: Усиление рассуждений малых языковых моделей', 'desc': 'MCTS-RAG - это новый подход, объединяющий retrieval-augmented generation (RAG) и Monte Carlo Tree Search (MCTS) для улучшения рассуждений малых языковых моделей в задачах, требующих обширных знаний. Метод динамически интегрирует поиск информации и рассуждение через итеративный процесс принятия решений. MCTS-RAG сочетает структурированное рассуждение с адаптивным поиском, что улучшает принятие решений, снижает галлюцинации и повышает фактическую точность. Эксперименты показывают, что этот метод позволяет малым языковым моделям достичь производительности, сравнимой с крупными моделями типа GPT-4, эффективно масштабируя вычисления во время вывода.'}, 'en': {'title': 'Enhancing Small Models with Smart Retrieval and Reasoning', 'desc': 'MCTS-RAG is a new method that improves how small language models handle complex tasks that require knowledge. It combines retrieval-augmented generation (RAG) with Monte Carlo Tree Search (MCTS) to enhance reasoning by providing relevant information during the decision-making process. This approach allows the model to access external facts while reasoning, leading to better accuracy and consistency in responses. Experiments show that MCTS-RAG enables smaller models to perform as well as larger models like GPT-4o on challenging datasets.'}, 'zh': {'title': 'MCTS-RAG：小型模型推理的新标准', 'desc': '我们介绍了一种新方法MCTS-RAG，它通过结合检索增强生成（RAG）和蒙特卡洛树搜索（MCTS），提升小型语言模型在知识密集型任务上的推理能力。MCTS-RAG通过迭代决策过程动态整合检索和推理，克服了传统RAG方法和MCTS推理的局限性。与标准RAG方法不同，MCTS-RAG能够更好地结合结构化推理和自适应检索，从而提高决策质量，减少幻觉现象，并确保更高的事实准确性和响应一致性。实验结果表明，该方法使小型语言模型的性能可与前沿大型语言模型（如GPT-4o）相媲美，树立了小型模型推理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2503.19950', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation', 'url': 'https://huggingface.co/papers/2503.19950', 'abstract': "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.", 'score': 7, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'b75e0acc68cc5153', 'authors': ['Han Chen', 'Zicong Jiang', 'Zining Zhang', 'Bingsheng He', 'Pingyi Luo', 'Mian Lu', 'Yuqiang Chen'], 'affiliations': ['4Paradigm', 'School of Computing National University of Singapore', 'School of Electronic and Information Engineering South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.19950.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие памяти для ускорения работы языковых моделей', 'desc': 'LogQuant - это новая техника 2-битной квантизации для KV-кэша при инференсе больших языковых моделей. Она применяет логарифмический механизм фильтрации для выборочного сжатия KV-кэша по всему контексту. В тестах LogQuant повышает пропускную способность на 25% и увеличивает размер батча на 60% без роста потребления памяти. Для сложных задач, таких как математика и завершение кода, LogQuant улучшает точность на 40-200% при том же коэффициенте сжатия.'}, 'en': {'title': 'LogQuant: Efficient 2-Bit Quantization for Enhanced LLM Performance', 'desc': 'LogQuant is a novel 2-bit quantization method designed for efficiently managing the KV Cache in large language model inference. Unlike previous techniques that prioritize later tokens or rely on early attention patterns, LogQuant employs a log-based filtering mechanism to compress the KV Cache more effectively. This approach not only reduces memory usage but also enhances performance, achieving a 25% increase in throughput and a 60% increase in batch size without additional memory costs. In challenging tasks like Math and Code Completion, LogQuant significantly boosts accuracy by 40% to 200% while maintaining the same compression ratio, making it a superior choice for LLM applications.'}, 'zh': {'title': 'LogQuant：高效的KV缓存量化技术', 'desc': 'LogQuant是一种创新的2位量化技术，专为大型语言模型（LLM）推理中的KV缓存设计。它通过应用基于对数的过滤机制，选择性地压缩KV缓存，从而在保持优越性能的同时显著节省内存。与以往方法不同，LogQuant避免了性能瓶颈和频繁的错误预测，提升了25%的吞吐量和60%的批处理大小。对于数学和代码补全等复杂任务，LogQuant在相同压缩比下提高了40%到200%的准确性，超越了类似技术。'}}}, {'id': 'https://huggingface.co/papers/2503.20271', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'url': 'https://huggingface.co/papers/2503.20271', 'abstract': "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data.", 'score': 6, 'issue_id': 2925, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '53462508b8990597', 'authors': ['Haoqin Tu', 'Weitao Feng', 'Hardy Chen', 'Hui Liu', 'Xianfeng Tang', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'UC Santa Cruz', 'UT Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2503.20271.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Прогресс в оценке и улучшении мультимодальных моделей вознаграждения', 'desc': 'Статья исследует эффективность моделей вознаграждения, контролируемых процессом (PRM), в области мультимодального машинного обучения. Авторы проводят сравнительный анализ визуально-языковых моделей (VLLM) в качестве моделей вознаграждения на различных бенчмарках. Они представляют новый бенчмарк ViLBench, специально разработанный для оценки интенсивных сигналов вознаграждения процесса. Исследование также демонстрирует перспективный подход к улучшению производительности VLLM с помощью сбора данных о вознаграждениях процесса и применения улучшенного алгоритма поиска по дереву.'}, 'en': {'title': 'Enhancing Multimodal Learning with Process-Supervised Rewards', 'desc': 'This paper explores the use of process-supervised reward models (PRMs) to provide detailed feedback for complex reasoning tasks in the multimodal domain. It benchmarks vision large language models (VLLMs) as output reward models (ORMs) and PRMs across various vision-language tasks, finding that neither consistently outperforms the other. The authors introduce ViLBench, a challenging benchmark that emphasizes the need for process reward signals, revealing that even advanced models like GPT-4o struggle with its complexity. Additionally, they demonstrate a method to enhance reward model performance by collecting a large dataset of vision-language process rewards, leading to measurable improvements in model accuracy.'}, 'zh': {'title': '提升视觉-语言模型的奖励评估', 'desc': '本文探讨了过程监督奖励模型（PRMs）在复杂任务中的应用，提供了详细的逐步反馈以帮助选择推理路径。尽管PRMs具有优势，但在多模态领域的评估仍然较少。我们首次对当前的视觉大语言模型（VLLMs）进行基准测试，发现输出奖励模型（ORMs）和过程奖励模型（PRMs）在不同任务中的表现并不一致。为此，我们引入了ViLBench，一个需要强烈过程奖励信号的视觉-语言基准，展示了当前VLLMs在此基准上的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.19462', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'url': 'https://huggingface.co/papers/2503.19462', 'abstract': 'Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challenges present in existing diffusion distillation methods and propose a novel efficient method, namely AccVideo, to reduce the inference steps for accelerating video diffusion models with synthetic dataset. We leverage the pretrained video diffusion model to generate multiple valid denoising trajectories as our synthetic dataset, which eliminates the use of useless data points during distillation. Based on the synthetic dataset, we design a trajectory-based few-step guidance that utilizes key data points from the denoising trajectories to learn the noise-to-video mapping, enabling video generation in fewer steps. Furthermore, since the synthetic dataset captures the data distribution at each diffusion timestep, we introduce an adversarial training strategy to align the output distribution of the student model with that of our synthetic dataset, thereby enhancing the video quality. Extensive experiments demonstrate that our model achieves 8.5x improvements in generation speed compared to the teacher model while maintaining comparable performance. Compared to previous accelerating methods, our approach is capable of generating videos with higher quality and resolution, i.e., 5-seconds, 720x1280, 24fps.', 'score': 5, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '721d2bb59c963434', 'authors': ['Haiyu Zhang', 'Xinyuan Chen', 'Yaohui Wang', 'Xihui Liu', 'Yunhong Wang', 'Yu Qiao'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19462.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#inference', '#video', '#dataset', '#synthetic'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео с помощью синтетических данных и дистилляции диффузионных моделей', 'desc': 'Статья представляет новый метод AccVideo для ускорения генерации видео с помощью диффузионных моделей. Авторы используют предобученную модель для создания синтетического набора данных с множественными траекториями денойзинга. Предложенный подход включает траекторно-ориентированное руководство по малошаговой генерации и состязательное обучение для улучшения качества видео. Эксперименты показывают 8.5-кратное ускорение генерации по сравнению с исходной моделью при сохранении сопоставимого качества.'}, 'en': {'title': 'Accelerating Video Generation with AccVideo', 'desc': 'This paper addresses the slow and resource-intensive process of video generation using diffusion models, which require many steps for denoising. The authors introduce AccVideo, a new method that reduces the number of inference steps needed by utilizing a synthetic dataset generated from a pretrained video diffusion model. By focusing on key data points from denoising trajectories, the model learns to map noise to video more efficiently. Additionally, an adversarial training strategy is employed to improve the quality of the generated videos, resulting in significant speed improvements while maintaining high resolution and quality.'}, 'zh': {'title': '加速视频生成，提升质量与效率', 'desc': '扩散模型在视频生成领域取得了显著进展，但其迭代去噪的特性导致生成视频需要大量推理步骤，速度慢且计算成本高。本文分析了现有扩散蒸馏方法中的挑战，并提出了一种新颖的高效方法AccVideo，以减少推理步骤，加速视频扩散模型。我们利用预训练的视频扩散模型生成多个有效的去噪轨迹作为合成数据集，从而在蒸馏过程中消除无用数据点。通过设计基于轨迹的少步引导，我们能够在更少的步骤中实现视频生成，同时引入对抗训练策略以提高视频质量。'}}}, {'id': 'https://huggingface.co/papers/2503.19846', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'url': 'https://huggingface.co/papers/2503.19846', 'abstract': "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels.", 'score': 4, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'a4039050131ab9d6', 'authors': ['Aaron Serianni', 'Tyler Zhu', 'Olga Russakovsky', 'Vikram V. Ramaswamy'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19846.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#cv', '#dataset', '#interpretability'], 'emoji': '👁️', 'ru': {'title': 'Новый взгляд на предвзятость нейросетей через призму внимания', 'desc': 'Статья представляет новый метод оценки предвзятости в моделях компьютерного зрения - Attention-IoU. В отличие от существующих методов, Attention-IoU анализирует внутренние представления модели, используя карты внимания. Авторы валидируют метрику на синтетическом наборе данных Waterbirds и применяют ее к набору CelebA, обнаруживая корреляции, выходящие за рамки различий в точности. Исследование показывает, что Attention-IoU может выявлять потенциальные искажающие переменные, не присутствующие в метках набора данных.'}, 'en': {'title': 'Unveiling Biases with Attention-IoU in Computer Vision Models', 'desc': "This paper addresses the issue of bias in computer vision models, which can be amplified by the datasets they are trained on. The authors introduce a new metric called Attention-IoU, which utilizes attention maps to uncover biases in a model's internal representations rather than just focusing on dataset distribution or performance metrics. They validate this metric using the Waterbirds dataset and further analyze the CelebA dataset to reveal hidden correlations that go beyond mere accuracy differences. By manipulating the training set, they demonstrate that Attention-IoU can identify confounding variables that are not explicitly labeled in the dataset, providing deeper insights into model biases."}, 'zh': {'title': '揭示模型内部偏见的注意力交并比', 'desc': '本文探讨了计算机视觉模型在不同数据集和任务中表现出的偏见。现有的量化分类模型偏见的方法主要关注数据集分布和模型在子群体上的表现，而忽视了模型内部的工作机制。我们提出了注意力交并比（Attention-IoU）指标，通过注意力图揭示模型内部表示中的偏见，并识别可能导致偏见的图像特征。通过对Waterbirds和CelebA数据集的分析，我们验证了Attention-IoU的有效性，并发现其能够揭示超出准确性差异的相关性。'}}}, {'id': 'https://huggingface.co/papers/2503.20756', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems', 'url': 'https://huggingface.co/papers/2503.20756', 'abstract': "Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.", 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '6b8affbfbdd5a426', 'authors': ['Chenxi Wang', 'Jizhan Fang', 'Xiang Chen', 'Bozhong Tian', 'Ziwen Xu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'Редактирование знаний для улучшения автономного вождения', 'desc': 'Статья описывает применение больших мультимодальных моделей (LMM) в системах автономного вождения (ADS). Авторы предлагают использовать редактирование знаний для улучшения работы моделей без полного переобучения. Они также представляют набор данных ADS-Edit для оценки редактирования знаний в контексте автономного вождения. Проведенные эксперименты показывают перспективность этого подхода для развития автономных транспортных средств.'}, 'en': {'title': 'Enhancing Autonomous Driving with Targeted Knowledge Editing', 'desc': "This paper discusses the use of Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS) and the challenges they face, such as traffic knowledge misunderstanding and complex road conditions. To overcome these issues, the authors propose a method called Knowledge Editing, which allows for specific adjustments to a model's behavior without needing to retrain it entirely. They also introduce ADS-Edit, a specialized dataset for multimodal knowledge editing in ADS, which includes diverse real-world scenarios and various data types. The paper presents experimental results that highlight the potential of knowledge editing to enhance the performance of autonomous driving systems."}, 'zh': {'title': '知识编辑助力自动驾驶系统的进步', 'desc': '本文探讨了大型多模态模型（LMMs）在自动驾驶系统（ADS）中的应用潜力。尽管LMMs有前景，但在交通知识理解、复杂路况和车辆多样性等方面面临挑战。为了解决这些问题，我们提出了知识编辑的方法，可以在不完全重训练的情况下，针对性地修改模型的行为。同时，我们还引入了ADS-Edit，这是一个专为自动驾驶设计的多模态知识编辑数据集，包含多种真实场景和数据类型。'}}}, {'id': 'https://huggingface.co/papers/2503.20220', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'url': 'https://huggingface.co/papers/2503.20220', 'abstract': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.', 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'f65c515bbb6af49b', 'authors': ['Weijie Guo', 'Guofeng Zhang', 'Wufei Ma', 'Alan Yuille'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20220.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#robotics', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Точная 3D-поза без 3D-разметки', 'desc': 'DINeMo - это новая нейронная сетевая модель для оценки 3D/6D позы объектов на уровне категорий без использования 3D-разметки. Модель использует псевдо-соответствия, полученные из крупных визуальных фундаментальных моделей, применяя двунаправленный метод генерации. DINeMo значительно превосходит предыдущие методы оценки 3D-позы с нулевым и малым количеством примеров, сокращая разрыв с полностью контролируемыми методами на 67.3%. Модель эффективно масштабируется при добавлении неразмеченных изображений в процессе обучения.'}, 'en': {'title': 'Revolutionizing 3D Pose Estimation with Unlabeled Data', 'desc': 'This paper introduces DINeMo, a new neural mesh model designed for category-level 3D/6D pose estimation without relying on 3D annotations. It utilizes pseudo-correspondence generated from large visual foundation models, allowing it to learn from unlabeled data effectively. The model employs a bidirectional approach to generate pseudo correspondences, combining local appearance features with global context. Experimental results show that DINeMo significantly improves performance in zero- and few-shot 3D pose estimation, achieving results close to fully-supervised methods while being more scalable and efficient.'}, 'zh': {'title': '无标注3D姿态估计的新突破', 'desc': '本论文提出了一种新的神经网格模型DINeMo，用于类别级的3D/6D姿态估计，旨在提高3D场景理解的能力。与以往依赖3D标注的学习方法不同，DINeMo通过利用大型视觉基础模型获得的伪对应关系进行训练，从而避免了对3D标注的依赖。我们采用双向伪对应生成方法，结合局部外观特征和全局上下文信息，显著提升了模型的鲁棒性。实验结果表明，DINeMo在车类数据集上的表现超越了之前的零样本和少样本3D姿态估计方法，缩小了与完全监督方法的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.20198', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models', 'url': 'https://huggingface.co/papers/2503.20198', 'abstract': 'Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \\ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \\ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \\ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.', 'score': 3, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'b2b966b253011624', 'authors': ['Alex Jinpeng Wang', 'Linjie Li', 'Zhengyuan Yang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.20198.jpg', 'data': {'categories': ['#long_context', '#cv', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Новая эра генерации изображений с длинными текстами', 'desc': 'Современные autoregressive и diffusion модели хорошо справляются с генерацией изображений с короткими текстами, но испытывают трудности с длинными текстами, такими как абзацы в документах. В этой работе представлена первая модель, специально разработанная для генерации изображений с длинными текстами, что закрывает важный пробел в существующих системах text-to-image. Основной проблемой является image tokenizer, который ограничивает качество генерации текста. Для решения этой проблемы был разработан новый бинарный tokenizer, оптимизированный для детального захвата текстовых особенностей, что позволило создать модель, превосходящую существующие решения в точности и гибкости генерации длинных текстов.'}, 'en': {'title': 'Revolutionizing Long-Text Image Generation with \\ModelName', 'desc': 'This paper introduces a new approach to generating long-form text in images, which has been a challenge for existing generative models. The authors identify that the image tokenizer is a key limitation in the quality of text generation. To overcome this, they propose a novel binary tokenizer that focuses on capturing detailed features of scene text. Their multimodal autoregressive model, named \\ModelName, demonstrates superior performance in generating high-quality long-text images, allowing for customizable text properties and paving the way for new applications in document and presentation generation.'}, 'zh': {'title': '长文本图像生成的新突破', 'desc': '最近，自回归和扩散模型的进展使得短文本图像生成表现出色。然而，生成连贯的长文本图像（如幻灯片或文档中的段落）仍然是当前生成模型面临的主要挑战。我们首次专注于长文本图像生成，填补了现有文本到图像系统的关键空白。通过分析最先进的自回归生成模型，我们发现图像分词器是影响文本生成质量的关键瓶颈，因此我们提出了一种新的文本专注的二进制分词器，以优化细节场景文本特征的捕捉。'}}}, {'id': 'https://huggingface.co/papers/2503.17358', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image', 'url': 'https://huggingface.co/papers/2503.17358', 'abstract': 'In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.', 'score': 3, 'issue_id': 2927, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '491a350f070233ec', 'authors': ['Jerred Chen', 'Ronald Clark'], 'affiliations': ['University of Oxford Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.17358.jpg', 'data': {'categories': ['#dataset', '#robotics', '#training', '#cv', '#benchmark'], 'emoji': '📷', 'ru': {'title': 'Размытие в движении как ключ к точному позиционированию камеры', 'desc': 'Статья представляет новый подход к оценке положения камеры в условиях сильного размытия изображения при быстром движении. Метод предсказывает поле плотного потока движения и карту глубины по одному размытому изображению. Затем восстанавливается мгновенная скорость камеры путем решения задачи наименьших квадратов. Модель обучается на синтетических данных и дообучается на реальных изображениях с помощью полностью дифференцируемого конвейера.'}, 'en': {'title': 'Harnessing Motion Blur for Enhanced Camera Motion Estimation', 'desc': "This paper introduces a new method for estimating camera motion in situations where fast movements cause motion blur, which typically hinders existing techniques. Instead of viewing motion blur as a problem, the authors utilize it to predict a dense motion flow field and a depth map from a single blurred image. They calculate the camera's instantaneous velocity by solving a linear least squares problem, effectively treating the motion blur as valuable information. The proposed framework is trained on a large dataset and shows superior performance in estimating camera velocities compared to traditional methods."}, 'zh': {'title': '利用运动模糊提升相机运动估计的创新方法', 'desc': '在许多机器人和虚拟现实/增强现实应用中，快速的相机运动会导致严重的运动模糊，现有的相机姿态估计方法因此失效。我们提出了一种新颖的框架，将运动模糊视为运动估计的丰富线索，而不是不必要的伪影。该方法通过从单一的运动模糊图像中直接预测密集的运动流场和单目深度图来工作。我们的模型经过大规模合成运动模糊数据集训练，并在真实数据上进行端到端的微调，最终在真实世界基准测试中表现出色，超越了现有的MASt3R和COLMAP等方法。'}}}, {'id': 'https://huggingface.co/papers/2503.20641', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'url': 'https://huggingface.co/papers/2503.20641', 'abstract': "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", 'score': 2, 'issue_id': 2932, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '52b4dbdb179d7229', 'authors': ['Han Wu', 'Yuxuan Yao', 'Shuqi Liu', 'Zehua Liu', 'Xiaojin Fu', 'Xiongwei Han', 'Xing Li', 'Hui-Ling Zhen', 'Tao Zhong', 'Mingxuan Yuan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.20641.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LLM: от длинного к короткому через объединение моделей', 'desc': 'Статья посвящена проблеме избыточного мышления в больших языковых моделях (LLM) при переходе от быстрого интуитивного мышления (System 1) к медленному аналитическому (System 2). Авторы предлагают метод объединения моделей (model merging) для достижения баланса между глубиной рассуждений и эффективностью. Эксперименты показывают, что этот подход может сократить среднюю длину ответа на 55% при сохранении или улучшении производительности. Исследование также выявляет сильную корреляцию между масштабом модели и эффективностью объединения.'}, 'en': {'title': 'Efficient Reasoning through Model Merging', 'desc': 'This paper discusses the transition from quick, intuitive reasoning (System 1) to more deliberate, analytical reasoning (System 2) in large language models (LLMs). It highlights the inefficiencies that arise when models overthink, leading to unnecessary complexity without significant gains in output quality. The authors propose a method called Long-to-Short (L2S) reasoning, which aims to optimize the balance between deep reasoning and efficiency. They introduce model merging as a solution, which combines the strengths of both reasoning systems, demonstrating that this approach can significantly reduce response length while maintaining or enhancing performance.'}, 'zh': {'title': '模型合并：高效的长到短推理解决方案', 'desc': '本文探讨了大型语言模型（LLMs）在复杂任务中从系统1推理到系统2推理的转变。尽管这种进步提高了推理的深度，但往往导致效率下降，模型可能会产生冗余的推理步骤。为了解决这个问题，长到短（L2S）推理提出了一种平衡推理深度与效率的方案。通过模型合并，我们能够将系统1模型的快速思维与系统2模型的系统性推理结合，从而在保持性能的同时显著减少响应长度。'}}}, {'id': 'https://huggingface.co/papers/2503.19953', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals', 'url': 'https://huggingface.co/papers/2503.19953', 'abstract': "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.", 'score': 2, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '01093e8b98f32607', 'authors': ['Stefan Stojanov', 'David Wendt', 'Seungwoo Kim', 'Rahul Venkatesh', 'Kevin Feigelis', 'Jiajun Wu', 'Daniel LK Yamins'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19953.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'Самообучаемая оценка движения в видео без размеченных данных', 'desc': 'Opt-CWM - это самообучаемый метод для оценки потока и окклюзии на основе предобученной модели предсказания следующего кадра. Он работает путем оптимизации контрфактических проб, извлекающих информацию о движении из базовой видеомодели. Этот подход позволяет избежать использования фиксированных эвристик и обучаться на неограниченных видеовходах. Opt-CWM достигает наилучших результатов в оценке движения на реальных видео без использования размеченных данных.'}, 'en': {'title': 'Revolutionizing Motion Estimation with Self-Supervised Learning', 'desc': 'This paper presents Opt-CWM, a self-supervised method for estimating motion in videos, which is crucial for applications like video generation and robotics. Unlike traditional approaches that rely on synthetic data or specific heuristics, Opt-CWM utilizes a pre-trained next-frame prediction model to learn motion information directly from real-world video inputs. The technique optimizes counterfactual probes, allowing it to adaptively extract flow and occlusion data without needing fixed rules. As a result, Opt-CWM achieves state-of-the-art performance in motion estimation while eliminating the requirement for labeled datasets.'}, 'zh': {'title': '自监督运动估计的新突破', 'desc': '本论文探讨了视频中的运动估计问题，这是计算机视觉中的一个重要课题，广泛应用于可控视频生成和机器人技术。现有的方法主要依赖合成数据训练或特定情境的启发式调整，这限制了模型在真实场景中的表现。我们提出了一种名为Opt-CWM的自监督技术，通过预训练的下一帧预测模型进行流动和遮挡估计。Opt-CWM通过优化反事实探针来提取运动信息，避免了固定启发式的需求，并在无需标注数据的情况下，在真实视频上实现了最先进的运动估计性能。'}}}, {'id': 'https://huggingface.co/papers/2503.16870', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'url': 'https://huggingface.co/papers/2503.16870', 'abstract': "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.", 'score': 2, 'issue_id': 2921, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'e8b397bd8ee5118a', 'authors': ['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee'], 'affiliations': ['Samsung Research, Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2503.16870.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная дистилляция знаний для больших языковых моделей', 'desc': "Статья представляет новый метод дистилляции знаний для обучения больших языковых моделей. Авторы показывают, что наивные подходы к разреженной дистилляции знаний, такие как кэширование Top-K вероятностей, дают смещенные оценки распределения вероятностей учителя. Предлагается метод 'Random Sampling Knowledge Distillation', основанный на выборке по важности, который обеспечивает несмещенные оценки и сохраняет градиент в ожидании. Этот метод позволяет быстрее обучать модели-ученики с минимальными накладными расходами по сравнению с обучением на основе кросс-энтропии, сохраняя при этом конкурентоспособную производительность."}, 'en': {'title': 'Unbiased Knowledge Distillation for Efficient Model Training', 'desc': "This paper discusses a new method for knowledge distillation in Large Language Models, focusing on the challenges of pre-training. The authors highlight that traditional methods, like caching Top-K probabilities, can lead to biased teacher probability distributions, which negatively affect the student's performance. They introduce 'Random Sampling Knowledge Distillation', an importance-sampling approach that provides unbiased estimates and maintains gradient preservation. This method allows for faster training with minimal overhead while achieving competitive results compared to full distillation across various model sizes."}, 'zh': {'title': '高效的知识蒸馏方法提升学生模型训练', 'desc': '知识蒸馏是一种有效的技术，可以从大型语言模型中提取知识。本文探讨了在预训练阶段应用知识蒸馏的挑战，特别是简单的稀疏知识蒸馏方法可能导致学生模型获得偏差的教师概率分布。我们提出了一种基于重要性采样的随机采样知识蒸馏方法，能够提供无偏估计，并在期望中保留梯度。该方法在存储稀疏logits的同时，能加快学生模型的训练速度，并在不同模型规模下保持竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.18929', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training', 'url': 'https://huggingface.co/papers/2503.18929', 'abstract': 'Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines.', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'ae8d68c1a89d88a1', 'authors': ['Brian R. Bartoldson', 'Siddarth Venkatraman', 'James Diffenderfer', 'Moksh Jain', 'Tal Ben-Nun', 'Seanie Lee', 'Minsu Kim', 'Johan Obando-Ceron', 'Yoshua Bengio', 'Bhavya Kailkhura'], 'affiliations': ['CIFAR Fellow', 'KAIST', 'Lawrence Livermore National Laboratory', 'Mila Quebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.18929.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'TBA: Масштабируемое обучение с подкреплением для крупных языковых моделей', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для крупных языковых моделей под названием Trajectory Balance with Asynchrony (TBA). TBA использует буфер воспроизведения опыта для улучшения исследования и разнообразия данных. Метод предлагает асинхронный подход, разделяющий процессы поиска и обучения, что значительно ускоряет общее время обучения. TBA демонстрирует улучшения производительности в задачах математических рассуждений, настройки предпочтений и автоматизированного тестирования безопасности по сравнению с существующими методами.'}, 'en': {'title': 'Boosting LLM Training with Efficient Replay and Exploration', 'desc': 'This paper introduces a new method called Trajectory Balance with Asynchrony (TBA) for improving reinforcement learning in large language models (LLMs). TBA allows the use of experience replay buffers, which help in better exploration by storing past experiences and using them for training. The method separates the training and search processes, leading to faster training times and enhanced diversity in the data sampled. Overall, TBA shows significant improvements in performance and efficiency on various post-training tasks compared to existing methods.'}, 'zh': {'title': '提升强化学习效率的轨迹平衡与异步方法', 'desc': '强化学习（RL）是大型语言模型（LLM）后训练的重要组成部分。现有的在线算法与经验重放缓冲区不兼容，而我们提出的轨迹平衡与异步（TBA）方法可以有效利用重放缓冲区的优势。TBA通过将计算资源更多地用于搜索，持续生成离线数据来更新策略，从而加速训练过程。该方法在数学推理、偏好调优和自动红队等任务上表现出显著的速度和性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.15893', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\n  Document Structure Analysis', 'url': 'https://huggingface.co/papers/2503.15893', 'abstract': "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are publicly available at https://github.com/microsoft/CompHRDoc.", 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '459fe3a2bedb5ed6', 'authors': ['Jiawei Wang', 'Kai Hu', 'Qiang Huo'], 'affiliations': ['Department of EEIS, University of Science and Technology of China, Hefei, 230026, China', 'Microsoft Research Asia, Beijing, 100080, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15893.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#benchmark'], 'emoji': '📄', 'ru': {'title': 'Унифицированный подход к анализу иерархической структуры документов', 'desc': 'Статья представляет новый подход к иерархическому анализу структуры документов (HDSA) под названием UniHDSA. Этот метод рассматривает различные подзадачи HDSA как проблемы предсказания отношений и объединяет метки предсказания отношений в единое пространство меток. UniHDSA использует единый модуль предсказания отношений для одновременной обработки нескольких задач на уровне страницы и документа. Авторы разработали мультимодальную систему на основе архитектуры Transformer для валидации эффективности UniHDSA.'}, 'en': {'title': 'Unified Relation Prediction for Enhanced Document Structure Analysis', 'desc': 'This paper introduces a new method called UniHDSA for Hierarchical Document Structure Analysis (HDSA), which focuses on understanding the layout and structure of documents. Unlike previous methods that tackled individual tasks separately, UniHDSA treats various HDSA tasks as relation prediction problems, allowing for a more integrated approach. The method uses a single relation prediction module to analyze both page-level and document-level structures simultaneously. Experimental results show that UniHDSA outperforms existing methods on benchmark datasets, demonstrating its effectiveness in document layout analysis.'}, 'zh': {'title': '统一关系预测，提升文档结构分析', 'desc': '文档结构分析对于理解文档的物理布局和逻辑结构至关重要，涉及信息检索、文档摘要和知识提取等应用。本文提出了一种统一的关系预测方法UniHDSA，旨在将文档结构分析的各个子任务视为关系预测问题，并将关系预测标签整合到一个统一的标签空间中。通过这种方法，单一的关系预测模块能够同时处理多个任务，无论是在页面级别还是文档级别的结构分析。实验结果表明，UniHDSA在层次文档结构分析基准Comp-HRDoc上达到了最先进的性能，并在大规模文档布局分析数据集DocLayNet上也取得了竞争力的结果，展示了该方法在各个子任务上的优越性。'}}}, {'id': 'https://huggingface.co/papers/2503.10997', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'url': 'https://huggingface.co/papers/2503.10997', 'abstract': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'd3b2354bbfc88139', 'authors': ['Aashish Anantha Ramakrishnan', 'Aadarsh Anantha Ramakrishnan', 'Dongwon Lee'], 'affiliations': ['National Institute of Technology, Tiruchirappalli', 'The Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10997.jpg', 'data': {'categories': ['#open_source', '#story_generation', '#multimodal', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'RONA: прагматическое разнообразие в генерации подписей к изображениям', 'desc': 'Статья представляет новый метод RONA для генерации разнообразных подписей к изображениям с помощью мультимодальных языковых моделей (MLLM). В отличие от традиционных подходов, RONA использует отношения согласованности для создания прагматически разнообразных подписей. Эксперименты показывают, что RONA превосходит базовые MLLM по общему разнообразию и соответствию эталонным подписям в различных доменах. Авторы предоставляют открытый исходный код своего метода.'}, 'en': {'title': 'RONA: Enhancing Caption Diversity with Pragmatic Cues', 'desc': 'This paper introduces RONA, a new prompting strategy designed for Multi-modal Large Language Models (MLLM) to improve the diversity of image captions. Unlike traditional methods that focus on syntactic and semantic variations, RONA emphasizes pragmatic cues to convey a central message alongside visual descriptions. By utilizing Coherence Relations, RONA enhances the way messages are communicated in relation to images. The results show that RONA outperforms existing MLLM baselines in generating captions that are more diverse and closely aligned with ground-truth descriptions across various domains.'}, 'zh': {'title': 'RONA：提升图像标题多样性的创新策略', 'desc': '本文提出了一种新的提示策略RONA，用于多模态大语言模型（MLLM），旨在提高图像标题的多样性。传统的写作助手生成的标题往往侧重于语法和语义的变化，而人类撰写的标题则更注重传达中心信息。RONA通过利用一致性关系作为变化的轴心，探索了与视觉内容结合的替代沟通方式。实验结果表明，RONA生成的标题在多样性和真实对齐度上优于现有的MLLM基线。'}}}, {'id': 'https://huggingface.co/papers/2503.20731', 'title': 'RecTable: Fast Modeling Tabular Data with Rectified Flow', 'url': 'https://huggingface.co/papers/2503.20731', 'abstract': 'Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable.', 'score': 0, 'issue_id': 2935, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'dfe7dcfbd0067c32', 'authors': ['Masane Fuchi', 'Tomohiro Takagi'], 'affiliations': ['Meiji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20731.jpg', 'data': {'categories': ['#open_source', '#architecture', '#dataset', '#optimization', '#diffusion', '#training'], 'emoji': '📊', 'ru': {'title': 'RecTable: быстрая генерация качественных табличных данных', 'desc': 'Авторы представляют RecTable - новый метод генерации табличных данных, основанный на технологии выпрямленного потока (rectified flow). RecTable использует простую архитектуру из нескольких блоков с управляемыми линейными юнитами. Метод применяет смешанное распределение шума и логит-нормальное распределение временных шагов. Эксперименты показывают, что RecTable достигает результатов, сопоставимых с современными диффузионными моделями, при меньшем времени обучения.'}, 'en': {'title': 'RecTable: Fast and Efficient Tabular Data Generation', 'desc': 'This paper presents RecTable, a novel approach for generating high-quality tabular data using rectified flow modeling. Unlike traditional methods like GANs and VAEs, RecTable significantly reduces training time while maintaining competitive performance. The architecture is straightforward, utilizing stacked gated linear unit blocks, which simplifies the model design. Additionally, the training strategies involve a mixed-type noise distribution and a logit-normal timestep distribution, enhancing efficiency and effectiveness in data generation.'}, 'zh': {'title': 'RecTable：高效生成表格数据的新方法', 'desc': '本论文介绍了一种新的模型RecTable，用于生成高质量的表格数据，超越了基于GAN和VAE的模型。RecTable采用了修正流建模，具有简单的架构，由几个堆叠的门控线性单元块组成。我们的方法在训练策略上也很简单，结合了混合类型的噪声分布和对数正态时间步分布。实验结果表明，RecTable在性能上与多种最先进的扩散和评分模型相当，同时减少了训练时间。'}}}, {'id': 'https://huggingface.co/papers/2503.17970', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\n  Pathological Images', 'url': 'https://huggingface.co/papers/2503.17970', 'abstract': "Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumor's aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at https://github.com/AIGeeksGroup/PathoHR.", 'score': 0, 'issue_id': 2928, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '7982f77e78076ec7', 'authors': ['Yang Luo', 'Shiru Wang', 'Jun Liu', 'Jiaxuan Xiao', 'Rundong Xue', 'Zeyu Zhang', 'Hao Zhang', 'Yu Lu', 'Yang Zhao', 'Yutong Xie'], 'affiliations': ['ANU', 'DLMU', 'Dartmouth', 'La Trobe', 'MBZUAI', 'NUP', 'SZTU', 'UCAS', 'XJLTU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2503.17970.jpg', 'data': {'categories': ['#cv', '#healthcare', '#training'], 'emoji': '🔬', 'ru': {'title': 'Повышение точности прогноза рака груди с помощью улучшения изображений и глубокого обучения', 'desc': 'PathoHR - это новый метод для точного прогнозирования выживаемости при раке молочной железы, который улучшает качество патологических изображений любого размера для более эффективного извлечения признаков. Он использует высокоразрешающий Vision Transformer для улучшения представления патч-изображений, оценивает различные метрики сходства для сравнения извлеченных признаков и демонстрирует, что улучшенные маленькие патчи могут достичь точности прогнозирования сравнимой или превосходящей большие необработанные патчи. PathoHR предлагает перспективный подход к интеграции улучшенного разрешения изображений с оптимизированным извлечением признаков для развития вычислительной патологии.'}, 'en': {'title': 'Enhancing Breast Cancer Survival Prediction with PathoHR', 'desc': 'This paper addresses the challenge of predicting breast cancer survival by focusing on the variability within tumors, which can complicate feature extraction from whole slide images (WSIs). The authors introduce PathoHR, a new pipeline that utilizes a high-resolution Vision Transformer (ViT) to improve the representation of tumor features at a patch level. They evaluate various similarity metrics to optimize the learning process, allowing for better capture of tumor characteristics. The results show that smaller, enhanced image patches can achieve similar or better prediction accuracy compared to larger patches, while also being more computationally efficient.'}, 'zh': {'title': '提升乳腺癌生存预测的PathoHR管道', 'desc': '本论文提出了一种名为PathoHR的新型管道，用于提高乳腺癌生存预测的准确性。由于肿瘤的异质性，病理图像中的不同区域可能表现出不同的形态和分子特征，这使得从全切片图像中提取代表性特征变得困难。PathoHR通过引入高分辨率的视觉变换器（ViT）来增强图像补丁的表示，从而实现更有效的特征学习。此外，实验结果表明，经过该管道增强的小图像补丁在预测准确性上可以与原始的大图像补丁相媲美，同时显著降低计算开销。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi (2)', '#alignment', '#architecture (8)', '#audio (1)', '#benchmark (10)', '#cv (10)', '#data', '#dataset (9)', '#diffusion (5)', '#ethics (2)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (13)', '#open_source (8)', '#optimization (10)', '#plp', '#rag (2)', '#reasoning (7)', '#rl (1)', '#rlhf', '#robotics (4)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (2)', '#training (10)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-27 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-27 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-27 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    