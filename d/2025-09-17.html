
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. September 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 сентября</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-16.html">⬅️ <span id="prev-date">16.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-18.html">➡️ <span id="next-date">18.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'};
        let feedDateNext = {'ru': '18.09', 'en': '09/18', 'zh': '9月18日'};
        let feedDatePrev = {'ru': '16.09', 'en': '09/16', 'zh': '9月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.13312', 'title': 'WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research', 'url': 'https://huggingface.co/papers/2509.13312', 'abstract': 'WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and one-shot generation paradigms that easily suffer from long-context failure issues like "loss in the middle" and hallucinations. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, source-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank for each part, it effectively mitigates long-context issues. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing high-quality, reliable, and well-structured reports.', 'score': 74, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '7c9823f6fb958040', 'authors': ['Zijian Li', 'Xin Guan', 'Bo Zhang', 'Shen Huang', 'Houquan Zhou', 'Shaopeng Lai', 'Ming Yan', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jun Zhang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13312.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#hallucinations', '#long_context', '#multimodal', '#agents'], 'emoji': '🕸️', 'ru': {'title': 'Адаптивное планирование и целенаправленный синтез для качественных исследовательских отчетов', 'desc': 'Статья представляет WebWeaver - двухагентную систему для решения задач глубокого открытого исследования. Первый агент (планировщик) итеративно собирает доказательства и оптимизирует план, создавая подробный план с ссылками на банк данных. Второй агент (писатель) выполняет иерархический процесс извлечения информации и написания, составляя отчет по частям. Такой подход позволяет избежать проблем с длинным контекстом и галлюцинациями. WebWeaver достигает нового уровня качества на основных бенчмарках глубокого открытого исследования.'}, 'en': {'title': 'WebWeaver: Revolutionizing AI Research with Adaptive Planning and Focused Synthesis', 'desc': 'This paper presents WebWeaver, a dual-agent framework designed to tackle open-ended deep research (OEDR) challenges in AI. It combines adaptive planning with focused synthesis to create high-quality reports by mimicking the human research process. The framework features a planner that dynamically interleaves evidence gathering with outline optimization, ensuring a comprehensive and reliable report structure. By using targeted retrieval from a memory bank, WebWeaver effectively addresses common issues like long-context failures and hallucinations, setting a new standard in OEDR performance.'}, 'zh': {'title': 'WebWeaver：人本研究的新方法', 'desc': 'WebWeaver是一个双代理框架，旨在解决开放式深度研究的挑战。它通过动态规划和聚焦合成，生成高质量、可靠的研究报告。该框架模拟人类研究过程，规划者在动态循环中优化大纲并获取证据，作家则逐步撰写报告。WebWeaver在多个开放式深度研究基准测试中表现出色，验证了其人本、迭代的方法论。'}}}, {'id': 'https://huggingface.co/papers/2509.13310', 'title': 'Scaling Agents via Continual Pre-training', 'url': 'https://huggingface.co/papers/2509.13310', 'abstract': 'AgentFounder, a deep research agent model incorporating Agentic Continual Pre-training, achieves state-of-the-art performance in agentic tasks while maintaining strong tool-use ability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.', 'score': 61, 'issue_id': 5932, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'b5b1de97f689d595', 'authors': ['Liangcai Su', 'Zhen Zhang', 'Guangyu Li', 'Zhuo Chen', 'Chenxi Wang', 'Maojia Song', 'Xinyu Wang', 'Kuan Li', 'Jialong Wu', 'Xuanzhong Chen', 'Zile Qiao', 'Zhongwang Zhang', 'Huifeng Yin', 'Shihao Cai', 'Runnan Fang', 'Zhengwei Tao', 'Wenbiao Yin', 'Chenxiong Qian', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13310.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#agi'], 'emoji': '🤖', 'ru': {'title': 'AgentFounder: новый шаг в создании эффективных агентных систем искусственного интеллекта', 'desc': 'Исследователи представили AgentFounder - модель глубокого исследовательского агента, использующую Агентное Континуальное Предобучение (Agentic CPT). Эта модель достигает наилучших результатов в агентных задачах, сохраняя при этом способность эффективно использовать инструменты. AgentFounder решает проблему недостаточной производительности пост-обученных моделей в агентных задачах. Модель показала высокие результаты на 10 бенчмарках, включая BrowseComp и HLE.'}, 'en': {'title': 'Empowering Agents with Continual Learning for Superior Performance', 'desc': 'The paper introduces AgentFounder, a deep research agent model that utilizes Agentic Continual Pre-training (Agentic CPT) to enhance performance in agentic tasks. Traditional post-training methods struggle with agentic tasks due to the lack of specialized foundational models, leading to optimization challenges. By implementing Agentic CPT, AgentFounder effectively learns diverse agentic behaviors while aligning with expert demonstrations. The model demonstrates superior performance on multiple benchmarks, showcasing its advanced tool-use capabilities and problem-solving skills.'}, 'zh': {'title': 'AgentFounder：代理任务的最优解', 'desc': '本文介绍了一种名为AgentFounder的深度研究代理模型，该模型结合了代理持续预训练（Agentic Continual Pre-training），在代理任务中实现了最先进的性能，同时保持了强大的工具使用能力。大型语言模型（LLMs）已经发展成为能够自主使用工具和进行多步骤推理的代理系统，但在代理任务中，基于通用基础模型的后训练方法表现不佳。我们发现问题的根源在于缺乏强大的代理基础模型，导致模型在后训练过程中需要同时学习多样的代理行为并与专家示范对齐，从而产生基本的优化矛盾。为了解决这个问题，我们首次提出将代理持续预训练纳入深度研究代理的训练流程，以构建强大的代理基础模型。'}}}, {'id': 'https://huggingface.co/papers/2509.13305', 'title': 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.13305', 'abstract': "WebSailor, a post-training methodology, enhances open-source models with systematic uncertainty reduction, matching proprietary agents' performance in complex information-seeking tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.", 'score': 52, 'issue_id': 5932, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'e7b960c7dacc4ae5', 'authors': ['Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Rui Ye', 'Yida Zhao', 'Liwen Zhang', 'Litu Ou', 'Dingchu Zhang', 'Xixi Wu', 'Jialong Wu', 'Xinyu Wang', 'Zile Qiao', 'Zhen Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13305.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#open_source', '#agents', '#agi'], 'emoji': '🧭', 'ru': {'title': 'WebSailor: открытый путь к сверхчеловеческому поиску информации', 'desc': 'WebSailor - это методология пост-обучения, которая улучшает работу моделей с открытым исходным кодом в сложных задачах поиска информации. Она основана на систематическом снижении неопределенности, что ранее было доступно только проприетарным агентным системам. Методология включает генерацию новых задач с высокой неопределенностью и эффективный алгоритм обучения с подкреплением DUPO. В результате WebSailor значительно превосходит все открытые агенты и сравнивается по производительности с проприетарными системами.'}, 'en': {'title': 'Closing the Gap: Empowering Open-Source Models with WebSailor', 'desc': "WebSailor is a new method that improves open-source machine learning models by reducing uncertainty in their decision-making processes. This technique allows these models to perform as well as proprietary systems in challenging tasks that require searching for information. The methodology includes creating difficult tasks that test the models' abilities and using a special training algorithm called Duplicating Sampling Policy Optimization (DUPO). By implementing these strategies, WebSailor helps open-source models achieve better performance in complex scenarios, bridging the gap with advanced proprietary agents."}, 'zh': {'title': 'WebSailor：缩小开源与专有模型的能力差距', 'desc': 'WebSailor是一种后训练方法，旨在通过系统性减少不确定性来增强开源模型的性能，使其在复杂的信息检索任务中与专有代理的表现相匹配。该方法的成功依赖于一种复杂的推理模式，这种模式在开源模型中缺失，即在广阔的信息环境中系统性地减少极端不确定性的能力。WebSailor通过结构化采样和信息模糊化生成新颖的高不确定性任务，并结合高效的代理强化学习训练算法，显著提升了开源代理在复杂信息检索任务中的表现。最终，WebSailor成功缩小了开源模型与专有系统之间的能力差距。'}}}, {'id': 'https://huggingface.co/papers/2509.13311', 'title': 'Towards General Agentic Intelligence via Environment Scaling', 'url': 'https://huggingface.co/papers/2509.13311', 'abstract': 'A scalable framework and two-phase fine-tuning strategy enhance function-calling capabilities of agents in diverse environments, improving performance on agentic benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.', 'score': 49, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'c97e7569e926b9a7', 'authors': ['Runnan Fang', 'Shihao Cai', 'Baixuan Li', 'Jialong Wu', 'Guangyu Li', 'Wenbiao Yin', 'Xinyu Wang', 'Xiaobin Wang', 'Liangcai Su', 'Zhen Zhang', 'Shibin Wu', 'Zhengwei Tao', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13311.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#agi', '#training', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Масштабируемое обучение агентов для улучшения вызова функций в разнообразных средах', 'desc': 'Статья представляет масштабируемую систему и двухфазную стратегию дообучения для улучшения способностей агентов к вызову функций в разнообразных средах. Авторы разработали фреймворк, автоматически создающий гетерогенные симулированные среды, расширяя спектр сценариев вызова функций. Предложенная стратегия обучения сначала наделяет агентов базовыми возможностями, а затем специализирует их для конкретных доменов. Эксперименты на различных бенчмарках показали значительное улучшение способностей моделей к вызову функций.'}, 'en': {'title': 'Scaling Agentic Intelligence for Enhanced Function-Calling', 'desc': 'This paper presents a scalable framework and a two-phase fine-tuning strategy to improve the function-calling abilities of agents in various environments. The authors emphasize that training agents in diverse settings is crucial for developing robust function-calling intelligence, which is essential for real-world applications of Large Language Models. They propose a method to systematically create varied simulated environments and a training approach that first builds basic agentic skills before refining them for specific tasks. Experimental results show that their model, AgentScaler, outperforms existing benchmarks in function-calling tasks, indicating significant advancements in agentic intelligence.'}, 'zh': {'title': '提升智能体函数调用能力的可扩展框架', 'desc': '本论文提出了一种可扩展的框架和两阶段微调策略，以增强智能体在多样化环境中的函数调用能力。研究表明，智能体的函数调用能力与其训练环境的多样性密切相关。我们设计了一个自动构建异构环境的框架，系统性地扩展了函数调用场景的空间。此外，通过两阶段的微调策略，我们首先赋予智能体基本能力，然后针对特定领域进行专业化训练，显著提升了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.13309', 'title': 'WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents', 'url': 'https://huggingface.co/papers/2509.13309', 'abstract': "WebResearcher, a deep-research framework, enhances AI agents' knowledge synthesis by reformulating research as a Markov Decision Process and using a scalable data synthesis engine, achieving superior performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.", 'score': 47, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'f064e62f63131809', 'authors': ['Zile Qiao', 'Guoxin Chen', 'Xuanzhong Chen', 'Donglei Yu', 'Wenbiao Yin', 'Xinyu Wang', 'Zhen Zhang', 'Baixuan Li', 'Huifeng Yin', 'Kuan Li', 'Rui Min', 'Minpeng Liao', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13309.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#science', '#dataset', '#training', '#agents', '#transfer_learning'], 'emoji': '🕸️', 'ru': {'title': 'WebResearcher: новый уровень глубокого исследования для ИИ', 'desc': 'WebResearcher - это новая система глубокого исследования, которая улучшает способность ИИ-агентов синтезировать знания. Она переформулирует процесс исследования как марковский процесс принятия решений и использует масштабируемый механизм синтеза данных. WebResearcher включает в себя итеративную парадигму глубокого исследования и систему WebFrontier для генерации качественных обучающих данных. Эксперименты показывают, что WebResearcher превосходит существующие системы по ряду сложных тестов.'}, 'en': {'title': 'Revolutionizing AI Knowledge Synthesis with WebResearcher', 'desc': "WebResearcher is a deep-research framework that improves how AI agents gather and synthesize knowledge by treating research as a Markov Decision Process. It features an iterative approach where agents create evolving reports while managing focused workspaces, which helps avoid distractions from irrelevant information. Additionally, the framework includes WebFrontier, a data synthesis engine that produces high-quality training data, enhancing the agents' ability to construct knowledge actively. Experiments show that WebResearcher outperforms existing systems across multiple benchmarks, demonstrating its effectiveness in autonomous knowledge discovery."}, 'zh': {'title': 'WebResearcher：提升AI知识综合的新框架', 'desc': 'WebResearcher是一个深度研究框架，通过将研究重新表述为马尔可夫决策过程，增强了人工智能代理的知识综合能力。该框架包含两个关键组件：一个是迭代深度研究范式，能够定期整合发现并生成不断演变的报告，克服了现有单一上下文方法的局限性；另一个是可扩展的数据合成引擎，能够生成高质量的训练数据，促进主动知识构建。实验结果表明，WebResearcher在多个基准测试中表现优异，甚至超越了许多前沿的专有系统。'}}}, {'id': 'https://huggingface.co/papers/2509.13313', 'title': 'ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization', 'url': 'https://huggingface.co/papers/2509.13313', 'abstract': "ReSum, a novel paradigm with periodic context summarization, enhances web agents' performance on knowledge-intensive tasks by overcoming context window limitations, achieving significant improvements over ReAct.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on BrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web agents.", 'score': 43, 'issue_id': 5932, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '3850ff93d847cdb9', 'authors': ['Xixi Wu', 'Kuan Li', 'Yida Zhao', 'Liwen Zhang', 'Litu Ou', 'Huifeng Yin', 'Zhongwang Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Minhao Cheng', 'Shuai Wang', 'Hong Cheng', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.13313.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#long_context', '#agents'], 'emoji': '🧠', 'ru': {'title': 'ReSum: раздвигая границы контекста для веб-агентов на основе LLM', 'desc': 'В статье представлена новая парадигма ReSum, которая улучшает производительность веб-агентов на основе больших языковых моделей (LLM) в задачах, требующих обширных знаний. ReSum преодолевает ограничения контекстного окна путем периодического суммирования контекста, что позволяет агентам проводить неограниченное исследование. Авторы также предлагают ReSum-GRPO - метод адаптации парадигмы, интегрирующий GRPO с сегментированным обучением траекторий. Эксперименты показывают, что ReSum превосходит существующую парадигму ReAct на 4.5% в среднем, а после обучения с ReSum-GRPO улучшение достигает 8.2%.'}, 'en': {'title': 'ReSum: Breaking Context Barriers for Smarter Web Agents', 'desc': 'ReSum is a new approach that improves the performance of web agents on complex knowledge tasks by addressing the limitations of context windows found in previous methods like ReAct. It allows agents to summarize their interactions periodically, which helps them keep track of important information without being constrained by limited context. This method enables agents to explore more effectively, even when dealing with complicated queries that involve many entities and relationships. The results show that ReSum significantly enhances performance, achieving better accuracy with fewer training samples compared to existing models.'}, 'zh': {'title': 'ReSum：突破上下文限制的智能体新方法', 'desc': 'ReSum是一种新颖的周期性上下文摘要方法，旨在提升网络智能体在知识密集型任务中的表现。传统的ReAct方法受到上下文窗口限制的影响，难以处理复杂查询。ReSum通过将不断增长的交互历史转换为紧凑的推理状态，克服了这些限制，使智能体能够在不受上下文约束的情况下进行无限探索。实验结果表明，ReSum在多个基准测试中相较于ReAct平均提高了4.5%的性能，经过ReSum-GRPO训练后，性能提升可达8.2%。'}}}, {'id': 'https://huggingface.co/papers/2509.13232', 'title': 'Single-stream Policy Optimization', 'url': 'https://huggingface.co/papers/2509.13232', 'abstract': "Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@k across the evaluated k values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.", 'score': 23, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'ecea2ef463a2c2ff', 'authors': ['Zhongwen Xu', 'Zihan Ding'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2509.13232.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'SPO: Революция в обучении языковых моделей', 'desc': 'Статья представляет новый метод оптимизации политики для больших языковых моделей (LLM) под названием Single-stream Policy Optimization (SPO). SPO устраняет проблемы групповых методов, такие как GRPO, предоставляя стабильный сигнал обучения с низкой дисперсией. Метод использует персистентный KL-адаптивный трекер значений и нормализует преимущества глобально по всему батчу. Эксперименты с Qwen3-8B показывают, что SPO обеспечивает более плавную сходимость и более высокую точность по сравнению с GRPO.'}, 'en': {'title': 'Streamlining Learning for Better Language Model Performance', 'desc': 'Single-stream Policy Optimization (SPO) enhances the training of Large Language Models (LLMs) by addressing the limitations of group-based policy-gradient methods. It eliminates issues like degenerate groups that disrupt learning signals and synchronization barriers that limit scalability. By using a persistent, KL-adaptive value tracker and normalizing advantages globally, SPO provides a stable and low-variance learning signal for each sample. This approach not only improves performance and efficiency but also supports adaptive curriculum learning through prioritized sampling, leading to significant gains in accuracy on challenging benchmarks.'}, 'zh': {'title': '单流策略优化：提升大语言模型的训练效率', 'desc': '单流策略优化（SPO）通过消除基于组的方法问题，改进了大语言模型的策略梯度训练。传统的基于组的方法虽然可以降低方差，但存在学习信号丢失和同步障碍等关键缺陷。SPO通过引入持久的KL自适应价值追踪器，提供了稳定、低方差的学习信号，从而提高了性能和效率。实验结果表明，SPO在多个数学基准测试中表现优于传统方法，展示了其在长时间跨度或工具集成环境中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.12815', 'title': 'Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation', 'url': 'https://huggingface.co/papers/2509.12815', 'abstract': 'Hunyuan3D Studio automates 3D asset creation using AI, integrating neural modules to transform concept images or text into high-quality, game-ready 3D models with optimized geometry and PBR textures.  \t\t\t\t\tAI-generated summary \t\t\t\t The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.', 'score': 18, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '3561c2f1ae6a316c', 'authors': ['Biwen Lei', 'Yang Li', 'Xinhai Liu', 'Shuhui Yang', 'Lixin Xu', 'Jingwei Huang', 'Ruining Tang', 'Haohan Weng', 'Jian Liu', 'Jing Xu', 'Zhen Zhou', 'Yiling Zhu', 'Jiankai Xing', 'Jiachen Xu', 'Changfeng Ma', 'Xinhao Yan', 'Yunhan Yang', 'Chunshi Wang', 'Duoteng Xu', 'Xueqi Ma', 'Yuguang Chen', 'Jing Li', 'Mingxin Yang', 'Sheng Zhang', 'Yifei Feng', 'Xin Huang', 'Di Luo', 'Zebin He', 'Puhua Jiang', 'Changrong Hu', 'Zihan Qin', 'Shiwei Miao', 'Haolin Liu', 'Yunfei Zhao', 'Zeqiang Lai', 'Qingxiang Lin', 'Zibo Zhao', 'Kunhong Li', 'Xianghui Yang', 'Huiwen Shi', 'Xin Yang', 'Yuxuan Wang', 'Zebin Yao', 'Yihang Lian', 'Sicong Liu', 'Xintong Han', 'Wangchen Qin', 'Caisheng Ouyang', 'Jianyin Liu', 'Tianwen Yuan', 'Shuai Jiang', 'Hong Duan', 'Yanqi Niu', 'Wencong Lin', 'Yifu Sun', 'Shirui Huang', 'Lin Niu', 'Gu Gong', 'Guojian Xiao', 'Bojian Zheng', 'Xiang Yuan', 'Qi Chen', 'Jie Xiao', 'Dongyang Zheng', 'Xiaofeng Yang', 'Kai Liu', 'Jianchen Zhu', 'Lifu Wang', 'Qinglin Lu', 'Jie Liu', 'Liang Dong', 'Fan Jiang', 'Ruibin Chen', 'Lei Wang', 'Chao Zhang', 'Jiaxin Lin', 'Hao Zhang', 'Zheng Ye', 'Peng He', 'Runzhou Wu', 'Yinhe Wu', 'Jiayao Du', 'Jupeng Chen', 'Xinyue Mao', 'Dongyuan Guo', 'Yixuan Tang', 'Yulin Tsai', 'Yonghao Tan', 'Jiaao Yu', 'Junlin Yu', 'Keren Zhang', 'Yifan Li', 'Peng Chen', 'Tian Liu', 'Di Wang', 'Yuhong Liu', 'Linus', 'Jie Jiang', 'Zhuo Chen', 'Chunchao Guo'], 'affiliations': ['Tencent Hunyuan Hunyuan3D Studio'], 'pdf_title_img': 'assets/pdf/title_img/2509.12815.jpg', 'data': {'categories': ['#games', '#optimization', '#3d'], 'emoji': '🎮', 'ru': {'title': 'ИИ революционизирует создание 3D-ассетов для игр', 'desc': 'Hunyuan3D Studio - это платформа для автоматизированного создания 3D-ассетов с использованием искусственного интеллекта. Система интегрирует нейронные модули для преобразования концепт-изображений или текста в высококачественные 3D-модели, готовые к использованию в играх. Hunyuan3D Studio оптимизирует геометрию и создает PBR-текстуры, значительно ускоряя рабочий процесс разработки игр. Платформа демонстрирует потенциал ИИ-ассистированных технологий в создании интерактивного контента.'}, 'en': {'title': 'Revolutionizing 3D Asset Creation with AI', 'desc': 'Hunyuan3D Studio is an innovative AI-powered platform that automates the creation of high-quality 3D assets for game development. It utilizes advanced neural modules to convert concept images or text into fully-realized 3D models, complete with optimized geometry and PBR textures. This system streamlines the traditionally labor-intensive process, allowing for faster production and easier access to 3D content creation. By meeting the technical standards of modern game engines, Hunyuan3D Studio enhances the efficiency of game production workflows.'}, 'zh': {'title': 'Hunyuan3D Studio：游戏资产创作的AI革命', 'desc': 'Hunyuan3D Studio 是一个基于人工智能的内容创作平台，旨在自动化和简化游戏制作流程。它通过集成先进的神经模块，将概念图像或文本快速转化为高质量的3D模型，模型具备优化的几何形状和高保真PBR纹理。该平台不仅提高了3D资产的生成效率，还确保生成的资产符合现代游戏引擎的技术要求。Hunyuan3D Studio 代表了游戏开发和互动媒体领域中AI辅助工作流程的重大进步。'}}}, {'id': 'https://huggingface.co/papers/2509.13317', 'title': '3D Aware Region Prompted Vision Language Model', 'url': 'https://huggingface.co/papers/2509.13317', 'abstract': 'A Spatial Region 3D (SR-3D) vision-language model unifies 2D and 3D representations by enriching 2D features with 3D positional embeddings, enabling flexible region prompting and accurate spatial reasoning across frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.', 'score': 8, 'issue_id': 5929, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'b8172aa326bbec5e', 'authors': ['An-Chieh Cheng', 'Yang Fu', 'Yukang Chen', 'Zhijian Liu', 'Xiaolong Li', 'Subhashree Radhakrishnan', 'Song Han', 'Yao Lu', 'Jan Kautz', 'Pavlo Molchanov', 'Hongxu Yin', 'Xiaolong Wang', 'Sifei Liu'], 'affiliations': ['MIT', 'NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.13317.jpg', 'data': {'categories': ['#benchmark', '#games', '#3d', '#cv', '#multimodal', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Объединение 2D и 3D для лучшего понимания пространства', 'desc': 'SR-3D - это модель машинного обучения, объединяющая 2D и 3D представления визуальных данных. Она обогащает 2D признаки 3D позиционными эмбеддингами, что позволяет гибко задавать регионы интереса и точно анализировать пространственные отношения между кадрами. Модель поддерживает разметку регионов ограничивающими рамками, сегментационными масками или напрямую в 3D, без необходимости полной разметки всех кадров. SR-3D достигает наилучших результатов как на общих задачах компьютерного зрения, так и на специализированных 3D пространственных бенчмарках.'}, 'en': {'title': 'Unifying 2D and 3D for Enhanced Scene Understanding', 'desc': 'The Spatial Region 3D (SR-3D) model integrates 2D and 3D visual data by enhancing 2D features with 3D positional information. This allows for flexible region prompting, enabling users to annotate images and 3D spaces without extensive labeling across multiple frames. By leveraging 3D embeddings, the model improves spatial reasoning, even when objects are not visible together in the same view. Experiments show that SR-3D outperforms existing methods in both 2D and 3D tasks, demonstrating its capability to understand scenes effectively, even in videos lacking 3D data.'}, 'zh': {'title': '统一2D与3D表示的空间区域3D模型', 'desc': '本文介绍了一种空间区域3D（SR-3D）视觉语言模型，它通过将2D特征与3D位置嵌入相结合，实现了2D和3D表示的统一。SR-3D支持灵活的区域提示，用户可以在任意帧上使用边界框或分割掩码进行标注，而无需进行繁琐的多帧标注。通过增强2D视觉特征，SR-3D能够在不同帧之间进行更准确的空间推理，即使感兴趣的物体不在同一视图中出现。实验结果表明，SR-3D在2D视觉语言和3D空间基准测试中均表现出色，证明了其在场景理解中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.12603', 'title': 'EconProver: Towards More Economical Test-Time Scaling for Automated\n  Theorem Proving', 'url': 'https://huggingface.co/papers/2509.12603', 'abstract': 'Two methods, dynamic CoT switching and Diverse parallel-scaled RL, reduce computational cost in ATP models while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance.', 'score': 6, 'issue_id': 5931, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '8ca0696473050199', 'authors': ['Mukai Li', 'Linfeng Song', 'Zhenwen Liang', 'Jiahao Xu', 'Shansan Gong', 'Qi Liu', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.12603.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#inference', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное автоматическое доказательство теорем: меньше затрат, та же мощность', 'desc': 'Статья представляет два метода для снижения вычислительных затрат в моделях автоматического доказательства теорем (ATP) при сохранении производительности. Первый метод - динамическое переключение цепочки рассуждений (CoT), снижающее избыточное потребление токенов. Второй метод - разнообразное параллельно-масштабируемое обучение с подкреплением (RL) с обучаемыми префиксами для повышения эффективности при ограниченном числе проходов. Эксперименты показывают, что предложенный подход EconProver достигает сравнимой производительности с базовыми методами при использовании лишь 12% вычислительных ресурсов.'}, 'en': {'title': 'Efficient ATP: Cutting Costs Without Cutting Performance', 'desc': 'This paper presents two innovative methods aimed at reducing the computational costs associated with Automated Theorem Proving (ATP) models while preserving their performance. The first method, dynamic Chain-of-Thought (CoT) switching, optimizes token usage by selectively activating CoT reasoning only when necessary. The second method, Diverse parallel-scaled reinforcement learning (RL), employs trainable prefixes to improve sampling efficiency under limited passes. Through experiments, the proposed EconProver demonstrates that it can achieve similar performance to existing models while using only 12% of the computational resources, offering a more efficient approach to ATP.'}, 'zh': {'title': '降低计算成本，提升自动定理证明效率', 'desc': '本文提出了两种方法，动态链式思维切换和多样化并行缩放强化学习，以降低自动定理证明（ATP）模型的计算成本，同时保持性能。研究表明，现有的测试时间缩放策略在推理时引入了显著的计算开销，而传统的成本分析往往只关注采样次数。通过系统比较不同的缩放策略，本文展示了当前开源方法的低效性，并提出了减少令牌使用和采样次数的有效方案。实验结果表明，所提出的EconProver在仅使用12%计算成本的情况下，性能与基线方法相当。'}}}, {'id': 'https://huggingface.co/papers/2509.12341', 'title': 'Exact Coset Sampling for Quantum Lattice Algorithms', 'url': 'https://huggingface.co/papers/2509.12341', 'abstract': 'A replacement for the domain-extension step in a quantum lattice algorithm uses a pair-shift difference construction to correct periodicity issues and enforce modular linear relations efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t We give a simple, fully correct, and assumption-light replacement for the contested "domain-extension" in Step 9 of a recent windowed-QFT lattice algorithm with complex-Gaussian windows~chen2024quantum. The published Step~9 suffers from a periodicity/support mismatch. We present a pair-shift difference construction that coherently cancels all unknown offsets, produces an exact uniform CRT-coset state over Z_{P}, and then uses the QFT to enforce the intended modular linear relation. The unitary is reversible, uses poly(log M_2) gates, and preserves the algorithm\'s asymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice.', 'score': 3, 'issue_id': 5930, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '30a8fdee577071f4', 'authors': ['Yifan Zhang'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2509.12341.jpg', 'data': {'categories': [], 'emoji': '🔬', 'ru': {'title': 'Усовершенствование квантового алгоритма решетки: эффективное расширение домена', 'desc': 'Статья представляет улучшение для алгоритма квантовой решетки. Авторы предлагают замену для шага расширения домена, используя конструкцию разности парных сдвигов. Это исправляет проблемы с периодичностью и эффективно обеспечивает модульные линейные соотношения. Новый метод является полностью корректным, не требует дополнительных предположений и сохраняет асимптотику исходного алгоритма.'}, 'en': {'title': 'Efficiently Correcting Periodicity in Quantum Lattice Algorithms', 'desc': "This paper introduces a new method to replace the problematic 'domain-extension' step in a quantum lattice algorithm. The proposed solution uses a pair-shift difference construction to address issues related to periodicity and support mismatches. By coherently canceling unknown offsets, it creates a uniform CRT-coset state and applies the Quantum Fourier Transform (QFT) to maintain the desired modular linear relations. This approach is efficient, reversible, and maintains the algorithm's performance characteristics."}, 'zh': {'title': '高效解决量子算法中的周期性问题', 'desc': '本文提出了一种替代量子格算法中域扩展步骤的简单方法，旨在解决周期性问题。我们使用了一种成对移位差构造，能够有效地消除未知偏移量，并生成一个精确的均匀余数类状态。该方法通过量子傅里叶变换（QFT）来强制执行预期的模线性关系，同时保持算法的渐近性质。我们的构造是可逆的，并且使用了多项式对数级别的门。'}}}, {'id': 'https://huggingface.co/papers/2509.06079', 'title': 'Multimodal Reasoning for Science: Technical Report and 1st Place\n  Solution to the ICML 2025 SeePhys Challenge', 'url': 'https://huggingface.co/papers/2509.06079', 'abstract': 'A caption-assisted reasoning framework bridges visual and textual modalities, achieving top performance in multimodal reasoning tasks like SeePhys and MathVerse.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner.', 'score': 3, 'issue_id': 5930, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 сентября', 'en': 'September 7', 'zh': '9月7日'}, 'hash': '58d2381d40c28302', 'authors': ['Hao Liang', 'Ruitao Wu', 'Bohan Zeng', 'Junbo Niu', 'Wentao Zhang', 'Bin Dong'], 'affiliations': ['Beihang University', 'Peking University', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2509.06079.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мост между зрением и языком: новый подход к мультимодальному ИИ', 'desc': 'Авторы представляют фреймворк для мультимодального рассуждения, который объединяет визуальные и текстовые модальности. Этот подход использует вспомогательные подписи для улучшения понимания изображений и текста. Фреймворк показал лучшие результаты на соревновании SeePhys и продемонстрировал эффективность на бенчмарке MathVerse для геометрических рассуждений. Предложенный метод превосходит даже современные языковые модели вроде GPT-3 в задачах мультимодального рассуждения.'}, 'en': {'title': 'Bridging Visual and Textual Modalities for Superior Multimodal Reasoning', 'desc': 'This paper presents a caption-assisted reasoning framework that enhances the integration of visual and textual information for multimodal reasoning tasks. The framework addresses the limitations of existing models, such as GPT-3, which struggle with multimodal scenarios despite their success in text-based reasoning. By achieving top performance in challenges like SeePhys and demonstrating strong generalization on the MathVerse benchmark, the proposed method showcases its effectiveness in geometric reasoning. The authors provide their code publicly, promoting further research and application in this area.'}, 'zh': {'title': '标题辅助推理：连接视觉与文本的桥梁', 'desc': '本文提出了一种基于标题辅助推理的框架，旨在有效连接视觉和文本模态，从而解决多模态推理中的挑战。尽管文本推理已有显著进展，但现有模型在多模态场景中表现仍不理想。我们的框架在ICML 2025 AI for Math Workshop中获得了第一名，证明了其有效性和鲁棒性。此外，我们还在MathVerse基准上验证了该方法在几何推理方面的广泛适用性。'}}}, {'id': 'https://huggingface.co/papers/2509.12521', 'title': 'Phi: Preference Hijacking in Multi-modal Large Language Models at\n  Inference Time', 'url': 'https://huggingface.co/papers/2509.12521', 'abstract': 'A novel method, Preference Hijacking (Phi), manipulates Multimodal Large Language Model (MLLM) response preferences using specially crafted images, demonstrating significant effectiveness across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Multimodal Large Language Models (MLLMs) have gained significant attention across various domains. However, their widespread adoption has also raised serious safety concerns. In this paper, we uncover a new safety risk of MLLMs: the output preference of MLLMs can be arbitrarily manipulated by carefully optimized images. Such attacks often generate contextually relevant yet biased responses that are neither overtly harmful nor unethical, making them difficult to detect. Specifically, we introduce a novel method, Preference Hijacking (Phi), for manipulating the MLLM response preferences using a preference hijacked image. Our method works at inference time and requires no model modifications. Additionally, we introduce a universal hijacking perturbation -- a transferable component that can be embedded into different images to hijack MLLM responses toward any attacker-specified preferences. Experimental results across various tasks demonstrate the effectiveness of our approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.', 'score': 2, 'issue_id': 5944, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '1fd3f34077643500', 'authors': ['Yifan Lan', 'Yuanpu Cao', 'Weitong Zhang', 'Lu Lin', 'Jinghui Chen'], 'affiliations': ['The Pennsylvania State University', 'The University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2509.12521.jpg', 'data': {'categories': ['#multimodal', '#inference', '#ethics', '#security'], 'emoji': '🎭', 'ru': {'title': 'Управление предпочтениями ИИ через визуальные манипуляции', 'desc': 'Новый метод под названием Preference Hijacking (Phi) позволяет манипулировать предпочтениями мультимодальных больших языковых моделей (MLLM) с помощью специально созданных изображений. Этот метод работает во время вывода и не требует изменений в модели. Phi демонстрирует значительную эффективность в различных задачах, что вызывает серьезные опасения по поводу безопасности MLLM. Авторы также представили универсальное возмущение, которое может быть встроено в различные изображения для управления ответами MLLM.'}, 'en': {'title': 'Hijacking Preferences: A New Threat to MLLMs', 'desc': 'This paper presents a new method called Preference Hijacking (Phi) that targets Multimodal Large Language Models (MLLMs) by manipulating their response preferences through specially designed images. The method reveals a significant safety risk, as it can generate biased yet contextually relevant outputs without being easily detectable. Phi operates during the inference phase and does not require any changes to the underlying model, making it a versatile tool for attackers. The research includes a universal hijacking perturbation that can be applied to various images, effectively steering MLLM responses towards desired preferences.'}, 'zh': {'title': '偏好劫持：操控多模态语言模型的安全风险', 'desc': '本文提出了一种新方法，称为偏好劫持（Preference Hijacking，Phi），用于操控多模态大型语言模型（MLLM）的响应偏好。通过精心设计的图像，Phi能够在推理时有效地改变MLLM的输出，且无需对模型进行修改。这种攻击方式可能生成上下文相关但带有偏见的响应，难以被检测。实验结果表明，Phi在多种任务中表现出显著的有效性，展示了MLLM安全性的新风险。'}}}, {'id': 'https://huggingface.co/papers/2509.11526', 'title': 'Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis', 'url': 'https://huggingface.co/papers/2509.11526', 'abstract': 'A novel MIL framework, MHIM-MIL, uses masked hard instance mining with a Siamese structure and momentum teacher to improve cancer diagnosis and subtyping accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.', 'score': 1, 'issue_id': 5929, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '50c5e6e28356cfc4', 'authors': ['Wenhao Tang', 'Sheng Huang', 'Heng Fang', 'Fengtao Zhou', 'Bo Liu', 'Qingshan Liu'], 'affiliations': ['CS, Hefei University of Technology, Hefei, China', 'CS, Hong Kong University of Science and Technology, Hong Kong, China', 'CS, Nanjing University of Posts and Telecommunications, Nanjing, China', 'School of Big Data & Software Engineering, Chongqing University, Chongqing, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.11526.jpg', 'data': {'categories': ['#benchmark', '#science', '#optimization', '#training', '#healthcare'], 'emoji': '🔬', 'ru': {'title': 'Новый метод машинного обучения повышает точность диагностики рака', 'desc': 'Статья представляет новый подход к множественному обучению с экземплярами (MIL) для улучшения диагностики и подтипирования рака. Метод MHIM-MIL использует маскированный поиск сложных экземпляров с сиамской структурой и учителем с импульсом. Он фокусируется на сложных для классификации примерах, в отличие от традиционных методов MIL. Экспериментальные результаты показывают превосходство MHIM-MIL над современными методами по производительности и эффективности.'}, 'en': {'title': 'Unlocking Cancer Insights with Hard Instance Mining', 'desc': 'The paper introduces a new Multiple Instance Learning (MIL) framework called MHIM-MIL, which focuses on improving cancer diagnosis and subtyping accuracy. It employs masked hard instance mining using a Siamese network structure and a momentum teacher to effectively identify and utilize challenging instances in gigapixel Whole Slide Images (WSIs). By masking easier instances, the framework encourages the model to learn from harder examples, which are essential for better discriminative boundary modeling. Experimental results show that MHIM-MIL significantly outperforms existing methods in various cancer-related tasks, demonstrating its effectiveness and efficiency in computational pathology.'}, 'zh': {'title': '掩蔽困难实例挖掘，提升癌症诊断准确性', 'desc': '本文提出了一种新的多实例学习框架MHIM-MIL，旨在提高癌症诊断和亚型分类的准确性。该框架采用了掩蔽困难实例挖掘技术，结合了Siamese结构和动量教师，以更好地识别难以分类的实例。通过使用类感知实例概率，MHIM-MIL能够掩蔽显著实例并隐式挖掘困难实例，从而优化学生模型的训练。实验结果表明，MHIM-MIL在癌症诊断、亚型分类和生存分析等任务上表现优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2509.11177', 'title': 'Optimal Brain Restoration for Joint Quantization and Sparsification of\n  LLMs', 'url': 'https://huggingface.co/papers/2509.11177', 'abstract': 'A framework combining quantization and pruning in LLMs through error compensation achieves significant speedup and memory reduction.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.', 'score': 1, 'issue_id': 5934, 'pub_date': '2025-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '69dce8903a88c970', 'authors': ['Hang Guo', 'Yawei Li', 'Luca Benini'], 'affiliations': ['ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2509.11177.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Оптимальное восстановление мозга: сжатие языковых моделей без потери качества', 'desc': 'Эта статья представляет новый подход к оптимизации больших языковых моделей (LLM), сочетающий квантование и прунинг. Авторы предлагают фреймворк Optimal Brain Restoration (OBR), который позволяет согласовать эти два метода через компенсацию ошибок. OBR минимизирует ухудшение производительности на целевых задачах, используя целевую функцию второго порядка на основе гессиана. Эксперименты показывают, что OBR позволяет применять агрессивное квантование W4A4KV4 с 50% разреженностью к существующим LLM, обеспечивая ускорение до 4.72x и уменьшение потребления памяти в 6.4 раза по сравнению с базовой моделью FP16.'}, 'en': {'title': 'Combining Quantization and Pruning for Efficient LLMs', 'desc': 'This paper presents a new framework called Optimal Brain Restoration (OBR) that combines quantization and pruning techniques to enhance the efficiency of Large Language Models (LLMs). By addressing the conflicting needs of quantization, which prefers compact weight ranges, and pruning, which benefits from high variance, OBR uses error compensation to align these methods effectively. The framework operates without requiring additional training, leveraging a second-order Hessian objective to minimize performance loss on downstream tasks. Experimental results demonstrate that OBR allows for significant compression, achieving up to 4.72 times speedup and 6.4 times memory reduction compared to traditional dense models.'}, 'zh': {'title': '量化与剪枝的完美结合，提升LLM性能！', 'desc': '本文提出了一种结合量化和剪枝的框架，旨在提高大型语言模型（LLM）的速度和减少内存使用。通过引入错误补偿机制，优化了量化和稀疏性之间的矛盾，克服了单一方法在压缩方面的局限性。我们提出的最优大脑恢复（OBR）方法，能够在不影响下游任务性能的情况下，实现更高效的模型压缩。实验结果表明，OBR方法在现有LLM上实现了高达4.72倍的加速和6.4倍的内存减少。'}}}, {'id': 'https://huggingface.co/papers/2509.10687', 'title': 'Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video\n  Generation', 'url': 'https://huggingface.co/papers/2509.10687', 'abstract': 'SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.', 'score': 1, 'issue_id': 5945, 'pub_date': '2025-09-12', 'pub_date_card': {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'}, 'hash': '628885e66116f7e7', 'authors': ['Hao Zhang', 'Chun-Han Yao', 'Simon Donné', 'Narendra Ahuja', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2509.10687.jpg', 'data': {'categories': ['#dataset', '#3d', '#cv', '#synthetic', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'SP4D: Генерация кинематически-осведомленных видео из монокулярных входных данных', 'desc': 'SP4D - это фреймворк для генерации парных RGB и кинематических видео частей объектов из монокулярных входных данных. Он использует двухветвевую модель диффузии, которая совместно синтезирует RGB-кадры и соответствующие карты сегментации частей. Для упрощения архитектуры и гибкого включения различного количества частей введена схема пространственного цветового кодирования. Модуль BiDiFuse улучшает согласованность между ветвями, поддерживаемую контрастной функцией потерь для согласованности частей.'}, 'en': {'title': 'Generating Kinematic Videos with SP4D: A New Era in Motion Understanding', 'desc': 'The paper introduces Stable Part Diffusion 4D (SP4D), a novel framework that generates paired RGB and kinematic part videos from single-camera inputs. It utilizes a dual-branch diffusion model to synthesize both RGB frames and part segmentation maps, focusing on kinematic parts that reflect object movement. A unique spatial color encoding scheme allows for flexible part counts and efficient sharing of latent representations between branches. The framework is trained on a large dataset, KinematicParts20K, and demonstrates strong generalization across various scenarios, making it effective for animation and motion tasks.'}, 'zh': {'title': '生成运动部件视频的新方法', 'desc': 'SP4D是一个生成配对RGB和运动部件视频的框架，使用单目输入和双分支扩散模型。与传统的基于外观的分割方法不同，SP4D学习生成与物体关节运动一致的运动部件。该框架采用空间颜色编码方案，使得分割分支能够共享RGB分支的潜在变量，并通过简单的后处理恢复分割。实验表明，SP4D在多种场景中表现出强大的泛化能力，适用于动画和运动相关任务。'}}}, {'id': 'https://huggingface.co/papers/2509.13177', 'title': 'ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic\n  Medical Datasets Generation', 'url': 'https://huggingface.co/papers/2509.13177', 'abstract': 'ROOM is a simulation framework that generates photorealistic bronchoscopy training data using patient CT scans, enabling the development and validation of autonomy algorithms in medical robotics.  \t\t\t\t\tAI-generated summary \t\t\t\t Continuum robots are advancing bronchoscopy procedures by accessing complex lung airways and enabling targeted interventions. However, their development is limited by the lack of realistic training and test environments: Real data is difficult to collect due to ethical constraints and patient safety concerns, and developing autonomy algorithms requires realistic imaging and physical feedback. We present ROOM (Realistic Optical Observation in Medicine), a comprehensive simulation framework designed for generating photorealistic bronchoscopy training data. By leveraging patient CT scans, our pipeline renders multi-modal sensor data including RGB images with realistic noise and light specularities, metric depth maps, surface normals, optical flow and point clouds at medically relevant scales. We validate the data generated by ROOM in two canonical tasks for medical robotics -- multi-view pose estimation and monocular depth estimation, demonstrating diverse challenges that state-of-the-art methods must overcome to transfer to these medical settings. Furthermore, we show that the data produced by ROOM can be used to fine-tune existing depth estimation models to overcome these challenges, also enabling other downstream applications such as navigation. We expect that ROOM will enable large-scale data generation across diverse patient anatomies and procedural scenarios that are challenging to capture in clinical settings. Code and data: https://github.com/iamsalvatore/room.', 'score': 0, 'issue_id': 5943, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': '2df41bc3ac53da60', 'authors': ['Salvatore Esposito', 'Matías Mattamala', 'Daniel Rebain', 'Francis Xiatian Zhang', 'Kevin Dhaliwal', 'Mohsen Khadem', 'Subramanian Ramamoorthy'], 'affiliations': ['University of British Columbia, Canada', 'University of Edinburgh, UK'], 'pdf_title_img': 'assets/pdf/title_img/2509.13177.jpg', 'data': {'categories': ['#cv', '#synthetic', '#data', '#robotics', '#training', '#transfer_learning', '#dataset'], 'emoji': '🫁', 'ru': {'title': 'Реалистичная симуляция бронхоскопии для обучения ИИ в медицинской робототехнике', 'desc': 'ROOM - это система симуляции для создания фотореалистичных тренировочных данных бронхоскопии на основе КТ-снимков пациентов. Она позволяет разрабатывать и валидировать алгоритмы автономности в медицинской робототехнике. ROOM генерирует мультимодальные сенсорные данные, включая RGB-изображения с реалистичным шумом и бликами, карты глубины, нормали поверхностей и облака точек в медицински значимых масштабах. Система была протестирована на задачах оценки позы по нескольким ракурсам и монокулярной оценки глубины, демонстрируя разнообразные проблемы для современных методов машинного обучения.'}, 'en': {'title': 'Revolutionizing Bronchoscopy Training with ROOM Simulation', 'desc': "ROOM is a simulation framework that creates realistic bronchoscopy training data from patient CT scans, which is essential for developing autonomy algorithms in medical robotics. It addresses the challenge of obtaining real data due to ethical and safety concerns by generating multi-modal sensor data, including RGB images and depth maps. The framework has been validated through tasks like multi-view pose estimation and monocular depth estimation, showcasing its ability to present challenges for current methods. By fine-tuning existing models with ROOM's data, it opens up possibilities for improved navigation and other applications in medical settings."}, 'zh': {'title': 'ROOM：医学机器人训练的逼真模拟框架', 'desc': 'ROOM是一个模拟框架，利用患者的CT扫描生成逼真的支气管镜训练数据，从而促进医疗机器人自主算法的开发和验证。该框架通过渲染多模态传感器数据，包括带有真实噪声和光泽的RGB图像、度量深度图、表面法线、光流和点云，提供了医学相关的尺度。我们在两个医学机器人经典任务中验证了ROOM生成的数据，展示了当前最先进的方法在转移到这些医学环境时必须克服的多样化挑战。此外，ROOM生成的数据可以用于微调现有的深度估计模型，支持导航等其他下游应用。'}}}, {'id': 'https://huggingface.co/papers/2509.12541', 'title': 'zELO: ELO-inspired Training Method for Rerankers and Embedding Models', 'url': 'https://huggingface.co/papers/2509.12541', 'abstract': 'A novel training methodology named zELO optimizes retrieval performance by treating ranking tasks as equivalent to a Thurstone model, resulting in state-of-the-art open-weight reranker models that outperform proprietary models across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel training methodology named zELO, which optimizes retrieval performance via the analysis that ranking tasks are statically equivalent to a Thurstone model. Based on the zELO method, we use unsupervised data in order train a suite of state-of-the-art open-weight reranker models: zerank-1 and zerank-1-small. These models achieve the highest retrieval scores in multiple domains, including finance, legal, code, and STEM, outperforming closed-source proprietary rerankers on both NDCG@10 and Recall. These models also demonstrate great versatility, maintaining their 0-shot performance on out-of-domain and private customer datasets. The training data included 112,000 queries and 100 documents per query, and was trained end-to-end from unannotated queries and documents in less than 10,000 H100-hours.', 'score': 0, 'issue_id': 5947, 'pub_date': '2025-09-16', 'pub_date_card': {'ru': '16 сентября', 'en': 'September 16', 'zh': '9月16日'}, 'hash': 'b06fdf71944d104d', 'authors': ['Nicholas Pipitone', 'Ghita Houir Alami', 'Advaith Avadhanam', 'Anton Kaminskyi', 'Ashley Khoo'], 'affiliations': ['ZeroEntropy'], 'pdf_title_img': 'assets/pdf/title_img/2509.12541.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'zELO: Революция в обучении ранжированию для эффективного поиска', 'desc': 'В статье представлен новый метод обучения под названием zELO, который оптимизирует эффективность поиска, рассматривая задачи ранжирования как эквивалентные модели Терстоуна. На основе метода zELO авторы обучили набор передовых моделей переранжирования с открытым весом: zerank-1 и zerank-1-small. Эти модели достигают наивысших показателей поиска в различных областях, превосходя проприетарные ранжировщики по метрикам NDCG@10 и Recall. Модели также демонстрируют высокую универсальность, сохраняя свою эффективность на доменах и наборах данных, не участвовавших в обучении.'}, 'en': {'title': 'zELO: Revolutionizing Retrieval with Thurstone Insights', 'desc': 'The paper presents a new training methodology called zELO, which enhances retrieval performance by framing ranking tasks in the context of a Thurstone model. This approach allows for the development of advanced open-weight reranker models, specifically zerank-1 and zerank-1-small, which achieve superior performance compared to proprietary models across various fields. The models were trained using unsupervised data, leveraging a large dataset of 112,000 queries and 100 documents per query, and demonstrated high retrieval scores on metrics like NDCG@10 and Recall. Additionally, they maintain strong performance even on out-of-domain and private datasets, showcasing their versatility and effectiveness.'}, 'zh': {'title': 'zELO：优化检索性能的新方法', 'desc': '本文介绍了一种新颖的训练方法zELO，该方法通过将排名任务视为与Thurstone模型等价来优化检索性能。基于zELO方法，我们使用无监督数据训练了一系列最先进的开放权重重排序模型，包括zerank-1和zerank-1-small。这些模型在多个领域（如金融、法律、代码和STEM）中实现了最高的检索分数，超越了闭源的专有重排序模型。模型在0-shot情况下也表现出色，能够在不同领域和私有客户数据集上保持良好的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.11481', 'title': 'RAPTOR: A Foundation Policy for Quadrotor Control', 'url': 'https://huggingface.co/papers/2509.11481', 'abstract': 'A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers).', 'score': 0, 'issue_id': 5945, 'pub_date': '2025-09-15', 'pub_date_card': {'ru': '15 сентября', 'en': 'September 15', 'zh': '9月15日'}, 'hash': '2ca96573142ae65f', 'authors': ['Jonas Eschmann', 'Dario Albani', 'Giuseppe Loianno'], 'affiliations': ['Autonomous Robotics Research Center, Technology Innovation Institute, Abu Dhabi, UAE', 'Department of Electrical Engineering and Computer Sciences (EECS), UC Berkeley, Berkeley, CA 94720, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.11481.jpg', 'data': {'categories': ['#agents', '#rl', '#transfer_learning', '#small_models', '#training', '#optimization', '#robotics'], 'emoji': '🚁', 'ru': {'title': 'Универсальное управление квадрокоптерами с помощью адаптивной нейросети', 'desc': 'Метод RAPTOR позволяет единой нейросетевой политике адаптироваться к различным квадрокоптерам без предварительного обучения. Он использует мета-имитационное обучение и обучение в контексте с рекуррентностью в скрытом слое. Авторы протестировали метод на 10 реальных квадрокоптерах разных размеров и конфигураций. Небольшая трехслойная политика с всего 2084 параметрами оказалась достаточной для мгновенной адаптации к широкому спектру платформ.'}, 'en': {'title': 'RAPTOR: One Policy to Control Them All!', 'desc': 'The paper introduces RAPTOR, a novel method that allows a single neural network policy to adapt quickly to different quadrotors without needing retraining. It leverages Meta-Imitation Learning and In-Context Learning, incorporating recurrence in the hidden layer to enhance adaptability. By training on a diverse set of 1000 quadrotors, the method distills knowledge into a compact policy with only 2084 parameters, enabling zero-shot adaptation to new environments. Extensive testing shows that this approach maintains performance across various conditions, demonstrating significant improvements over traditional specialized models.'}, 'zh': {'title': 'RAPTOR：四旋翼控制的零样本适应新方法', 'desc': '本文提出了一种名为RAPTOR的方法，旨在通过元模仿学习和上下文学习，使单一神经网络策略能够在零样本情况下适应多种四旋翼无人机。与传统的强化学习方法不同，RAPTOR能够有效应对不同的飞行器类型和环境变化，避免了过拟合问题。该方法通过在隐藏层中引入递归结构，实现了对新平台的快速适应。实验表明，使用仅2084个参数的三层小型策略，RAPTOR能够在毫秒级别内适应未见过的四旋翼无人机。'}}}, {'id': 'https://huggingface.co/papers/2509.10696', 'title': 'Struct-Bench: A Benchmark for Differentially Private Structured Text\n  Generation', 'url': 'https://huggingface.co/papers/2509.10696', 'abstract': 'Struct-Bench is a framework and benchmark for evaluating synthetic structured datasets with natural language components, addressing challenges in differentially private synthetic data generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Differentially private (DP) synthetic data generation is a promising technique for utilizing private datasets that otherwise cannot be exposed for model training or other analytics. While much research literature has focused on generating private unstructured text and image data, in enterprise settings, structured data (e.g., tabular) is more common, often including natural language fields or components. Existing synthetic data evaluation techniques (e.g., FID) struggle to capture the structural properties and correlations of such datasets. In this work, we propose Struct-Bench, a framework and benchmark for evaluating synthetic datasets derived from structured datasets that contain natural language data. The Struct-Bench framework requires users to provide a representation of their dataset structure as a Context-Free Grammar (CFG). Our benchmark comprises 5 real-world and 2 synthetically generated datasets, each annotated with CFGs. We show that these datasets demonstrably present a great challenge even for state-of-the-art DP synthetic data generation methods. Struct-Bench also includes reference implementations of different metrics and a leaderboard, thereby providing researchers a standardized evaluation platform to benchmark and investigate privacy-preserving synthetic data generation methods. Further, we also present a case study showing how to use Struct-Bench to improve the synthetic data quality of Private Evolution (PE) on structured data. The benchmark and the leaderboard have been publicly made available at https://struct-bench.github.io.', 'score': 0, 'issue_id': 5947, 'pub_date': '2025-09-12', 'pub_date_card': {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'}, 'hash': '9d848e126ee8da08', 'authors': ['Shuaiqi Wang', 'Vikas Raunak', 'Arturs Backurs', 'Victor Reis', 'Pei Zhou', 'Sihao Chen', 'Longqi Yang', 'Zinan Lin', 'Sergey Yekhanin', 'Giulia Fanti'], 'affiliations': ['Carnegie Mellon University', 'Microsoft Corporation', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.10696.jpg', 'data': {'categories': ['#benchmark', '#leakage', '#open_source', '#synthetic', '#dataset'], 'emoji': '🔒', 'ru': {'title': 'Структурированная оценка приватных синтетических данных', 'desc': 'Struct-Bench - это фреймворк и бенчмарк для оценки синтетических структурированных датасетов с компонентами естественного языка. Он решает проблемы в генерации дифференциально приватных синтетических данных. Фреймворк требует от пользователей предоставления представления структуры их датасета в виде контекстно-свободной грамматики (КСГ). Бенчмарк включает 7 датасетов, метрики и таблицу лидеров, предоставляя исследователям стандартизированную платформу для оценки методов генерации приватных синтетических данных.'}, 'en': {'title': 'Struct-Bench: Elevating Synthetic Data Evaluation for Structured Datasets', 'desc': 'Struct-Bench is a new framework designed to evaluate synthetic datasets that include structured data with natural language elements, particularly in the context of differentially private data generation. It addresses the limitations of existing evaluation methods that fail to capture the complexities of structured datasets, which are common in enterprise applications. By requiring users to define their dataset structure using Context-Free Grammar (CFG), Struct-Bench provides a standardized way to assess the quality of synthetic data. The framework includes a benchmark with real-world and synthetic datasets, along with metrics and a leaderboard to facilitate research in privacy-preserving synthetic data generation.'}, 'zh': {'title': 'Struct-Bench：评估合成结构化数据的新框架', 'desc': 'Struct-Bench是一个用于评估合成结构化数据集的框架和基准，特别是那些包含自然语言成分的数据集。该框架解决了差分隐私合成数据生成中的挑战，尤其是在企业环境中，结构化数据（如表格数据）更为常见。现有的合成数据评估技术难以捕捉这些数据集的结构特性和相关性。通过提供上下文无关文法（CFG）来表示数据集结构，Struct-Bench为研究人员提供了一个标准化的评估平台，以便基准测试和研究隐私保护的合成数据生成方法。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (7)', '#agi (3)', '#alignment', '#architecture', '#audio', '#benchmark (8)', '#cv (3)', '#data (1)', '#dataset (5)', '#diffusion (1)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (3)', '#interpretability', '#leakage (1)', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (13)', '#plp', '#rag', '#reasoning (5)', '#rl (4)', '#rlhf', '#robotics (2)', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (3)', '#training (12)', '#transfer_learning (3)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-17 21:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-17 21:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-17 21:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    