{
    "date": {
        "ru": "6 –Ω–æ—è–±—Ä—è",
        "en": "November 6",
        "zh": "11Êúà6Êó•"
    },
    "time_utc": "2025-11-06 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-06",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.03276",
            "title": "Diffusion Language Models are Super Data Learners",
            "url": "https://huggingface.co/papers/2511.03276",
            "abstract": "Diffusion language models outperform autoregressive models in low-data settings due to any-order modeling, iterative bidirectional denoising, and Monte Carlo augmentation, and maintain advantages even at scale.  \t\t\t\t\tAI-generated summary \t\t\t\t Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.",
            "score": 124,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 –Ω–æ—è–±—Ä—è",
                "en": "November 5",
                "zh": "11Êúà5Êó•"
            },
            "hash": "3393b24fd7b4f0a3",
            "authors": [
                "Jinjie Ni",
                "Qian Liu",
                "Longxu Dou",
                "Chao Du",
                "Zili Wang",
                "Hang Yan",
                "Tianyu Pang",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab",
                "Shanghai Qiji Zhifeng Co., Ltd.",
                "StepFun"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03276.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#low_resource",
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#plp",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç –∞–≤—Ç–æRegression-–Ω—ã–µ –ø—Ä–∏ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –ø–æ—á–µ–º—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∞–≤—Ç–æRegression-–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç—Ä—ë–º —Ñ–∞–∫—Ç–æ—Ä–∞–º: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ª—é–±–æ–º –ø–æ—Ä—è–¥–∫–µ, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–æ–º—É —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—é –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–æ—á–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è (crossover) –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–º–µ—â–∞–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä—ë–º–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–æ —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–∞–∂–µ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏. –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å 1.7 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 10 –º–ª—Ä–¥ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ Python, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æRegression-–Ω—É—é –º–æ–¥–µ–ª—å —Å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –±—é–¥–∂–µ—Ç–∞–º–∏. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –º–∞–ª—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö HellaSwag –∏ MMLU –¥–∞–∂–µ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä—é–∫–æ–≤."
                },
                "en": {
                    "title": "Diffusion Models: The New Leaders in Low-Data Language Tasks",
                    "desc": "This paper discusses how diffusion language models (DLMs) perform better than autoregressive (AR) models when there is limited data available. The authors identify three key reasons for this advantage: DLMs can model data in any order, they use iterative bidirectional denoising for better training efficiency, and they incorporate Monte Carlo augmentation to enhance performance. They demonstrate that even with a smaller dataset, DLMs can achieve higher accuracy by training longer, and this trend continues as the model size increases. The findings suggest that DLMs maintain their superiority over AR models even as the amount of data grows, challenging previous assumptions about model performance in low-data scenarios."
                },
                "zh": {
                    "title": "Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩéÊï∞ÊçÆÁéØÂ¢É‰∏≠ÁöÑ‰ºòÂäø",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàDLMÔºâÂú®‰ΩéÊï∞ÊçÆÁéØÂ¢É‰∏ãÁöÑ‰ºòÂäøÔºåÁâπÂà´ÊòØÂú®‰ªª‰ΩïÈ°∫Â∫èÂª∫Ê®°„ÄÅËø≠‰ª£ÂèåÂêëÂéªÂô™ÂíåËíôÁâπÂç°Ê¥õÂ¢ûÂº∫Á≠âÊñπÈù¢„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåDLMÈÄöËøáËÆ≠ÁªÉÊõ¥Â§öÁöÑÂë®ÊúüÔºåËÉΩÂ§üÊåÅÁª≠Ë∂ÖË∂äËá™ÂõûÂΩíÊ®°ÂûãÔºàARÔºâ„ÄÇÈöèÁùÄÊï∞ÊçÆÈáèÁöÑÂ¢ûÂä†ÔºåDLMÁöÑ‰ºòÂäø‰ºöÈÄêÊ∏êÂáèÂ∞èÔºå‰ΩÜÂú®Â§ßËßÑÊ®°Ê®°Âûã‰∏≠‰ªçÁÑ∂‰øùÊåÅÊòéÊòæÁöÑ‰ºòÂäø„ÄÇËÆ∫ÊñáËøòÊåáÂá∫ÔºåÈ™åËØÅ‰∫§ÂèâÁÜµÁöÑ‰∏äÂçáÂπ∂‰∏ç‰∏ÄÂÆöÊÑèÂë≥ÁùÄ‰∏ãÊ∏∏ÊÄßËÉΩÁöÑ‰∏ãÈôç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03334",
            "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal\n  Interactions",
            "url": "https://huggingface.co/papers/2511.03334",
            "abstract": "UniAVGen, a unified framework using dual Diffusion Transformers and Asymmetric Cross-Modal Interaction, enhances audio-video generation by ensuring synchronization and consistency with fewer training samples.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.",
            "score": 51,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 –Ω–æ—è–±—Ä—è",
                "en": "November 5",
                "zh": "11Êúà5Êó•"
            },
            "hash": "24a0cd36ecc1379b",
            "authors": [
                "Guozhen Zhang",
                "Zixiang Zhou",
                "Teng Hu",
                "Ziqiao Peng",
                "Youliang Zhang",
                "Yi Chen",
                "Yuan Zhou",
                "Qinglin Lu",
                "Limin Wang"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "State Key Laboratory for Novel Software Technology, Nanjing University",
                "Tencent Hunyuan",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03334.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#audio",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Å –º–µ–Ω—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
                    "desc": "UniAVGen ‚Äî –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–≤–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ Diffusion Transformers –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã –ª–µ–∂–∏—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã—Ä–∞–≤–Ω–µ–Ω–Ω—ã–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –∑–≤—É–∫–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≥—É–±, –∞ —Ç–∞–∫–∂–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å Face-Aware Modulation –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–∫—Ü–µ–Ω—Ç–∏—Ä—É–µ—Ç –≤–∞–∂–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ü–∞, –∞ Modality-Aware Classifier-Free Guidance —É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ UniAVGen —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ—à–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ, –æ–∑–≤—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∏ —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ –ø–æ –∞—É–¥–∏–æ."
                },
                "en": {
                    "title": "UniAVGen: Synchronizing Audio and Video with Fewer Samples!",
                    "desc": "UniAVGen is a novel framework designed to improve the generation of audio and video together by using advanced machine learning techniques. It employs dual Diffusion Transformers to create a shared space for audio and video data, ensuring they are synchronized and semantically consistent. The framework features an Asymmetric Cross-Modal Interaction mechanism that allows for precise alignment of audio and video, enhancing the quality of generated content. With fewer training samples required, UniAVGen demonstrates significant improvements in synchronization and consistency compared to existing methods."
                },
                "zh": {
                    "title": "Èü≥ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á∫™ÂÖÉÔºöUniAVGen",
                    "desc": "UniAVGenÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®ÂèåÈáçÊâ©Êï£ÂèòÊç¢Âô®Âíå‰∏çÂØπÁß∞Ë∑®Ê®°ÊÄÅ‰∫§‰∫íÔºåÊèêÂçáÈü≥È¢ëËßÜÈ¢ëÁîüÊàêÁöÑÂêåÊ≠•ÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂèåÂàÜÊîØËÅîÂêàÂêàÊàêÊû∂ÊûÑÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ËøûË¥ØÁöÑË∑®Ê®°ÊÄÅÊΩúÂú®Á©∫Èó¥„ÄÇÂÖ∂Ê†∏ÂøÉÊòØ‰∏Ä‰∏™‰∏çÂØπÁß∞Ë∑®Ê®°ÊÄÅ‰∫§‰∫íÊú∫Âà∂ÔºåÁ°Æ‰øù‰∫ÜÁ≤æÁ°ÆÁöÑÊó∂Á©∫ÂêåÊ≠•ÂíåËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåUniAVGenÂú®‰ΩøÁî®Êõ¥Â∞ëÁöÑËÆ≠ÁªÉÊ†∑Êú¨Êó∂ÔºåËÉΩÂ§üÂú®Èü≥È¢ëËßÜÈ¢ëÂêåÊ≠•„ÄÅÈü≥Ëâ≤‰∏ÄËá¥ÊÄßÂíåÊÉÖÊÑü‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞Âá∫ÊòæËëó‰ºòÂäø„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03001",
            "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied\n  Environments with Tool Augmentation",
            "url": "https://huggingface.co/papers/2511.03001",
            "abstract": "LEGO-Eval and LEGO-Bench improve the evaluation and generation of realistic 3D scenes by aligning detailed instructions with scene components, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.",
            "score": 46,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "84cac177d315512a",
            "authors": [
                "Gyeom Hwangbo",
                "Hyungjoo Chae",
                "Minseok Kang",
                "Hyeonjong Ju",
                "Soohyun Oh",
                "Jinyoung Yeo"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Yonsei University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03001.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#agents",
                    "#dataset",
                    "#science"
                ],
                "emoji": "üèóÔ∏è",
                "ru": {
                    "title": "–¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ 3D-—Å—Ü–µ–Ω–∞–º–∏",
                    "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LEGO-Eval –∏ LEGO-Bench ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ü–µ–Ω–∫–∏ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –¢–µ–∫—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞—é—Ç –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å–∫–ª–∞–¥–∫–∏, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LEGO-Eval —è–≤–Ω–æ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å—Ü–µ–Ω—ã —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ ‚Äî —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ü–µ–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 10%."
                },
                "en": {
                    "title": "Enhancing 3D Scene Realism with LEGO-Eval and LEGO-Bench",
                    "desc": "The paper introduces LEGO-Eval and LEGO-Bench, two tools designed to enhance the evaluation and generation of realistic 3D scenes. LEGO-Eval provides a framework that accurately assesses how well generated scenes align with detailed, fine-grained instructions, addressing the shortcomings of existing evaluation methods. LEGO-Bench offers a set of complex instructions that reflect real-world environments, which are crucial for training embodied agents effectively. The results show that these new tools significantly improve the alignment assessment and highlight the limitations of current scene generation methods."
                },
                "zh": {
                    "title": "ÊèêÂçá3DÂú∫ÊôØÁîüÊàêÁöÑÁúüÂÆûÊÑü‰∏éËØÑ‰º∞Á≤æÂ∫¶",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫ÜLEGO-EvalÂíåLEGO-BenchÔºåÊó®Âú®ÊîπÂñÑ3DÂú∫ÊôØÁöÑËØÑ‰º∞ÂíåÁîüÊàê„ÄÇÁé∞ÊúâÁöÑÁîüÊàêÊñπÊ≥ïÂ∏∏Â∏∏Êó†Ê≥ïÁîüÊàêÂÖ∑ÊúâÁúüÂÆûÁ©∫Èó¥Â∏ÉÂ±ÄÂíåÁâ©‰ΩìÂ±ûÊÄßÁöÑÂú∫ÊôØÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫Êåá‰ª§‰∏çÂ§üËØ¶ÁªÜ„ÄÇÈÄöËøáÂºïÂÖ•Êõ¥ÁªÜËá¥ÁöÑÊåá‰ª§ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊåáÂØº3DÂú∫ÊôØÂêàÊàêÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÂú∫ÊôØÁöÑÁúüÂÆûÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLEGO-EvalÂú®ËØÑ‰º∞Âú∫ÊôØ‰∏éÊåá‰ª§ÁöÑÂØπÈΩêÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊàêÂäüÁéá‰πüÊòæËëóÊèêÈ´ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02734",
            "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents",
            "url": "https://huggingface.co/papers/2511.02734",
            "abstract": "CostBench evaluates Large Language Model agents' cost-aware planning and adaptability in response to dynamic changes, revealing significant gaps in current models' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "981b46d11b82e81e",
            "authors": [
                "Jiayu Liu",
                "Cheng Qian",
                "Zhaochen Su",
                "Qing Zong",
                "Shijue Huang",
                "Bingxiang He",
                "Yi R. Fung"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02734.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "üí∞",
                "ru": {
                    "title": "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤: –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                    "desc": "CostBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω—è—é—â–µ–π—Å—è —Å—Ä–µ–¥–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á, CostBench —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞—Ç—Ä–∞—Ç –∏ –ø–µ—Ä–µ–ø–ª–∞–Ω–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ—Ç–∫–∞–∑—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–¥—É—â–∏—Ö LLM-–º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞—Ö–æ–¥–∏—Ç—å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: –¥–∞–∂–µ GPT-5 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –º–µ–Ω–µ–µ 75% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö –∏ —Ç–µ—Ä—è–µ—Ç –æ–∫–æ–ª–æ 40% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –ø–æ–º–µ—Ö–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è –±–æ–ª–µ–µ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—É—é —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é –∫ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã–º –∏–∑–º–µ–Ω–µ–Ω–∏—è–º."
                },
                "en": {
                    "title": "CostBench: Bridging the Gap in Cost-Aware Planning for LLMs",
                    "desc": "The paper introduces CostBench, a new benchmark for evaluating Large Language Model (LLM) agents on their ability to plan cost-effectively and adapt to changing conditions. It highlights that current assessments focus mainly on task completion, neglecting the important aspect of resource efficiency. CostBench is designed to test agents in a travel-planning context, incorporating various tasks with customizable costs and dynamic events that mimic real-world unpredictability. The findings reveal that existing models, including GPT-5, struggle with cost-aware planning, achieving low performance rates, especially under dynamic scenarios, indicating a need for improvement in economic reasoning and adaptability."
                },
                "zh": {
                    "title": "ÊèêÂçá‰ª£ÁêÜÁöÑÊàêÊú¨ÊÑèËØÜ‰∏éÈÄÇÂ∫îËÉΩÂäõ",
                    "desc": "CostBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ËøõË°åÊàêÊú¨ÊÑèËØÜËßÑÂàíÂíåÈÄÇÂ∫îËÉΩÂäõÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑„ÄÇÂΩìÂâçÁöÑËØÑ‰º∞‰∏ªË¶ÅÂÖ≥Ê≥®‰ªªÂä°ÂÆåÊàêÔºåËÄåÂøΩËßÜ‰∫ÜËµÑÊ∫êÊïàÁéáÂíåÈÄÇÂ∫îÊÄßÔºåËøôÂØπ‰∫é‰ª£ÁêÜÂú®ÂèòÂåñÁéØÂ¢É‰∏≠Âà∂ÂÆöÂíåË∞ÉÊï¥ÊàêÊú¨ÊúÄ‰ºòËÆ°ÂàíËá≥ÂÖ≥ÈáçË¶Å„ÄÇCostBench ËÆæËÆ°‰∫ÜÂ§öÁßç‰ªªÂä°ÔºåÊîØÊåÅ‰∏çÂêåÁöÑÂ∑•ÂÖ∑ÂíåÂèØÂÆöÂà∂ÁöÑÊàêÊú¨ÔºåÂπ∂Ê®°ÊãüÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÈÄöËøáÂØπÈ¢ÜÂÖàÊ®°ÂûãÁöÑËØÑ‰º∞ÔºåÂèëÁé∞ÂÆÉ‰ª¨Âú®ÊàêÊú¨ÊÑèËØÜËßÑÂàíÊñπÈù¢Â≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä®ÊÄÅÊù°‰ª∂‰∏ãË°®Áé∞Êõ¥Â∑Æ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02818",
            "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
            "url": "https://huggingface.co/papers/2511.02818",
            "abstract": "Orion-MSP, a tabular in-context learning architecture, addresses limitations in current models by incorporating multi-scale processing, block-sparse attention, and a Perceiver-style memory, achieving state-of-the-art performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "3b39df0f331db829",
            "authors": [
                "Mohamed Bouadi",
                "Pratinav Seth",
                "Aditya Tanna",
                "Vinay Kumar Sankarapu"
            ],
            "affiliations": [
                "Lexsi Labs, India & France"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02818.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "üìä",
                "ru": {
                    "title": "–ú–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
                    "desc": "Orion-MSP ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (in-context learning) –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —É–ª–∞–≤–ª–∏–≤–∞—é—â—É—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–ª–æ—á–Ω–æ-—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (block-sparse attention) —Å –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π –ª–æ–∫–∞–ª—å–Ω—ã—Ö, –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —à–∏—Ä–æ–∫–∏–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏ –±–µ–∑ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø–∞–º—è—Ç—å –≤ —Å—Ç–∏–ª–µ Perceiver, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö –∏ —É—Ç–æ—á–Ω—è—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ. –ù–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö Orion-MSP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—è—Å—å –Ω–∞ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã."
                },
                "en": {
                    "title": "Revolutionizing Tabular Data Learning with Orion-MSP",
                    "desc": "Orion-MSP is a new architecture designed for in-context learning with tabular data, which is commonly used in real-world applications. It improves upon existing models by using multi-scale processing to better understand complex feature interactions and block-sparse attention to manage large datasets efficiently. Additionally, it incorporates a Perceiver-style memory that allows for effective communication between different components of the model. As a result, Orion-MSP achieves top performance on various benchmarks while being scalable for high-dimensional data."
                },
                "zh": {
                    "title": "Orion-MSPÔºöÈ´òÊïàÁöÑË°®Ê†ºÊï∞ÊçÆÂ≠¶‰π†Êñ∞Ê†áÂáÜ",
                    "desc": "Orion-MSPÊòØ‰∏ÄÁßçÊñ∞ÁöÑË°®Ê†ºÊï∞ÊçÆ‰∏ä‰∏ãÊñáÂ≠¶‰π†Êû∂ÊûÑÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÈÄöËøáÂ§öÂ∞∫Â∫¶Â§ÑÁêÜ„ÄÅÂùóÁ®ÄÁñèÊ≥®ÊÑèÂäõÂíåPerceiverÈ£éÊ†ºÁöÑËÆ∞ÂøÜÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÁâπÂæÅ‰πãÈó¥ÁöÑÂ±ÇÊ¨°‰∫§‰∫í„ÄÇËØ•Ê®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºå‰∏îÂú®Â§ÑÁêÜÈ´òÁª¥Ë°®Ê†ºÊó∂ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊâ©Â±ïÊÄß„ÄÇOrion-MSP‰∏∫È´òÊïàÁöÑË°®Ê†ºÊï∞ÊçÆ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÆæÁ´ã‰∫ÜÊñ∞ÁöÑÊ†áÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02802",
            "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
            "url": "https://huggingface.co/papers/2511.02802",
            "abstract": "TabTune is a unified library that standardizes the workflow for tabular foundation models, supporting various adaptation strategies and evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune .",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "0f9518cd87a3d413",
            "authors": [
                "Aditya Tanna",
                "Pratinav Seth",
                "Mohamed Bouadi",
                "Utsav Avaiya",
                "Vinay Kumar Sankarapu"
            ],
            "affiliations": [
                "Lexsi Labs, India & France"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02802.jpg",
            "data": {
                "categories": [],
                "emoji": "üìä",
                "ru": {
                    "title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–∞–±—É–ª—è—Ä–Ω—ã—Ö foundation models",
                    "desc": "TabTune ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è —Ç–∞–±—É–ª—è—Ä–Ω—ã—Ö foundation models, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º –º–æ–¥–µ–ª—è–º. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è zero-shot inference, meta-learning, supervised fine-tuning –∏ parameter-efficient fine-tuning. TabTune –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö —Å —É—á—ë—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–π –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å—é –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –º–æ–¥—É–ª–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∏ fairness, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ç–∞–±—É–ª—è—Ä–Ω—ã—Ö foundation models."
                },
                "en": {
                    "title": "Streamlining Tabular Model Workflows with TabTune",
                    "desc": "TabTune is a comprehensive library designed to streamline the workflow for tabular foundation models in machine learning. It addresses challenges like inconsistent preprocessing and fragmented APIs by providing a unified interface for various adaptation strategies, including zero-shot inference and supervised fine-tuning. The library also automates preprocessing and integrates evaluation metrics for performance, calibration, and fairness, ensuring reliable deployment of models. By promoting extensibility and reproducibility, TabTune facilitates consistent benchmarking across different adaptation methods for tabular data."
                },
                "zh": {
                    "title": "TabTuneÔºöË°®Ê†ºÊ®°ÂûãÁöÑÁªü‰∏ÄÂ∑•‰ΩúÊµÅËß£ÂÜ≥ÊñπÊ°à",
                    "desc": "TabTuneÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ∫ìÔºåÊó®Âú®Ê†áÂáÜÂåñË°®Ê†ºÂü∫Á°ÄÊ®°ÂûãÁöÑÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂÆÉÊîØÊåÅÂ§öÁßçÈÄÇÂ∫îÁ≠ñÁï•ÂíåËØÑ‰º∞ÊåáÊ†áÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®È¢ÑÂ§ÑÁêÜ„ÄÅAPIÂíåÂæÆË∞ÉËøáÁ®ã‰∏≠ÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇÈÄöËøáÂçï‰∏ÄÊé•Âè£ÔºåTabTuneÊèê‰æõÂØπ‰∏ÉÁßçÊúÄÂÖàËøõÊ®°ÂûãÁöÑ‰∏ÄËá¥ËÆøÈóÆÔºåÊîØÊåÅÈõ∂Ê†∑Êú¨Êé®ÁêÜ„ÄÅÂÖÉÂ≠¶‰π†„ÄÅÁõëÁù£ÂæÆË∞ÉÂíåÂèÇÊï∞È´òÊïàÂæÆË∞ÉÁ≠âÁ≠ñÁï•„ÄÇËØ•Ê°ÜÊû∂Ëá™Âä®ÂåñÊ®°ÂûãÊÑüÁü•ÁöÑÈ¢ÑÂ§ÑÁêÜÔºåÂÜÖÈÉ®ÁÆ°ÁêÜÊû∂ÊûÑÂºÇÊûÑÊÄßÔºåÂπ∂ÈõÜÊàêÊÄßËÉΩ„ÄÅÊ†°ÂáÜÂíåÂÖ¨Âπ≥ÊÄßËØÑ‰º∞Ê®°ÂùóÔºå‰øÉËøõ‰∫ÜË°®Ê†ºÂü∫Á°ÄÊ®°ÂûãÁöÑÂèØÊâ©Â±ïÊÄßÂíåÂèØÈáçÂ§çÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.01294",
            "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
            "url": "https://huggingface.co/papers/2511.01294",
            "abstract": "Kinematify is an automated framework that synthesizes articulated objects from RGB images or textual descriptions, addressing challenges in inferring kinematic topologies and estimating joint parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-11-03",
            "pub_date_card": {
                "ru": "3 –Ω–æ—è–±—Ä—è",
                "en": "November 3",
                "zh": "11Êúà3Êó•"
            },
            "hash": "91edb1158b49e80b",
            "authors": [
                "Jiawei Wang",
                "Dingyou Wang",
                "Jiaming Hu",
                "Qixuan Zhang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "affiliations": [
                "Deemos Technology Co., Ltd."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.01294.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#cv",
                    "#multimodal",
                    "#robotics",
                    "#synthetic"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ —à–∞—Ä–Ω–∏—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞",
                    "desc": "Kinematify ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —à–∞—Ä–Ω–∏—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏: –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–µ–π –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Å–≤–æ–±–æ–¥—ã –∏ –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—É—Å—Ç–∞–≤–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –º–µ—Ç–æ–¥–æ–º –¥–µ—Ä–µ–≤–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—É—Å—Ç–∞–≤–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Automating Articulated Object Synthesis for Robotics",
                    "desc": "Kinematify is an innovative framework that automates the creation of articulated object models from RGB images or textual descriptions. It tackles the complex problems of determining kinematic structures and estimating joint parameters, which are crucial for robotic manipulation and simulation tasks. By integrating Monte Carlo Tree Search (MCTS) for structural inference with geometry-driven optimization, Kinematify generates accurate and functional models for objects with high degrees of freedom. The framework shows significant advancements in accuracy and scalability compared to existing methods, making it a valuable tool for robotics and motion planning."
                },
                "zh": {
                    "title": "KinematifyÔºöËá™Âä®ÂêàÊàêÂÖ≥ËäÇÁâ©‰ΩìÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "KinematifyÊòØ‰∏Ä‰∏™Ëá™Âä®ÂåñÊ°ÜÊû∂ÔºåÂèØ‰ª•‰ªéRGBÂõæÂÉèÊàñÊñáÊú¨ÊèèËø∞‰∏≠ÂêàÊàêÂÖ≥ËäÇÁâ©‰Ωì„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÊé®Êñ≠È´òËá™Áî±Â∫¶Áâ©‰ΩìÁöÑËøêÂä®ÊãìÊâëÂíå‰º∞ËÆ°ÂÖ≥ËäÇÂèÇÊï∞ÁöÑÊåëÊàò„ÄÇÈÄöËøáÁªìÂêàËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÂíåÂá†‰ΩïÈ©±Âä®ÁöÑ‰ºòÂåñÔºåKinematifyËÉΩÂ§üÁîüÊàêÁâ©ÁêÜ‰∏ÄËá¥‰∏îÂäüËÉΩÊúâÊïàÁöÑÊèèËø∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKinematifyÂú®Ê≥®ÂÜåÂíåËøêÂä®ÊãìÊâëÂáÜÁ°ÆÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03628",
            "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
            "url": "https://huggingface.co/papers/2511.03628",
            "abstract": "LiveTradeBench evaluates LLMs in dynamic trading environments to assess decision-making under real-time uncertainty and market volatility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 –Ω–æ—è–±—Ä—è",
                "en": "November 5",
                "zh": "11Êúà5Êó•"
            },
            "hash": "d928cc902d93698c",
            "authors": [
                "Haofei Yu",
                "Fenghai Li",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois, Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03628.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#leakage"
                ],
                "emoji": "üìà",
                "ru": {
                    "title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–µ: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º —Ä—ã–Ω–∫–∞",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è LiveTradeBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ –ø—Ä–∏–Ω—è—Ç–∏—é —Ä–µ—à–µ–Ω–∏–π –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ä–µ–¥–µ —Å —Ä–µ–∞–ª—å–Ω–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é —Ä—ã–Ω–∫–∞. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏. LLM-–∞–≥–µ–Ω—Ç—ã —É–ø—Ä–∞–≤–ª—è—é—Ç –ø–æ—Ä—Ç—Ñ–µ–ª—è–º–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–∫—Ç–∏–≤–æ–≤, –±–∞–ª–∞–Ω—Å–∏—Ä—É—è —Ä–∏—Å–∫ –∏ –¥–æ—Ö–æ–¥, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç —É—Å–ø–µ—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–µ, —á—Ç–æ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Evaluating LLMs in Real-Time Trading: Bridging Static Tests and Dynamic Markets",
                    "desc": "The paper introduces LiveTradeBench, a novel framework for evaluating large language models (LLMs) in dynamic trading scenarios. Unlike traditional benchmarks that assess isolated reasoning, LiveTradeBench simulates real-time market conditions, allowing LLMs to make decisions under uncertainty and volatility. It incorporates live data streaming, multi-asset portfolio management, and diverse market environments to provide a comprehensive evaluation of LLM performance. The findings reveal that high scores in static evaluations do not guarantee effective trading strategies, highlighting the need for benchmarks that reflect real-world decision-making challenges."
                },
                "zh": {
                    "title": "ÂÆûÊó∂‰∫§ÊòìÁéØÂ¢É‰∏ãÁöÑÂÜ≥Á≠ñËØÑ‰º∞",
                    "desc": "LiveTradeBench ÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âä®ÊÄÅ‰∫§ÊòìÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõÁöÑÂ∑•ÂÖ∑„ÄÇÂÆÉÈÄöËøáÂÆûÊó∂Â∏ÇÂú∫Êï∞ÊçÆÊµÅÂíåÊñ∞ÈóªÔºåÊ∂àÈô§‰∫ÜÂØπÁ¶ªÁ∫øÂõûÊµãÁöÑ‰æùËµñÔºå‰ªéËÄåÊçïÊçâÁúüÂÆûÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËØ•Âπ≥Âè∞ÊîØÊåÅÂ§öËµÑ‰∫ßÁªÑÂêàÁÆ°ÁêÜÔºåÊï¥ÂêàÈ£éÈô©ÁÆ°ÁêÜÂíåË∑®ËµÑ‰∫ßÊé®ÁêÜÔºåÈÄÇÁî®‰∫é‰∏çÂêåÂ∏ÇÂú∫ÁéØÂ¢ÉÁöÑËØÑ‰º∞„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÈùôÊÄÅËØÑ‰º∞‰∏éÁúüÂÆû‰∏ñÁïåËÉΩÂäõ‰πãÈó¥Â≠òÂú®Â∑ÆË∑ùÔºåÂº∫Ë∞É‰∫ÜÂú®ÂÆûÊó∂‰∏çÁ°ÆÂÆöÊÄß‰∏ãÊµãËØïËøûÁª≠ÂÜ≥Á≠ñÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03146",
            "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity",
            "url": "https://huggingface.co/papers/2511.03146",
            "abstract": "MME-CC is a vision-grounded benchmark that evaluates multimodal large language models' cognitive capacity across spatial, geometric, and knowledge-based reasoning tasks, revealing weaknesses in spatial and geometric reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 –Ω–æ—è–±—Ä—è",
                "en": "November 5",
                "zh": "11Êúà5Êó•"
            },
            "hash": "44c56671af0489ab",
            "authors": [
                "Kaiyuan Zhang",
                "Chenghao Yang",
                "Zhoufutu Wen",
                "Sihang Yuan",
                "Qiuyue Wang",
                "Chaoyi Huang",
                "Guosheng Zhu",
                "He Wang",
                "Huawenyu Lu",
                "Jianing Wen",
                "Jianpeng Jiao",
                "Lishu Luo",
                "Longxiang Liu",
                "Sijin Wu",
                "Xiaolei Zhu",
                "Xuanliang Zhang",
                "Ge Zhang",
                "Yi Lin",
                "Guang Shi",
                "Chaoyou Fu",
                "Wenhao Huang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03146.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#cv",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è: —Ä–∞—Å–∫—Ä—ã–≤–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MME-CC, –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ, –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∏ –∑–Ω–∞–Ω–∏–µ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 16 –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤—ã—è–≤–∏–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–∞–±–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏, –≥–¥–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30%. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–æ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏: –æ—à–∏–±–∫–∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏, –Ω–µ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–º–µ–Ω–µ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏ –ø–ª–æ—Ö–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –†–∞–±–æ—Ç–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã—Ö –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π MLLMs –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ ‚Üí —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∞) –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞."
                },
                "en": {
                    "title": "Evaluating Cognitive Capacity in Multimodal Models",
                    "desc": "The paper introduces MME-CC, a benchmark designed to evaluate the cognitive abilities of multimodal large language models (MLLMs) in reasoning tasks that involve visual information. It categorizes these tasks into spatial, geometric, and knowledge-based reasoning, highlighting the models' weaknesses in spatial and geometric reasoning. The study conducts experiments on 16 MLLMs, revealing that while some closed-source models perform better overall, they still struggle with tasks requiring spatial and geometric understanding. The findings emphasize the need for a deeper focus on cognitive capacity in the development and evaluation of MLLMs."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑËÆ§Áü•ËÉΩÂäõ",
                    "desc": "MME-CCÊòØ‰∏Ä‰∏™Âü∫‰∫éËßÜËßâÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Á©∫Èó¥„ÄÅÂá†‰ΩïÂíåÁü•ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËÆ§Áü•ËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜËøá‰∫éÂº∫Ë∞ÉÊñáÊú¨Êé®ÁêÜÔºåÊú™ËÉΩÁ≥ªÁªüÂú∞ÊçïÊçâ‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÁöÑËÆ§Áü•Ë°å‰∏∫„ÄÇÈÄöËøáÂØπ16‰∏™‰ª£Ë°®ÊÄßÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Á©∫Èó¥ÂíåÂá†‰ΩïÊé®ÁêÜËÉΩÂäõÊôÆÈÅçËæÉÂº±Ôºå‰∏îÂ≠òÂú®Â∏∏ËßÅÁöÑÈîôËØØÊ®°Âºè„ÄÇÊàë‰ª¨Â∏åÊúõËøôÈ°πÂ∑•‰ΩúËÉΩÂ§üÊé®Âä®Â∞ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ§Áü•ËÉΩÂäõ‰Ωú‰∏∫ËØÑ‰º∞ÂíåÊ®°ÂûãËÆæËÆ°ÁöÑÊ†∏ÂøÉ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02309",
            "title": "The Sequential Edge: Inverse-Entropy Voting Beats Parallel\n  Self-Consistency at Matched Compute",
            "url": "https://huggingface.co/papers/2511.02309",
            "abstract": "Sequential scaling in language model reasoning outperforms parallel scaling across multiple models and benchmarks, with inverse-entropy weighted voting further enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "bddb0cb10cb55da4",
            "authors": [
                "Aman Sharma",
                "Paras Chopra"
            ],
            "affiliations": [
                "Lossfunk"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02309.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#inference",
                    "#reasoning"
                ],
                "emoji": "üîÑ",
                "ru": {
                    "title": "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–≤–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º, –≥–¥–µ –∫–∞–∂–¥–∞—è –Ω–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–ø—ã—Ç–∫–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º –±—é–¥–∂–µ—Ç–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ 95,6% –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π. –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Sequential Scaling: The Future of Language Model Reasoning",
                    "desc": "This paper investigates the effectiveness of sequential scaling versus parallel scaling in language model reasoning. It finds that running fewer chains that build on each other through sequential steps leads to better accuracy than running multiple independent chains at the same time. The authors introduce a new method called inverse-entropy weighted voting, which further improves the accuracy of sequential scaling by weighing answers based on the reliability of their reasoning. Overall, the study suggests a significant shift in how we optimize inference time for language models, favoring sequential refinement over traditional parallel approaches."
                },
                "zh": {
                    "title": "È°∫Â∫èÊâ©Â±ïË∂ÖË∂äÂπ∂Ë°åÊâ©Â±ïÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÂáÜÁ°ÆÊÄß",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ‰∏≠ÁöÑÊµãËØïÊó∂Èó¥Êâ©Â±ïÈóÆÈ¢òÔºåÊØîËæÉ‰∫ÜÂπ∂Ë°åÊâ©Â±ïÂíåÈ°∫Â∫èÊâ©Â±ïÁöÑÊïàÊûú„ÄÇÂú®Áõ∏ÂêåÁöÑ‰ª§ÁâåÈ¢ÑÁÆóÂíåËÆ°ÁÆóËµÑÊ∫ê‰∏ãÔºåÈ°∫Â∫èÊâ©Â±ïÈÄöËøáÈÄêÊ≠•ÊîπËøõÁöÑÊñπÂºèÔºåÊòæËëó‰ºò‰∫éÂπ∂Ë°åËá™‰∏ÄËá¥ÊÄßÊñπÊ≥ï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ°∫Â∫èÊâ©Â±ïÂú®95.6%ÁöÑÈÖçÁΩÆ‰∏≠Ë°®Áé∞Êõ¥‰Ω≥ÔºåÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫ÜÊúÄÈ´ò46.7%„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊó†ÂÖ≥ÊñπÊ≥ï‚Äî‚ÄîÈÄÜÁÜµÂä†ÊùÉÊäïÁ•®ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÈ°∫Â∫èÊâ©Â±ïÁöÑÂáÜÁ°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.03718",
            "title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask",
            "url": "https://huggingface.co/papers/2511.03718",
            "abstract": "A perspectivist annotation scheme for the HCRC MapTask corpus reveals how understanding emerges, diverges, and repairs in collaborative dialogue, highlighting the role of multiplicity discrepancies in referential misalignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-05",
            "pub_date_card": {
                "ru": "5 –Ω–æ—è–±—Ä—è",
                "en": "November 5",
                "zh": "11Êúà5Êó•"
            },
            "hash": "6ce52f670b595fdb",
            "authors": [
                "Nan Li",
                "Albert Gatt",
                "Massimo Poesio"
            ],
            "affiliations": [
                "Utrecht University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.03718.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#data",
                    "#interpretability"
                ],
                "emoji": "üó∫Ô∏è",
                "ru": {
                    "title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ: –∫–∞–∫ –≤—ã—è–≤–∏—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏—Å—Ç—Å–∫–∞—è —Å—Ö–µ–º–∞ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ –¥–∏–∞–ª–æ–≥–æ–≤ HCRC MapTask, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–¥–µ–ª—å–Ω–æ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ —Å–ª—É—à–∞—é—â–µ–≥–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ LLM —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ —Å—Ö–µ–º—ã –¥–ª—è –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 13 —Ç—ã—Å—è—á –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å—Å—ã–ª–æ–∫ –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤–∑–∞–∏–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ª–Ω—ã–µ –Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ–¥–∫–∏ –ø–æ—Å–ª–µ —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –Ω–æ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–∑—ã–≤–∞—é—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å–æ–∑–¥–∞—ë—Ç —Ä–µ—Å—É—Ä—Å –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö –∏ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≤–∏—Å—è—â–µ–µ –æ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –≤ –¥–∏–∞–ª–æ–≥–µ."
                },
                "en": {
                    "title": "Understanding Misalignment in Collaborative Dialogue",
                    "desc": "This paper presents a new annotation scheme for analyzing collaborative dialogue in the HCRC MapTask corpus. It focuses on how participants build mutual understanding, even when they may refer to different things. The authors introduce a method to track how understanding develops, diverges, and is repaired during conversations. Their findings highlight that while misunderstandings are uncommon, discrepancies in perspective can lead to significant misalignments in reference, providing insights into how language models can better handle these situations."
                },
                "zh": {
                    "title": "Êè≠Á§∫Âçè‰ΩúÂØπËØù‰∏≠ÁöÑÁêÜËß£‰∏éËØØËß£",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÈáäÊñπÊ°àÔºåÁî®‰∫éÂàÜÊûêHCRC MapTaskËØ≠ÊñôÂ∫ì‰∏≠ÁöÑÂçè‰ΩúÂØπËØù„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®‰∏çÂØπÁß∞ÁöÑÂØπËØù‰∏≠ÔºåÂèÇ‰∏éËÄÖÂèØËÉΩ‰ºöËØØ‰ª•‰∏∫ÂΩºÊ≠§ËææÊàê‰∏ÄËá¥ÔºåÂÆûÈôÖ‰∏äÂç¥Âú®Êåá‰ª£‰∏çÂêåÁöÑÂÆû‰Ωì„ÄÇÈÄöËøáÂØπÊØè‰∏™ÂèÇËÄÉË°®ËææÁöÑÂèëË®ÄËÄÖÂíåÂê¨ËÄÖÁöÑÁêÜËß£ËøõË°åÂçïÁã¨ÊçïÊçâÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜÁêÜËß£ÊòØÂ¶Ç‰ΩïÈÄêÊ∏êÂΩ¢Êàê„ÄÅÂàÜÊ≠ßÂíå‰øÆÂ§çÁöÑ„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÂ∞ΩÁÆ°ËØçÊ±áÂèò‰ΩìÁªü‰∏ÄÂêéÂÆåÂÖ®ËØØËß£ÁöÑÊÉÖÂÜµÂæàÂ∞ëÔºå‰ΩÜÂ§öÊ†∑ÊÄßÂ∑ÆÂºÇ‰ºöÁ≥ªÁªüÊÄßÂú∞ÂºïÂèëÂàÜÊ≠ßÔºåË°®ÊòéË°®Èù¢‰∏äÁöÑÂÖ±ËØÜÂèØËÉΩÊé©Áõñ‰∫ÜÊåá‰ª£ÁöÑ‰∏ç‰∏ÄËá¥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.02358",
            "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation",
            "url": "https://huggingface.co/papers/2511.02358",
            "abstract": "M-Solomon, a multimodal embedder, adaptively augments queries using a Multimodal LLM, improving performance and reducing embedding latency compared to baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "8acc02d495d5f5bc",
            "authors": [
                "Wongyu Kim",
                "Hochang Lee",
                "Sanghak Lee",
                "Yoonsung Kim",
                "Jaehyun Park"
            ],
            "affiliations": [
                "NC AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.02358.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤: –±–µ—Ä—ë–º augmentation —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ",
                    "desc": "M-Solomon ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –≤—Å—Ç—Ä–∞–∏–≤–∞—Ç–µ–ª—å (embedder), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é LLM –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –¥–µ–ª–∏—Ç –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –¥–≤–µ –≥—Ä—É–ø–ø—ã: —Ç–µ, –∫–æ—Ç–æ—Ä—ã–º –Ω—É–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –∏ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–º –æ–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ–∑ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –ª–∏–±–æ –ø—Ä–µ—Ñ–∏–∫—Å /augment –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –ª–∏–±–æ /embed –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ M-Solomon –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, —Ç–∞–∫ –∏ –º–µ—Ç–æ–¥—ã —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –ø—Ä–∏ —ç—Ç–æ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –∑–∞–¥–µ—Ä–∂–∫—É –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."
                },
                "en": {
                    "title": "Adaptive Query Augmentation for Enhanced Embedding Efficiency",
                    "desc": "M-Solomon is a novel multimodal embedder that enhances query performance by adaptively deciding when to augment queries using a Multimodal Large Language Model (MLLM). It categorizes queries into two groups: those that benefit from augmentation and those that do not, optimizing the embedding process. By generating synthetic augmentations only for the necessary queries, M-Solomon significantly reduces embedding latency while improving overall performance. Experimental results demonstrate that M-Solomon outperforms both traditional methods without augmentation and those that always apply augmentation, showcasing its efficiency and effectiveness in multimodal environments."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÊü•ËØ¢Â¢ûÂº∫ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÂµåÂÖ•ÊïàÁéá",
                    "desc": "M-SolomonÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÂµåÂÖ•Âô®ÔºåÈÄöËøáËá™ÈÄÇÂ∫îÂ¢ûÂº∫Êü•ËØ¢ÔºåÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊù•ÊèêÈ´òÊÄßËÉΩÂπ∂ÂáèÂ∞ëÂµåÂÖ•Âª∂Ëøü„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂ∞ÜËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÊü•ËØ¢ÂàÜ‰∏∫‰∏§ÁªÑÔºå‰∏ÄÁªÑÈúÄË¶ÅÂ¢ûÂº∫ÔºåÂè¶‰∏ÄÁªÑ‰∏çÈúÄË¶Å„ÄÇÊé•ÁùÄÔºåM-SolomonÈÄöËøáÁîüÊàêÂêàÈÄÇÁöÑÂ¢ûÂº∫‰ø°ÊÅØÊù•Â§ÑÁêÜÈúÄË¶ÅÂ¢ûÂº∫ÁöÑÊü•ËØ¢ÔºåÂπ∂Âú®ÂøÖË¶ÅÊó∂ËøõË°åËá™ÈÄÇÂ∫îÊü•ËØ¢Â¢ûÂº∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåM-SolomonÂú®ÂµåÂÖ•Âª∂Ëøü‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåÂêåÊó∂Âú®ÊÄßËÉΩ‰∏ä‰πüÊúâÊòæËëóÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.04583",
            "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration\n  from a Baseline Paper",
            "url": "https://huggingface.co/papers/2511.04583",
            "abstract": "Jr. AI Scientist, an autonomous AI system, mimics novice researcher workflows to generate scientifically valuable papers, outperforming fully automated systems but with identified limitations and risks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-06",
            "pub_date_card": {
                "ru": "6 –Ω–æ—è–±—Ä—è",
                "en": "November 6",
                "zh": "11Êúà6Êó•"
            },
            "hash": "ab749d992d943b95",
            "authors": [
                "Atsuyuki Miyai",
                "Mashiro Toyooka",
                "Takashi Otonari",
                "Zaiying Zhao",
                "Kiyoharu Aizawa"
            ],
            "affiliations": [
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.04583.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#science",
                    "#alignment"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ö–æ–≥–¥–∞ AI —É—á–∏—Ç—Å—è –¥–µ–ª–∞—Ç—å –Ω–∞—É–∫—É –∫–∞–∫ —Å—Ç—É–¥–µ–Ω—Ç: —É—Å–ø–µ—Ö–∏ –∏ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è Jr. AI Scientist ‚Äî –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞—á–∏–Ω–∞—é—â–µ–≥–æ —É—á—ë–Ω–æ–≥–æ: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–±–æ—Ç—É, —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä—É—é—â–∏–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω–æ —Ü–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—é –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ Agents4Science, –≤—ã—è–≤–ª—è—è –∫–∞–∫ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫ –∏ –µ—ë –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ —Ä–∏—Å–∫–æ–≤ AI Scientist –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è AI-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞—É–∫–∏."
                },
                "en": {
                    "title": "Empowering Novice Research: Jr. AI Scientist's Journey in AI-Driven Science",
                    "desc": "The paper introduces Jr. AI Scientist, an autonomous AI system designed to replicate the research workflow of a novice researcher. It analyzes existing literature, formulates hypotheses, conducts experiments, and writes scientific papers, outperforming fully automated systems in generating valuable contributions. The study evaluates its performance through automated assessments and peer reviews, revealing that it achieves higher scores than previous models. However, the authors also highlight significant limitations and risks associated with its use, emphasizing the need for careful consideration in the application of AI in scientific research."
                },
                "zh": {
                    "title": "Ê®°‰ªøÂàùÂ≠¶ËÄÖÔºåÊé®Âä®ÁßëÂ≠¶ËøõÊ≠•ÁöÑAIÁ≥ªÁªü",
                    "desc": "Jr. AI Scientist ÊòØ‰∏Ä‰∏™Ëá™‰∏ªÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÔºåÊ®°‰ªøÂàùÂ≠¶ËÄÖÁ†îÁ©∂ËÄÖÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÁîüÊàêÂÖ∑ÊúâÁßëÂ≠¶‰ª∑ÂÄºÁöÑËÆ∫Êñá„ÄÇÂÆÉÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÂÆåÂÖ®Ëá™Âä®ÂåñÁöÑÁ≥ªÁªüÔºå‰ΩÜ‰πüÂ≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÊÄßÂíåÈ£éÈô©„ÄÇËØ•Á≥ªÁªüÈÄöËøáÂàÜÊûê‰∫∫Á±ªÂØºÂ∏àÊèê‰æõÁöÑÂü∫Á°ÄËÆ∫ÊñáÔºåÊèêÂá∫ÊîπËøõÂÅáËÆæÔºåÂπ∂ÈÄöËøá‰∏•Ê†ºÂÆûÈ™åÈ™åËØÅËøô‰∫õÂÅáËÆæÔºåÊúÄÁªàÊí∞ÂÜôÂá∫Á†îÁ©∂ÁªìÊûúÁöÑËÆ∫Êñá„ÄÇÂ∞ΩÁÆ° Jr. AI Scientist ÂèñÂæó‰∫ÜËæÉÈ´òÁöÑËØÑÂÆ°ÂàÜÊï∞Ôºå‰ΩÜ‰ªçÈúÄÂÖ≥Ê≥®ÂÖ∂Âú®Â∫îÁî®‰∏≠ÁöÑÊΩúÂú®È£éÈô©ÂíåÊú™Êù•Á†îÁ©∂ÁöÑÊåëÊàò„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-11-05.html",
    "link_next": "2025-11-07.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11Êúà5Êó•"
    },
    "short_date_next": {
        "ru": "07.11",
        "en": "11/07",
        "zh": "11Êúà7Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}