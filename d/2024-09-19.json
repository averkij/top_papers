{
    "date": {
        "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 19",
        "zh": "9æœˆ19æ—¥"
    },
    "time_utc": "2024-09-19 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-19",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.12186",
            "title": "Qwen2.5-Coder Technical Report",
            "url": "https://huggingface.co/papers/2409.12186",
            "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.",
            "score": 125,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "3a409a257f1d480b",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#data",
                    "#plp",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Qwen2.5-Coder: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2.5-Coder, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ CodeQwen1.5. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Qwen2.5 Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 5,5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Qwen2.5-Coder Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering Code Generation with Qwen2.5-Coder!",
                    "desc": "The Qwen2.5-Coder series represents a major advancement in code generation models, succeeding the CodeQwen1.5. It consists of two versions, Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B, which are built on the Qwen2.5 architecture and trained on an extensive dataset of over 5.5 trillion tokens. This model excels in various code-related tasks, achieving state-of-the-art performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair. The enhancements in data processing and model training are designed to foster greater adoption among developers for practical applications."
                },
                "zh": {
                    "title": "Qwen2.5-Coderï¼šä»£ç ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Qwen2.5-Coderç³»åˆ—ï¼Œè¿™æ˜¯å¯¹å…¶å‰èº«CodeQwen1.5çš„é‡è¦å‡çº§ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å‹ï¼šQwen2.5-Coder-1.5Bå’ŒQwen2.5-Coder-7Bï¼Œä¸“æ³¨äºä»£ç ç”Ÿæˆã€‚Qwen2.5-CoderåŸºäºQwen2.5æ¶æ„ï¼Œç»è¿‡è¶…è¿‡5.5ä¸‡äº¿ä¸ªæ ‡è®°çš„é¢„è®­ç»ƒï¼Œå±•ç°å‡ºå“è¶Šçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šé¡¹ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡çš„æ›´å¤§æ¨¡å‹ï¼Œæ¨åŠ¨äº†ä»£ç æ™ºèƒ½ç ”ç©¶çš„å‰æ²¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12191",
            "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
            "url": "https://huggingface.co/papers/2409.12191",
            "abstract": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at https://github.com/QwenLM/Qwen2-VL.",
            "score": 73,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "298712ce7466399d",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#open_source",
                    "#small_models",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2-VL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Naive Dynamic Resolution. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ M-RoPE Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Dynamic Resolution for Enhanced Visual Understanding",
                    "desc": "The Qwen2-VL Series is an upgraded version of the Qwen-VL models that changes how visual data is processed by using a Naive Dynamic Resolution mechanism. This allows the model to handle images of different resolutions more flexibly, resulting in better and more accurate visual representations. It also incorporates Multimodal Rotary Position Embedding (M-RoPE) to effectively combine positional information from text, images, and videos. By scaling the model size and training data, the Qwen2-VL-72B model achieves performance on par with top models like GPT-4o and Claude3.5-Sonnet in multimodal tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€åˆ†è¾¨ç‡ï¼Œæå‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼",
                    "desc": "Qwen2-VLç³»åˆ—æ˜¯å¯¹ä¹‹å‰Qwen-VLæ¨¡å‹çš„é«˜çº§å‡çº§ï¼Œé‡æ–°å®šä¹‰äº†è§†è§‰å¤„ç†ä¸­çš„ä¼ ç»Ÿé¢„è®¾åˆ†è¾¨ç‡æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ç®€å•åŠ¨æ€åˆ†è¾¨ç‡æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¹¶ç”Ÿæˆä¸åŒæ•°é‡çš„è§†è§‰æ ‡è®°ã€‚è¯¥æ¨¡å‹è¿˜é›†æˆäº†å¤šæ¨¡æ€æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆM-RoPEï¼‰ï¼Œæœ‰æ•ˆèåˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘çš„ä½ç½®ä¿¡æ¯ã€‚é€šè¿‡ç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘å¤„ç†èŒƒå¼ï¼ŒQwen2-VLå¢å¼ºäº†æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„ç ”ç©¶ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12181",
            "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
            "url": "https://huggingface.co/papers/2409.12181",
            "abstract": "Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development.",
            "score": 43,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "40d004b4e127be2d",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Long-Context Understanding in Language Models",
                    "desc": "This paper discusses the challenges of training language models to understand long documents. It highlights the difficulty in comparing different methods for extending models to handle longer contexts due to varying data and model types. The authors propose a standardized evaluation protocol to assess the performance of these long-context models. Their findings indicate that perplexity remains a key performance metric, while approximate attention methods tend to underperform, and fine-tuning methods are effective but struggle with extrapolation."
                },
                "zh": {
                    "title": "æå‡é•¿æ–‡æœ¬ç†è§£çš„å…³é”®åœ¨äºå›°æƒ‘åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå›°æƒ‘åº¦æ˜¯è¯„ä¼°é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡çš„é‡è¦æŒ‡æ ‡ã€‚å½“å‰çš„è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œè€Œç²¾ç¡®å¾®è°ƒçš„æ–¹æ³•åœ¨å…¶æ‰©å±•èŒƒå›´å†…é€šå¸¸æœ‰æ•ˆã€‚ä½œè€…è¿˜æä¾›äº†å¼€æºä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12183",
            "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
            "url": "https://huggingface.co/papers/2409.12183",
            "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.",
            "score": 36,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "062e35c77608607b",
            "data": {
                "categories": [
                    "#reasoning",
                    "#survey",
                    "#dataset",
                    "#math",
                    "#inference",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ 'Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ'?",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT) Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 20 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoT Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ CoT Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking the Power of Thought: CoT's Role in Symbolic Reasoning",
                    "desc": "This paper investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models (LLMs) for various tasks. Through a meta-analysis of over 100 studies and evaluations on 20 datasets, the authors find that CoT significantly enhances performance mainly in math and logic tasks, while showing limited benefits for other tasks. They also discover that generating answers directly without CoT yields similar accuracy, except for questions involving symbolic reasoning. The study suggests that CoT should be used selectively to optimize performance and reduce computational costs, and advocates for exploring new methods beyond traditional CoT prompting."
                },
                "zh": {
                    "title": "æ€ç»´é“¾ï¼šæå‡é€»è¾‘ä¸æ•°å­¦ä»»åŠ¡çš„å…³é”®",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æç¤ºå¼•å¯¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨æ•ˆæœã€‚æˆ‘ä»¬å¯¹100å¤šç¯‡ä½¿ç”¨CoTçš„è®ºæ–‡è¿›è¡Œäº†å®šé‡å…ƒåˆ†æï¼Œå¹¶åœ¨14ä¸ªæ¨¡å‹ä¸Šè¯„ä¼°äº†20ä¸ªæ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒCoTåœ¨æ•°å­¦æˆ–é€»è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè€Œåœ¨å…¶ä»–ç±»å‹ä»»åŠ¡ä¸­çš„æå‡åˆ™è¾ƒå°ã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒCoTåœ¨ç¬¦å·æ‰§è¡Œæ–¹é¢çš„æ”¹è¿›æ˜¯å…¶æ€§èƒ½æå‡çš„ä¸»è¦åŸå› ï¼Œä½†ç›¸è¾ƒäºä½¿ç”¨ç¬¦å·æ±‚è§£å™¨ï¼Œå…¶è¡¨ç°ä»ç„¶ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11901",
            "title": "LLMs + Persona-Plug = Personalized LLMs",
            "url": "https://huggingface.co/papers/2409.11901",
            "abstract": "Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their individual interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. Some of them involve fine-tuning a unique personalized LLM for each user, which is too expensive for widespread application. Alternative approaches introduce personalization information in a plug-and-play manner by retrieving the user's relevant historical texts as demonstrations. However, this retrieval-based strategy may break the continuity of the user history and fail to capture the user's overall styles and patterns, hence leading to sub-optimal performance. To address these challenges, we propose a novel personalized LLM model, . It constructs a user-specific embedding for each individual by modeling all her historical contexts through a lightweight plug-in user embedder module. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing more personalized outputs without tuning their own parameters. Extensive experiments on various tasks in the language model personalization (LaMP) benchmark demonstrate that the proposed model significantly outperforms existing personalized LLM approaches.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "d24ec09831b2ffd9",
            "data": {
                "categories": [
                    "#personalization",
                    "#training",
                    "#alignment",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "PersoNULL: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PersoNULL. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ (embedding) Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²ÑĞµ ĞµĞ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ. Ğ­Ñ‚Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ LLM Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ²Ñ‹Ñ‡ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ±ĞµĞ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PersoNULL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM."
                },
                "en": {
                    "title": "Personalized Language Models Made Easy!",
                    "desc": "This paper discusses the importance of personalization in language tasks, highlighting that users with similar needs may still desire different outputs based on their unique preferences. It critiques existing methods that either require expensive fine-tuning of large language models (LLMs) or rely on retrieval-based strategies that can disrupt user history. The authors propose a new model that creates a user-specific embedding by analyzing all historical contexts through a lightweight module, allowing LLMs to better capture individual user styles. Experimental results show that this approach significantly improves performance in generating personalized outputs compared to previous methods."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è¯­è¨€æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "ä¸ªæ€§åŒ–åœ¨è®¸å¤šè¯­è¨€ä»»åŠ¡å’Œåº”ç”¨ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› ä¸ºå…·æœ‰ç›¸åŒéœ€æ±‚çš„ç”¨æˆ·å¯èƒ½ä¼šæ ¹æ®ä¸ªäººå…´è¶£åå¥½ä¸åŒçš„è¾“å‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡è½»é‡çº§çš„ç”¨æˆ·åµŒå…¥æ¨¡å—ä¸ºæ¯ä¸ªç”¨æˆ·æ„å»ºç‰¹å®šçš„åµŒå…¥ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ•æ‰ç”¨æˆ·çš„ä¹ æƒ¯å’Œåå¥½ã€‚ä¸ä¼ ç»Ÿçš„ä¸ªæ€§åŒ–æ–¹æ³•ä¸åŒï¼Œè¯¥æ¨¡å‹æ— éœ€è°ƒæ•´è‡ªèº«å‚æ•°å³å¯ç”Ÿæˆæ›´ä¸ªæ€§åŒ–çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–LLMæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11564",
            "title": "Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey",
            "url": "https://huggingface.co/papers/2409.11564",
            "abstract": "Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "6b85a58b33baead9",
            "data": {
                "categories": [
                    "#survey",
                    "#training",
                    "#rl",
                    "#alignment",
                    "#rlhf",
                    "#multimodal"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹: ĞºĞ»ÑÑ‡ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (preference tuning) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Aligning Models with Human Preferences: A Survey on Preference Tuning",
                    "desc": "This paper surveys the advancements in preference tuning, which is essential for aligning deep generative models with human preferences. It covers reinforcement learning frameworks, various preference tuning tasks, and the models and datasets used across language, speech, and vision. The paper provides a detailed analysis of different preference tuning methods and discusses their applications in real-world tasks. Finally, it outlines future research directions to foster innovation in the field of preference tuning and model alignment."
                },
                "zh": {
                    "title": "åå¥½è°ƒä¼˜ï¼šå¯¹é½æ¨¡å‹ä¸äººç±»éœ€æ±‚çš„å…³é”®",
                    "desc": "åå¥½è°ƒä¼˜æ˜¯å°†æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦è¿‡ç¨‹ã€‚æœ¬æ–‡ç»¼è¿°äº†åå¥½è°ƒä¼˜å’Œäººç±»åé¦ˆæ•´åˆçš„æœ€æ–°è¿›å±•ï¼Œåˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šé¦–å…ˆä»‹ç»å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€åå¥½è°ƒä¼˜ä»»åŠ¡ã€æ¨¡å‹å’Œæ•°æ®é›†ï¼›å…¶æ¬¡æ·±å…¥åˆ†æå„ç§åå¥½è°ƒä¼˜æ–¹æ³•ï¼›æœ€åæ¢è®¨åå¥½è°ƒä¼˜åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨åŠæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å±•ç¤ºåå¥½è°ƒä¼˜å’Œæ¨¡å‹å¯¹é½çš„æœ€æ–°æ–¹æ³•ï¼Œä¿ƒè¿›ç ”ç©¶è€…å’Œä»ä¸šè€…å¯¹è¯¥é¢†åŸŸçš„ç†è§£å’Œåˆ›æ–°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12136",
            "title": "GRIN: GRadient-INformed MoE",
            "url": "https://huggingface.co/papers/2409.12136",
            "abstract": "Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16times3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "bb28b0c9d4b617d5",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#math",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GRIN: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GRIN. GRIN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ GRIN Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 6.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» GRIN Ğ´Ğ»Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking the Power of Mixture-of-Experts with GRIN",
                    "desc": "This paper presents a new approach called GRIN for training Mixture-of-Experts (MoE) models, which are designed to scale better than traditional dense models by using sparse computation. The challenge with MoE is that the discrete routing of experts complicates the standard backpropagation process, which is essential for optimizing deep learning models. GRIN addresses this issue by using sparse gradient estimation to improve expert routing and implementing model parallelism to prevent token loss. The results show that the proposed MoE model, with fewer activated parameters, outperforms a larger dense model and achieves competitive performance on various language tasks."
                },
                "zh": {
                    "title": "GRINï¼šæå‡æ··åˆä¸“å®¶æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°è®­ç»ƒæ–¹æ³•",
                    "desc": "æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šè¿‡ç¨€ç–è®¡ç®—å’Œä¸“å®¶è·¯ç”±å®ç°äº†æ¯”å¯†é›†æ¨¡å‹æ›´æœ‰æ•ˆçš„æ‰©å±•ã€‚ç”±äºç¦»æ•£çš„ä¸“å®¶è·¯ç”±ä¼šé˜»ç¢æ ‡å‡†çš„åå‘ä¼ æ’­ï¼Œä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRINï¼ˆåŸºäºæ¢¯åº¦çš„ä¿¡æ¯çš„MoEè®­ç»ƒï¼‰ï¼Œå®ƒé€šè¿‡ç¨€ç–æ¢¯åº¦ä¼°è®¡æ¥ä¼˜åŒ–ä¸“å®¶è·¯ç”±ï¼Œå¹¶é…ç½®æ¨¡å‹å¹¶è¡Œä»¥é¿å…ä¸¢å¤±æ ‡è®°ã€‚é€šè¿‡åœ¨è‡ªå›å½’è¯­è¨€å»ºæ¨¡ä¸­åº”ç”¨GRINï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ€§èƒ½ä¼˜äº7Bå¯†é›†æ¨¡å‹çš„MoEæ¨¡å‹ï¼Œå±•ç¤ºäº†GRINåœ¨æå‡MoEæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12139",
            "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
            "url": "https://huggingface.co/papers/2409.12139",
            "abstract": "With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://takinaudiollm.github.io.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "11d685ad0e258a9d",
            "data": {
                "categories": [
                    "#audio",
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ½Ğ¸Ğ³: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Takin AudioLLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ½Ğ¸Ğ³ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Takin TTS Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Takin VC Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¸ Takin Morphing Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğ¼ÑƒÑ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹, Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞµÑ€Ğ¸Ğ¸ Takin AudioLLM."
                },
                "en": {
                    "title": "Revolutionizing Audiobook Production with Zero-Shot Customization",
                    "desc": "The paper presents Takin AudioLLM, a set of advanced models for audiobook production that enable zero-shot personalized speech generation. It includes Takin TTS, which uses a neural codec language model to produce high-quality, natural-sounding speech without prior training on specific data. Takin VC enhances speaker similarity through a joint modeling approach, while Takin Morphing allows users to customize speech characteristics like timbre and prosody. The effectiveness of these models is demonstrated through extensive experiments, showcasing their ability to generate human-like speech tailored to individual preferences."
                },
                "zh": {
                    "title": "é›¶-shotä¸ªæ€§åŒ–è¯­éŸ³å®šåˆ¶çš„æœªæ¥",
                    "desc": "éšç€å¤§æ•°æ®å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£çš„åˆ°æ¥ï¼Œé›¶-shotä¸ªæ€§åŒ–å¿«é€Ÿå®šåˆ¶æˆä¸ºä¸€ä¸ªé‡è¦è¶‹åŠ¿ã€‚æœ¬æ–‡ä»‹ç»äº†Takin AudioLLMç³»åˆ—æŠ€æœ¯å’Œæ¨¡å‹ï¼Œä¸»è¦åŒ…æ‹¬Takin TTSã€Takin VCå’ŒTakin Morphingï¼Œä¸“ä¸ºæœ‰å£°ä¹¦åˆ¶ä½œè€Œè®¾è®¡ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿå®ç°é›¶-shotè¯­éŸ³ç”Ÿæˆï¼Œç”Ÿæˆçš„é«˜è´¨é‡è¯­éŸ³å‡ ä¹ä¸çœŸå®äººå£°æ— å¼‚ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«éœ€æ±‚å®šåˆ¶è¯­éŸ³å†…å®¹ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†Takin AudioLLMç³»åˆ—æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08425",
            "title": "SoloAudio: Target Sound Extraction with Language-oriented Audio Diffusion Transformer",
            "url": "https://huggingface.co/papers/2409.08425",
            "abstract": "In this paper, we introduce SoloAudio, a novel diffusion-based generative model for target sound extraction (TSE). Our approach trains latent diffusion models on audio, replacing the previous U-Net backbone with a skip-connected Transformer that operates on latent features. SoloAudio supports both audio-oriented and language-oriented TSE by utilizing a CLAP model as the feature extractor for target sounds. Furthermore, SoloAudio leverages synthetic audio generated by state-of-the-art text-to-audio models for training, demonstrating strong generalization to out-of-domain data and unseen sound events. We evaluate this approach on the FSD Kaggle 2018 mixture dataset and real data from AudioSet, where SoloAudio achieves the state-of-the-art results on both in-domain and out-of-domain data, and exhibits impressive zero-shot and few-shot capabilities. Source code and demos are released.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "85f96fee2e333d85",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#multilingual",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "SoloAudio - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ²ÑƒĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ U-Net Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLAP Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². SoloAudio Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FSD Kaggle 2018 Ğ¸ AudioSet, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… zero-shot Ğ¸ few-shot."
                },
                "en": {
                    "title": "SoloAudio: Revolutionizing Target Sound Extraction with Diffusion Models",
                    "desc": "This paper presents SoloAudio, a new generative model that uses diffusion techniques for extracting specific sounds from audio. It innovatively replaces the traditional U-Net architecture with a Transformer that processes latent audio features, enhancing performance. SoloAudio is versatile, supporting both audio and language-based sound extraction by employing a CLAP model for feature extraction. The model is trained on synthetic audio from advanced text-to-audio systems, achieving top results on various datasets and demonstrating strong abilities in zero-shot and few-shot scenarios."
                },
                "zh": {
                    "title": "SoloAudioï¼šç›®æ ‡å£°éŸ³æå–çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹SoloAudioï¼Œç”¨äºç›®æ ‡å£°éŸ³æå–ï¼ˆTSEï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘ä¸Šè®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨è·³è·ƒè¿æ¥çš„Transformeræ›¿ä»£äº†ä¹‹å‰çš„U-Netéª¨å¹²ç½‘ç»œã€‚SoloAudioé€šè¿‡åˆ©ç”¨CLAPæ¨¡å‹ä½œä¸ºç›®æ ‡å£°éŸ³çš„ç‰¹å¾æå–å™¨ï¼Œæ”¯æŒéŸ³é¢‘å¯¼å‘å’Œè¯­è¨€å¯¼å‘çš„TSEã€‚æ­¤å¤–ï¼ŒSoloAudioåˆ©ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ç”Ÿæˆçš„åˆæˆéŸ³é¢‘è¿›è¡Œè®­ç»ƒï¼Œåœ¨æœªè§å£°éŸ³äº‹ä»¶å’Œé¢†åŸŸå¤–æ•°æ®ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12193",
            "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
            "url": "https://huggingface.co/papers/2409.12193",
            "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "e21a2ff70200771f",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ 2D Ğº 3D: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Vista3D - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ñ Gaussian Splatting Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑĞ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ (SDF). Vista3D Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ 2D Ğ¸ 3D-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Swift and Consistent 3D Generation with Vista3D",
                    "desc": "Vista3D is a novel framework designed for rapid 3D object generation from limited visual input. It employs a two-phase approach, starting with a coarse phase that uses Gaussian Splatting to create initial geometry from a single image. The fine phase enhances this geometry by extracting a Signed Distance Function (SDF) and optimizing it through a differentiable isosurface representation. By utilizing a disentangled representation and harmonizing gradients from 2D and 3D diffusion priors, Vista3D achieves a remarkable balance between consistency and diversity in the generated 3D models."
                },
                "zh": {
                    "title": "Vista3Dï¼šå¿«é€Ÿç”Ÿæˆä¸‰ç»´ç‰©ä½“çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVista3Dçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»ç‰©ä½“çš„å¯è§éƒ¨åˆ†å¿«é€Ÿç”Ÿæˆå…¶éšè—çš„ä¸‰ç»´ç»´åº¦ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼šç²—ç•¥é˜¶æ®µå’Œç²¾ç»†é˜¶æ®µã€‚åœ¨ç²—ç•¥é˜¶æ®µï¼ŒVista3Dé€šè¿‡é«˜æ–¯ç‚¹äº‘ä»å•å¼ å›¾åƒä¸­å¿«é€Ÿç”Ÿæˆåˆå§‹å‡ ä½•å½¢çŠ¶ï¼›åœ¨ç²¾ç»†é˜¶æ®µï¼Œåˆ™ç›´æ¥ä»å­¦ä¹ åˆ°çš„é«˜æ–¯ç‚¹äº‘ä¸­æå–å¸¦ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰ï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†çš„ç­‰å€¼é¢è¡¨ç¤ºè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒVista3Dé€šè¿‡ä½¿ç”¨è§£è€¦è¡¨ç¤ºå’Œç‹¬ç«‹çš„éšå¼å‡½æ•°ï¼Œæå‡äº†ç”Ÿæˆè´¨é‡ï¼Œèƒ½å¤Ÿæ•æ‰ç‰©ä½“çš„å¯è§å’Œè¢«é®æŒ¡çš„éƒ¨åˆ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.09401",
            "title": "Towards Diverse and Efficient Audio Captioning via Diffusion Models",
            "url": "https://huggingface.co/papers/2409.09401",
            "abstract": "We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive diffusion model tailored for diverse and efficient audio captioning. Although existing captioning models relying on language backbones have achieved remarkable success in various captioning tasks, their insufficient performance in terms of generation speed and diversity impede progress in audio understanding and multimedia applications. Our diffusion-based framework offers unique advantages stemming from its inherent stochasticity and holistic context modeling in captioning. Through rigorous evaluation, we demonstrate that DAC not only achieves SOTA performance levels compared to existing benchmarks in the caption quality, but also significantly outperforms them in terms of generation speed and diversity. The success of DAC illustrates that text generation can also be seamlessly integrated with audio and visual generation tasks using a diffusion backbone, paving the way for a unified, audio-related generative model across different modalities.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "a78b001ecd3e1a38",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "DAC: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DAC (Diffusion-based Audio Captioning). Ğ­Ñ‚Ğ¾ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. DAC Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ£ÑĞ¿ĞµÑ… DAC Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Audio Captioning with Diffusion Models",
                    "desc": "The paper presents Diffusion-based Audio Captioning (DAC), a new model designed for creating captions for audio content. Unlike traditional models that rely heavily on language processing, DAC uses a diffusion approach that enhances both the speed and variety of generated captions. This model excels in generating high-quality captions while also being faster and more diverse than existing methods. The findings suggest that DAC can effectively combine text generation with audio and visual tasks, promoting a more integrated approach to multimedia understanding."
                },
                "zh": {
                    "title": "åŸºäºæ‰©æ•£çš„éŸ³é¢‘æè¿°ï¼šé€Ÿåº¦ä¸å¤šæ ·æ€§çš„çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„éŸ³é¢‘æè¿°æ¨¡å‹ï¼ˆDACï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§éè‡ªå›å½’çš„æ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºé«˜æ•ˆå¤šæ ·çš„éŸ³é¢‘æè¿°ã€‚ç°æœ‰çš„æè¿°æ¨¡å‹è™½ç„¶åœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç”Ÿæˆé€Ÿåº¦å’Œå¤šæ ·æ€§æ–¹é¢çš„ä¸è¶³é™åˆ¶äº†éŸ³é¢‘ç†è§£å’Œå¤šåª’ä½“åº”ç”¨çš„è¿›å±•ã€‚æˆ‘ä»¬çš„æ‰©æ•£æ¡†æ¶é€šè¿‡å›ºæœ‰çš„éšæœºæ€§å’Œæ•´ä½“ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜DACåœ¨æè¿°è´¨é‡ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç”Ÿæˆé€Ÿåº¦å’Œå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11074",
            "title": "RoMath: A Mathematical Reasoning Benchmark in Romanian",
            "url": "https://huggingface.co/papers/2409.11074",
            "abstract": "Mathematics has long been conveyed through natural language, primarily for human understanding. With the rise of mechanized mathematics and proof assistants, there is a growing need to understand informal mathematical text, yet most existing benchmarks focus solely on English, overlooking other languages. This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite comprising three datasets: RoMath-Baccalaureate, RoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical domains and difficulty levels, aiming to improve non-English language models and promote multilingual AI development. By focusing on Romanian, a low-resource language with unique linguistic features, RoMath addresses the limitations of Anglo-centric models and emphasizes the need for dedicated resources beyond simple automatic translation. We benchmark several open-weight language models, highlighting the importance of creating resources for underrepresented languages. We make the code and dataset available.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "4dd29be6c679fb86",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multilingual",
                    "#math",
                    "#benchmark",
                    "#open_source",
                    "#low_resource"
                ],
                "emoji": "ğŸ‡·ğŸ‡´",
                "ru": {
                    "title": "RoMath: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RoMath - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒĞ¼Ñ‹Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. RoMath Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Romanian Mathematics: RoMath for Multilingual AI",
                    "desc": "This paper presents RoMath, a benchmark suite designed to enhance mathematical reasoning in Romanian, a low-resource language. It includes three datasets that cover various mathematical topics and difficulty levels, aiming to support the development of multilingual AI models. The study highlights the limitations of existing benchmarks that primarily focus on English, advocating for resources that cater to underrepresented languages. By evaluating open-weight language models on these datasets, the paper underscores the importance of creating dedicated tools for non-English mathematical understanding."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šè¯­è¨€æ•°å­¦æ¨ç†çš„é©å‘½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†RoMathï¼Œè¿™æ˜¯ä¸€ä¸ªç½—é©¬å°¼äºšæ•°å­¦æ¨ç†åŸºå‡†å¥—ä»¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ•°æ®é›†ï¼šRoMath-Baccalaureateã€RoMath-Competitionså’ŒRoMath-Syntheticã€‚è¿™äº›æ•°æ®é›†æ¶µç›–äº†å¤šç§æ•°å­¦é¢†åŸŸå’Œéš¾åº¦çº§åˆ«ï¼Œæ—¨åœ¨æå‡éè‹±è¯­è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä¿ƒè¿›å¤šè¯­è¨€äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚é€šè¿‡å…³æ³¨ç½—é©¬å°¼äºšè¯­è¿™ä¸€ä½èµ„æºè¯­è¨€ï¼ŒRoMathè§£å†³äº†ä»¥è‹±è¯­ä¸ºä¸­å¿ƒæ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†è¶…è¶Šç®€å•è‡ªåŠ¨ç¿»è¯‘çš„ä¸“ç”¨èµ„æºçš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªå¼€æ”¾æƒé‡è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œçªå‡ºäº†ä¸ºä»£è¡¨æ€§ä¸è¶³è¯­è¨€åˆ›å»ºèµ„æºçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12001",
            "title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning",
            "url": "https://huggingface.co/papers/2409.12001",
            "abstract": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "2ff547dffde5361c",
            "data": {
                "categories": [
                    "#survey",
                    "#dataset",
                    "#rl",
                    "#data",
                    "#agents",
                    "#benchmark",
                    "#games",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑÑƒ Ğ² Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ (MARL) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 80 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ½Ğ¸Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ MARL."
                },
                "en": {
                    "title": "Enhancing Data Awareness in Offline Multi-Agent Reinforcement Learning",
                    "desc": "This paper addresses the challenges in offline multi-agent reinforcement learning (MARL) by highlighting the importance of data quality and consistency. It critiques the current practices where many studies create their own datasets without a standardized approach, leading to unclear results. The authors propose a framework that includes guidelines for dataset generation, a standardized repository for existing datasets, and tools for dataset analysis. These contributions aim to enhance data awareness and improve the overall performance of algorithms in offline MARL."
                },
                "zh": {
                    "title": "æå‡ç¦»çº¿MARLçš„æ•°æ®ä½¿ç”¨ä¸æ„è¯†",
                    "desc": "ç¦»çº¿å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ˜¯ä¸€ä¸ªåˆ©ç”¨é™æ€æ•°æ®é›†å¯»æ‰¾å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœ€ä¼˜æ§åˆ¶ç­–ç•¥çš„ç ”ç©¶æ–¹å‘ã€‚å°½ç®¡è¯¥é¢†åŸŸä»¥æ•°æ®é©±åŠ¨ä¸ºç‰¹å¾ï¼Œä½†ç›®å‰çš„ç ”ç©¶å¾€å¾€å¿½è§†äº†æ•°æ®çš„é‡è¦æ€§ï¼Œå¯¼è‡´ç®—æ³•æ€§èƒ½ä¸æ•°æ®é›†ä¹‹é—´çš„å…³ç³»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬é€šè¿‡æ–‡çŒ®è°ƒæŸ¥è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œå¹¶æŒ‡å‡ºå¤§å¤šæ•°ç ”ç©¶ç”Ÿæˆçš„æ•°æ®é›†ç¼ºä¹ä¸€è‡´çš„æ–¹æ³•è®ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰é¡¹å…³é”®è´¡çŒ®ï¼Œä»¥æ”¹å–„ç¦»çº¿MARLä¸­çš„æ•°æ®ä½¿ç”¨å’Œæ•°æ®æ„è¯†ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ–°æ•°æ®é›†çš„æ˜ç¡®æŒ‡å—ã€å¯¹80å¤šä¸ªç°æœ‰æ•°æ®é›†çš„æ ‡å‡†åŒ–ä»¥åŠä¸€å¥—åˆ†æå·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11363",
            "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
            "url": "https://huggingface.co/papers/2409.11363",
            "abstract": "AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "83eb0c2355b12707",
            "data": {
                "categories": [
                    "#science",
                    "#cv",
                    "#healthcare",
                    "#agents",
                    "#benchmark",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "CORE-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CORE-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 270 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 90 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½ Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: AutoGPT Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CORE-Agent, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¸ GPT-4o-mini. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 21% Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ÑƒÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "CORE-Bench: Advancing AI in Scientific Reproducibility",
                    "desc": "This paper presents CORE-Bench, a benchmark designed to evaluate AI agents on their ability to achieve computational reproducibility in scientific research. It consists of 270 tasks derived from 90 scientific papers across three fields: computer science, social science, and medicine, with varying levels of difficulty. The benchmark allows for efficient evaluation of agents, significantly reducing the time required for testing. Results from baseline agents indicate that while current performance is low, there is substantial potential for improvement in automating scientific tasks."
                },
                "zh": {
                    "title": "æå‡ç§‘å­¦ç ”ç©¶çš„å¯é‡å¤æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†CORE-Benchï¼ˆè®¡ç®—å¯é‡å¤æ€§ä»£ç†åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç§‘å­¦ç ”ç©¶ä¸­å¯é‡å¤æ€§ä»»åŠ¡è¡¨ç°çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«270ä¸ªä»»åŠ¡ï¼ŒåŸºäº90ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œæ¶µç›–è®¡ç®—æœºç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦å’ŒåŒ»å­¦ä¸‰ä¸ªé¢†åŸŸã€‚ä»»åŠ¡åˆ†ä¸ºä¸‰ç§éš¾åº¦çº§åˆ«ï¼ŒåŒ…æ‹¬ä»…è¯­è¨€å’Œè§†è§‰-è¯­è¨€ä»»åŠ¡ã€‚é€šè¿‡å¿«é€Ÿä¸”å¯å¹¶è¡Œçš„è¯„ä¼°ç³»ç»Ÿï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ˜¾è‘—èŠ‚çœè¯„ä¼°æ—¶é—´ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶ä»£ç†çš„å‘å±•æä¾›æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11315",
            "title": "fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction",
            "url": "https://huggingface.co/papers/2409.11315",
            "abstract": "Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "75e76c540084b86e",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#cv",
                    "#healthcare",
                    "#graphs",
                    "#benchmark",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° 3D-Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ğ¾Ñ‚ Ñ„ĞœĞ Ğ¢ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… fMRI-3D, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ„ĞœĞ Ğ¢-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 15 ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ 4768 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MinD-3D Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ„ĞœĞ Ğ¢. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾-Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ³Ğ»ÑƒĞ±Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ 3D Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Reconstructing 3D Visuals from Brain Signals with MinD-3D",
                    "desc": "This paper introduces a method called MinD-3D for reconstructing 3D visuals from fMRI data, which is crucial for understanding brain activity related to visual processing. The authors present a new dataset, fMRI-3D, containing data from 15 participants and 4768 3D objects, enhancing the diversity of training data for machine learning models. The MinD-3D framework utilizes a neuro-fusion encoder to extract features from fMRI signals, followed by a diffusion model and a generative transformer decoder to create accurate 3D object reconstructions. The study establishes new evaluation metrics and demonstrates that MinD-3D achieves high accuracy in both semantic and structural aspects, contributing to cognitive neuroscience and computer vision fields."
                },
                "zh": {
                    "title": "ä»fMRIæ•°æ®é‡å»º3Dè§†è§‰çš„åˆ›æ–°æ¢ç´¢",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºRecon3DMindçš„æ–¹æ³•ï¼Œç”¨äºä»åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ•°æ®ä¸­é‡å»º3Dè§†è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬åˆ›å»ºäº†fMRI-3Dæ•°æ®é›†ï¼ŒåŒ…å«15åå‚ä¸è€…çš„4768ä¸ª3Då¯¹è±¡ï¼Œæ•°æ®é›†åˆ†ä¸ºfMRI-Shapeå’ŒfMRI-Objaverseä¸¤ä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MinD-3Dæ¡†æ¶ï¼Œé€šè¿‡ç¥ç»èåˆç¼–ç å™¨æå–fMRIç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰ç‰¹å¾ï¼Œæœ€ç»ˆé‡å»º3Då¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinD-3Dåœ¨è¯­ä¹‰å’Œç©ºé—´å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¢å¼ºäº†æˆ‘ä»¬å¯¹äººè„‘å¤„ç†3Dè§†è§‰ä¿¡æ¯çš„ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12134",
            "title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework",
            "url": "https://huggingface.co/papers/2409.12134",
            "abstract": "In tackling the challenge of Multi-Document Summarization (MDS), numerous methods have been proposed, spanning both extractive and abstractive summarization techniques. However, each approach has its own limitations, making it less effective to rely solely on either one. An emerging and promising strategy involves a synergistic fusion of extractive and abstractive summarization methods. Despite the plethora of studies in this domain, research on the combined methodology remains scarce, particularly in the context of Vietnamese language processing. This paper presents a novel Vietnamese MDS framework leveraging a two-component pipeline architecture that integrates extractive and abstractive techniques. The first component employs an extractive approach to identify key sentences within each document. This is achieved by a modification of the pre-trained BERT network, which derives semantically meaningful phrase embeddings using siamese and triplet network structures. The second component utilizes the VBD-LLaMA2-7B-50b model for abstractive summarization, ultimately generating the final summary document. Our proposed framework demonstrates a positive performance, attaining ROUGE-2 scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art baselines.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "596ba994e9995eba",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multilingual",
                    "#transfer_learning",
                    "#architecture",
                    "#low_resource"
                ],
                "emoji": "ğŸ‡»ğŸ‡³",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ BERT Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ VBD-LLaMA2-7B-50b Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„ĞµÑ€Ğ°Ñ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ROUGE-2 Ğ² 39.6% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ VN-MDS."
                },
                "en": {
                    "title": "Fusing Extractive and Abstractive Techniques for Enhanced Vietnamese Summarization",
                    "desc": "This paper addresses the challenge of Multi-Document Summarization (MDS) by proposing a new framework that combines both extractive and abstractive summarization techniques. The first part of the framework uses a modified BERT model to extract key sentences from documents, leveraging siamese and triplet networks for better semantic understanding. The second part employs the VBD-LLaMA2-7B-50b model to generate a coherent summary from the extracted information. The results show that this integrated approach achieves a ROUGE-2 score of 39.6% on the VN-MDS dataset, surpassing existing methods in the field."
                },
                "zh": {
                    "title": "èåˆæå–ä¸ç”Ÿæˆï¼Œæå‡å¤šæ–‡æ¡£æ‘˜è¦æ•ˆæœ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤šæ–‡æ¡£æ‘˜è¦ï¼ˆMDSï¼‰çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæå–å¼å’Œç”Ÿæˆå¼æ‘˜è¦çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç»„ä»¶ç®¡é“æ¶æ„ï¼Œé¦–å…ˆé€šè¿‡ä¿®æ”¹é¢„è®­ç»ƒçš„BERTç½‘ç»œæå–æ¯ä¸ªæ–‡æ¡£ä¸­çš„å…³é”®å¥å­ã€‚æ¥ç€ï¼Œä½¿ç”¨VBD-LLaMA2-7B-50bæ¨¡å‹è¿›è¡Œç”Ÿæˆå¼æ‘˜è¦ï¼Œæœ€ç»ˆç”Ÿæˆæ‘˜è¦æ–‡æ¡£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨VN-MDSæ•°æ®é›†ä¸Šå–å¾—äº†39.6%çš„ROUGE-2åˆ†æ•°ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12106",
            "title": "Measuring Human and AI Values based on Generative Psychometrics with Large Language Models",
            "url": "https://huggingface.co/papers/2409.12106",
            "abstract": "Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. We begin by fine-tuning an LLM for accurate perception-level value measurement and verifying the capability of LLMs to parse texts into perceptions, forming the core of the GPV pipeline. Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for value-aligned AI.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "f795400e3345e3d7",
            "data": {
                "categories": [
                    "#multilingual",
                    "#training",
                    "#ethics",
                    "#data",
                    "#interpretability",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Generative Psychometrics for Values (GPV) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). GPV Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ fine-tuned LLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ±Ğ»Ğ¾Ğ³Ğ°Ğ¼, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğ¼ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ GPV Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ°Ğ¼Ğ¸Ñ… LLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¸ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Harnessing AI to Measure Human Values with Precision",
                    "desc": "This paper presents Generative Psychometrics for Values (GPV), a new method for measuring human values using large language models (LLMs). The authors fine-tune an LLM to accurately assess perceptions of values from text, establishing a robust pipeline for value measurement. They demonstrate that GPV outperforms traditional psychological tools when applied to human-written blogs, showing its reliability and validity. Additionally, the paper explores how LLM values can be measured in a context-sensitive manner and discusses the implications of these values for LLM safety."
                },
                "zh": {
                    "title": "åˆ©ç”¨AIæ¨åŠ¨ä»·å€¼æµ‹é‡çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»·å€¼æµ‹é‡æ–°æ–¹æ³•ï¼Œç§°ä¸ºç”Ÿæˆå¿ƒç†æµ‹é‡å­¦ï¼ˆGPVï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ–‡æœ¬çš„é€‰æ‹©æ€§æ„ŸçŸ¥è¿›è¡Œç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨å‡†ç¡®æµ‹é‡äººç±»çš„ä»·å€¼è§‚ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒLLMï¼ŒéªŒè¯å…¶è§£ææ–‡æœ¬ä¸ºæ„ŸçŸ¥çš„èƒ½åŠ›ï¼Œå¹¶å°†å…¶åº”ç”¨äºäººç±»æ’°å†™çš„åšå®¢ä¸­ï¼Œå±•ç¤ºäº†å…¶ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰©å±•äº†GPVä»¥æµ‹é‡LLMçš„ä»·å€¼ï¼Œæ­ç¤ºäº†ä¸åŒä»·å€¼ä½“ç³»å¯¹LLMå®‰å…¨æ€§çš„å½±å“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-18.html",
    "link_next": "2024-09-20.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "18.09",
        "en": "09/18",
        "zh": "9æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.09",
        "en": "09/20",
        "zh": "9æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 3,
        "#video": 1,
        "#multimodal": 4,
        "#math": 3,
        "#multilingual": 4,
        "#architecture": 12,
        "#healthcare": 2,
        "#training": 9,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 3,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 3,
        "#science": 2,
        "#low_resource": 2,
        "#personalization": 0
    }
}