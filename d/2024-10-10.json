{
    "date": {
        "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 10",
        "zh": "10æœˆ10æ—¥"
    },
    "time_utc": "2024-10-11 20:13",
    "issue_id": 59,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08196",
            "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
            "url": "https://huggingface.co/papers/2410.08196",
            "abstract": "Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. The code is released at https://github.com/mathllm/MathCoder2 .",
            "score": 27,
            "issue_id": 52,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MathCode-Pile Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 19.2 Ğ¼Ğ»Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº, ÑƒÑ‡ĞµĞ±Ğ½Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ LaTeX, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ğ¸Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "\"Code Your Way to Better Math Reasoning!\"",
                    "desc": "This paper presents a new method for improving the mathematical reasoning skills of large language models by generating mathematical code with reasoning steps. The authors create a high-quality dataset called MathCode-Pile, which includes math-related web data, code, textbooks, and synthetic data. They extract LaTeX expressions and conditions from this dataset to generate code that accurately reflects mathematical reasoning. Training models with this dataset significantly enhances their mathematical abilities, resulting in the MathCoder2 family of models."
                },
                "zh": {
                    "title": "ä»£ç ä¸æ¨ç†ï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ•°å­¦ä»£ç å’Œç›¸åº”çš„æ¨ç†æ­¥éª¤æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°å­¦é¢„è®­ç»ƒæ•°æ®é›†ï¼Œç»“åˆäº†æ•°å­¦ç›¸å…³çš„ç½‘ç»œæ•°æ®ã€ä½¿ç”¨æ•°å­¦åŒ…çš„ä»£ç ã€æ•°å­¦æ•™ç§‘ä¹¦å’Œåˆæˆæ•°æ®ã€‚é€šè¿‡æå–LaTeXè¡¨è¾¾å¼ã€æ¡ä»¶å’Œç»“æœï¼Œç”Ÿæˆç›¸åº”çš„ä»£ç æ¥å‡†ç¡®æ•æ‰æ•°å­¦æ¨ç†è¿‡ç¨‹ã€‚æœ€ç»ˆï¼Œä½¿ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—æé«˜äº†æ•°å­¦èƒ½åŠ›ï¼Œå½¢æˆäº†MathCoder2æ¨¡å‹å®¶æ—ã€‚"
                }
            },
            "hash": "0ef94d1fd5147144",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05265",
            "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
            "url": "https://huggingface.co/papers/2410.05265",
            "abstract": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at https://github.com/ChenMnZ/PrefixQuant.",
            "score": 25,
            "issue_id": 51,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "PrefixQuant: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "PrefixQuant - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑÑ‹ Ğ² KV-ĞºÑÑˆĞµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ². PrefixQuant Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "PrefixQuant: Revolutionizing LLM Quantization with Token-Wise Precision",
                    "desc": "The paper introduces PrefixQuant, a new method for quantizing large language models that focuses on token-wise outliers, which are often overlooked by existing methods. PrefixQuant works by identifying and managing high-frequency outlier tokens offline, thus avoiding the need for expensive per-token dynamic quantization during inference. This approach allows for efficient per-tensor static quantization, which not only improves memory efficiency and inference speed but also enhances model performance. The results show that PrefixQuant significantly outperforms previous methods in terms of perplexity and accuracy while being faster than traditional FP16 models."
                },
                "zh": {
                    "title": "PrefixQuantï¼šé«˜æ•ˆé™æ€é‡åŒ–çš„æ–°çªç ´",
                    "desc": "é‡åŒ–å¯¹äºéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥æé«˜å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚ç°æœ‰çš„æ¿€æ´»é‡åŒ–æ–¹æ³•ä¸»è¦è§£å†³é€šé“çº§å¼‚å¸¸å€¼çš„é—®é¢˜ï¼Œå¾€å¾€å¿½ç•¥äº†ä»¤ç‰Œçº§å¼‚å¸¸å€¼ï¼Œä»è€Œä¾èµ–äºæ˜‚è´µçš„æ¯ä»¤ç‰ŒåŠ¨æ€é‡åŒ–ã€‚PrefixQuantæ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ç¦»çº¿éš”ç¦»å¼‚å¸¸ä»¤ç‰Œã€‚å®ƒé€šè¿‡åœ¨KVç¼“å­˜ä¸­å‰ç¼€é«˜é¢‘å¼‚å¸¸ä»¤ç‰Œï¼Œé˜²æ­¢æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå¼‚å¸¸ä»¤ç‰Œï¼Œä»è€Œç®€åŒ–é‡åŒ–è¿‡ç¨‹ã€‚"
                }
            },
            "hash": "a083aa646dcae575",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03450",
            "title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
            "url": "https://huggingface.co/papers/2410.03450",
            "abstract": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.",
            "score": 23,
            "issue_id": 51,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#rag"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MART: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MART Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). MART Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ MLLM-Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ¿Ğ¾ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Task Success: MART's Smart Trajectory Retrieval",
                    "desc": "This paper introduces a new method called MLLM as ReTriever (MART) to improve the performance of embodied agents in complex tasks. MART fine-tunes a machine learning language model (MLLM) to retrieve task-relevant trajectory data by focusing on the effectiveness of the trajectories rather than just surface-level similarities. The method also uses Trajectory Abstraction to summarize trajectories with fewer tokens, helping agents understand key milestones. Experimental results show that MART significantly enhances task success rates in new environments compared to existing methods."
                },
                "zh": {
                    "title": "é€šè¿‡MARTæ–¹æ³•æå‡å…·èº«æ™ºèƒ½ä½“çš„ä»»åŠ¡æˆåŠŸç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºMLLMä½œä¸ºæ£€ç´¢å™¨ï¼ˆMARTï¼‰ï¼Œé€šè¿‡åˆ©ç”¨äº¤äº’æ•°æ®æ¥å¾®è°ƒMLLMæ£€ç´¢å™¨ï¼Œä»¥æé«˜å…·èº«æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨è½¨è¿¹çš„è¡¨é¢ç›¸ä¼¼æ€§ï¼Œè€Œå¿½ç•¥äº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚MARTæ–¹æ³•é€šè¿‡åå¥½å­¦ä¹ æ¥ä¼˜åŒ–æ£€ç´¢å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿä¼˜å…ˆè€ƒè™‘æœªè§ä»»åŠ¡ä¸­è½¨è¿¹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€‚"
                }
            },
            "hash": "db11faae830e2807",
            "pub_date_card": {
                "ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 4",
                "zh": "10æœˆ4æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08207",
            "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
            "url": "https://huggingface.co/papers/2410.08207",
            "abstract": "Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.",
            "score": 15,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#editing",
                    "#multimodal"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "DICE: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DICE (Discrete Inversion for Controllable Editing) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞºĞ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. DICE Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ VQ-Diffusion, Paella Ğ¸ RoBERTa. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DICE: Precision Editing in Discrete Diffusion Models",
                    "desc": "The paper introduces DICE, a novel method for precise content editing in discrete diffusion models, which are used in tasks like image generation and language modeling. DICE records noise sequences and masking patterns during the reverse diffusion process, allowing for accurate data reconstruction and flexible editing without predefined masks. This approach is tested on models like VQ-Diffusion and RoBERTa, showing that it maintains high data fidelity while improving editing capabilities. DICE opens up new possibilities for detailed content manipulation in discrete data spaces."
                },
                "zh": {
                    "title": "DICEï¼šç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç²¾ç¡®å¯æ§ç¼–è¾‘",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDICEçš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯æ§ç¼–è¾‘ã€‚DICEé€šè¿‡è®°å½•å™ªå£°åºåˆ—å’Œæ©ç æ¨¡å¼ï¼Œå®ç°äº†å¯¹ç¦»æ•£æ•°æ®çš„ç²¾ç¡®é‡å»ºå’Œçµæ´»ç¼–è¾‘ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDICEä¸éœ€è¦é¢„å®šä¹‰çš„æ©ç æˆ–æ³¨æ„åŠ›æ“ä½œã€‚å®éªŒè¡¨æ˜ï¼ŒDICEåœ¨å›¾åƒå’Œæ–‡æœ¬é¢†åŸŸéƒ½èƒ½ä¿æŒé«˜æ•°æ®ä¿çœŸåº¦ï¼ŒåŒæ—¶å¢å¼ºç¼–è¾‘èƒ½åŠ›ã€‚"
                }
            },
            "hash": "49494862b22f09dc",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07869",
            "title": "Benchmarking Agentic Workflow Generation",
            "url": "https://huggingface.co/papers/2410.07869",
            "abstract": "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset will be available at https://github.com/zjunlp/WorFBench.",
            "score": 13,
            "issue_id": 54,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WorFBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¯Ğœ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WorFBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ WorFEval - Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Bridging the Workflow Gap in Large Language Models",
                    "desc": "The paper introduces WorFBench, a benchmark designed to evaluate the workflow generation capabilities of Large Language Models (LLMs) across diverse scenarios and complex graph structures. It also presents WorFEval, an evaluation protocol that uses subsequence and subgraph matching to assess the accuracy of these workflows. The study reveals a significant gap in the sequence and graph planning abilities of LLMs, including a 15% gap in GPT-4's performance. Additionally, the research shows that the generated workflows can improve the efficiency and effectiveness of downstream tasks."
                },
                "zh": {
                    "title": "WorFBenchï¼šæå‡LLMå·¥ä½œæµç¨‹ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ¨ç†å’Œè§„åˆ’ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å·¥ä½œæµç¨‹ã€‚ç°æœ‰çš„å·¥ä½œæµç¨‹è¯„ä¼°æ¡†æ¶å­˜åœ¨åœºæ™¯è¦†ç›–æœ‰é™ã€ç»“æ„ç®€å•å’Œè¯„ä¼°æ ‡å‡†æ¾æ•£ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†WorFBenchï¼Œä¸€ä¸ªç»Ÿä¸€çš„å·¥ä½œæµç¨‹ç”ŸæˆåŸºå‡†ï¼Œå…·æœ‰å¤šæ–¹é¢çš„åœºæ™¯å’Œå¤æ‚çš„å›¾å½¢å·¥ä½œæµç¨‹ç»“æ„ã€‚é€šè¿‡å¯¹ä¸åŒç±»å‹çš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°åºåˆ—è§„åˆ’èƒ½åŠ›å’Œå›¾å½¢è§„åˆ’èƒ½åŠ›ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç”šè‡³GPT-4ä¹Ÿå­˜åœ¨çº¦15%çš„å·®è·ã€‚"
                }
            },
            "hash": "ce1da2a3c48bcb17",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08164",
            "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
            "url": "https://huggingface.co/papers/2410.08164",
            "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.",
            "score": 9,
            "issue_id": 52,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#human_computer_interaction",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Agent S: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼",
                    "desc": "Agent S - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Agent S Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Agent-Computer Interface Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ GUI Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Agent S: Revolutionizing Task Automation with Intelligent GUI Interaction",
                    "desc": "Agent S is a framework designed to automate complex tasks on computers by interacting with their graphical interfaces. It tackles challenges like acquiring specific knowledge, planning long tasks, and managing changing interfaces using experience-augmented hierarchical planning. This approach combines learning from external searches and internal experiences to improve task execution. The framework shows significant improvements in task success rates and generalizes well across different operating systems."
                },
                "zh": {
                    "title": "Agent Sï¼šè‡ªåŠ¨åŒ–äººæœºäº¤äº’çš„æœªæ¥",
                    "desc": "Agent S æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢å®ç°è®¡ç®—æœºçš„è‡ªä¸»äº¤äº’ï¼Œè‡ªåŠ¨åŒ–å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚å®ƒé€šè¿‡å¼•å…¥ç»éªŒå¢å¼ºçš„å±‚æ¬¡è§„åˆ’ï¼Œè§£å†³äº†è·å–é¢†åŸŸçŸ¥è¯†ã€é•¿ä»»åŠ¡è§„åˆ’å’Œå¤„ç†åŠ¨æ€ç•Œé¢ç­‰æŒ‘æˆ˜ã€‚Agent S ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥æå‡ GUI ä»£ç†çš„æ¨ç†å’Œæ§åˆ¶èƒ½åŠ›ã€‚åœ¨ OSWorld åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent S çš„æˆåŠŸç‡æ¯”åŸºçº¿é«˜å‡º 9.37%ï¼Œå¹¶åœ¨ WindowsAgentArena åŸºå‡†ä¸­å±•ç¤ºäº†å¹¿æ³›çš„é€šç”¨æ€§ã€‚"
                }
            },
            "hash": "ccbad559d2898387",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07303",
            "title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
            "url": "https://huggingface.co/papers/2410.07303",
            "abstract": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing boldsymbol v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.",
            "score": 9,
            "issue_id": 51,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Rectified Diffusion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºÑƒÑ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Rectified Diffusion ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Speed Up and Simplify: Revolutionizing Image Generation with Rectified Diffusion",
                    "desc": "The paper discusses how diffusion models, which are used for generating images, can be slow because they require solving complex equations. The authors propose a new method called Rectified Diffusion, which simplifies the process by using a pretrained model to match noise with samples, making the training faster and more efficient. They argue that previous methods focused too much on making the path straight, but the real goal should be to create a path that naturally fits the model's needs. Their approach not only speeds up the process but also improves the quality of the generated images while reducing training costs."
                },
                "zh": {
                    "title": "æ ¡æ­£æ‰©æ•£ï¼šç®€åŒ–è®­ç»ƒï¼Œæå‡æ€§èƒ½",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢æœ‰å¾ˆå¤§æå‡ï¼Œä½†ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºâ€œæ ¡æ­£æ‰©æ•£â€çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥åŒ¹é…å™ªå£°å’Œæ ·æœ¬å¯¹ï¼Œä»è€Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œç›´çº¿åŒ–å¹¶ä¸æ˜¯æ ¡æ­£çš„å¿…è¦ç›®æ ‡ï¼Œå…³é”®åœ¨äºå®ç°ä¸€é˜¶è¿‘ä¼¼çš„ODEè·¯å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸ŠéªŒè¯ï¼Œè¡¨ç°ä¼˜å¼‚ä¸”è®­ç»ƒæˆæœ¬æ›´ä½ã€‚"
                }
            },
            "hash": "08c053399c56f831",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04751",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "url": "https://huggingface.co/papers/2410.04751",
            "abstract": "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.",
            "score": 7,
            "issue_id": 52,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#cv",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLVM) Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLVM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°Ğ¼, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLVM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLVM Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Perception in Large Language and Vision Models",
                    "desc": "The paper explores the performance of large language and vision models (LLVMs) in perception-related tasks, revealing a gap between their advanced reasoning abilities and basic perceptual skills. It highlights that LLVMs process images globally, sometimes solving math problems without detailed perception, and that their cross-modal alignment may compromise original perceptual capabilities. The study finds that the lower layers of these models are crucial for visual understanding, suggesting that improvements in these areas could enhance performance. The authors propose future research directions to develop more effective LLVMs and create more challenging evaluation benchmarks."
                },
                "zh": {
                    "title": "æ¢ç´¢å¤§å‹è¯­è¨€ä¸è§†è§‰æ¨¡å‹çš„æ„ŸçŸ¥å¥¥ç§˜",
                    "desc": "å¤§å‹è¯­è¨€ä¸è§†è§‰æ¨¡å‹ï¼ˆLLVMsï¼‰åœ¨è®¸å¤šéœ€è¦æ„ŸçŸ¥å’Œè®¤çŸ¥èƒ½åŠ›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŸºæœ¬æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å›¾åƒæ—¶å…·æœ‰å…¨å±€å¤„ç†çš„ç‰¹æ€§ï¼Œå³ä½¿è§†è§‰ç‰‡æ®µé¡ºåºè¢«æ‰“ä¹±ï¼Œå®ƒä»¬ä»èƒ½ä¿æŒä¸€å®šçš„ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶ï¼Œæœ‰æ—¶ä¸éœ€è¦å®Œå…¨æ„ŸçŸ¥è¯¦ç»†çš„æ•°æ®ä¿¡æ¯ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºï¼Œæ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½è¿‡åº¦é€‚åº”å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œå¯¼è‡´å…¶è§†è§‰ç¼–ç å™¨çš„åŸå§‹æ„ŸçŸ¥èƒ½åŠ›æœ‰æ‰€ä¸‹é™ã€‚"
                }
            },
            "hash": "4712ab5ada7bb4c9",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08151",
            "title": "Progressive Autoregressive Video Diffusion Models",
            "url": "https://huggingface.co/papers/2410.08151",
            "abstract": "Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/.",
            "score": 7,
            "issue_id": 51,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ ÑˆÑƒĞ¼Ğ° Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ (1440 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ 24 FPS) Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ·ĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Breaking Barriers: Extending Video Diffusion to One Minute",
                    "desc": "The paper introduces a method to extend current video diffusion models to generate longer videos by using an autoregressive approach. By assigning progressively increasing noise levels to latent frames, the model can maintain high-quality video generation without abrupt scene changes. This technique allows for fine-grained conditioning among latent frames and large overlaps in attention windows, enabling the generation of videos up to one minute long. The results demonstrate state-of-the-art performance in long video generation, overcoming previous limitations of short clip production."
                },
                "zh": {
                    "title": "é€æ­¥å»å™ªï¼šé•¿è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€æ”¹å˜å…¶æ¶æ„ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å…³é”®æ–¹æ³•ï¼Œå³ä¸ºæ½œåœ¨å¸§åˆ†é…é€æ­¥å¢åŠ çš„å™ªå£°æ°´å¹³ï¼Œè€Œä¸æ˜¯å•ä¸€çš„å™ªå£°æ°´å¹³ã€‚è¿™ç§é€æ­¥çš„è§†é¢‘å»å™ªæ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆè§†é¢‘å¸§ï¼Œè€Œä¸ä¼šå‡ºç°è´¨é‡ä¸‹é™æˆ–åœºæ™¯çªå˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œèƒ½å¤Ÿç”Ÿæˆé•¿è¾¾ä¸€åˆ†é’Ÿçš„è§†é¢‘ã€‚"
                }
            },
            "hash": "599024cc2a308da1",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05248",
            "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
            "url": "https://huggingface.co/papers/2410.05248",
            "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.",
            "score": 6,
            "issue_id": 52,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "SFTMix: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "SFTMix - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Mixup Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼ĞµĞ½ĞµĞµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ…. SFTMix Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "SFTMix: Elevating Instruction-Tuning Without Expensive Datasets",
                    "desc": "The paper introduces SFTMix, a new method to improve instruction-tuning in large language models without relying on expensive, high-quality datasets. SFTMix uses training dynamics to identify examples with varying confidence levels and applies a Mixup-based regularization to balance learning. This approach helps prevent overfitting on confident examples and enhances learning on less confident ones, leading to better performance across various tasks. The method is shown to be adaptable to different models and scalable to any dataset size, making it a versatile tool in natural language processing."
                },
                "zh": {
                    "title": "SFTMixï¼šæ— éœ€ç²¾å¿ƒç­–åˆ’æ•°æ®é›†çš„æŒ‡ä»¤å¾®è°ƒæ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSFTMixçš„æ–°æ–¹æ³•ï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè€ŒSFTMixé€šè¿‡åˆ©ç”¨æ•°æ®é›†çš„å†…åœ¨ç‰¹æ€§ï¼Œå‡å°‘äº†å¯¹ç²¾å¿ƒç­–åˆ’æ•°æ®é›†çš„ä¾èµ–ã€‚SFTMixé€šè¿‡è¯†åˆ«ä¸åŒä¿¡å¿ƒæ°´å¹³çš„ä¾‹å­ï¼Œå¹¶åº”ç”¨ä¸€ç§åŸºäºMixupçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ¥æ”¹å–„æ¨¡å‹åœ¨ä¸ç¡®å®šä¾‹å­ä¸Šçš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒSFTMixåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            },
            "hash": "960aef6ca0d9e59b",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06508",
            "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning",
            "url": "https://huggingface.co/papers/2410.06508",
            "abstract": "Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.",
            "score": 6,
            "issue_id": 51,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AlphaLLM-CPL: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ MCTS",
                    "desc": "AlphaLLM-CPL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS). ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ MCTS. AlphaLLM-CPL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºÑƒÑ€Ñ€Ğ¸ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Boosting LLM Reasoning with Smart MCTS Learning",
                    "desc": "This paper introduces AlphaLLM-CPL, a new training framework that helps large language models (LLMs) improve their reasoning skills using Monte Carlo Tree Search (MCTS). Unlike previous methods, AlphaLLM-CPL makes better use of the detailed path information from MCTS by creating stepwise trajectory pairs, which helps in more effective learning. It also uses a technique called curriculum preference learning to adjust the order of learning steps, focusing on the most important ones and avoiding overfitting. Tests on math problems show that AlphaLLM-CPL greatly enhances the reasoning abilities of LLMs compared to older methods."
                },
                "zh": {
                    "title": "AlphaLLM-CPLï¼šé€šè¿‡MCTSè¡Œä¸ºè’¸é¦è‡ªæˆ‘æå‡LLMæ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶AlphaLLM-CPLï¼Œç”¨äºé€šè¿‡è’™ç‰¹å¡ç½—æ ‘æœç´¢ï¼ˆMCTSï¼‰è¡Œä¸ºè’¸é¦æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚AlphaLLM-CPLé€šè¿‡æ„å»ºæ¥è‡ªåŒä¸€çˆ¶èŠ‚ç‚¹çš„å­èŠ‚ç‚¹çš„é€æ­¥è½¨è¿¹å¯¹ï¼Œæä¾›äº†æ›´æœ‰æ•ˆçš„MCTSè¡Œä¸ºè’¸é¦ã€‚å®ƒè¿˜å¼•å…¥äº†è¯¾ç¨‹åå¥½å­¦ä¹ ï¼ŒåŠ¨æ€è°ƒæ•´è½¨è¿¹å¯¹çš„è®­ç»ƒé¡ºåºï¼Œä»¥ä¼˜å…ˆè€ƒè™‘å…³é”®å­¦ä¹ æ­¥éª¤å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaLLM-CPLåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„MCTSè¡Œä¸ºè’¸é¦æ–¹æ³•ã€‚"
                }
            },
            "hash": "e3c203504e6848e3",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05210",
            "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
            "url": "https://huggingface.co/papers/2410.05210",
            "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.",
            "score": 6,
            "issue_id": 50,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Fine-grained Selective Calibrated CLIP (FSC-CLIP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. FSC-CLIP Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FSC-CLIP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Balancing Act: Enhancing Compositional Understanding Without Compromise",
                    "desc": "The paper introduces FSC-CLIP, a method to improve how vision and language models understand compositions without losing their ability to handle multiple types of data at once. Traditional methods often harm the model's ability to work with both images and text by using a global hard negative loss, which can confuse the model. FSC-CLIP uses a local hard negative loss and selective calibrated regularization to provide more precise guidance, helping the model learn better without losing its multi-modal skills. Tests show that FSC-CLIP matches top models in understanding compositions while keeping its multi-modal strengths intact."
                },
                "zh": {
                    "title": "FSC-CLIPï¼šæå‡ç»„åˆç†è§£ï¼Œä¸æŸå¤šæ¨¡æ€è¡¨ç°",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¢å¼ºé¢„è®­ç»ƒè§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„ç»„åˆç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¸å½±å“é›¶æ ·æœ¬å¤šæ¨¡æ€ä»»åŠ¡çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•é€šå¸¸é€šè¿‡ä½¿ç”¨å…¨å±€ç¡¬è´Ÿæ ·æœ¬æŸå¤±æ¥æé«˜ç»„åˆæ¨ç†ï¼Œä½†è¿™ä¼šæŸå®³å¤šæ¨¡æ€èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»†ç²’åº¦é€‰æ‹©æ€§æ ¡å‡†CLIPï¼ˆFSC-CLIPï¼‰ï¼Œç»“åˆäº†å±€éƒ¨ç¡¬è´Ÿæ ·æœ¬æŸå¤±å’Œé€‰æ‹©æ€§æ ¡å‡†æ­£åˆ™åŒ–ã€‚è¿™äº›åˆ›æ–°æä¾›äº†ç»†ç²’åº¦çš„è´Ÿç›‘ç£ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„è¡¨ç¤ºå®Œæ•´æ€§ã€‚"
                }
            },
            "hash": "aec4ac6f174a5a13",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06154",
            "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
            "url": "https://huggingface.co/papers/2410.06154",
            "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
            "score": 5,
            "issue_id": 54,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "GLOV: LLM ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GLOV, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ñ Ñ†ĞµĞ»ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. GLOV Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ LLM Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ VLM. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ñ‹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "GLOV: Boosting Vision-Language Models with Smart Language Prompts",
                    "desc": "The paper introduces GLOV, a method that uses Large Language Models (LLMs) as implicit optimizers to improve Vision-Language Models (VLMs) for vision tasks. GLOV works by generating and ranking prompts for VLMs, using a fitness function to measure their effectiveness. The method also adjusts the LLM's output by incorporating an offset vector, which guides the language generation towards what the VLM prefers. This approach significantly boosts the performance of VLMs on various datasets, achieving up to 57.5% improvement in recognition tasks."
                },
                "zh": {
                    "title": "GLOVï¼šæå‡è§†è§‰ä»»åŠ¡æ€§èƒ½çš„éšå¼ä¼˜åŒ–å™¨",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•GLOVï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ä½œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„éšå¼ä¼˜åŒ–å™¨ï¼Œä»¥å¢å¼ºè§†è§‰ä»»åŠ¡ã€‚GLOVé€šè¿‡å…ƒæç¤ºå‘LLMæä¾›ä»»åŠ¡æè¿°ï¼Œå¹¶æŸ¥è¯¢é€‚åˆçš„VLMæç¤ºï¼Œè¿™äº›æç¤ºæ ¹æ®çº¯åº¦æµ‹é‡è¿›è¡Œæ’åã€‚åœ¨æ¯ä¸ªä¼˜åŒ–æ­¥éª¤ä¸­ï¼Œæ’åçš„æç¤ºä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹è¾“å…¥LLMï¼Œä»¥å¸®åŠ©å…¶äº†è§£VLMåå¥½çš„æ–‡æœ¬æç¤ºç±»å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨æ¯ä¸ªä¼˜åŒ–æ­¥éª¤ä¸­æ·»åŠ åç§»å‘é‡æ¥å¼•å¯¼LLMç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚"
                }
            },
            "hash": "1b375d9257a67241",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07041",
            "title": "Emergent properties with repeated examples",
            "url": "https://huggingface.co/papers/2410.07041",
            "abstract": "We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.",
            "score": 5,
            "issue_id": 50,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#math"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ - Ğ¼Ğ°Ñ‚ÑŒ ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ñ‡ĞµĞ¼ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ¸Ğ· Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Repetition Over Diversity: A New Path to Better Learning",
                    "desc": "This paper explores how transformers perform when trained with repeated examples versus a larger variety of single-use examples. It finds that using a smaller set of repeated examples can lead to better performance than using a larger, more diverse dataset. The study introduces a two-set training method, which combines repeated examples with normal sampling, resulting in faster learning and improved outcomes. This research provides insights into the balance between memorization and generalization in deep learning."
                },
                "zh": {
                    "title": "é‡å¤è®­ç»ƒï¼šè¶…è¶Šæ•°æ®å¤šæ ·æ€§çš„åŠ›é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨ç®—æ³•ç”Ÿæˆçš„æ•°æ®é›†ä¸Šï¼ŒTransformeræ¨¡å‹çš„æ€§èƒ½ä¸è®­ç»ƒæ ·æœ¬é‡å¤æ¬¡æ•°ä¹‹é—´çš„å…³ç³»ã€‚åœ¨æœ€å¤§å…¬çº¦æ•°ã€æ¨¡ä¹˜æ³•å’ŒçŸ©é˜µç‰¹å¾å€¼ä¸‰ä¸ªæ•°å­¦é—®é¢˜ä¸Šï¼Œç ”ç©¶è¡¨æ˜åœ¨å›ºå®šè®­ç»ƒæ­¥æ•°ä¸‹ï¼Œä½¿ç”¨è¾ƒå°é‡å¤æ ·æœ¬é›†è®­ç»ƒçš„æ¨¡å‹ä¼˜äºä½¿ç”¨è¾ƒå¤§å•æ¬¡æ ·æœ¬é›†çš„æ¨¡å‹ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†åŒé›†è®­ç»ƒæ³•ï¼Œå³åœ¨æ­£å¸¸é‡‡æ ·çš„åŸºç¡€ä¸Šé‡å¤ä½¿ç”¨ä¸€å°éƒ¨åˆ†éšæœºæ ·æœ¬ï¼Œå¯ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦å¹¶æé«˜æ€§èƒ½ã€‚è¿™è¡¨æ˜é‡å¤çš„å¥½å¤„å¯èƒ½è¶…è¿‡æ•°æ®å¤šæ ·æ€§çš„å¥½å¤„ã€‚"
                }
            },
            "hash": "eac06638db21722d",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07137",
            "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
            "url": "https://huggingface.co/papers/2410.07137",
            "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a \"null model\" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",
            "score": 4,
            "issue_id": 56,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾: Ğ´Ğ°Ğ¶Ğµ 'Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ' Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¯Ğœ!",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ´Ğ°Ğ¶Ğµ 'Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ', Ğ²ÑĞµĞ³Ğ´Ğ° Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚Ñ€ÑĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Unmasking the Illusion: Ensuring Fair Play in LLM Benchmarks",
                    "desc": "The paper discusses how automatic benchmarks for evaluating language models, like AlpacaEval 2.0, can be manipulated to falsely boost a model's performance. It reveals that even a simple model that outputs the same response regardless of input can achieve high scores on these benchmarks. This highlights the vulnerability of current evaluation systems to gaming tactics, which can lead to misleading promotional claims about a model's capabilities. The authors emphasize the need for developing robust anti-cheating mechanisms to ensure the reliability of automatic benchmarks."
                },
                "zh": {
                    "title": "æ­ç¤ºè‡ªåŠ¨åŒ–è¯„ä¼°åŸºå‡†çš„æ¼æ´ï¼šå‘¼å”¤åä½œå¼Šæœºåˆ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†è‡ªåŠ¨åŒ–è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†çš„æ¼æ´ï¼Œè¿™äº›åŸºå‡†å¦‚AlpacaEval 2.0ç­‰ï¼Œè™½ç„¶æˆæœ¬ä½ä¸”å¯æ‰©å±•ï¼Œä½†å®¹æ˜“è¢«æ“æ§ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯ç®€å•çš„â€œç©ºæ¨¡å‹â€ä¹Ÿèƒ½é€šè¿‡å›ºå®šè¾“å‡ºæ¥æ¬ºéª—è¿™äº›åŸºå‡†ï¼Œè·å¾—é«˜èƒœç‡ã€‚è®ºæ–‡å¼ºè°ƒäº†éœ€è¦å¼€å‘åä½œå¼Šæœºåˆ¶ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®æ€§èƒ½ã€‚"
                }
            },
            "hash": "fdf529020c19324c",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08049",
            "title": "Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations",
            "url": "https://huggingface.co/papers/2410.08049",
            "abstract": "This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet promoting further research and development in the community.",
            "score": 4,
            "issue_id": 51,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ConvNets",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ´ĞµÑ€ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… (ConvNets). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ´ĞµÑ€ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚ĞµĞºĞ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ UniRepLKNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ConvNets Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ¸Ñ… ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°ÑĞ»Ğ¾ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ConvNets Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ vision transformers."
                },
                "en": {
                    "title": "\"Think Big: Revolutionizing ConvNets with Large Kernels\"",
                    "desc": "This paper introduces a new approach to designing Convolutional Neural Networks (ConvNets) by using large convolutional kernels instead of many smaller ones. The authors present the UniRepLKNet architecture, which is optimized for capturing extensive spatial information efficiently. Their model achieves high accuracy and performance across various tasks, outperforming traditional models and vision transformers. The research highlights the advantages of large-kernel ConvNets, such as larger receptive fields and a shift from texture to shape bias, offering faster inference speeds."
                },
                "zh": {
                    "title": "å¤§å·ç§¯æ ¸ï¼šç°ä»£å·ç§¯ç¥ç»ç½‘ç»œçš„æ–°æ–¹å‘",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†åœ¨è®¾è®¡ç°ä»£å·ç§¯ç¥ç»ç½‘ç»œæ—¶ä½¿ç”¨å¤§å·ç§¯æ ¸çš„èŒƒå¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å°‘é‡å¤§å·ç§¯æ ¸æ¯”å †å å¤šä¸ªå°å·ç§¯æ ¸æ›´ä¼˜ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€å¥—é’ˆå¯¹å¤§å·ç§¯æ ¸ç½‘ç»œçš„æ¶æ„è®¾è®¡æŒ‡å—ï¼Œä¼˜åŒ–å…¶æ•ˆç‡å’Œæ€§èƒ½ã€‚UniRepLKNetæ¶æ„å±•ç¤ºäº†å¤§å·ç§¯æ ¸ç½‘ç»œåœ¨æ•æ‰å¹¿æ³›ç©ºé—´ä¿¡æ¯æ–¹é¢çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚"
                }
            },
            "hash": "8c8b1af09bf67bbe",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08115",
            "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
            "url": "https://huggingface.co/papers/2410.08115",
            "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).",
            "score": 4,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Optima: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM",
                    "desc": "Optima - Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾ĞºĞµĞ½-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Monte Carlo Tree Search, Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Optima Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama 3 8B."
                },
                "en": {
                    "title": "Optima: Revolutionizing Multi-Agent Communication and Efficiency",
                    "desc": "The paper introduces Optima, a framework designed to improve communication efficiency and task effectiveness in multi-agent systems using large language models. Optima uses a generate, rank, select, and train approach with a reward function to balance task performance and communication readability. It explores reinforcement learning techniques like Supervised Fine-Tuning and Direct Preference Optimization, integrating Monte Carlo Tree Search methods to enhance data generation. The framework demonstrates significant performance improvements in multi-agent tasks, achieving better results with fewer resources compared to traditional methods."
                },
                "zh": {
                    "title": "Optimaï¼šæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•ˆç‡çš„æ–°æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOptimaçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€šä¿¡æ•ˆç‡å’Œä»»åŠ¡æ•ˆæœã€‚Optimaé€šè¿‡ç”Ÿæˆã€æ’åºã€é€‰æ‹©å’Œè®­ç»ƒçš„è¿­ä»£è¿‡ç¨‹ï¼Œç»“åˆå¥–åŠ±å‡½æ•°æ¥å¹³è¡¡ä»»åŠ¡è¡¨ç°ã€ä»¤ç‰Œæ•ˆç‡å’Œé€šä¿¡å¯è¯»æ€§ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œå¹¶ç»“åˆè’™ç‰¹å¡ç½—æ ‘æœç´¢æŠ€æœ¯æ¥ç”Ÿæˆæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimaåœ¨ä¿¡æ¯ä¸å¯¹ç§°é—®ç­”å’Œå¤æ‚æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚"
                }
            },
            "hash": "b7cacda3f030e1bd",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05603",
            "title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition",
            "url": "https://huggingface.co/papers/2410.05603",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term \"task superposition\". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of \"LLMs as superposition of simulators\", and raise questions about the mechanisms enabling simultaneous task execution.",
            "score": 3,
            "issue_id": 57,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#inference",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸Ğ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ 'ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡'. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ñ… LLM Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº LLM Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unleashing the Power of Task Superposition in LLMs",
                    "desc": "This paper investigates a unique ability of Large Language Models (LLMs) called 'task superposition', where they can handle multiple distinct in-context learning tasks at once during a single inference. The authors provide evidence that this capability is present across different LLM families and sizes, even when models are trained to learn one task at a time. They explain that this ability is possible due to the expressive power of transformers, and they explore how LLMs internally manage task vectors during superposition. The study also finds that larger models can perform more tasks simultaneously and better adjust their output, offering new insights into the potential of LLMs."
                },
                "zh": {
                    "title": "æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡å åŠ èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡æ¨ç†ä¸­åŒæ—¶æ‰§è¡Œå¤šç§ä¸åŒçš„ä»»åŠ¡ï¼Œè¿™ç§èƒ½åŠ›è¢«ç§°ä¸ºâ€œä»»åŠ¡å åŠ â€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ¨¡å‹è¢«è®­ç»ƒä¸ºä¸€æ¬¡åªå­¦ä¹ ä¸€ä¸ªä»»åŠ¡ï¼Œè¿™ç§ç°è±¡ä»ç„¶ä¼šå‡ºç°ã€‚ç†è®ºä¸Šï¼Œè¿™ç§èƒ½åŠ›åœ¨å˜å‹å™¨çš„è¡¨è¾¾èƒ½åŠ›èŒƒå›´å†…ã€‚æ›´å¤§çš„æ¨¡å‹å¯ä»¥åŒæ—¶è§£å†³æ›´å¤šçš„ä»»åŠ¡ï¼Œå¹¶æ›´å¥½åœ°æ ¡å‡†å…¶è¾“å‡ºåˆ†å¸ƒã€‚"
                }
            },
            "hash": "a6c2dccffed8ffb4",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07484",
            "title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents",
            "url": "https://huggingface.co/papers/2410.07484",
            "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.",
            "score": 2,
            "issue_id": 59,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#planning",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ LLM Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ WALL-E Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (MPC) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ² Minecraft Ğ¸ ALFWorld WALL-E Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning LLMs with the World: A New Era of Efficient Exploration",
                    "desc": "The paper explores the potential of large language models (LLMs) to serve as effective world models for model-based agents by aligning them with specific environment dynamics through rule learning. This alignment is achieved using a neurosymbolic approach that involves inducing, updating, and pruning rules based on comparisons between agent-explored trajectories and LLM predictions. The resulting model, which combines the LLM with learned rules, enhances exploration and learning efficiency, as demonstrated by the WALL-E agent in open-world challenges like Minecraft and ALFWorld. WALL-E outperforms existing methods by achieving higher success rates with fewer resources, such as replanning time and tokens, required for reasoning."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼šæ™ºèƒ½ä½“ä¸–ç•Œæ¨¡å‹çš„æ–°å¯èƒ½",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å¯ä»¥ç›´æ¥ä½œä¸ºåŸºäºæ¨¡å‹çš„æ™ºèƒ½ä½“çš„å¼ºå¤§ä¸–ç•Œæ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è§„åˆ™å­¦ä¹ ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†LLMä¸å…¶éƒ¨ç½²ç¯å¢ƒå¯¹é½ï¼Œä»è€Œå¼¥åˆçŸ¥è¯†å·®è·ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ™ºèƒ½ä½“æ¢ç´¢è½¨è¿¹å’Œä¸–ç•Œæ¨¡å‹é¢„æµ‹æ¥å­¦ä¹ è§„åˆ™ã€‚æœ€ç»ˆçš„ä¸–ç•Œæ¨¡å‹ç”±LLMå’Œå­¦ä¹ åˆ°çš„è§„åˆ™ç»„æˆï¼Œåœ¨Minecraftå’ŒALFWorldç­‰å¼€æ”¾ä¸–ç•ŒæŒ‘æˆ˜ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            },
            "hash": "6e6aca942958f302",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04808",
            "title": "LPZero: Language Model Zero-cost Proxy Search from Zero",
            "url": "https://huggingface.co/papers/2410.04808",
            "abstract": "In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily rely on expert knowledge and incur significant trial-and-error costs. Particularly in NLP tasks, most existing ZC proxies fail to surpass the performance of the naive baseline. To address these challenges, we introduce a novel framework, LPZero, which is the first to automatically design ZC proxies for various tasks, achieving higher ranking consistency than human-designed proxies. Specifically, we model the ZC proxy as a symbolic equation and incorporate a unified proxy search space that encompasses existing ZC proxies, which are composed of a predefined set of mathematical symbols. To heuristically search for the best ZC proxy, LPZero incorporates genetic programming to find the optimal symbolic composition. We propose a Rule-based Pruning Strategy (RPS), which preemptively eliminates unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's superior ranking ability and performance on downstream tasks compared to current approaches.",
            "score": 2,
            "issue_id": 56,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "LPZero: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "LPZero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ (ZC) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (NAS). ĞĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ZC-Ğ¿Ñ€Ğ¾ĞºÑĞ¸ ĞºĞ°Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸. LPZero Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (RPS) Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±ĞµÑĞ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° FlexiBERT, GPT-2 Ğ¸ LLaMA-7B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LPZero Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "\"LPZero: Automating Efficiency in Neural Architecture Search\"",
                    "desc": "The paper introduces LPZero, a novel framework for Zero-shot Neural Architecture Search (NAS) that reduces computational demands by automatically designing Zero-cost (ZC) proxies. Unlike existing methods that rely heavily on expert knowledge, LPZero uses genetic programming to explore a unified search space of symbolic equations, improving ranking consistency across tasks. The framework includes a Rule-based Pruning Strategy (RPS) to eliminate ineffective proxies early, enhancing efficiency and performance. Experiments show that LPZero outperforms current methods in ranking and downstream tasks, particularly in natural language processing applications."
                },
                "zh": {
                    "title": "LPZeroï¼šè‡ªåŠ¨åŒ–é›¶æˆæœ¬ä»£ç†è®¾è®¡çš„çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶LPZeroï¼Œç”¨äºè‡ªåŠ¨è®¾è®¡é›¶æˆæœ¬ä»£ç†ï¼ˆZC proxiesï¼‰ï¼Œä»¥å‡å°‘ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰çš„è®¡ç®—éœ€æ±‚ã€‚LPZeroé€šè¿‡å°†ZCä»£ç†å»ºæ¨¡ä¸ºç¬¦å·æ–¹ç¨‹ï¼Œå¹¶ç»“åˆç»Ÿä¸€çš„ä»£ç†æœç´¢ç©ºé—´ï¼Œæ¥æé«˜ä»£ç†çš„æ’åä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶ä½¿ç”¨é—ä¼ ç¼–ç¨‹æ¥å¯»æ‰¾æœ€ä½³çš„ç¬¦å·ç»„åˆï¼Œå¹¶é€šè¿‡è§„åˆ™å‰ªæç­–ç•¥ï¼ˆRPSï¼‰æå‰æ·˜æ±°ä¸ç†æƒ³çš„ä»£ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒLPZeroåœ¨FlexiBERTã€GPT-2å’ŒLLaMA-7Bç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            },
            "hash": "58e4dff9a00bb4f7",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07707",
            "title": "MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2410.07707",
            "abstract": "Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page",
            "score": 2,
            "issue_id": 53,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MotionGS: Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²",
                    "desc": "MotionGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ 3D-ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ ĞµĞ³Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğº ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ· ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MotionGS Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "MotionGS: Mastering Dynamic 3D Scenes with Motion-Aware Gaussians",
                    "desc": "The paper introduces MotionGS, a new framework for reconstructing dynamic 3D scenes using Gaussian splatting. It addresses the challenge of optimizing object motion by incorporating explicit motion priors through an optical flow decoupling module. This module separates camera movement from object motion, allowing for more accurate deformation of 3D Gaussians. The framework also includes a camera pose refinement module to improve the accuracy of camera poses, resulting in superior performance compared to existing methods."
                },
                "zh": {
                    "title": "MotionGSï¼šåŠ¨æ€åœºæ™¯é‡å»ºçš„æ–°çªç ´",
                    "desc": "åŠ¨æ€åœºæ™¯é‡å»ºä¸€ç›´æ˜¯3Dè§†è§‰é¢†åŸŸçš„é•¿æœŸæŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œ3Dé«˜æ–¯ç‚¹çš„å‡ºç°ä¸ºè¿™ä¸ªé—®é¢˜æä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯å˜å½¢3Dé«˜æ–¯ç‚¹æ¡†æ¶MotionGSï¼Œé€šè¿‡å¼•å…¥å…‰æµè§£è€¦æ¨¡å—æ¥æŒ‡å¯¼3Dé«˜æ–¯çš„å˜å½¢ã€‚å®éªŒè¡¨æ˜ï¼ŒMotionGSåœ¨å•ç›®åŠ¨æ€åœºæ™¯ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            },
            "hash": "f7341783586984de",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05629",
            "title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
            "url": "https://huggingface.co/papers/2410.05629",
            "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.",
            "score": 1,
            "issue_id": 58,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Vector-ICL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Vector-ICL Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº few-shot ICL, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼."
                },
                "en": {
                    "title": "Expanding LLM Horizons: From Text to Vectors",
                    "desc": "This paper investigates the ability of large language models (LLMs) to perform in-context learning (ICL) on continuous vectors from various domains. By using lightweight projectors to align input data with the LLM's embedding space, the study introduces Vector-ICL, where LLMs can effectively learn from these projected vectors. The research shows that pretraining projectors with general language modeling objectives is crucial for enabling Vector-ICL, and task-specific finetuning further improves its performance. Experiments demonstrate that Vector-ICL often outperforms few-shot ICL and domain-specific models across multiple tasks and modalities, suggesting LLMs' potential to handle vector data beyond traditional text tokens."
                },
                "zh": {
                    "title": "å‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šè¶…è¶Šä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬æ•°æ®ä¸Šå±•ç¤ºäº†å‡ºè‰²çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶è¿™äº›èƒ½åŠ›æ˜¯å¦å¯ä»¥æ‰©å±•åˆ°æ¥è‡ªä¸åŒé¢†åŸŸçš„è¿ç»­å‘é‡ï¼Œè¿™äº›å‘é‡æ˜¯é€šè¿‡é»‘ç®±é¢„è®­ç»ƒç¼–ç å™¨è·å¾—çš„ã€‚é€šè¿‡è½»é‡çº§æŠ•å½±å™¨å°†è¾“å…¥æ•°æ®ä¸LLMçš„åµŒå…¥ç©ºé—´å¯¹é½ï¼Œæˆ‘ä»¬å‘ç°LLMå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å’Œå­¦ä¹ è¿™äº›æŠ•å½±å‘é‡ï¼Œè¿™è¢«ç§°ä¸ºå‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆVector-ICLï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæŠ•å½±å™¨ä¸ä¸€èˆ¬è¯­è¨€å»ºæ¨¡ç›®æ ‡å¯ä»¥å®ç°Vector-ICLï¼Œè€Œç‰¹å®šä»»åŠ¡çš„å¾®è°ƒè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚"
                }
            },
            "hash": "0f95824fcb35b2e3",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03437",
            "title": "Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs",
            "url": "https://huggingface.co/papers/2410.03437",
            "abstract": "Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.",
            "score": 1,
            "issue_id": 58,
            "pub_date": "2024-10-04",
            "pub_date_ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ¦“",
                "ru": {
                    "title": "Zebra: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ”Ğ£Ğ§ĞŸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "Zebra - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… (Ğ”Ğ£Ğ§ĞŸ). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Zebra Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Zebra Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ”Ğ£Ğ§ĞŸ."
                },
                "en": {
                    "title": "Zebra: Transforming PDE Solutions with In-Context Learning",
                    "desc": "The paper introduces Zebra, a new generative auto-regressive transformer model designed to solve time-dependent parametric partial differential equations (PDEs) without needing gradient adaptation during inference. Zebra leverages in-context learning, inspired by large language models, to dynamically adapt to new tasks by conditioning on input sequences that include context trajectories. This approach allows Zebra to handle varying context sizes and provides uncertainty quantification by sampling multiple solution trajectories. The model is evaluated on various challenging PDE scenarios, showing its adaptability, robustness, and superior performance compared to existing methods."
                },
                "zh": {
                    "title": "Zebraï¼šæ— éœ€æ¢¯åº¦è°ƒæ•´çš„PDEè§£å†³æ–¹æ¡ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºZebraçš„æ–°å‹ç”Ÿæˆè‡ªå›å½’Transformerï¼Œç”¨äºè§£å†³å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰ã€‚Zebraé€šè¿‡åœ¨é¢„è®­ç»ƒå’Œæ¨ç†æ—¶åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŠ¨æ€é€‚åº”æ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€åœ¨æ¨ç†æ—¶è¿›è¡Œæ¢¯åº¦è°ƒæ•´ã€‚å®ƒèƒ½å¤Ÿçµæ´»å¤„ç†ä»»æ„å¤§å°çš„ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œå¹¶é€šè¿‡é‡‡æ ·å¤šä¸ªè§£è½¨è¿¹æ¥æ”¯æŒä¸ç¡®å®šæ€§é‡åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒZebraåœ¨å„ç§å¤æ‚çš„PDEåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¾ˆå¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚"
                }
            },
            "hash": "49b78243ef220e9c",
            "pub_date_card": {
                "ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 4",
                "zh": "10æœˆ4æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08159",
            "title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2410.08159",
            "abstract": "Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model with the same architecture as standard language models. DART does not rely on image quantization, enabling more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.",
            "score": 1,
            "issue_id": 57,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "DART: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DART - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸. DART Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ DART Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑÑƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DART: Revolutionizing Image Generation with Unified AR and Diffusion",
                    "desc": "The paper introduces DART, a new model for generating images that combines autoregressive and diffusion techniques in a non-Markovian framework. Unlike traditional diffusion models, DART does not rely on a step-by-step noise addition process, allowing it to use the generation path more efficiently. By using a transformer-based architecture similar to language models, DART can denoise image patches without needing image quantization, improving image quality and flexibility. The model also supports training with both text and image data, achieving strong results in generating images from text descriptions."
                },
                "zh": {
                    "title": "DARTï¼šçªç ´æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆå›¾åƒç”Ÿæˆæ–°æ ‡æ†",
                    "desc": "æ‰©æ•£æ¨¡å‹æ˜¯ç›®å‰è§†è§‰ç”Ÿæˆçš„ä¸»æµæ–¹æ³•ï¼Œä½†å…¶é©¬å°”å¯å¤«æ€§è´¨é™åˆ¶äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ç”Ÿæˆè½¨è¿¹ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDARTçš„æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è‡ªå›å½’å’Œæ‰©æ•£æ–¹æ³•ï¼Œé‡‡ç”¨éé©¬å°”å¯å¤«æ¡†æ¶ã€‚DARTé€šè¿‡è‡ªå›å½’æ¨¡å‹åœ¨ç©ºé—´å’Œå…‰è°±ä¸Šè¿­ä»£å»å™ªå›¾åƒå—ï¼Œä¸ä¾èµ–å›¾åƒé‡åŒ–ï¼Œä»è€Œæé«˜äº†å›¾åƒå»ºæ¨¡çš„æ•ˆæœã€‚DARTè¿˜å¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­åŒæ—¶è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œå±•ç¤ºäº†åœ¨ç±»æ¡ä»¶å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ç«äº‰åŠ›ã€‚"
                }
            },
            "hash": "40e6937b715fa538",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        }
    ],
    "weekday": 3,
    "link_prev": "2024-10-09.html",
    "link_next": "2024-10-11.html",
    "date_en": "10 October",
    "date_prev": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": {
        "ru": "09.10",
        "en": "10/09",
        "zh": "10æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.10",
        "en": "10/11",
        "zh": "10æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 10,
        "#agents": 7,
        "#cv": 8,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 8,
        "#math": 3,
        "#multilingual": 0,
        "#architecture": 4,
        "#medicine": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#editing": 0,
        "#human_computer_interaction": 0,
        "#planning": 0
    }
}