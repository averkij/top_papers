{
    "date": {
        "ru": "1 Ğ¸ÑĞ»Ñ",
        "en": "July 1",
        "zh": "7æœˆ1æ—¥"
    },
    "time_utc": "2025-07-01 15:12",
    "weekday": 1,
    "issue_id": 4582,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.23044",
            "title": "Ovis-U1 Technical Report",
            "url": "https://huggingface.co/papers/2506.23044",
            "abstract": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.",
            "score": 36,
            "issue_id": 4573,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "caa82e446dde84a7",
            "authors": [
                "Guo-Hua Wang",
                "Shanshan Zhao",
                "Xinjie Zhang",
                "Liangfu Cao",
                "Pengxin Zhan",
                "Lunhao Duan",
                "Shiyin Lu",
                "Minghao Fu",
                "Xiaohao Chen",
                "Jianshan Zhao",
                "Yang Li",
                "Qing-Guo Chen"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23044.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#architecture",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "Ovis-U1: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ovis-U1 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ovis-U1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Ovis-U1: Unifying Text and Image Mastery",
                    "desc": "Ovis-U1 is a powerful machine learning model with 3 billion parameters that excels in understanding and generating images from text, as well as editing images. It uses a unique training method that combines language understanding and image generation, leading to improved performance over models that focus on just one of these tasks. The model features a diffusion-based visual decoder and a bidirectional token refiner, which enhance its capabilities in generating high-quality images. Ovis-U1 has achieved impressive scores on various benchmarks, outperforming other state-of-the-art models in multimodal tasks."
                },
                "zh": {
                    "title": "Ovis-U1ï¼šå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹çš„çªç ´",
                    "desc": "Ovis-U1æ˜¯ä¸€ä¸ªæ‹¥æœ‰30äº¿å‚æ•°çš„ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆäº†å¤šæ¨¡æ€ç†è§£ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨åŸºäºæ‰©æ•£çš„è§†è§‰è§£ç å™¨å’ŒåŒå‘ä»¤ç‰Œç²¾ç‚¼å™¨ï¼Œä½¿å¾—å›¾åƒç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ä¸é¢†å…ˆæ¨¡å‹å¦‚GPT-4oç›¸å½“ã€‚ä¸ä¸€äº›ä½¿ç”¨å›ºå®šå¤šè¯­è¨€æ¨¡å‹è¿›è¡Œç”Ÿæˆä»»åŠ¡çš„æ¨¡å‹ä¸åŒï¼ŒOvis-U1é‡‡ç”¨äº†ä¸€ç§æ–°çš„ç»Ÿä¸€è®­ç»ƒæ–¹æ³•ï¼Œä»è¯­è¨€æ¨¡å‹å¼€å§‹è®­ç»ƒã€‚é€šè¿‡å°†ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç»“åˆï¼ŒOvis-U1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23858",
            "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
            "url": "https://huggingface.co/papers/2506.23858",
            "abstract": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.",
            "score": 23,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "684efafe36e7bbc8",
            "authors": [
                "Jianzong Wu",
                "Liang Hou",
                "Haotian Yang",
                "Xin Tao",
                "Ye Tian",
                "Pengfei Wan",
                "Di Zhang",
                "Yunhai Tong"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23858.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VMoBA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "VMoBA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. VMoBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ 1D-2D-3D, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Accelerating Video Generation with Sparse Attention",
                    "desc": "VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality."
                },
                "zh": {
                    "title": "VMoBAï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ–°ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶",
                    "desc": "VMoBAæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³å…¨æ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒè§†é¢‘å˜æ¢å™¨çš„æ³¨æ„åŠ›æ¨¡å¼è¿›è¡Œæ·±å…¥åˆ†æï¼ŒVMoBAèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘æ•°æ®çš„æ—¶ç©ºç‰¹æ€§ã€‚è¯¥æœºåˆ¶é€šè¿‡ä¸‰é¡¹å…³é”®æ”¹è¿›æå‡äº†åŸæœ‰çš„MoBAæ¡†æ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€é€‚åº”æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å¼çš„åˆ†å±‚é€’å½’å—åˆ’åˆ†æ–¹æ¡ˆã€ä¼˜å…ˆé€‰æ‹©æœ€æ˜¾è‘—çš„æŸ¥è¯¢-é”®å—äº¤äº’çš„å…¨å±€å—é€‰æ‹©ï¼Œä»¥åŠåŸºäºé˜ˆå€¼çš„å—é€‰æ‹©æ¥åŠ¨æ€ç¡®å®šå…³æ³¨å—çš„æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVMoBAåœ¨é•¿åºåˆ—è®­ç»ƒä¸­æ˜¾è‘—åŠ é€ŸVDMsçš„è®­ç»ƒï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸å…¨æ³¨æ„åŠ›æœºåˆ¶ç›¸å½“æˆ–æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24123",
            "title": "Calligrapher: Freestyle Text Image Customization",
            "url": "https://huggingface.co/papers/2506.24123",
            "abstract": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.",
            "score": 21,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "ee3cd26fec757450",
            "authors": [
                "Yue Ma",
                "Qingyan Bai",
                "Hao Ouyang",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Hongyu Liu",
                "Zichen Liu",
                "Haofan Wang",
                "Jingye Chen",
                "Yujun Shen",
                "Qifeng Chen"
            ],
            "affiliations": [
                "Ant Group, China",
                "Hong Kong University of Science and Technology, China",
                "InstantX, Independent Research Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24123.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "âœ’ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ»Ğ¸Ğ³Ñ€Ğ°Ñ„: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Calligrapher - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Calligrapher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Digital Typography with Style Consistency",
                    "desc": "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."
                },
                "zh": {
                    "title": "Calligrapherï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ•°å­—æ’ç‰ˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Calligrapher æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªè’¸é¦å’Œå±€éƒ¨é£æ ¼æ³¨å…¥æŠ€æœ¯ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸”é£æ ¼ä¸€è‡´çš„æ•°å­—æ’ç‰ˆã€‚è¯¥æ¡†æ¶è§£å†³äº†æ’ç‰ˆå®šåˆ¶ä¸­é£æ ¼æ§åˆ¶å’Œæ•°æ®ä¾èµ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ã€‚é¦–å…ˆï¼Œå¼€å‘äº†ä¸€ç§è‡ªè’¸é¦æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨æ„å»ºä»¥é£æ ¼ä¸ºä¸­å¿ƒçš„æ’ç‰ˆåŸºå‡†ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¯è®­ç»ƒçš„é£æ ¼ç¼–ç å™¨å¼•å…¥å±€éƒ¨é£æ ¼æ³¨å…¥æ¡†æ¶ï¼Œæå–å‚è€ƒå›¾åƒä¸­çš„å¼ºå¥é£æ ¼ç‰¹å¾ï¼Œä»è€Œæå‡ç›®æ ‡é£æ ¼çš„å¯¹é½ç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24119",
            "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.24119",
            "abstract": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
            "score": 17,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "fbb4b5b047892d14",
            "authors": [
                "Bo Liu",
                "Leon Guertler",
                "Simon Yu",
                "Zichen Liu",
                "Penghui Qi",
                "Daniel Balcells",
                "Mickel Liu",
                "Cheston Tan",
                "Weiyan Shi",
                "Min Lin",
                "Wee Sun Lee",
                "Natasha Jaques"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), A*STAR",
                "National University of Singapore",
                "Northeastern University",
                "Plastic Labs",
                "Sea AI Lab",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24119.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#games",
                    "#transfer_learning",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPIRAL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñƒ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²ĞµÑ€ÑĞ¸Ğ¹ ÑĞ°Ğ¼Ğ¸Ñ… ÑĞµĞ±Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ³Ñ€Ğµ ĞšÑƒĞ½-Ğ¿Ğ¾ĞºĞµÑ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B-Base Ğ½Ğ° 8.6% Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ 8.4% Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼."
                },
                "en": {
                    "title": "Empowering AI Reasoning through Self-Play in Zero-Sum Games",
                    "desc": "The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models."
                },
                "zh": {
                    "title": "è‡ªæˆ‘å¯¹å¼ˆï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPIRALçš„è‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›¶å’Œæ¸¸æˆæå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡å‹ä¸è‡ªèº«ä¸æ–­æ”¹è¿›çš„ç‰ˆæœ¬è¿›è¡Œå¤šè½®å¯¹å¼ˆï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥ç›‘ç£çš„éœ€æ±‚ã€‚SPIRALèƒ½å¤Ÿç”Ÿæˆæ— é™çš„é€æ­¥æŒ‘æˆ˜é—®é¢˜ï¼Œä½¿æ¨¡å‹å¿…é¡»é€‚åº”æ›´å¼ºçš„å¯¹æ‰‹ï¼Œä»è€Œå®ç°è‡ªæˆ‘æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SPIRALè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†é›¶å’Œæ¸¸æˆåœ¨è‡ªä¸»æ¨ç†å‘å±•ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22832",
            "title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
            "url": "https://huggingface.co/papers/2506.22832",
            "abstract": "A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model's reasoning trace contradicts that of an independent, frozen vision-language model (\"listener\") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
            "score": 17,
            "issue_id": 4575,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "70be97df80ce7e08",
            "authors": [
                "Alexander Gambashidze",
                "Li Pengyi",
                "Matvey Skripkin",
                "Andrey Galichin",
                "Anton Gusarov",
                "Konstantin Sobolev",
                "Andrey Kuznetsov",
                "Ivan Oseledets"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute, Moscow, Russia",
                "Skolkovo Institute of Science and Technology, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22832.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#alignment",
                    "#rl",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑˆĞ°Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸ÑÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ listener-augmented Group Relative Policy Optimization (GRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-'ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageReward Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½ÑĞ°Ğ½ÑĞ°Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Model Reasoning with Listener-Augmented Rewards",
                    "desc": "This paper presents a new framework called listener-augmented Group Relative Policy Optimization (GRPO) to improve reward models for aligning vision-language models with human preferences. The framework addresses the issue of reasoning accuracy by incorporating a 'listener' model that re-evaluates the reasoning process of the main model, providing calibrated confidence scores. This approach not only enhances the accuracy of the model's outputs but also reduces contradictions in reasoning when compared to traditional methods. The results show significant improvements in both accuracy and out-of-distribution performance, demonstrating the effectiveness of listener-based rewards in training robust models."
                },
                "zh": {
                    "title": "å¢å¼ºæ¨ç†ï¼Œæå‡äººç±»åå¥½å¯¹é½çš„å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°è¯„ä¼°æ¨ç†è¿‡ç¨‹æ¥æ”¹å–„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæé«˜è§†è§‰-è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ç²¾åº¦å’Œåœ¨åˆ†å¸ƒå¤–çš„è¡¨ç°ã€‚å½“å‰çš„å¥–åŠ±æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸”ç›‘ç£å¾®è°ƒå®¹æ˜“å¯¼è‡´è®°å¿†åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªâ€œå¬ä¼—â€æ¨¡å‹ï¼Œå®ƒç‹¬ç«‹äºæ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œé‡æ–°è¯„ä¼°ï¼Œå¹¶æä¾›æ›´ä¸ºå‡†ç¡®çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„å¥–åŠ±æœºåˆ¶ä¸ä»…é¼“åŠ±æ¨ç†æ¨¡å‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜ä¿ƒä½¿å…¶ç”Ÿæˆå¯¹ç‹¬ç«‹æ¨¡å‹å…·æœ‰è¯´æœåŠ›çš„è§£é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17930",
            "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
            "url": "https://huggingface.co/papers/2506.17930",
            "abstract": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent \"gibberish\" can remarkably improve performance across diverse tasks. Notably, the \"gibberish\" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",
            "score": 12,
            "issue_id": 4570,
            "pub_date": "2025-06-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ½Ñ",
                "en": "June 22",
                "zh": "6æœˆ22æ—¥"
            },
            "hash": "346a389b3fbfb2bd",
            "authors": [
                "Jianyu Wang",
                "Zhiqiang Hu",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "MiroMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17930.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ‘ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PromptQuine. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 'Ğ±ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ñ‹' Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². PromptQuine Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking LLM Potential with 'Gibberish' Prompts!",
                    "desc": "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."
                },
                "zh": {
                    "title": "èƒ¡è¨€ä¹±è¯­ï¼Œæå‡æ¨¡å‹è¡¨ç°çš„ç§˜å¯†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºè®¾è®¡èŒƒå¼ï¼Œç§°ä¸ºPromptQuineï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºæ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°†éšæœºç¤ºä¾‹ä¿®å‰ªæˆçœ‹ä¼¼æ— æ„ä¹‰çš„â€œèƒ¡è¨€ä¹±è¯­â€å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ–¹æ³•ã€‚å°½ç®¡å‘ç°æœ‰æ•ˆçš„ä¿®å‰ªç­–ç•¥å¹¶ä¸ç®€å•ï¼Œä½†PromptQuineæ¡†æ¶é€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼ï¼Œè‡ªåŠ¨æœç´¢ä¿®å‰ªç­–ç•¥ï¼Œåˆ©ç”¨ä½æ•°æ®ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¸Œæœ›èƒ½ä¸ºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœºåˆ¶ç ”ç©¶æä¾›æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨æ›´å¼€æ”¾çš„æœç´¢ç®—æ³•çš„å‘å±•ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„LLMæç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23542",
            "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
            "url": "https://huggingface.co/papers/2506.23542",
            "abstract": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
            "score": 10,
            "issue_id": 4570,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "9c9286ea4d796818",
            "authors": [
                "Weida Wang",
                "Changyong He",
                "Jin Zeng",
                "Di Qiu"
            ],
            "affiliations": [
                "Google",
                "School of Computer Science and Technology, Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23542.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#dataset",
                    "#graphs",
                    "#synthetic",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ToF: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Time-of-Flight (ToF), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹. Ğ¡ĞµÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing ToF Depth Images with Motion-Invariant Graph Fusion",
                    "desc": "This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets."
                },
                "zh": {
                    "title": "æå‡ToFæ·±åº¦å›¾åƒçš„å»å™ªæ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶é—´é£è¡Œï¼ˆToFï¼‰æ·±åº¦å»å™ªç½‘ç»œï¼Œåˆ©ç”¨è¿åŠ¨ä¸å˜å›¾èåˆå’Œè‡ªé€‚åº”æ»¤æ³¢å™¨æ¥æé«˜æ—¶é—´ç¨³å®šæ€§å’Œç©ºé—´æ¸…æ™°åº¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å¸§å¤„ç†æˆ–å¤šå¸§å¤„ç†ï¼Œä½†æœªè€ƒè™‘ä¸åŒå¸§ä¸­å¯¹åº”åƒç´ çš„æ·±åº¦å˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œç©ºé—´æ¨¡ç³Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å›¾ç»“æ„çš„æ—¶é—´è‡ªç›¸ä¼¼æ€§ï¼Œå®ç°è·¨å¸§å‡ ä½•æ³¨æ„åŠ›çš„å›¾èåˆï¼Œå¹¶ç»“åˆå›¾çš„å¹³æ»‘æ€§å…ˆéªŒå’ŒToFå™ªå£°åˆ†å¸ƒçš„æ•°æ®ä¿¡åº¦é¡¹ï¼Œæ„å»ºæœ€å¤§åéªŒå»å™ªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨åˆæˆDVToFæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®Kinectv2æ•°æ®é›†ä¸Šå±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17417",
            "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
            "url": "https://huggingface.co/papers/2506.17417",
            "abstract": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.",
            "score": 8,
            "issue_id": 4570,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "57576e8287f4515e",
            "authors": [
                "Mingyuan Wu",
                "Meitang Li",
                "Jingcheng Yang",
                "Jize Jiang",
                "Kaizhuo Yan",
                "Zhaoheng Li",
                "Minjia Zhang",
                "Klara Nahrstedt"
            ],
            "affiliations": [
                "University of Illinois Urbana Champaign",
                "University of Michigan Ann Arbor"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17417.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¿Ğ¾Ğ±ĞµĞ´Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Boosting VLM Reasoning with Inference-Time Techniques",
                    "desc": "This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities."
                },
                "zh": {
                    "title": "æ¨ç†æ—¶æŠ€æœ¯æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶æŠ€æœ¯å¦‚ä½•æå‡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè§£ç æ—¶é—´ç¼©æ”¾å’Œè‡ªæˆ‘ä¿®æ­£ç­‰æŠ€æœ¯åœ¨æ²¡æœ‰å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è¡¨ç°ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹æœªèƒ½æ˜¾ç¤ºå‡ºè‡ªæˆ‘ä¿®æ­£çš„ä¼˜åŠ¿ï¼Œä½†åŸºäºç”Ÿæˆçš„æ–¹æ³•åœ¨æå‡æ€§èƒ½æ–¹é¢æ˜æ˜¾ä¼˜äºåŸºäºéªŒè¯çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„VLMåœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¸Šä»ç¼ºä¹å¼ºå¤§çš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23151",
            "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation",
            "url": "https://huggingface.co/papers/2506.23151",
            "abstract": "MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.",
            "score": 7,
            "issue_id": 4578,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "ad8371e7e40b14b4",
            "authors": [
                "Vladislav Bargatin",
                "Egor Chistov",
                "Alexander Yakovenko",
                "Dmitriy Vatolin"
            ],
            "affiliations": [
                "Lomonosov Moscow State University, Moscow, Russia",
                "MSU Institute for Artificial Intelligence, Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23151.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "MEMFOF - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ GPU Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Spring, Sintel Ğ¸ KITTI-2015. MEMFOF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ RAFT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 2,09 Ğ“Ğ‘ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 1080p, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ½Ğ° Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "MEMFOF: Efficient Optical Flow for High-Resolution Images",
                    "desc": "MEMFOF is a novel optical flow estimation method designed to be memory-efficient while maintaining high accuracy for high-resolution images. It significantly reduces GPU memory usage during both training and inference, allowing for full 1080p processing without the need for downsampling. By optimizing design choices from existing architectures and incorporating multi-frame estimation, MEMFOF achieves state-of-the-art performance across various benchmarks. This method not only excels in accuracy but also enhances runtime efficiency, making it a robust solution for real-time applications in optical flow estimation."
                },
                "zh": {
                    "title": "MEMFOFï¼šé«˜æ•ˆå…‰æµä¼°è®¡çš„æ–°é€‰æ‹©",
                    "desc": "MEMFOFæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„å¤šå¸§å…‰æµä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘GPUå†…å­˜ä½¿ç”¨ã€‚è¯¥æ–¹æ³•åœ¨1080pè¾“å…¥æ—¶ä»…éœ€2.09 GBçš„GPUå†…å­˜ï¼Œè®­ç»ƒæ—¶ä¸º28.5 GBï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åŸç”Ÿ1080pä¸‹è¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€è£å‰ªæˆ–ä¸‹é‡‡æ ·ã€‚MEMFOFé€šè¿‡æ•´åˆå‡å°‘çš„ç›¸å…³ä½“ç§¯å’Œé«˜åˆ†è¾¨ç‡è®­ç»ƒåè®®ï¼Œä¼˜åŒ–äº†RAFTç±»æ¶æ„çš„è®¾è®¡é€‰æ‹©ï¼Œä»è€Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸å…¶ä»–èµ„æºå¯†é›†å‹æ–¹æ³•ç›¸æ¯”ï¼ŒMEMFOFåœ¨å‡†ç¡®æ€§å’Œè¿è¡Œæ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨é«˜åˆ†è¾¨ç‡å…‰æµä¼°è®¡ä¸­çš„ç¨³å¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.16500",
            "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
            "url": "https://huggingface.co/papers/2506.16500",
            "abstract": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.",
            "score": 7,
            "issue_id": 4572,
            "pub_date": "2025-06-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ½Ñ",
                "en": "June 19",
                "zh": "6æœˆ19æ—¥"
            },
            "hash": "8a6cd2aa2a56bf51",
            "authors": [
                "Samir Khaki",
                "Xiuyu Li",
                "Junxian Guo",
                "Ligeng Zhu",
                "Chenfeng Xu",
                "Konstantinos N. Plataniotis",
                "Amir Yazdanbakhsh",
                "Kurt Keutzer",
                "Song Han",
                "Zhijian Liu"
            ],
            "affiliations": [
                "Google DeepMind",
                "MIT",
                "UC Berkeley",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.16500.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "SparseLoRA: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "SparseLoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¼ SVD-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SparseLoRA ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 2,2 Ñ€Ğ°Ğ· Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¾ 1,6 Ñ€Ğ°Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "SparseLoRA: Speeding Up LLM Fine-Tuning with Smart Sparsity",
                    "desc": "SparseLoRA is a novel method designed to enhance the efficiency of fine-tuning large language models (LLMs) by utilizing a sparse subset of weights for loss and gradient calculations. Unlike previous methods that only reduce the number of trainable parameters, SparseLoRA significantly cuts down on computational costs and speeds up the fine-tuning process. It employs a lightweight, training-free singular value decomposition (SVD) estimator to dynamically select which weights to use, ensuring optimal performance across different layers and training steps. Experimental results demonstrate that SparseLoRA can reduce computational costs by up to 2.2 times and improve speed by up to 1.6 times, all while maintaining high accuracy on various tasks."
                },
                "zh": {
                    "title": "SparseLoRAï¼šåŠ é€Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒçš„ç¨€ç–æ–¹æ³•",
                    "desc": "SparseLoRAæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ç¨€ç–æƒé‡å­é›†æ¥å‡å°‘å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒçš„è®¡ç®—æˆæœ¬å’ŒåŠ é€Ÿè¿‡ç¨‹ã€‚ä¸å…¶ä»–å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒSparseLoRAä¸ä»…é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œè¿˜æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è½»é‡çº§çš„è®­ç»ƒæ— å…³å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ç¨€ç–æ€§ä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶é€‰æ‹©éœ€è¦è®¡ç®—çš„æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseLoRAåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†æœ€å¤š2.2å€ï¼Œé€Ÿåº¦æå‡äº†æœ€å¤š1.6å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22598",
            "title": "RExBench: Can coding agents autonomously implement AI research\n  extensions?",
            "url": "https://huggingface.co/papers/2506.22598",
            "abstract": "RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.",
            "score": 4,
            "issue_id": 4582,
            "pub_date": "2025-06-27",
            "pub_date_card": {
                "ru": "27 Ğ¸ÑĞ½Ñ",
                "en": "June 27",
                "zh": "6æœˆ27æ—¥"
            },
            "hash": "236eb718627b6f97",
            "authors": [
                "Nicholas Edwards",
                "Yukyung Lee",
                "Yujun",
                "Mao",
                "Yulu Qin",
                "Sebastian Schuster",
                "Najoung Kim"
            ],
            "affiliations": [
                "Boston University",
                "University College London",
                "University of Vienna"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22598.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#agents"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "RExBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 12 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ”Ğ°Ğ¶Ğµ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 40% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Evaluating LLM Agents: The Challenge of Autonomous Research Extensions",
                    "desc": "RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints."
                },
                "zh": {
                    "title": "è¯„ä¼°LLMä»£ç†çš„ç ”ç©¶æ‰©å±•èƒ½åŠ›",
                    "desc": "RExBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è‡ªä¸»å®ç°ç ”ç©¶æ‰©å±•èƒ½åŠ›çš„åŸºå‡†å·¥å…·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„LLMä»£ç†åœ¨æ²¡æœ‰äººç±»æç¤ºçš„æƒ…å†µä¸‹ï¼Œæ— æ³•æˆåŠŸå®Œæˆå¤§å¤šæ•°ç ”ç©¶æ‰©å±•ä»»åŠ¡ã€‚å°½ç®¡åœ¨æä¾›é¢å¤–çš„äººç±»æç¤ºåï¼ŒæˆåŠŸç‡æœ‰æ‰€æé«˜ï¼Œä½†ä»ç„¶ä½äº40%ã€‚è¿™è¡¨æ˜å½“å‰çš„ä»£ç†åœ¨å¤„ç†ç°å®ç ”ç©¶æ‰©å±•ä»»åŠ¡æ—¶ï¼Œä»ç„¶éœ€è¦å¤§é‡çš„äººç±»æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23219",
            "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
            "url": "https://huggingface.co/papers/2506.23219",
            "abstract": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
            "score": 3,
            "issue_id": 4573,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "86191833a27a1741",
            "authors": [
                "Jie Feng",
                "Shengyuan Wang",
                "Tianhui Liu",
                "Yanxin Xi",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
                "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China",
                "School of Electronic and Information Engineering, Beijing Jiaotong University, China",
                "University of Helsinki, Finland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23219.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "UrbanLLaVA: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "UrbanLLaVA - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UrbanLLaVA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "UrbanLLaVA: Revolutionizing Urban Data Processing with Multi-Modal AI",
                    "desc": "UrbanLLaVA is a multi-modal large language model specifically designed to handle various urban datasets, excelling in both single-modal and complex cross-modal tasks. It addresses the limitations of existing models by providing a unified framework that processes multiple types of urban data simultaneously. The model is trained using a diverse urban instruction dataset and employs a multi-stage training approach to enhance spatial reasoning and domain knowledge. Experimental results indicate that UrbanLLaVA significantly outperforms other models in urban research, demonstrating strong generalization across different cities."
                },
                "zh": {
                    "title": "UrbanLLaVAï¼šåŸå¸‚æ•°æ®å¤„ç†çš„æ–°çªç ´",
                    "desc": "UrbanLLaVAæ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†åŸå¸‚æ•°æ®é›†ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å•æ¨¡æ€å’Œå¤æ‚è·¨æ¨¡æ€åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæå‡äº†ç©ºé—´æ¨ç†å’Œé¢†åŸŸçŸ¥è¯†çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUrbanLLaVAåœ¨å¤šä¸ªåŸå¸‚çš„ä»»åŠ¡ä¸­å‡ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22992",
            "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
            "url": "https://huggingface.co/papers/2506.22992",
            "abstract": "MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.",
            "score": 3,
            "issue_id": 4574,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "19689e84c5482c65",
            "authors": [
                "Yulun Jiang",
                "Yekun Chai",
                "Maria BrbiÄ‡",
                "Michael Moor"
            ],
            "affiliations": [
                "EPFL",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22992.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MARBLE: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "MARBLE - ÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡: M-Portal Ğ¸ M-Cube, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ². Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° MARBLE, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° M-Portal Ğ¸ 0% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° M-Cube. MARBLE Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "MARBLE: Unveiling the Limits of Multimodal Reasoning in AI",
                    "desc": "MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning."
                },
                "zh": {
                    "title": "MARBLEï¼šæ­ç¤ºå¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜",
                    "desc": "MARBLEæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨æ­ç¤ºç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨é€æ­¥æ¨ç†å’Œè®¡åˆ’åˆ¶å®šæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†åŒ…å«ä¸¤ä¸ªé«˜åº¦æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒM-Portalå’ŒM-Cubeï¼Œè¦æ±‚åœ¨ç©ºé—´ã€è§†è§‰å’Œç‰©ç†çº¦æŸä¸‹è¿›è¡Œå¤šæ­¥éª¤è®¡åˆ’çš„åˆ¶å®šå’Œç†è§£ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨MARBLEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ‰€æœ‰12ä¸ªå…ˆè¿›æ¨¡å‹åœ¨M-Portalä¸Šçš„è¡¨ç°æ¥è¿‘éšæœºï¼Œè€Œåœ¨M-Cubeä¸Šçš„å‡†ç¡®ç‡ä¸º0%ã€‚é€šè¿‡æ­ç¤ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼ŒMARBLEå¸Œæœ›æ¨åŠ¨ä¸‹ä¸€ä»£æ¨¡å‹çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ¨ç†æ­¥éª¤ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œè®¡åˆ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23394",
            "title": "Teaching a Language Model to Speak the Language of Tools",
            "url": "https://huggingface.co/papers/2506.23394",
            "abstract": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
            "score": 2,
            "issue_id": 4570,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "19b24559e749a260",
            "authors": [
                "Simeon Emanuilov"
            ],
            "affiliations": [
                "Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23394.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»Ğ³Ğ°Ñ€ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ BgGPT Ğ½Ğ° Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° TUCAN Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Empowering Multilingual Models for Effective Tool Use",
                    "desc": "This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications."
                },
                "zh": {
                    "title": "æå‡å¤šè¯­è¨€å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ä¿åŠ åˆ©äºšè¯­çš„åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ã€‚å¤§å¤šæ•°å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­è¯­è¨€ä¸­ç¼ºä¹å¯é çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚ç ”ç©¶é€šè¿‡å¯¹BgGPTæ¨¡å‹ç³»åˆ—è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œä½¿ç”¨ä¸€ä¸ªåŒ…å«10,035ä¸ªåŠŸèƒ½è°ƒç”¨ç¤ºä¾‹çš„åŒè¯­æ•°æ®é›†ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œç ”ç©¶æ¨å‡ºçš„TUCANæ¨¡å‹åœ¨åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†28.75%ï¼Œå¹¶ä¸”åœ¨å“åº”æ ¼å¼ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22694",
            "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
            "url": "https://huggingface.co/papers/2506.22694",
            "abstract": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.",
            "score": 2,
            "issue_id": 4573,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "a9c9ccf73356a002",
            "authors": [
                "Raghavv Goel",
                "Sudhanshu Agrawal",
                "Mukul Gagrani",
                "Junyoung Park",
                "Yifan Zao",
                "He Zhang",
                "Tian Liu",
                "Yiping Yang",
                "Xin Yuan",
                "Jiuyan Lu",
                "Chris Lott",
                "Mingu Lee"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.22694.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ VocabTrim, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ°. VocabTrim Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ head-Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ°, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¥Ğ¾Ñ‚Ñ ÑÑ‚Ğ¾ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 16% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.2-3B-Instruct Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Speed Up Drafting with VocabTrim!",
                    "desc": "This paper presents VocabTrim, a technique designed to enhance drafter-based speculative decoding by optimizing the vocabulary used in the drafting process. By limiting the vocabulary to only the most frequently sampled tokens from the target model, VocabTrim reduces the inference overhead associated with drafting, particularly in memory-constrained environments. Although this approach may slightly lower the acceptance rate of generated sequences, it significantly accelerates drafting speed, especially on edge devices. The results demonstrate a notable 16% improvement in memory-bound speed-up for Llama-3 models on Spec-Bench, showcasing the effectiveness of this method."
                },
                "zh": {
                    "title": "VocabTrimï¼šæå‡æ¨æµ‹è§£ç é€Ÿåº¦çš„å…³é”®æŠ€æœ¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVocabTrimçš„æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è‰æ‹Ÿè¯­è¨€æ¨¡å‹çš„è¯æ±‡é‡æ¥æ”¹å–„åŸºäºè‰æ‹Ÿçš„æ¨æµ‹è§£ç æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é™ä½åœ¨å†…å­˜å—é™ç¯å¢ƒä¸­çš„è‰æ‹Ÿå»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¤§è¯æ±‡é‡çš„ç›®æ ‡è¯­è¨€æ¨¡å‹ã€‚VocabTrimé€šè¿‡é‡æ„è‰æ‹Ÿè¯­è¨€æ¨¡å‹çš„å¤´éƒ¨ï¼Œä»…ä¿ç•™ç›®æ ‡æ¨¡å‹ä¸­æœ€å¸¸è¢«é‡‡æ ·çš„æœ‰é™è¯æ±‡ï¼Œä»è€Œæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚å°½ç®¡é™åˆ¶è¯æ±‡ä¼šç•¥å¾®é™ä½æ¥å—ç‡ï¼Œä½†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ˜¾è‘—æé«˜äº†å†…å­˜å—é™é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.23135",
            "title": "RoboScape: Physics-informed Embodied World Model",
            "url": "https://huggingface.co/papers/2506.23135",
            "abstract": "RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.",
            "score": 1,
            "issue_id": 4574,
            "pub_date": "2025-06-29",
            "pub_date_card": {
                "ru": "29 Ğ¸ÑĞ½Ñ",
                "en": "June 29",
                "zh": "6æœˆ29æ—¥"
            },
            "hash": "421522bfdd825b96",
            "authors": [
                "Yu Shang",
                "Xin Zhang",
                "Yinzhou Tang",
                "Lei Jin",
                "Chen Gao",
                "Wei Wu",
                "Yong Li"
            ],
            "affiliations": [
                "Manifold AI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.23135.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#science",
                    "#open_source",
                    "#robotics",
                    "#video",
                    "#games",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RoboScape: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "RoboScape - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "RoboScape: Realistic Robotic Video Generation with Physics Awareness",
                    "desc": "RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos."
                },
                "zh": {
                    "title": "RoboScapeï¼šæå‡æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„ç‰©ç†çœŸå®æ„Ÿ",
                    "desc": "RoboScape æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆæ—¶é—´æ·±åº¦é¢„æµ‹å’Œå…³é”®ç‚¹åŠ¨æ€å­¦ä¹ ï¼Œæå‡äº†æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„è§†è§‰çœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§ã€‚å½“å‰çš„ä¸–ç•Œæ¨¡å‹åœ¨å»ºæ¨¡ä¸‰ç»´å‡ ä½•å’Œè¿åŠ¨åŠ¨æ€æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººåœºæ™¯ä¸­ä¸å¤ŸçœŸå®ã€‚æœ¬æ–‡æå‡ºçš„ RoboScape é€šè¿‡è”åˆå­¦ä¹  RGB è§†é¢‘ç”Ÿæˆå’Œç‰©ç†çŸ¥è¯†ï¼Œé‡‡ç”¨äº†ä¸¤ä¸ªå…³é”®çš„ç‰©ç†ä¿¡æ¯è”åˆè®­ç»ƒä»»åŠ¡ï¼Œå¢å¼ºäº†è§†é¢‘æ¸²æŸ“ä¸­çš„ä¸‰ç»´å‡ ä½•ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboScape åœ¨å¤šç§æœºå™¨äººåœºæ™¯ä¸­ç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ›´é«˜çš„è§†è§‰çœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§ï¼Œå¹¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­éªŒè¯äº†å…¶å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.22753",
            "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
            "url": "https://huggingface.co/papers/2506.22753",
            "abstract": "The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.",
            "score": 1,
            "issue_id": 4578,
            "pub_date": "2025-06-28",
            "pub_date_card": {
                "ru": "28 Ğ¸ÑĞ½Ñ",
                "en": "June 28",
                "zh": "6æœˆ28æ—¥"
            },
            "hash": "a52ba639d6bd763f",
            "authors": [
                "Jianing Zhang",
                "Jiayi Zhu",
                "Feiyu Ji",
                "Xiaokang Yang",
                "Xiaoyun Yuan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.22753.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#cv",
                    "#data",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ· Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Degradation-Modeled Multipath Diffusion Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ·. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ·. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Metalens Imaging with Adaptive Degradation Modeling",
                    "desc": "The Degradation-Modeled Multipath Diffusion framework enhances the image quality of metalenses by integrating natural image priors and specialized modules. It effectively addresses the challenges of optical degradation while balancing detail, fidelity, and perceptual quality. Unlike traditional methods that require extensive calibration or large datasets, this approach utilizes pretrained models and a tunable decoder for better control over the inference process. The introduction of a spatially varying degradation-aware attention module allows for adaptive modeling of complex degradations, leading to superior image reconstruction results."
                },
                "zh": {
                    "title": "é™è§£å»ºæ¨¡æå‡é‡‘å±é€é•œå›¾åƒè´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é™è§£å»ºæ¨¡çš„å¤šè·¯å¾„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è‡ªç„¶å›¾åƒå…ˆéªŒæ¥æ”¹å–„é‡‘å±é€é•œçš„å›¾åƒè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å®šæ¨¡å—å¹³è¡¡ç»†èŠ‚ã€ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶è§£å†³å…‰å­¦é™è§£é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºç²¾ç¡®çš„å…‰å­¦æ ¡å‡†æˆ–åºå¤§çš„é…å¯¹æ•°æ®é›†ï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§è‡ªç„¶å›¾åƒå…ˆéªŒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸå’Œæ¸…æ™°å›¾åƒé‡å»ºæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.21448",
            "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
            "url": "https://huggingface.co/papers/2506.21448",
            "abstract": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io.",
            "score": 1,
            "issue_id": 4573,
            "pub_date": "2025-06-26",
            "pub_date_card": {
                "ru": "26 Ğ¸ÑĞ½Ñ",
                "en": "June 26",
                "zh": "6æœˆ26æ—¥"
            },
            "hash": "fefdbdbfb0394a3c",
            "authors": [
                "Huadai Liu",
                "Jialei Wang",
                "Kaicheng Luo",
                "Wen Wang",
                "Qian Chen",
                "Zhou Zhao",
                "Wei Xue"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (HKUST)",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.21448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#audio"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ ĞºĞ°Ğº Ğ·Ğ²ÑƒĞºĞ¾Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€: Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ThinkSound - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ°, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AudioCoT Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ²ÑƒĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ThinkSound Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "ThinkSound: Revolutionizing Video-to-Audio Generation with Chain-of-Thought Reasoning",
                    "desc": "ThinkSound is a new framework that enhances video-to-audio generation by using Chain-of-Thought reasoning with a multimodal large language model. It addresses the challenge of creating high-quality audio that accurately reflects the visual content by breaking the process into three stages: foundational foley generation, interactive object-centric refinement, and targeted editing. Each stage utilizes contextually aligned reasoning to guide the audio generation, ensuring that the soundscapes are semantically coherent and tailored to user inputs. The framework also introduces the AudioCoT dataset, which connects visual elements, text descriptions, and sound synthesis, leading to state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "ThinkSoundï¼šè§†é¢‘ç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "ThinkSoundæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´æ¨ç†ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä»è§†é¢‘ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘ï¼Œè¾¾åˆ°äº†å„é¡¹åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœã€‚å°½ç®¡ç«¯åˆ°ç«¯çš„è§†é¢‘åˆ°éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å·²æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç”ŸæˆçœŸå®æ•æ‰è§†è§‰å†…å®¹ç»†å¾®å·®åˆ«çš„é«˜ä¿çœŸéŸ³é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªäº’è¡¥é˜¶æ®µï¼šåŸºç¡€éŸ³æ•ˆç”Ÿæˆã€äº¤äº’å¼å¯¹è±¡ä¸­å¿ƒç»†åŒ–å’ŒåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç›®æ ‡ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥AudioCoTæ•°æ®é›†ï¼ŒThinkSoundåœ¨è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆçš„å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨éŸ³é¢‘æŒ‡æ ‡å’Œé“¾å¼æ€ç»´æŒ‡æ ‡ä¸Šå‡å–å¾—äº†é¢†å…ˆæˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.17080",
            "title": "Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs",
            "url": "https://huggingface.co/papers/2506.17080",
            "abstract": "Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.",
            "score": 1,
            "issue_id": 4578,
            "pub_date": "2025-06-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ½Ñ",
                "en": "June 20",
                "zh": "6æœˆ20æ—¥"
            },
            "hash": "f35759eeab0ef755",
            "authors": [
                "Ricardo Rei",
                "Nuno M. Guerreiro",
                "JosÃ© Pombal",
                "JoÃ£o Alves",
                "Pedro Teixeirinha",
                "Amin Farajian",
                "AndrÃ© F. T. Martins"
            ],
            "affiliations": [
                "Instituto Superior TÃ©cnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
                "Instituto de TelecomunicaÃ§Ãµes",
                "MICS, CentraleSupÃ©lec, UniversitÃ© Paris-Saclay",
                "Unbabel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.17080.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#training",
                    "#machine_translation"
                ],
                "emoji": "ğŸ—¼",
                "ru": {
                    "title": "Tower+: Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "Tower+ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Tower+ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Tower+ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Tower+: Bridging Translation and General-Purpose Language Tasks",
                    "desc": "Tower+ is a suite of fine-tuned language models that excels in both translation and multilingual text tasks. The models are trained using a unique recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning, allowing them to maintain strong general-purpose capabilities while specializing in translation. By carefully curating data at each training stage, Tower+ achieves a balance between translation performance and general tasks like code generation and problem-solving. The results show that even smaller models can outperform larger existing models, demonstrating the effectiveness of this training approach."
                },
                "zh": {
                    "title": "Tower+: ç¿»è¯‘ä¸å¤šè¯­è¨€èƒ½åŠ›çš„å®Œç¾å¹³è¡¡",
                    "desc": "Tower+æ˜¯ä¸€å¥—ç»è¿‡ç²¾ç»†è°ƒä¼˜çš„è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç¿»è¯‘å’Œå¤šè¯­è¨€é€šç”¨æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒTower+å®ç°äº†ç¿»è¯‘ä¸“ä¸šåŒ–ä¸å¤šè¯­è¨€é€šç”¨èƒ½åŠ›ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µç²¾å¿ƒç”Ÿæˆå’Œæ•´ç†æ•°æ®ï¼Œä»¥å¢å¼ºç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªè§„æ¨¡ä¸Šå¼€å‘ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¸¸å¸¸è¶…è¶Šæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-30.html",
    "link_next": "2025-07-02.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "30.06",
        "en": "06/30",
        "zh": "6æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "02.07",
        "en": "07/02",
        "zh": "7æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 10,
        "#agents": 1,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 1,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 1
    }
}