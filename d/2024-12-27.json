{
    "date": {
        "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 27",
        "zh": "12æœˆ27æ—¥"
    },
    "time_utc": "2024-12-27 03:15",
    "weekday": 4,
    "issue_id": 1350,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18547",
            "title": "Token-Budget-Aware LLM Reasoning",
            "url": "https://huggingface.co/papers/2412.18547",
            "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
            "score": 12,
            "issue_id": 1328,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "9a018cda2c47f064",
            "authors": [
                "Tingxu Han",
                "Chunrong Fang",
                "Shiyu Zhao",
                "Shiqing Ma",
                "Zhenyu Chen",
                "Zhenting Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Rutgers University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18547.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (CoT), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency in LLMs with Token Budgets",
                    "desc": "This paper addresses the reasoning efficiency of large language models (LLMs) by introducing a token-budget-aware framework. It highlights that while Chain-of-Thought (CoT) reasoning improves performance, it also increases token usage and costs. The authors propose a method to dynamically estimate token budgets based on the complexity of reasoning tasks, allowing for more efficient use of tokens. Experimental results demonstrate that this approach reduces token costs with minimal impact on performance, providing a balance between efficiency and accuracy in LLM reasoning."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†ï¼Œé™ä½æˆæœ¬ï¼",
                    "desc": "æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²è‡³å…³é‡è¦ã€‚è™½ç„¶é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ˜¾è‘—çš„ä»¤ç‰Œä½¿ç”¨å¼€é”€ï¼Œå¢åŠ äº†æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°å½“å‰LLMçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»¤ç‰Œé¢„ç®—çš„LLMæ¨ç†æ¡†æ¶ï¼ŒåŠ¨æ€ä¼°è®¡ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œä»è€Œåœ¨ä¿æŒæ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18319",
            "title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search",
            "url": "https://huggingface.co/papers/2412.18319",
            "abstract": "In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code will be available at https://github.com/HJYao00/Mulberry",
            "score": 8,
            "issue_id": 1341,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "cb1baee5756c3113",
            "authors": [
                "Huanjin Yao",
                "Jiaxing Huang",
                "Wenhao Wu",
                "Jingyi Zhang",
                "Yibo Wang",
                "Shunyu Liu",
                "Yingjie Wang",
                "Yuxin Song",
                "Haocheng Feng",
                "Li Shen",
                "Dacheng Tao"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Nanyang Technological University",
                "Sun Yat-sen University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18319.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#agents",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Collective Monte Carlo Tree Search (CoMCTS). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CoMCTS Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Mulberry-260k Ñ Ğ´ĞµÑ€ĞµĞ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mulberry, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Collaborative Reasoning for Enhanced MLLM Performance",
                    "desc": "This paper introduces a new method called Collective Monte Carlo Tree Search (CoMCTS) for training machine learning language models (MLLMs) to reason through questions step-by-step. CoMCTS enhances traditional tree search techniques by incorporating collective learning from multiple models, allowing them to collaboratively explore and identify effective reasoning paths. The authors create a dataset named Mulberry-260k, which contains structured reasoning nodes for various questions, facilitating the training of their model, Mulberry. Experimental results show that Mulberry outperforms existing models in reasoning tasks, demonstrating the effectiveness of the CoMCTS approach."
                },
                "zh": {
                    "title": "é›†ä½“å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å¹¶è§£å†³é—®é¢˜ï¼Œé€šè¿‡å­¦ä¹ æ¯ä¸ªæ¨ç†æ­¥éª¤ç›´è‡³æœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¨ç†æ–¹æ³•ï¼Œç§°ä¸ºé›†ä½“è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆCoMCTSï¼‰ï¼Œå®ƒå°†é›†ä½“å­¦ä¹ çš„æ¦‚å¿µå¼•å…¥åˆ°æ ‘æœç´¢ä¸­ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†è·¯å¾„æœç´¢å’Œå­¦ä¹ ã€‚CoMCTSçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤šä¸ªæ¨¡å‹çš„é›†ä½“çŸ¥è¯†ï¼Œé€šè¿‡æ‰©å±•ã€æ¨¡æ‹Ÿã€é”™è¯¯å®šä½ã€åå‘ä¼ æ’­å’Œé€‰æ‹©ç­‰å››ä¸ªè¿­ä»£æ“ä½œï¼ŒååŒæ¨æµ‹ã€æœç´¢å¹¶è¯†åˆ«æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚é€šè¿‡ä½¿ç”¨Mulberry-260kæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†Mulberryæ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡é€æ­¥æ¨ç†å’Œåæ€çš„èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17726",
            "title": "VidTwin: Video VAE with Decoupled Structure and Dynamics",
            "url": "https://huggingface.co/papers/2412.17726",
            "abstract": "Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. Our code has been released at https://github.com/microsoft/VidTok/tree/main/vidtwin.",
            "score": 1,
            "issue_id": 1350,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "df96145ae3273ac6",
            "authors": [
                "Yuchi Wang",
                "Junliang Guo",
                "Xinyi Xie",
                "Tianyu He",
                "Xu Sun",
                "Jiang Bian"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "Microsoft Research Asia",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17726.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "VidTwin: ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ²ÑƒÑ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ VidTwin, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ². VidTwin Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 0,20% Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ (PSNR 28,14 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MCL-JCV). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞµÑ‘ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Decoupling Video for Better Generation with VidTwin",
                    "desc": "This paper introduces VidTwin, a new type of video autoencoder that enhances video generation by separating video data into two different latent spaces. The Structure latent vectors focus on the overall content and global movements, while the Dynamics latent vectors capture detailed and quick movements. The model uses an Encoder-Decoder architecture with specialized submodules to efficiently extract these latent representations. VidTwin achieves impressive compression and reconstruction quality, making it suitable for various generative tasks and future research in video representation."
                },
                "zh": {
                    "title": "VidTwinï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç´§å‡‘çš„è§†é¢‘è‡ªç¼–ç å™¨VidTwinï¼Œèƒ½å¤Ÿå°†è§†é¢‘è§£è€¦ä¸ºä¸¤ä¸ªä¸åŒçš„æ½œåœ¨ç©ºé—´ï¼šç»“æ„æ½œåœ¨å‘é‡å’ŒåŠ¨æ€æ½œåœ¨å‘é‡ã€‚ç»“æ„æ½œåœ¨å‘é‡æ•æ‰æ•´ä½“å†…å®¹å’Œå…¨å±€è¿åŠ¨ï¼Œè€ŒåŠ¨æ€æ½œåœ¨å‘é‡åˆ™è¡¨ç¤ºç»†å¾®çš„ç»†èŠ‚å’Œå¿«é€Ÿè¿åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªå­æ¨¡å—åˆ†åˆ«æå–è¿™ä¸¤ä¸ªæ½œåœ¨ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVidTwinåœ¨é‡å»ºè´¨é‡å’Œå‹ç¼©ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºåç»­ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å…·æœ‰å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18495",
            "title": "How \"Real\" is Your Real-Time Simultaneous Speech-to-Text Translation System?",
            "url": "https://huggingface.co/papers/2412.18495",
            "abstract": "Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker's speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends, and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions.",
            "score": 1,
            "issue_id": 1345,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "40843b43c87690db",
            "authors": [
                "Sara Papi",
                "Peter Polak",
                "OndÅ™ej Bojar",
                "Dominik MachÃ¡Äek"
            ],
            "affiliations": [
                "Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Czech Republic",
                "Fondazione Bruno Kessler, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18495.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#benchmark",
                    "#architecture",
                    "#multimodal",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚ (SimulST). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ SimulST ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼ SimulST."
                },
                "en": {
                    "title": "Advancing Simultaneous Speech-to-Text Translation for Real-World Applications",
                    "desc": "This paper addresses the challenges in simultaneous speech-to-text translation (SimulST), which translates spoken language into text in real-time. It highlights that most research has focused on simplified, pre-segmented speech, neglecting the complexities of unbounded speech scenarios. The authors conducted a literature review of 110 papers to identify critical issues and inconsistencies in terminology that hinder progress in the field. They propose a standardized framework for SimulST systems, analyze current trends, and provide recommendations for future research to improve real-world applications."
                },
                "zh": {
                    "title": "æ¨åŠ¨åŒæ—¶è¯­éŸ³ç¿»è¯‘çš„æ ‡å‡†åŒ–ä¸å‘å±•",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†åŒæ—¶è¯­éŸ³ç¿»è¯‘ï¼ˆSimulSTï¼‰çš„æŒ‘æˆ˜ä¸å‘å±•ã€‚å°½ç®¡SimulSTæ—¨åœ¨å®æ—¶å°†æºè¯­è¨€è¯­éŸ³ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€æ–‡æœ¬ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶é›†ä¸­äºäººç±»é¢„å…ˆåˆ†æ®µçš„è¯­éŸ³ï¼Œå¿½è§†äº†å®é™…åº”ç”¨ä¸­çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å¯¹110ç¯‡æ–‡çŒ®è¿›è¡Œäº†å¹¿æ³›çš„å›é¡¾ï¼Œæ­ç¤ºäº†å½“å‰ç ”ç©¶ä¸­çš„å…³é”®é—®é¢˜ï¼Œå¹¶æå‡ºäº†æ ‡å‡†åŒ–çš„æœ¯è¯­å’Œåˆ†ç±»æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†å…·ä½“çš„å»ºè®®å’Œæœªæ¥æ–¹å‘ï¼Œä»¥æ¨åŠ¨SimulSTé¢†åŸŸå‘æ›´ç°å®å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆå‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17998",
            "title": "WavePulse: Real-time Content Analytics of Radio Livestreams",
            "url": "https://huggingface.co/papers/2412.17998",
            "abstract": "Radio remains a pervasive medium for mass information dissemination, with AM/FM stations reaching more Americans than either smartphone-based social networking or live television. Increasingly, radio broadcasts are also streamed online and accessed over the Internet. We present WavePulse, a framework that records, documents, and analyzes radio content in real-time. While our framework is generally applicable, we showcase the efficacy of WavePulse in a collaborative project with a team of political scientists focusing on the 2024 Presidential Elections. We use WavePulse to monitor livestreams of 396 news radio stations over a period of three months, processing close to 500,000 hours of audio streams. These streams were converted into time-stamped, diarized transcripts and analyzed to track answer key political science questions at both the national and state levels. Our analysis revealed how local issues interacted with national trends, providing insights into information flow. Our results demonstrate WavePulse's efficacy in capturing and analyzing content from radio livestreams sourced from the Web. Code and dataset can be accessed at https://wave-pulse.io.",
            "score": 1,
            "issue_id": 1342,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "f1e86c8c69c0a7d6",
            "authors": [
                "Govind Mittal",
                "Sarthak Gupta",
                "Shruti Wagle",
                "Chirag Chopra",
                "Anthony J DeMattee",
                "Nasir Memon",
                "Mustaque Ahamad",
                "Chinmay Hegde"
            ],
            "affiliations": [
                "Georgia Institute of Technology Atlanta, GA, USA",
                "New York University Tandon School of Engineering Brooklyn, NY, USA",
                "The Carter Center Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17998.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ“»",
                "ru": {
                    "title": "WavePulse: ĞŸÑ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒĞ»ÑŒÑĞ° Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ´Ğ¸Ğ¾ÑÑ„Ğ¸Ñ€",
                    "desc": "WavePulse - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°Ğ´Ğ¸Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° 396 Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ¾ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¹ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµÑ… Ğ¼ĞµÑÑÑ†ĞµĞ², Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 500 000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ğ»Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ² ÑÑ‚ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² insights Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "WavePulse: Real-Time Analysis of Radio Content for Political Insights",
                    "desc": "WavePulse is a framework designed to record, document, and analyze radio content in real-time, particularly focusing on online streams. It was applied in a project with political scientists to study the 2024 Presidential Elections by monitoring 396 news radio stations over three months. The framework processed nearly 500,000 hours of audio, converting them into time-stamped, diarized transcripts for analysis. This analysis provided valuable insights into how local issues influenced national trends in political discourse."
                },
                "zh": {
                    "title": "WavePulseï¼šå®æ—¶åˆ†æå¹¿æ’­å†…å®¹çš„å¼ºå¤§å·¥å…·",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†WavePulseæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿå®æ—¶è®°å½•ã€è®°å½•å’Œåˆ†æå¹¿æ’­å†…å®¹ã€‚æˆ‘ä»¬åœ¨2024å¹´æ€»ç»Ÿé€‰ä¸¾çš„é¡¹ç›®ä¸­å±•ç¤ºäº†WavePulseçš„æœ‰æ•ˆæ€§ï¼Œç›‘æµ‹äº†396ä¸ªæ–°é—»ç”µå°çš„ç›´æ’­ï¼Œå¤„ç†äº†è¿‘50ä¸‡å°æ—¶çš„éŸ³é¢‘æµã€‚é€šè¿‡å°†éŸ³é¢‘æµè½¬æ¢ä¸ºå¸¦æ—¶é—´æˆ³çš„é€å­—ç¨¿ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ†æåœ°æ–¹é—®é¢˜ä¸å›½å®¶è¶‹åŠ¿ä¹‹é—´çš„äº’åŠ¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒWavePulseåœ¨æ•æ‰å’Œåˆ†æç½‘ç»œå¹¿æ’­å†…å®¹æ–¹é¢å…·æœ‰å¾ˆé«˜çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18609",
            "title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models",
            "url": "https://huggingface.co/papers/2412.18609",
            "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B parameters), creating a substantial computational burden when processing multi-frame videos. Our method introduces a novel Spatio-Temporal Alignment Block (STAB) that directly processes video inputs without requiring pre-trained encoders while using only 45M parameters for visual processing - at least a 6.5times reduction compared to traditional approaches. The STAB architecture combines Local Spatio-Temporal Encoding for fine-grained feature extraction, efficient spatial downsampling through learned attention and separate mechanisms for modeling frame-level and video-level relationships. Our model achieves comparable or superior performance to encoder-based approaches for open-ended video question answering on standard benchmarks. The fine-grained video question-answering evaluation demonstrates our model's effectiveness, outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key aspects like correctness and temporal understanding. Extensive ablation studies validate our architectural choices and demonstrate the effectiveness of our spatio-temporal modeling approach while achieving 3-4times faster processing speeds than previous methods. Code is available at https://github.com/jh-yi/Video-Panda.",
            "score": 1,
            "issue_id": 1341,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "2cf82ae4b3c11ce8",
            "authors": [
                "Jinhui Yi",
                "Syed Talal Wasim",
                "Yanan Luo",
                "Muzammal Naseer",
                "Juergen Gall"
            ],
            "affiliations": [
                "Khalifa University",
                "University of Bonn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18609.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (STAB), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 45 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞĞ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ 3-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Video-Language Understanding with STAB",
                    "desc": "This paper introduces a new method for understanding videos in relation to language without using heavy encoders, which are typically resource-intensive. The proposed Spatio-Temporal Alignment Block (STAB) processes video inputs directly and uses only 45 million parameters, significantly less than traditional models. It employs techniques like Local Spatio-Temporal Encoding and learned attention for efficient feature extraction and relationship modeling. The results show that this approach not only matches but often surpasses the performance of existing encoder-based models in video question answering tasks, while also being faster and more efficient."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘è¯­è¨€ç†è§£çš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ— ç¼–ç å™¨è§†é¢‘è¯­è¨€ç†è§£æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½è®¡ç®—è´Ÿæ‹…çš„åŒæ—¶å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„è§†é¢‘è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§å‹å›¾åƒç¼–ç å™¨æˆ–è§†é¢‘ç¼–ç å™¨ï¼Œå¤„ç†å¤šå¸§è§†é¢‘æ—¶ä¼šé€ æˆå·¨å¤§çš„è®¡ç®—å‹åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ—¶ç©ºå¯¹é½æ¨¡å—ï¼ˆSTABï¼‰ï¼Œç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œä½¿ç”¨ä»…45Må‚æ•°è¿›è¡Œè§†è§‰å¤„ç†ï¼Œå‡å°‘äº†è‡³å°‘6.5å€çš„è®¡ç®—é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¼€æ”¾å¼è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸åŸºäºç¼–ç å™¨çš„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ­£ç¡®æ€§å’Œæ—¶é—´ç†è§£ç­‰å…³é”®æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„ç¼–ç å™¨æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17780",
            "title": "PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion",
            "url": "https://huggingface.co/papers/2412.17780",
            "abstract": "Peptide therapeutics, a major class of medicines, have achieved remarkable success across diseases such as diabetes and cancer, with landmark examples such as GLP-1 receptor agonists revolutionizing the treatment of type-2 diabetes and obesity. Despite their success, designing peptides that satisfy multiple conflicting objectives, such as target binding affinity, solubility, and membrane permeability, remains a major challenge. Classical drug development and structure-based design are ineffective for such tasks, as they fail to optimize global functional properties critical for therapeutic efficacy. Existing generative frameworks are largely limited to continuous spaces, unconditioned outputs, or single-objective guidance, making them unsuitable for discrete sequence optimization across multiple properties. To address this, we present PepTune, a multi-objective discrete diffusion model for the simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with state-dependent masking schedules and penalty-based objectives. To guide the diffusion process, we propose a Monte Carlo Tree Search (MCTS)-based strategy that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTS integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity inherent to discrete spaces. Using PepTune, we generate diverse, chemically-modified peptides optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling characteristics on various disease-relevant targets. In total, our results demonstrate that MCTS-guided discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces.",
            "score": 1,
            "issue_id": 1340,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "9607ccf742aa57fb",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Management and Technology Program, University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17780.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#architecture",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "PepTune: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "PepTune - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ SMILES. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ Masked Discrete Language Model (MDLM) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾. PepTune ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ñ‹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MCTS ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "PepTune: Revolutionizing Peptide Design with Multi-Objective Optimization",
                    "desc": "This paper introduces PepTune, a novel multi-objective discrete diffusion model designed to generate and optimize therapeutic peptides. Traditional methods struggle with the complex requirements of peptide design, such as balancing binding affinity and solubility. PepTune utilizes a Masked Discrete Language Model framework and incorporates a Monte Carlo Tree Search strategy to effectively navigate the challenges of discrete optimization. The results show that PepTune can produce diverse peptides that meet multiple therapeutic criteria, showcasing its potential in drug development."
                },
                "zh": {
                    "title": "PepTuneï¼šå¤šç›®æ ‡è‚½è®¾è®¡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPepTuneçš„å¤šç›®æ ‡ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŒæ—¶ç”Ÿæˆå’Œä¼˜åŒ–æ²»ç–—æ€§è‚½çš„SMILESç»“æ„ã€‚è¯¥æ¨¡å‹åŸºäºæ©è”½ç¦»æ•£è¯­è¨€æ¨¡å‹ï¼ˆMDLMï¼‰æ¡†æ¶ï¼Œç¡®ä¿ç”Ÿæˆæœ‰æ•ˆçš„è‚½ç»“æ„ï¼Œå¹¶é€šè¿‡çŠ¶æ€ä¾èµ–çš„æ©è”½å’Œæƒ©ç½šç›®æ ‡æ¥ä¼˜åŒ–å¤šä¸ªæ²»ç–—ç‰¹æ€§ã€‚ä¸ºäº†å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç­–ç•¥ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œé€æ­¥ä¼˜åŒ–Paretoæœ€ä¼˜åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPepTuneèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„åŒ–å­¦ä¿®é¥°è‚½ï¼Œä¼˜åŒ–ç›®æ ‡ç»“åˆäº²å’ŒåŠ›ã€è†œé€šé€æ€§ã€æº¶è§£åº¦ç­‰å¤šä¸ªç‰¹æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œå¤šç›®æ ‡åºåˆ—è®¾è®¡çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 2,
        "#video": 2,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) zhÅng tuÄ« lÇ de zhÃ²ng yÃ o xÃ¬ng. suÄ« rÃ¡n Chain-of-Thought (CoT) tuÄ« lÇ fÄng fÇ tÅng guÃ² jiÄng wÃ¨n tÃ­ fÄ“n jiÄ› wÃ©i zhÅng jiÄn bÃ¹ zhÃ²u lÃ¡i tÃ­ gÄo LLM xÃ¬ng nÃ©ng, dÃ n yÄ› zÄ“ng jiÄ le lÃ¬ng pÃ¡i shÇ yÃ²ng de kÄi xiÇo, dÇo zhÃ¬ chÃ©ng bÄ›n zÄ“ng jiÄ. yÃ¡n jiÅ« fÄ xiÃ n, dÄng qiÃ¡n LLMs de tuÄ« lÇ guÃ² chÃ©ng guÃ² yÃº rÇ’ng chÃ¡ng, kÄ› yÇ tÅng guÃ² zÃ i tÃ­ shÃ¬ zhÅng bÄo hÃ¡n hÃ© lÇ de lÃ¬ng pÃ¡i yÃ¹ suÃ n lÃ¡i yÄ suÅ, dÃ n lÃ¬ng pÃ¡i yÃ¹ suÃ n de xuÇn zÃ© duÃ¬ shÃ­ jÃ¬ yÄ suÅ xiÃ o guÇ’ zhÃ¬ guÄn zhÃ²ng yÃ o. zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ lÃ¬ng pÃ¡i yÃ¹ suÃ n gÇn zhÄ« de LLM tuÄ« lÇ kuÃ ng jiÃ , gÄ“n jÃ¹ tuÄ« lÇ fÃº zÃ  xÃ¬ng dÃ²ng tÃ i gÅ« sÇ”an bÃ¹ tÃ³ng wÃ¨n tÃ­ de lÃ¬ng pÃ¡i yÃ¹ suÃ n, bÃ¬ng shÇ yÃ²ng gÅ« sÇ”an de lÃ¬ng pÃ¡i yÃ¹ suÃ n zhÇ dÇo tuÄ« lÇ guÃ² chÃ©ng. shÃ­ yÃ n biÇo mÃ­ng, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i CoT tuÄ« lÇ zhÅng yÇ’u xiÃ o de jiÇn shÇo le lÃ¬ng pÃ¡i chÃ©ng bÄ›n, jÇn lÃ¼Ã¨ wÄ“i jÄ«ng le xÃ¬ng nÃ©ng, tÃ­ gÅng le yÄ« zhÇ’ng zÃ i LLM tuÄ« lÇ zhÅng pÃ­ng hÃ©ng xiÃ o yÃ¬ hÃ© zhÇ”n quÃ¨ xÃ¬ng de shÃ­ yÃ²ng jiÄ› juÃ© fÄng Ã n. dÃ i mÇ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'ä»¤ç‰Œ', 'pinyin': 'lÃ¬ng pÃ¡i', 'trans': 'token'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ng bÄ›n', 'trans': 'cost'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ng jiÄ', 'trans': 'increase'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'discover'}, {'word': 'å½“å‰', 'pinyin': 'dÄng qiÃ¡n', 'trans': 'current'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'}, {'word': 'å†—é•¿', 'pinyin': 'rÇ’ng chÃ¡ng', 'trans': 'tedious'}, {'word': 'å‹ç¼©', 'pinyin': 'yÄ suÅ', 'trans': 'compress'}, {'word': 'æç¤º', 'pinyin': 'tÃ­ shÃ¬', 'trans': 'prompt'}, {'word': 'åŒ…å«', 'pinyin': 'bÄo hÃ¡n', 'trans': 'include'}, {'word': 'åˆç†', 'pinyin': 'hÃ© lÇ', 'trans': 'reasonable'}, {'word': 'é¢„ç®—', 'pinyin': 'yÃ¹ suÃ n', 'trans': 'budget'}, {'word': 'è‡³å…³é‡è¦', 'pinyin': 'zhÃ¬ guÄn zhÃ²ng yÃ o', 'trans': 'crucial'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perceive'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'æ ¹æ®', 'pinyin': 'gÄ“n jÃ¹', 'trans': 'based on'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'}, {'word': 'ä¼°ç®—', 'pinyin': 'gÅ« suÃ n', 'trans': 'estimate'}, {'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guide'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'ç•¥å¾®', 'pinyin': 'lÃ¼Ã¨ wÄ“i', 'trans': 'slightly'}, {'word': 'é™ä½', 'pinyin': 'jiÃ ng dÄ«', 'trans': 'lower'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}, {'word': 'å¹³è¡¡', 'pinyin': 'pÃ­ng hÃ©ng', 'trans': 'balance'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ› juÃ© fÄng Ã n', 'trans': 'solution'}, {'word': 'å®ç”¨', 'pinyin': 'shÃ­ yÃ²ng', 'trans': 'practical'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}