{
    "date": {
        "ru": "27 декабря",
        "en": "December 27",
        "zh": "12月27日"
    },
    "time_utc": "2024-12-27 06:14",
    "weekday": 4,
    "issue_id": 1353,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 3,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами",
                    "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, достигающую высокой производительности среди моделей аналогичного масштаба. Авторы использовали улучшенный конвейер обработки данных, robust-оптимизацию и эффективный подход к отжигу для повышения эффективности предобучения. YuLan-Mini, обученная на 1.08 триллионах токенов, показывает результаты, сопоставимые с ведущими промышленными моделями, требующими значительно больше данных. Исследователи предоставляют полные детали состава данных для каждой фазы обучения, чтобы облегчить воспроизведение результатов."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "高效预训练，YuLan-Mini引领语言模型新潮流",
                    "desc": "本论文介绍了YuLan-Mini，一个具有2.42亿参数的强大基础模型，能够在同类模型中实现顶尖性能。我们提出了一种有效的预训练方法，重点在于通过三项关键技术贡献来提高训练效率：精细的数据处理流程结合了数据清洗和调度策略，强大的优化方法减少了训练不稳定性，以及有效的退火方法结合了目标数据选择和长上下文训练。YuLan-Mini在1.08T的训练数据上表现出色，其性能可与需要更多数据的行业领先模型相媲美。"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12月26日"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12月30日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。\n\nzhè piān wén zhāng tǎo lùn le dà yǔ yán mó xíng (LLMs) zhōng tuī lǐ de zhòng yào xìng. suī rán Chain-of-Thought (CoT) tuī lǐ fāng fǎ tōng guò jiāng wèn tí fēn jiě wéi zhōng jiān bù zhòu lái tí gāo LLM xìng néng, dàn yě zēng jiā le lìng pái shǐ yòng de kāi xiǎo, dǎo zhì chéng běn zēng jiā. yán jiū fā xiàn, dāng qián LLMs de tuī lǐ guò chéng guò yú rǒng cháng, kě yǐ tōng guò zài tí shì zhōng bāo hán hé lǐ de lìng pái yù suàn lái yā suō, dàn lìng pái yù suàn de xuǎn zé duì shí jì yā suō xiào guǒ zhì guān zhòng yào. zuò zhě tí chū le yī gè lìng pái yù suàn gǎn zhī de LLM tuī lǐ kuàng jià, gēn jù tuī lǐ fú zà xìng dòng tài gū sǔan bù tóng wèn tí de lìng pái yù suàn, bìng shǐ yòng gū sǔan de lìng pái yù suàn zhǐ dǎo tuī lǐ guò chéng. shí yàn biǎo míng, zhè zhǒng fāng fǎ zài CoT tuī lǐ zhōng yǒu xiào de jiǎn shǎo le lìng pái chéng běn, jǐn lüè wēi jīng le xìng néng, tí gōng le yī zhǒng zài LLM tuī lǐ zhōng píng héng xiào yì hé zhǔn què xìng de shí yòng jiě jué fāng àn. dài mǎ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '令牌', 'pinyin': 'lìng pái', 'trans': 'token'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'}, {'word': '增加', 'pinyin': 'zēng jiā', 'trans': 'increase'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'discover'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '冗长', 'pinyin': 'rǒng cháng', 'trans': 'tedious'}, {'word': '压缩', 'pinyin': 'yā suō', 'trans': 'compress'}, {'word': '提示', 'pinyin': 'tí shì', 'trans': 'prompt'}, {'word': '包含', 'pinyin': 'bāo hán', 'trans': 'include'}, {'word': '合理', 'pinyin': 'hé lǐ', 'trans': 'reasonable'}, {'word': '预算', 'pinyin': 'yù suàn', 'trans': 'budget'}, {'word': '至关重要', 'pinyin': 'zhì guān zhòng yào', 'trans': 'crucial'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '根据', 'pinyin': 'gēn jù', 'trans': 'based on'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '估算', 'pinyin': 'gū suàn', 'trans': 'estimate'}, {'word': '指导', 'pinyin': 'zhǐ dǎo', 'trans': 'guide'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '略微', 'pinyin': 'lüè wēi', 'trans': 'slightly'}, {'word': '降低', 'pinyin': 'jiàng dī', 'trans': 'lower'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '平衡', 'pinyin': 'píng héng', 'trans': 'balance'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '实用', 'pinyin': 'shí yòng', 'trans': 'practical'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}