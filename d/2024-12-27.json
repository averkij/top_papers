{
    "date": {
        "ru": "27 декабря",
        "en": "December 27",
        "zh": "12月27日"
    },
    "time_utc": "2024-12-27 11:08",
    "weekday": 4,
    "issue_id": 1358,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 18,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами",
                    "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, достигающую высокой производительности среди моделей аналогичного масштаба. Авторы использовали улучшенный конвейер обработки данных, robust-оптимизацию и эффективный подход к отжигу для повышения эффективности предобучения. YuLan-Mini, обученная на 1.08 триллионах токенов, показывает результаты, сопоставимые с ведущими промышленными моделями, требующими значительно больше данных. Исследователи предоставляют полные детали состава данных для каждой фазы обучения, чтобы облегчить воспроизведение результатов."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "高效预训练，YuLan-Mini引领语言模型新潮流",
                    "desc": "本论文介绍了YuLan-Mini，一个具有2.42亿参数的强大基础模型，能够在同类模型中实现顶尖性能。我们提出了一种有效的预训练方法，重点在于通过三项关键技术贡献来提高训练效率：精细的数据处理流程结合了数据清洗和调度策略，强大的优化方法减少了训练不稳定性，以及有效的退火方法结合了目标数据选择和长上下文训练。YuLan-Mini在1.08T的训练数据上表现出色，其性能可与需要更多数据的行业领先模型相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17483",
            "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
            "url": "https://huggingface.co/papers/2412.17483",
            "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
            "score": 8,
            "issue_id": 1355,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "e390119f893ae13b",
            "authors": [
                "Chenlong Deng",
                "Zhisong Zhang",
                "Kelong Mao",
                "Shuaiyi Li",
                "Xinting Huang",
                "Dong Yu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#optimization",
                    "#long_context",
                    "#data"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях",
                    "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения обработки длинного контекста в больших языковых моделях. Авторы изучают эффективность этих методов по сравнению с моделями полного внимания и выявляют потенциальные проблемы, связанные со сжатием. Они обнаруживают три ключевых паттерна отказа и предлагают две стратегии для их смягчения: тонкое автокодирование и посегментную оценку важности токенов. Работа предоставляет ценные выводы о сжатии контекста на основе гист-токенов и предлагает практические стратегии улучшения возможностей сжатия."
                },
                "en": {
                    "title": "Enhancing Long-Context Processing with Gist-Based Compression",
                    "desc": "This paper explores gist-based context compression methods to enhance the ability of large language models to process long contexts. It investigates how these methods can serve as alternatives to full attention models and identifies potential failure patterns that may occur during compression. The authors demonstrate that while gist-based compression performs well in certain tasks, it struggles with others, revealing specific issues like information loss at boundaries and unexpected surprises. To address these challenges, they propose strategies such as fine-grained autoencoding and segment-wise token importance estimation to improve the effectiveness of context compression."
                },
                "zh": {
                    "title": "提升长上下文处理的要旨压缩策略",
                    "desc": "本文深入研究了基于要旨的上下文压缩方法，以改善大型语言模型的长上下文处理。我们关注两个关键问题：这些方法能多好地替代全注意力模型，以及压缩可能出现的失败模式。实验表明，基于要旨的压缩在检索增强生成和长文档问答等任务中表现接近无损，但在合成回忆等任务中面临挑战。为了解决这些问题，我们提出了两种有效策略：细粒度自编码和基于段的标记重要性估计，以提高压缩能力。"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12月26日"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12月30日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。",
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "pinyin": "这篇文章介绍了YuLan-Mini，一个具有24.2亿参数的基础模型。该模型在预训练中通过三个关键技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08万亿个记号上训练，性能媲美需要更多数据的行业领先模型。详细信息可在GitHub上找到。\n\nZhè piān wénzhāng jièshào le YuLan-Mini, yīgè jùyǒu 24.2 yì cānshù de jīchǔ móxíng. Gǎi móxíng zài yùxùnliàn zhōng tōngguò sān gè guǎnjiàn jìshù gòngxiàn tígāo le xùnliàn xiàolǜ: shùjù guǎndào, yōuhuà fāngfǎ hé tuìhuǒ fāngfǎ. YuLan-Mini zài 1.08 wàn yì gè jìhào shàng xùnliàn, xìngnéng jīměi xūyào gèng duō shùjù de hángyè lǐngxiān móxíng. Xiángxì xìnxī kě zài GitHub shàng zhǎo dào.",
        "vocab": "[\n    {\"word\": \"篇\", \"pinyin\": \"piān\", \"trans\": \"article, piece of writing\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jièshào\", \"trans\": \"introduce, present\"},\n    {\"word\": \"具有\", \"pinyin\": \"jùyǒu\", \"trans\": \"possess, have\"},\n    {\"word\": \"亿\", \"pinyin\": \"yì\", \"trans\": \"hundred million\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshù\", \"trans\": \"parameter\"},\n    {\"word\": \"基础\", \"pinyin\": \"jīchǔ\", \"trans\": \"basic, foundation\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yùxùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōngguò\", \"trans\": \"through, by means of\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key, crucial\"},\n    {\"word\": \"技术\", \"pinyin\": \"jìshù\", \"trans\": \"technology, skill\"},\n    {\"word\": \"贡献\", \"pinyin\": \"gòngxiàn\", \"trans\": \"contribution\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve, raise\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"数据\", \"pinyin\": \"shùjù\", \"trans\": \"data\"},\n    {\"word\": \"管道\", \"pinyin\": \"guǎndào\", \"trans\": \"pipeline\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōuhuà\", \"trans\": \"optimization\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"退火\", \"pinyin\": \"tuìhuǒ\", \"trans\": \"annealing\"},\n    {\"word\": \"记号\", \"pinyin\": \"jìhào\", \"trans\": \"symbol, token\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"媲美\", \"pinyin\": \"pìměi\", \"trans\": \"rival, match\"},\n    {\"word\": \"行业\", \"pinyin\": \"hángyè\", \"trans\": \"industry\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐngxiān\", \"trans\": \"leading, ahead\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"信息\", \"pinyin\": \"xìnxī\", \"trans\": \"information\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces YuLan-Mini, a foundational model with 2.42 billion parameters. During pre-training, the model's training efficiency was enhanced through three key technological contributions: the data pipeline, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub.",
        "update_ts": "2024-12-27 09:10"
    }
}