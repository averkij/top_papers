{
    "date": {
        "ru": "27 декабря",
        "en": "December 27",
        "zh": "12月27日"
    },
    "time_utc": "2024-12-29 01:49",
    "weekday": 4,
    "issue_id": 1375,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 36,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами",
                    "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, достигающую высокой производительности среди моделей аналогичного масштаба. Авторы использовали улучшенный конвейер обработки данных, robust-оптимизацию и эффективный подход к отжигу для повышения эффективности предобучения. YuLan-Mini, обученная на 1.08 триллионах токенов, показывает результаты, сопоставимые с ведущими промышленными моделями, требующими значительно больше данных. Исследователи предоставляют полные детали состава данных для каждой фазы обучения, чтобы облегчить воспроизведение результатов."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "高效预训练，YuLan-Mini引领语言模型新潮流",
                    "desc": "本论文介绍了YuLan-Mini，一个具有2.42亿参数的强大基础模型，能够在同类模型中实现顶尖性能。我们提出了一种有效的预训练方法，重点在于通过三项关键技术贡献来提高训练效率：精细的数据处理流程结合了数据清洗和调度策略，强大的优化方法减少了训练不稳定性，以及有效的退火方法结合了目标数据选择和长上下文训练。YuLan-Mini在1.08T的训练数据上表现出色，其性能可与需要更多数据的行业领先模型相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17483",
            "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
            "url": "https://huggingface.co/papers/2412.17483",
            "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
            "score": 21,
            "issue_id": 1355,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "e390119f893ae13b",
            "authors": [
                "Chenlong Deng",
                "Zhisong Zhang",
                "Kelong Mao",
                "Shuaiyi Li",
                "Xinting Huang",
                "Dong Yu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#optimization",
                    "#long_context",
                    "#data"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях",
                    "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения обработки длинного контекста в больших языковых моделях. Авторы изучают эффективность этих методов по сравнению с моделями полного внимания и выявляют потенциальные проблемы, связанные со сжатием. Они обнаруживают три ключевых паттерна отказа и предлагают две стратегии для их смягчения: тонкое автокодирование и посегментную оценку важности токенов. Работа предоставляет ценные выводы о сжатии контекста на основе гист-токенов и предлагает практические стратегии улучшения возможностей сжатия."
                },
                "en": {
                    "title": "Enhancing Long-Context Processing with Gist-Based Compression",
                    "desc": "This paper explores gist-based context compression methods to enhance the ability of large language models to process long contexts. It investigates how these methods can serve as alternatives to full attention models and identifies potential failure patterns that may occur during compression. The authors demonstrate that while gist-based compression performs well in certain tasks, it struggles with others, revealing specific issues like information loss at boundaries and unexpected surprises. To address these challenges, they propose strategies such as fine-grained autoencoding and segment-wise token importance estimation to improve the effectiveness of context compression."
                },
                "zh": {
                    "title": "提升长上下文处理的要旨压缩策略",
                    "desc": "本文深入研究了基于要旨的上下文压缩方法，以改善大型语言模型的长上下文处理。我们关注两个关键问题：这些方法能多好地替代全注意力模型，以及压缩可能出现的失败模式。实验表明，基于要旨的压缩在检索增强生成和长文档问答等任务中表现接近无损，但在合成回忆等任务中面临挑战。为了解决这些问题，我们提出了两种有效策略：细粒度自编码和基于段的标记重要性估计，以提高压缩能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18176",
            "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation",
            "url": "https://huggingface.co/papers/2412.18176",
            "abstract": "Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models inherently lack collaborative filtering information, relying primarily on textual content data neglecting other modalities and thus failing to achieve optimal recommendation performance. To address this limitation, we propose Molar, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively. Molar employs an MLLM to generate unified item representations from both textual and non-textual data, facilitating comprehensive multimodal modeling and enriching item embeddings. Additionally, it incorporates collaborative filtering signals through a post-alignment mechanism, which aligns user representations from content-based and ID-based models, ensuring precise personalization and robust performance. By seamlessly combining multimodal content with collaborative filtering insights, Molar captures both user interests and contextual semantics, leading to superior recommendation accuracy. Extensive experiments validate that Molar significantly outperforms traditional and LLM-based baselines, highlighting its strength in utilizing multimodal data and collaborative signals for sequential recommendation tasks. The source code is available at https://anonymous.4open.science/r/Molar-8B06/.",
            "score": 9,
            "issue_id": 1360,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "61dbc52b7cb93a06",
            "authors": [
                "Yucong Luo",
                "Qitao Qin",
                "Hao Zhang",
                "Mingyue Cheng",
                "Ruiran Yan",
                "Kefan Wang",
                "Jie Ouyang"
            ],
            "affiliations": [
                "State Key Laboratory of Cognitive Intelligence",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#games",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Molar: Мультимодальные рекомендации с учетом коллаборативной фильтрации",
                    "desc": "Статья представляет Molar - новую систему последовательных рекомендаций, основанную на мультимодальных больших языковых моделях. Molar объединяет текстовые и нетекстовые данные для создания комплексных представлений элементов, используя MLLM. Система включает сигналы коллаборативной фильтрации через механизм пост-выравнивания, который согласует представления пользователей из моделей на основе контента и ID. Эксперименты показывают, что Molar превосходит традиционные методы и подходы на основе LLM в задачах последовательных рекомендаций."
                },
                "en": {
                    "title": "Molar: Uniting Multimodal Data for Superior Sequential Recommendations",
                    "desc": "This paper introduces Molar, a new framework for sequential recommendation systems that combines multiple types of data, known as modalities, with user identification information. Traditional large language models (LLMs) often miss collaborative filtering insights, which are crucial for personalized recommendations. Molar addresses this by using a multimodal approach that generates unified item representations from both textual and non-textual data, enhancing the quality of item embeddings. The framework also aligns user representations from different models to improve personalization, resulting in better recommendation accuracy compared to existing methods."
                },
                "zh": {
                    "title": "Molar：多模态协同过滤的序列推荐新框架",
                    "desc": "本文提出了一种名为Molar的多模态大语言序列推荐框架，旨在解决现有大语言模型在推荐系统中缺乏协同过滤信息的问题。Molar通过整合文本和非文本数据，生成统一的项目表示，从而有效捕捉协同信号。该框架还采用后对齐机制，将基于内容和基于ID的用户表示进行对齐，以确保个性化推荐的准确性。实验结果表明，Molar在序列推荐任务中显著优于传统和基于大语言模型的基线，展示了其在利用多模态数据和协同信号方面的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18072",
            "title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks",
            "url": "https://huggingface.co/papers/2412.18072",
            "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications. Project page is available at https://davidhalladay.github.io/mmfactory_demo.",
            "score": 8,
            "issue_id": 1360,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "09fb1e8965a639a4",
            "authors": [
                "Wan-Cyuan Fan",
                "Tanzila Rahman",
                "Leonid Sigal"
            ],
            "affiliations": [
                "CIFAR AI Chair",
                "University of British Columbia",
                "Vector Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18072.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agi",
                    "#optimization",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🏭",
                "ru": {
                    "title": "MMFactory: универсальная фабрика решений для визуальных задач",
                    "desc": "MMFactory - это универсальная платформа для решения сложных визуальных задач с использованием различных моделей машинного обучения. Она предлагает программные решения на основе описания задачи, нескольких примеров ввода-вывода и опциональных ограничений по ресурсам. MMFactory использует мультиагентную систему на основе больших языковых моделей для генерации исполняемых, разнообразных и надежных решений. Экспериментальные результаты показывают, что MMFactory превосходит существующие методы, предоставляя современные решения, адаптированные под требования пользователя."
                },
                "en": {
                    "title": "MMFactory: Tailored Solutions for Visual Tasks in Machine Learning",
                    "desc": "This paper presents MMFactory, a universal framework designed to improve the process of selecting and deploying machine learning models for visual tasks. It addresses the limitations of existing approaches by providing a solution search engine that considers user constraints and offers a variety of programmatic solutions. MMFactory utilizes a committee-based solution proposer that engages multiple agents to generate diverse and robust solutions tailored to specific user needs. Experimental results demonstrate that MMFactory outperforms current methods, delivering state-of-the-art solutions that align with user-defined specifications."
                },
                "zh": {
                    "title": "MMFactory：满足用户需求的通用视觉任务解决方案",
                    "desc": "随着基础模型和视觉语言模型的进步，许多通用和特定用途的模型被开发出来以应对各种视觉任务。然而，没有单一模型能够处理所有潜在用户设想的任务。为了解决这一问题，本文提出了MMFactory，这是一个通用框架，能够根据任务描述和样本输入输出对，建议多样化的程序解决方案，并评估其性能和资源特性。实验结果表明，MMFactory在满足用户需求方面优于现有方法，提供了最先进的解决方案。"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12月26日"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12月30日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个名为YuLan-Mini的大型语言模型。该模型有2.42B参数，性能卓越。研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。YuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。项目细节可以在GitHub上找到。",
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "pinyin": "这篇文章介绍了一个名为YuLan-Mini的大型语言模型。\nZhè piān wénzhāng jièshào le yīgè míngwèi YuLan-Mini de dàxíng yǔyán móxíng.\n\n该模型有2.42B参数，性能卓越。\nGǎi móxíng yǒu 2.42B cānshù, xìngnéng zhuóyuè.\n\n研究团队通过三种技术贡献提高了训练效率：数据管道、优化方法和退火方法。\nYánjiū tuánduì tōngguò sān zhǒng jìshù gòngxiàn tígāo le xùnliàn xiàolǜ: shùjù guǎndào, yōuhuà fāngfǎ hé tuìhuǒ fāngfǎ.\n\nYuLan-Mini在1.08T tokens上训练，性能与需要更多数据的行业领先模型相当。\nYuLan-Mini zài 1.08T tokens shàng xùnliàn, xìngnéng yǔ xūyào gèng duō shùjù de hángyè lǐngxiān móxíng xiāngdāng.\n\n项目细节可以在GitHub上找到。\nXiàngmù xìjié kěyǐ zài GitHub shàng zhǎo dào.",
        "vocab": "[\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔyán móxíng\", \"trans\": \"language model\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshù\", \"trans\": \"parameters\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"卓越\", \"pinyin\": \"zhuóyuè\", \"trans\": \"outstanding\"},\n    {\"word\": \"研究团队\", \"pinyin\": \"yánjiū tuánduì\", \"trans\": \"research team\"},\n    {\"word\": \"技术贡献\", \"pinyin\": \"jìshù gòngxiàn\", \"trans\": \"technical contributions\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"数据管道\", \"pinyin\": \"shùjù guǎndào\", \"trans\": \"data pipeline\"},\n    {\"word\": \"优化方法\", \"pinyin\": \"yōuhuà fāngfǎ\", \"trans\": \"optimization methods\"},\n    {\"word\": \"退火方法\", \"pinyin\": \"tuìhuǒ fāngfǎ\", \"trans\": \"annealing methods\"},\n    {\"word\": \"tokens\", \"pinyin\": \"tokens\", \"trans\": \"tokens\"},\n    {\"word\": \"行业领先\", \"pinyin\": \"hángyè lǐngxiān\", \"trans\": \"industry-leading\"},\n    {\"word\": \"项目细节\", \"pinyin\": \"xiàngmù xìjié\", \"trans\": \"project details\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces a large language model named YuLan-Mini. The model has 2.42B parameters and delivers outstanding performance. The research team enhanced training efficiency through three technical contributions: data pipelines, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08T tokens and performs comparably to industry-leading models that require more data. Project details can be found on GitHub.",
        "update_ts": "2024-12-28 12:38"
    }
}