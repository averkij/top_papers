{
    "date": {
        "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 27",
        "zh": "12æœˆ27æ—¥"
    },
    "time_utc": "2024-12-29 01:49",
    "weekday": 4,
    "issue_id": 1375,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 36,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YuLan-Mini - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2.42 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, robust-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. YuLan-Mini, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 1.08 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "é«˜æ•ˆé¢„è®­ç»ƒï¼ŒYuLan-Miniå¼•é¢†è¯­è¨€æ¨¡å‹æ–°æ½®æµ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†YuLan-Miniï¼Œä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°é¡¶å°–æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®æ¥æé«˜è®­ç»ƒæ•ˆç‡ï¼šç²¾ç»†çš„æ•°æ®å¤„ç†æµç¨‹ç»“åˆäº†æ•°æ®æ¸…æ´—å’Œè°ƒåº¦ç­–ç•¥ï¼Œå¼ºå¤§çš„ä¼˜åŒ–æ–¹æ³•å‡å°‘äº†è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚YuLan-Miniåœ¨1.08Tçš„è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½å¯ä¸éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17483",
            "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
            "url": "https://huggingface.co/papers/2412.17483",
            "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
            "score": 21,
            "issue_id": 1355,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "e390119f893ae13b",
            "authors": [
                "Chenlong Deng",
                "Zhisong Zhang",
                "Kelong Mao",
                "Shuaiyi Li",
                "Xinting Huang",
                "Dong Yu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#optimization",
                    "#long_context",
                    "#data"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ: Ñ‚Ğ¾Ğ½ĞºĞ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¸ÑÑ‚-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Long-Context Processing with Gist-Based Compression",
                    "desc": "This paper explores gist-based context compression methods to enhance the ability of large language models to process long contexts. It investigates how these methods can serve as alternatives to full attention models and identifies potential failure patterns that may occur during compression. The authors demonstrate that while gist-based compression performs well in certain tasks, it struggles with others, revealing specific issues like information loss at boundaries and unexpected surprises. To address these challenges, they propose strategies such as fine-grained autoencoding and segment-wise token importance estimation to improve the effectiveness of context compression."
                },
                "zh": {
                    "title": "æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„è¦æ—¨å‹ç¼©ç­–ç•¥",
                    "desc": "æœ¬æ–‡æ·±å…¥ç ”ç©¶äº†åŸºäºè¦æ—¨çš„ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ï¼Œä»¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚æˆ‘ä»¬å…³æ³¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè¿™äº›æ–¹æ³•èƒ½å¤šå¥½åœ°æ›¿ä»£å…¨æ³¨æ„åŠ›æ¨¡å‹ï¼Œä»¥åŠå‹ç¼©å¯èƒ½å‡ºç°çš„å¤±è´¥æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¦æ—¨çš„å‹ç¼©åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé•¿æ–‡æ¡£é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘æ— æŸï¼Œä½†åœ¨åˆæˆå›å¿†ç­‰ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æœ‰æ•ˆç­–ç•¥ï¼šç»†ç²’åº¦è‡ªç¼–ç å’ŒåŸºäºæ®µçš„æ ‡è®°é‡è¦æ€§ä¼°è®¡ï¼Œä»¥æé«˜å‹ç¼©èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18176",
            "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation",
            "url": "https://huggingface.co/papers/2412.18176",
            "abstract": "Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models inherently lack collaborative filtering information, relying primarily on textual content data neglecting other modalities and thus failing to achieve optimal recommendation performance. To address this limitation, we propose Molar, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively. Molar employs an MLLM to generate unified item representations from both textual and non-textual data, facilitating comprehensive multimodal modeling and enriching item embeddings. Additionally, it incorporates collaborative filtering signals through a post-alignment mechanism, which aligns user representations from content-based and ID-based models, ensuring precise personalization and robust performance. By seamlessly combining multimodal content with collaborative filtering insights, Molar captures both user interests and contextual semantics, leading to superior recommendation accuracy. Extensive experiments validate that Molar significantly outperforms traditional and LLM-based baselines, highlighting its strength in utilizing multimodal data and collaborative signals for sequential recommendation tasks. The source code is available at https://anonymous.4open.science/r/Molar-8B06/.",
            "score": 9,
            "issue_id": 1360,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "61dbc52b7cb93a06",
            "authors": [
                "Yucong Luo",
                "Qitao Qin",
                "Hao Zhang",
                "Mingyue Cheng",
                "Ruiran Yan",
                "Kefan Wang",
                "Jie Ouyang"
            ],
            "affiliations": [
                "State Key Laboratory of Cognitive Intelligence",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18176.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Molar: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Molar - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Molar Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MLLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾ÑÑ‚-Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ID. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Molar Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Molar: Uniting Multimodal Data for Superior Sequential Recommendations",
                    "desc": "This paper introduces Molar, a new framework for sequential recommendation systems that combines multiple types of data, known as modalities, with user identification information. Traditional large language models (LLMs) often miss collaborative filtering insights, which are crucial for personalized recommendations. Molar addresses this by using a multimodal approach that generates unified item representations from both textual and non-textual data, enhancing the quality of item embeddings. The framework also aligns user representations from different models to improve personalization, resulting in better recommendation accuracy compared to existing methods."
                },
                "zh": {
                    "title": "Molarï¼šå¤šæ¨¡æ€ååŒè¿‡æ»¤çš„åºåˆ—æ¨èæ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMolarçš„å¤šæ¨¡æ€å¤§è¯­è¨€åºåˆ—æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­ç¼ºä¹ååŒè¿‡æ»¤ä¿¡æ¯çš„é—®é¢˜ã€‚Molaré€šè¿‡æ•´åˆæ–‡æœ¬å’Œéæ–‡æœ¬æ•°æ®ï¼Œç”Ÿæˆç»Ÿä¸€çš„é¡¹ç›®è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆæ•æ‰ååŒä¿¡å·ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨åå¯¹é½æœºåˆ¶ï¼Œå°†åŸºäºå†…å®¹å’ŒåŸºäºIDçš„ç”¨æˆ·è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œä»¥ç¡®ä¿ä¸ªæ€§åŒ–æ¨èçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMolaråœ¨åºåˆ—æ¨èä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®å’ŒååŒä¿¡å·æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18072",
            "title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks",
            "url": "https://huggingface.co/papers/2412.18072",
            "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications. Project page is available at https://davidhalladay.github.io/mmfactory_demo.",
            "score": 8,
            "issue_id": 1360,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "09fb1e8965a639a4",
            "authors": [
                "Wan-Cyuan Fan",
                "Tanzila Rahman",
                "Leonid Sigal"
            ],
            "affiliations": [
                "CIFAR AI Chair",
                "University of British Columbia",
                "Vector Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18072.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agi",
                    "#optimization",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MMFactory: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "MMFactory - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼. MMFactory Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MMFactory Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "MMFactory: Tailored Solutions for Visual Tasks in Machine Learning",
                    "desc": "This paper presents MMFactory, a universal framework designed to improve the process of selecting and deploying machine learning models for visual tasks. It addresses the limitations of existing approaches by providing a solution search engine that considers user constraints and offers a variety of programmatic solutions. MMFactory utilizes a committee-based solution proposer that engages multiple agents to generate diverse and robust solutions tailored to specific user needs. Experimental results demonstrate that MMFactory outperforms current methods, delivering state-of-the-art solutions that align with user-defined specifications."
                },
                "zh": {
                    "title": "MMFactoryï¼šæ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„é€šç”¨è§†è§‰ä»»åŠ¡è§£å†³æ–¹æ¡ˆ",
                    "desc": "éšç€åŸºç¡€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œè®¸å¤šé€šç”¨å’Œç‰¹å®šç”¨é€”çš„æ¨¡å‹è¢«å¼€å‘å‡ºæ¥ä»¥åº”å¯¹å„ç§è§†è§‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ²¡æœ‰å•ä¸€æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ‰€æœ‰æ½œåœ¨ç”¨æˆ·è®¾æƒ³çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MMFactoryï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡æè¿°å’Œæ ·æœ¬è¾“å…¥è¾“å‡ºå¯¹ï¼Œå»ºè®®å¤šæ ·åŒ–çš„ç¨‹åºè§£å†³æ–¹æ¡ˆï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½å’Œèµ„æºç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMFactoryåœ¨æ»¡è¶³ç”¨æˆ·éœ€æ±‚æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºYuLan-Miniçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸‰ç§æŠ€æœ¯è´¡çŒ®æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼šæ•°æ®ç®¡é“ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æ–¹æ³•ã€‚YuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½ä¸éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“ã€‚é¡¹ç›®ç»†èŠ‚å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚",
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºYuLan-Miniçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«gÃ¨ mÃ­ngwÃ¨i YuLan-Mini de dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng.\n\nè¯¥æ¨¡å‹æœ‰2.42Bå‚æ•°ï¼Œæ€§èƒ½å“è¶Šã€‚\nGÇi mÃ³xÃ­ng yÇ’u 2.42B cÄnshÃ¹, xÃ¬ngnÃ©ng zhuÃ³yuÃ¨.\n\nç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸‰ç§æŠ€æœ¯è´¡çŒ®æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼šæ•°æ®ç®¡é“ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æ–¹æ³•ã€‚\nYÃ¡njiÅ« tuÃ¡nduÃ¬ tÅngguÃ² sÄn zhÇ’ng jÃ¬shÃ¹ gÃ²ngxiÃ n tÃ­gÄo le xÃ¹nliÃ n xiÃ olÇœ: shÃ¹jÃ¹ guÇndÃ o, yÅuhuÃ  fÄngfÇ hÃ© tuÃ¬huÇ’ fÄngfÇ.\n\nYuLan-Miniåœ¨1.08T tokensä¸Šè®­ç»ƒï¼Œæ€§èƒ½ä¸éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“ã€‚\nYuLan-Mini zÃ i 1.08T tokens shÃ ng xÃ¹nliÃ n, xÃ¬ngnÃ©ng yÇ” xÅ«yÃ o gÃ¨ng duÅ shÃ¹jÃ¹ de hÃ¡ngyÃ¨ lÇngxiÄn mÃ³xÃ­ng xiÄngdÄng.\n\né¡¹ç›®ç»†èŠ‚å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚\nXiÃ ngmÃ¹ xÃ¬jiÃ© kÄ›yÇ zÃ i GitHub shÃ ng zhÇo dÃ o.",
        "vocab": "[\n    {\"word\": \"è¯­è¨€æ¨¡å‹\", \"pinyin\": \"yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"language model\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄnshÃ¹\", \"trans\": \"parameters\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å“è¶Š\", \"pinyin\": \"zhuÃ³yuÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ç ”ç©¶å›¢é˜Ÿ\", \"pinyin\": \"yÃ¡njiÅ« tuÃ¡nduÃ¬\", \"trans\": \"research team\"},\n    {\"word\": \"æŠ€æœ¯è´¡çŒ®\", \"pinyin\": \"jÃ¬shÃ¹ gÃ²ngxiÃ n\", \"trans\": \"technical contributions\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ olÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"æ•°æ®ç®¡é“\", \"pinyin\": \"shÃ¹jÃ¹ guÇndÃ o\", \"trans\": \"data pipeline\"},\n    {\"word\": \"ä¼˜åŒ–æ–¹æ³•\", \"pinyin\": \"yÅuhuÃ  fÄngfÇ\", \"trans\": \"optimization methods\"},\n    {\"word\": \"é€€ç«æ–¹æ³•\", \"pinyin\": \"tuÃ¬huÇ’ fÄngfÇ\", \"trans\": \"annealing methods\"},\n    {\"word\": \"tokens\", \"pinyin\": \"tokens\", \"trans\": \"tokens\"},\n    {\"word\": \"è¡Œä¸šé¢†å…ˆ\", \"pinyin\": \"hÃ¡ngyÃ¨ lÇngxiÄn\", \"trans\": \"industry-leading\"},\n    {\"word\": \"é¡¹ç›®ç»†èŠ‚\", \"pinyin\": \"xiÃ ngmÃ¹ xÃ¬jiÃ©\", \"trans\": \"project details\"},\n    {\"word\": \"GitHub\", \"pinyin\": \"GitHub\", \"trans\": \"GitHub\"}\n]",
        "trans": "This article introduces a large language model named YuLan-Mini. The model has 2.42B parameters and delivers outstanding performance. The research team enhanced training efficiency through three technical contributions: data pipelines, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08T tokens and performs comparably to industry-leading models that require more data. Project details can be found on GitHub.",
        "update_ts": "2024-12-28 12:38"
    }
}