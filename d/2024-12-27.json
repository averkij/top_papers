{
    "date": {
        "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 27",
        "zh": "12æœˆ27æ—¥"
    },
    "time_utc": "2024-12-27 06:14",
    "weekday": 4,
    "issue_id": 1353,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 3,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ YuLan-Mini - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2.42 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, robust-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. YuLan-Mini, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 1.08 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "é«˜æ•ˆé¢„è®­ç»ƒï¼ŒYuLan-Miniå¼•é¢†è¯­è¨€æ¨¡å‹æ–°æ½®æµ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†YuLan-Miniï¼Œä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°é¡¶å°–æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®æ¥æé«˜è®­ç»ƒæ•ˆç‡ï¼šç²¾ç»†çš„æ•°æ®å¤„ç†æµç¨‹ç»“åˆäº†æ•°æ®æ¸…æ´—å’Œè°ƒåº¦ç­–ç•¥ï¼Œå¼ºå¤§çš„ä¼˜åŒ–æ–¹æ³•å‡å°‘äº†è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚YuLan-Miniåœ¨1.08Tçš„è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½å¯ä¸éœ€è¦æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸åª²ç¾ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12æœˆ30æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) zhÅng tuÄ« lÇ de zhÃ²ng yÃ o xÃ¬ng. suÄ« rÃ¡n Chain-of-Thought (CoT) tuÄ« lÇ fÄng fÇ tÅng guÃ² jiÄng wÃ¨n tÃ­ fÄ“n jiÄ› wÃ©i zhÅng jiÄn bÃ¹ zhÃ²u lÃ¡i tÃ­ gÄo LLM xÃ¬ng nÃ©ng, dÃ n yÄ› zÄ“ng jiÄ le lÃ¬ng pÃ¡i shÇ yÃ²ng de kÄi xiÇo, dÇo zhÃ¬ chÃ©ng bÄ›n zÄ“ng jiÄ. yÃ¡n jiÅ« fÄ xiÃ n, dÄng qiÃ¡n LLMs de tuÄ« lÇ guÃ² chÃ©ng guÃ² yÃº rÇ’ng chÃ¡ng, kÄ› yÇ tÅng guÃ² zÃ i tÃ­ shÃ¬ zhÅng bÄo hÃ¡n hÃ© lÇ de lÃ¬ng pÃ¡i yÃ¹ suÃ n lÃ¡i yÄ suÅ, dÃ n lÃ¬ng pÃ¡i yÃ¹ suÃ n de xuÇn zÃ© duÃ¬ shÃ­ jÃ¬ yÄ suÅ xiÃ o guÇ’ zhÃ¬ guÄn zhÃ²ng yÃ o. zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ lÃ¬ng pÃ¡i yÃ¹ suÃ n gÇn zhÄ« de LLM tuÄ« lÇ kuÃ ng jiÃ , gÄ“n jÃ¹ tuÄ« lÇ fÃº zÃ  xÃ¬ng dÃ²ng tÃ i gÅ« sÇ”an bÃ¹ tÃ³ng wÃ¨n tÃ­ de lÃ¬ng pÃ¡i yÃ¹ suÃ n, bÃ¬ng shÇ yÃ²ng gÅ« sÇ”an de lÃ¬ng pÃ¡i yÃ¹ suÃ n zhÇ dÇo tuÄ« lÇ guÃ² chÃ©ng. shÃ­ yÃ n biÇo mÃ­ng, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i CoT tuÄ« lÇ zhÅng yÇ’u xiÃ o de jiÇn shÇo le lÃ¬ng pÃ¡i chÃ©ng bÄ›n, jÇn lÃ¼Ã¨ wÄ“i jÄ«ng le xÃ¬ng nÃ©ng, tÃ­ gÅng le yÄ« zhÇ’ng zÃ i LLM tuÄ« lÇ zhÅng pÃ­ng hÃ©ng xiÃ o yÃ¬ hÃ© zhÇ”n quÃ¨ xÃ¬ng de shÃ­ yÃ²ng jiÄ› juÃ© fÄng Ã n. dÃ i mÇ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'ä»¤ç‰Œ', 'pinyin': 'lÃ¬ng pÃ¡i', 'trans': 'token'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ng bÄ›n', 'trans': 'cost'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ng jiÄ', 'trans': 'increase'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'discover'}, {'word': 'å½“å‰', 'pinyin': 'dÄng qiÃ¡n', 'trans': 'current'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'}, {'word': 'å†—é•¿', 'pinyin': 'rÇ’ng chÃ¡ng', 'trans': 'tedious'}, {'word': 'å‹ç¼©', 'pinyin': 'yÄ suÅ', 'trans': 'compress'}, {'word': 'æç¤º', 'pinyin': 'tÃ­ shÃ¬', 'trans': 'prompt'}, {'word': 'åŒ…å«', 'pinyin': 'bÄo hÃ¡n', 'trans': 'include'}, {'word': 'åˆç†', 'pinyin': 'hÃ© lÇ', 'trans': 'reasonable'}, {'word': 'é¢„ç®—', 'pinyin': 'yÃ¹ suÃ n', 'trans': 'budget'}, {'word': 'è‡³å…³é‡è¦', 'pinyin': 'zhÃ¬ guÄn zhÃ²ng yÃ o', 'trans': 'crucial'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perceive'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'æ ¹æ®', 'pinyin': 'gÄ“n jÃ¹', 'trans': 'based on'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'}, {'word': 'ä¼°ç®—', 'pinyin': 'gÅ« suÃ n', 'trans': 'estimate'}, {'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guide'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'ç•¥å¾®', 'pinyin': 'lÃ¼Ã¨ wÄ“i', 'trans': 'slightly'}, {'word': 'é™ä½', 'pinyin': 'jiÃ ng dÄ«', 'trans': 'lower'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}, {'word': 'å¹³è¡¡', 'pinyin': 'pÃ­ng hÃ©ng', 'trans': 'balance'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ› juÃ© fÄng Ã n', 'trans': 'solution'}, {'word': 'å®ç”¨', 'pinyin': 'shÃ­ yÃ²ng', 'trans': 'practical'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}