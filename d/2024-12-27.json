{
    "date": {
        "ru": "27 декабря",
        "en": "December 27",
        "zh": "12月27日"
    },
    "time_utc": "2024-12-27 08:13",
    "weekday": 4,
    "issue_id": 1355,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17743",
            "title": "YuLan-Mini: An Open Data-efficient Language Model",
            "url": "https://huggingface.co/papers/2412.17743",
            "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
            "score": 16,
            "issue_id": 1353,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "8d62bad0fb313231",
            "authors": [
                "Yiwen Hu",
                "Huatong Song",
                "Jia Deng",
                "Jiapeng Wang",
                "Jie Chen",
                "Kun Zhou",
                "Yutao Zhu",
                "Jinhao Jiang",
                "Zican Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17743.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#optimization",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Эффективное предобучение больших языковых моделей с меньшими ресурсами",
                    "desc": "Статья представляет YuLan-Mini - языковую модель с 2.42 миллиардами параметров, достигающую высокой производительности среди моделей аналогичного масштаба. Авторы использовали улучшенный конвейер обработки данных, robust-оптимизацию и эффективный подход к отжигу для повышения эффективности предобучения. YuLan-Mini, обученная на 1.08 триллионах токенов, показывает результаты, сопоставимые с ведущими промышленными моделями, требующими значительно больше данных. Исследователи предоставляют полные детали состава данных для каждой фазы обучения, чтобы облегчить воспроизведение результатов."
                },
                "en": {
                    "title": "YuLan-Mini: Efficient Pre-Training for Powerful Language Models",
                    "desc": "This paper introduces YuLan-Mini, a large language model with 2.42 billion parameters that performs exceptionally well compared to other models of similar size. The authors focus on improving the pre-training process by implementing a sophisticated data pipeline, a strong optimization technique to reduce instability, and a targeted data selection strategy during training. By utilizing 1.08 trillion tokens, YuLan-Mini achieves results that rival those of larger models that require more data. The paper also provides comprehensive details on the training data to support reproducibility in future research."
                },
                "zh": {
                    "title": "高效预训练，YuLan-Mini引领语言模型新潮流",
                    "desc": "本论文介绍了YuLan-Mini，一个具有2.42亿参数的强大基础模型，能够在同类模型中实现顶尖性能。我们提出了一种有效的预训练方法，重点在于通过三项关键技术贡献来提高训练效率：精细的数据处理流程结合了数据清洗和调度策略，强大的优化方法减少了训练不稳定性，以及有效的退火方法结合了目标数据选择和长上下文训练。YuLan-Mini在1.08T的训练数据上表现出色，其性能可与需要更多数据的行业领先模型相媲美。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17483",
            "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
            "url": "https://huggingface.co/papers/2412.17483",
            "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
            "score": 7,
            "issue_id": 1355,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "e390119f893ae13b",
            "authors": [
                "Chenlong Deng",
                "Zhisong Zhang",
                "Kelong Mao",
                "Shuaiyi Li",
                "Xinting Huang",
                "Dong Yu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#optimization",
                    "#long_context",
                    "#data"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Сжатие без потерь: оптимизация обработки длинного контекста в больших языковых моделях",
                    "desc": "Это исследование посвящено методам сжатия контекста на основе гист-токенов для улучшения обработки длинного контекста в больших языковых моделях. Авторы изучают эффективность этих методов по сравнению с моделями полного внимания и выявляют потенциальные проблемы, связанные со сжатием. Они обнаруживают три ключевых паттерна отказа и предлагают две стратегии для их смягчения: тонкое автокодирование и посегментную оценку важности токенов. Работа предоставляет ценные выводы о сжатии контекста на основе гист-токенов и предлагает практические стратегии улучшения возможностей сжатия."
                },
                "en": {
                    "title": "Enhancing Long-Context Processing with Gist-Based Compression",
                    "desc": "This paper explores gist-based context compression methods to enhance the ability of large language models to process long contexts. It investigates how these methods can serve as alternatives to full attention models and identifies potential failure patterns that may occur during compression. The authors demonstrate that while gist-based compression performs well in certain tasks, it struggles with others, revealing specific issues like information loss at boundaries and unexpected surprises. To address these challenges, they propose strategies such as fine-grained autoencoding and segment-wise token importance estimation to improve the effectiveness of context compression."
                },
                "zh": {
                    "title": "提升长上下文处理的要旨压缩策略",
                    "desc": "本文深入研究了基于要旨的上下文压缩方法，以改善大型语言模型的长上下文处理。我们关注两个关键问题：这些方法能多好地替代全注意力模型，以及压缩可能出现的失败模式。实验表明，基于要旨的压缩在检索增强生成和长文档问答等任务中表现接近无损，但在合成回忆等任务中面临挑战。为了解决这些问题，我们提出了两种有效策略：细粒度自编码和基于段的标记重要性估计，以提高压缩能力。"
                }
            }
        }
    ],
    "link_prev": "2024-12-26.html",
    "link_next": "2024-12-30.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "26.12",
        "en": "12/26",
        "zh": "12月26日"
    },
    "short_date_next": {
        "ru": "30.12",
        "en": "12/30",
        "zh": "12月30日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 2,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。\n\nzhè piān wén zhāng tǎo lùn le dà yǔ yán mó xíng (LLMs) zhōng tuī lǐ de zhòng yào xìng. suī rán Chain-of-Thought (CoT) tuī lǐ fāng fǎ tōng guò jiāng wèn tí fēn jiě wéi zhōng jiān bù zhòu lái tí gāo LLM xìng néng, dàn yě zēng jiā le lìng pái shǐ yòng de kāi xiǎo, dǎo zhì chéng běn zēng jiā. yán jiū fā xiàn, dāng qián LLMs de tuī lǐ guò chéng guò yú rǒng cháng, kě yǐ tōng guò zài tí shì zhōng bāo hán hé lǐ de lìng pái yù suàn lái yā suō, dàn lìng pái yù suàn de xuǎn zé duì shí jì yā suō xiào guǒ zhì guān zhòng yào. zuò zhě tí chū le yī gè lìng pái yù suàn gǎn zhī de LLM tuī lǐ kuàng jià, gēn jù tuī lǐ fú zà xìng dòng tài gū sǔan bù tóng wèn tí de lìng pái yù suàn, bìng shǐ yòng gū sǔan de lìng pái yù suàn zhǐ dǎo tuī lǐ guò chéng. shí yàn biǎo míng, zhè zhǒng fāng fǎ zài CoT tuī lǐ zhōng yǒu xiào de jiǎn shǎo le lìng pái chéng běn, jǐn lüè wēi jīng le xìng néng, tí gōng le yī zhǒng zài LLM tuī lǐ zhōng píng héng xiào yì hé zhǔn què xìng de shí yòng jiě jué fāng àn. dài mǎ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '令牌', 'pinyin': 'lìng pái', 'trans': 'token'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'}, {'word': '增加', 'pinyin': 'zēng jiā', 'trans': 'increase'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'discover'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '冗长', 'pinyin': 'rǒng cháng', 'trans': 'tedious'}, {'word': '压缩', 'pinyin': 'yā suō', 'trans': 'compress'}, {'word': '提示', 'pinyin': 'tí shì', 'trans': 'prompt'}, {'word': '包含', 'pinyin': 'bāo hán', 'trans': 'include'}, {'word': '合理', 'pinyin': 'hé lǐ', 'trans': 'reasonable'}, {'word': '预算', 'pinyin': 'yù suàn', 'trans': 'budget'}, {'word': '至关重要', 'pinyin': 'zhì guān zhòng yào', 'trans': 'crucial'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '根据', 'pinyin': 'gēn jù', 'trans': 'based on'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '估算', 'pinyin': 'gū suàn', 'trans': 'estimate'}, {'word': '指导', 'pinyin': 'zhǐ dǎo', 'trans': 'guide'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '略微', 'pinyin': 'lüè wēi', 'trans': 'slightly'}, {'word': '降低', 'pinyin': 'jiàng dī', 'trans': 'lower'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '平衡', 'pinyin': 'píng héng', 'trans': 'balance'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '实用', 'pinyin': 'shí yòng', 'trans': 'practical'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}