{
    "date": {
        "ru": "12 Ğ¸ÑĞ½Ñ",
        "en": "June 12",
        "zh": "6æœˆ12æ—¥"
    },
    "time_utc": "2025-06-12 06:17",
    "weekday": 3,
    "issue_id": 4255,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.09995",
            "title": "PlayerOne: Egocentric World Simulator",
            "url": "https://huggingface.co/papers/2506.09995",
            "abstract": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.",
            "score": 18,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "e7cba5eb3e340a0f",
            "authors": [
                "Yuanpeng Tu",
                "Hao Luo",
                "Xi Chen",
                "Xiang Bai",
                "Fan Wang",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "HKU",
                "HUST",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09995.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ PlayerOne",
                    "desc": "PlayerOne - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. PlayerOne Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Egocentric World Simulation with PlayerOne",
                    "desc": "PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments."
                },
                "zh": {
                    "title": "å¼€åˆ›è‡ªæˆ‘ä¸­å¿ƒç°å®ä¸–ç•Œæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ",
                    "desc": "PlayerOneæ˜¯ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç°å®ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æ•æ‰çš„å›¾åƒæ„å»ºå’Œç”Ÿæˆè§†é¢‘ã€‚å®ƒé‡‡ç”¨ç²—åˆ°ç»†çš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡çš„è‡ªæˆ‘ä¸­å¿ƒæ–‡æœ¬-è§†é¢‘å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒæ­¥è¿åŠ¨-è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†éƒ¨åˆ†è§£è€¦çš„è¿åŠ¨æ³¨å…¥æ–¹æ¡ˆï¼Œä»¥å®ç°å¯¹éƒ¨åˆ†è¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶é€šè¿‡è”åˆé‡å»ºæ¡†æ¶ç¡®ä¿é•¿è§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlayerOneåœ¨æ§åˆ¶äººç±»è¿åŠ¨å’Œå»ºæ¨¡å¤šæ ·åœºæ™¯æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09113",
            "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "url": "https://huggingface.co/papers/2506.09113",
            "abstract": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
            "score": 17,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "d44df125bd718f2f",
            "authors": [
                "Yu Gao",
                "Haoyuan Guo",
                "Tuyen Hoang",
                "Weilin Huang",
                "Lu Jiang",
                "Fangyuan Kong",
                "Huixia Li",
                "Jiashi Li",
                "Liang Li",
                "Xiaojie Li",
                "Xunsong Li",
                "Yifu Li",
                "Shanchuan Lin",
                "Zhijie Lin",
                "Jiawei Liu",
                "Shu Liu",
                "Xiaonan Nie",
                "Zhiwu Qing",
                "Yuxi Ren",
                "Li Sun",
                "Zhi Tian",
                "Rui Wang",
                "Sen Wang",
                "Guoqiang Wei",
                "Guohong Wu",
                "Jie Wu",
                "Ruiqi Xia",
                "Fei Xiao",
                "Xuefeng Xiao",
                "Jiangqiao Yan",
                "Ceyuan Yang",
                "Jianchao Yang",
                "Runkai Yang",
                "Tao Yang",
                "Yihang Yang",
                "Zilyu Ye",
                "Xuejiao Zeng",
                "Yan Zeng",
                "Heng Zhang",
                "Yang Zhao",
                "Xiaozheng Zheng",
                "Peihao Zhu",
                "Jiaxin Zou",
                "Feilong Zuo"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.09113.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#rlhf",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾",
                    "desc": "Seedance 1.0 - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²ÑƒÑ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğµ RLHF. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼, Seedance 1.0 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ~10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Seedance 1.0: Fast and High-Quality Video Generation Revolutionized",
                    "desc": "Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds."
                },
                "zh": {
                    "title": "Seedance 1.0ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "Seedance 1.0 æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€æœ‰æ•ˆçš„æ¶æ„è®¾è®¡ã€åè®­ç»ƒä¼˜åŒ–å’Œæ¨¡å‹åŠ é€ŸæŠ€æœ¯ï¼Œæä¾›äº†å“è¶Šçš„è´¨é‡å’Œé€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šæºæ•°æ®æ•´ç†å’Œç²¾å‡†çš„è§†é¢‘å­—å¹•ï¼Œå¢å¼ºäº†å¯¹å¤šæ ·åœºæ™¯çš„å…¨é¢å­¦ä¹ èƒ½åŠ›ã€‚å®ƒçš„é«˜æ•ˆæ¶æ„æ”¯æŒå¤šé•œå¤´ç”Ÿæˆï¼Œå¹¶åŒæ—¶å­¦ä¹ æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä»»åŠ¡ã€‚Seedance 1.0 é€šè¿‡å¤šé˜¶æ®µè’¸é¦ç­–ç•¥å®ç°äº†çº¦10å€çš„æ¨ç†åŠ é€Ÿï¼Œèƒ½å¤Ÿåœ¨41.4ç§’å†…ç”Ÿæˆ5ç§’çš„1080pè§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09003",
            "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
            "url": "https://huggingface.co/papers/2506.09003",
            "abstract": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).",
            "score": 12,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "794ea1a282cfa727",
            "authors": [
                "Lei Zhang",
                "Jiaxi Yang",
                "Min Yang",
                "Jian Yang",
                "Mouxiang Chen",
                "Jiajun Zhang",
                "Zeyu Cui",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",
                "University of Science and Technology of China, Hefei, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09003.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "SWE-Flow: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ TDD Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "SWE-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (TDD). ĞĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (RDG). SWE-Flow Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² TDD-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Automating TDD with SWE-Flow: Smarter Development Schedules",
                    "desc": "SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding."
                },
                "zh": {
                    "title": "SWE-Flowï¼šè‡ªåŠ¨åŒ–æµ‹è¯•é©±åŠ¨å¼€å‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "SWE-Flowæ˜¯ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼ŒåŸºäºæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰æ–¹æ³•ã€‚å®ƒé€šè¿‡è‡ªåŠ¨æ¨æ–­å•å…ƒæµ‹è¯•ä¸­çš„å¼€å‘æ­¥éª¤ï¼Œç”Ÿæˆç»“æ„åŒ–çš„å¼€å‘è®¡åˆ’ï¼Œä»è€Œæé«˜äº†åœ¨çœŸå®é¡¹ç›®ä¸Šå¾®è°ƒå¼€æ”¾æ¨¡å‹çš„æ€§èƒ½ã€‚SWE-Flowçš„æ ¸å¿ƒæ˜¯æ„å»ºè¿è¡Œæ—¶ä¾èµ–å›¾ï¼ˆRDGï¼‰ï¼Œå‡†ç¡®æ•æ‰å‡½æ•°ä¹‹é—´çš„äº¤äº’ï¼Œç¡®ä¿æ¯ä¸€æ­¥ç”Ÿæˆéƒ¨åˆ†ä»£ç åº“åŠç›¸åº”çš„å•å…ƒæµ‹è¯•ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä»çœŸå®çš„GitHubé¡¹ç›®ä¸­ç”Ÿæˆäº†å¤§é‡çš„è®­ç»ƒå’Œæµ‹è¯•å®ä¾‹ï¼Œæ˜¾è‘—æå‡äº†åŸºäºTDDçš„ç¼–ç æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08889",
            "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
            "url": "https://huggingface.co/papers/2506.08889",
            "abstract": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
            "score": 10,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "a6e46d58b91f0fad",
            "authors": [
                "Yizhao Gao",
                "Shuming Guo",
                "Shijie Cao",
                "Yuqing Xia",
                "Yu Cheng",
                "Lei Wang",
                "Lingxiao Ma",
                "Yutao Sun",
                "Tianzhu Ye",
                "Li Dong",
                "Hayden Kwok-Hay So",
                "Yu Hua",
                "Ting Cao",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Microsoft Research",
                "Peking University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08889.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "SeerAttention-R - ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ´ĞµÑ€ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 0,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ² 4000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğµ AIME Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ TileLang, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 9 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention-3 Ğ½Ğ° GPU H100 Ğ¿Ñ€Ğ¸ 90% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. SeerAttention-R Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models",
                    "desc": "SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware."
                },
                "zh": {
                    "title": "SeerAttention-Rï¼šé«˜æ•ˆç¨€ç–æ³¨æ„åŠ›æ¨ç†æ¡†æ¶",
                    "desc": "SeerAttention-Ræ˜¯ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œä¸“ä¸ºæ¨ç†æ¨¡å‹çš„é•¿è§£ç è€Œè®¾è®¡ã€‚å®ƒé€šè¿‡è‡ªè’¸é¦é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§ï¼ŒåŒæ—¶å»é™¤äº†æŸ¥è¯¢æ± åŒ–ï¼Œä»¥é€‚åº”è‡ªå›å½’è§£ç ã€‚è¯¥æ¡†æ¶è½»é‡ä¸”çµæ´»ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒSeerAttention-Råœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ä»¥4Kä»¤ç‰Œé¢„ç®—ä¿æŒæ¥è¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ï¼Œå¹¶åœ¨H100 GPUä¸Šå®ç°äº†é«˜è¾¾9å€çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09790",
            "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
            "url": "https://huggingface.co/papers/2506.09790",
            "abstract": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.",
            "score": 9,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "6fb3fee31c3739a5",
            "authors": [
                "Zhenran Xu",
                "Yiyu Wang",
                "Xue Yang",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce, China",
                "Harbin Institute of Technology (Shenzhen), China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09790.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ComfyUI-R1: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ComfyUI-R1 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ğ˜Ğ˜-Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ComfyUI-R1 Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 4000 Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Automating AI Art Workflows with ComfyUI-R1",
                    "desc": "ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities."
                },
                "zh": {
                    "title": "ComfyUI-R1ï¼šè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆçš„æ¨ç†æ¨¡å‹",
                    "desc": "ComfyUI-R1 æ˜¯ä¸€ä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸“æ³¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡é•¿é“¾æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¸®åŠ©ç”¨æˆ·åˆ›å»ºå®šåˆ¶åŒ–çš„å·¥ä½œæµï¼Œé™ä½äº†å­¦ä¹ æ›²çº¿ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº† 4K å·¥ä½œæµçš„æ•°æ®é›†ï¼Œç»è¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒï¼Œç¡®ä¿äº†å·¥ä½œæµçš„æ ¼å¼æœ‰æ•ˆæ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComfyUI-R1 åœ¨æ ¼å¼æœ‰æ•ˆæ€§å’Œ F1 åˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†é•¿é“¾æ¨ç†åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09984",
            "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
            "url": "https://huggingface.co/papers/2506.09984",
            "abstract": "A novel framework for end-to-end human animation with multi-modal conditions enables high-quality video generation through explicit layout control and region-specific modality matching.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.",
            "score": 5,
            "issue_id": 4252,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "97a59a9be9ba0dc3",
            "authors": [
                "Zhenzhi Wang",
                "Jiaqi Yang",
                "Jianwen Jiang",
                "Chao Liang",
                "Gaojie Lin",
                "Zerong Zheng",
                "Ceyuan Yang",
                "Dahua Lin"
            ],
            "affiliations": [
                "ByteDance",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09984.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Human Animation with Multi-Modal Control",
                    "desc": "This paper presents a new framework for creating human animations that can incorporate multiple types of input, such as text, images, and audio. Unlike previous methods that only animate one subject at a time, this approach allows for complex interactions between multiple characters and objects in a video. The framework uses a mask predictor to accurately match visual elements to specific regions in the video, ensuring that each character's appearance is correctly represented. Additionally, it integrates audio cues in a way that aligns with the visual layout, resulting in high-quality, controllable animations that reflect rich human interactions."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„é«˜è´¨é‡äººç±»åŠ¨ç”»ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯äººç±»åŠ¨ç”»æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šä¸ªæ¦‚å¿µçš„ç²¾ç¡®æ§åˆ¶ï¼Œå…è®¸äººç±»ä¸ç‰©ä½“ä¹‹é—´çš„ä¸°å¯Œäº¤äº’ã€‚é€šè¿‡åŒºåŸŸç‰¹å®šçš„æ¡ä»¶ç»‘å®šï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼Œå¹¶ç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„æ˜¾å¼å¸ƒå±€æ§åˆ¶ä¼˜äºç°æœ‰çš„éšå¼æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09937",
            "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2506.09937",
            "abstract": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  \t\t\t\t\tAI-generated summary \t\t\t\t While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.",
            "score": 2,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "2a59fafef3ba118c",
            "authors": [
                "Qiao Gu",
                "Yuanliang Ju",
                "Shengxiang Sun",
                "Igor Gilitschenski",
                "Haruki Nishimura",
                "Masha Itkina",
                "Florian Shkurti"
            ],
            "affiliations": [
                "Toyota Research Institute (TRI)",
                "University of Toronto (UofT)",
                "UofT Robotics Institute",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09937.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#optimization",
                    "#agents",
                    "#robotics",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SAFE: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²",
                    "desc": "SAFE - ÑÑ‚Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² VLA Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± ÑƒÑĞ¿ĞµÑ…Ğµ Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. SAFE Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ°Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SAFE: Generalizing Failure Detection for Vision-Language-Action Models",
                    "desc": "SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods."
                },
                "zh": {
                    "title": "SAFEï¼šæ™ºèƒ½æœºå™¨äººæ•…éšœæ£€æµ‹çš„æ–°æ–¹æ³•",
                    "desc": "SAFEæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æ•…éšœæ£€æµ‹å™¨ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ¨¡å‹çš„é«˜å±‚å†…éƒ¨ç‰¹å¾æ¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚å°½ç®¡è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–°ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†è®©æœºå™¨äººå®‰å…¨åœ°ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¤ŸåŠæ—¶å‘å‡ºè­¦æŠ¥çš„æ•…éšœæ£€æµ‹å™¨ï¼Œä»¥ä¾¿æœºå™¨äººå¯ä»¥åœæ­¢ã€å›æº¯æˆ–è¯·æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬æå‡ºçš„SAFEèƒ½å¤Ÿä»VLAçš„å†…éƒ¨ç‰¹å¾ä¸­å­¦ä¹ ï¼Œå¹¶é¢„æµ‹ä»»åŠ¡å¤±è´¥çš„å¯èƒ½æ€§ï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ•…éšœæ£€æµ‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08001",
            "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
            "url": "https://huggingface.co/papers/2506.08001",
            "abstract": "A new reParameterized training algorithm named POET uses Orthogonal Equivalence Transformation to optimize neurons, providing stable optimization and improved generalization for training large-scale neural networks including LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
            "score": 1,
            "issue_id": 4255,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "b47d919d8313c30a",
            "authors": [
                "Zeju Qiu",
                "Simon Buchholz",
                "Tim Z. Xiao",
                "Maximilian Dax",
                "Bernhard SchÃ¶lkopf",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08001.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "POET: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "POET - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. POET Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ POET Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "POET: Optimizing Neurons for Stable and Scalable Training",
                    "desc": "The paper introduces POET, a new training algorithm designed to enhance the optimization of neurons in large-scale neural networks, particularly large language models (LLMs). POET employs Orthogonal Equivalence Transformation, which reparameterizes neurons using two learnable orthogonal matrices alongside a fixed random weight matrix. This method ensures the stability of the optimization process and improves the generalization capabilities of the models. The authors also present efficient approximations that allow POET to be flexible and scalable, demonstrating its effectiveness through extensive experiments."
                },
                "zh": {
                    "title": "POETï¼šä¼˜åŒ–ç¥ç»å…ƒçš„é‡å‚æ•°åŒ–è®­ç»ƒæ–°ç®—æ³•",
                    "desc": "POETæ˜¯ä¸€ç§æ–°çš„é‡å‚æ•°åŒ–è®­ç»ƒç®—æ³•ï¼Œåˆ©ç”¨æ­£äº¤ç­‰ä»·å˜æ¢æ¥ä¼˜åŒ–ç¥ç»å…ƒã€‚è¯¥ç®—æ³•é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„æ­£äº¤çŸ©é˜µå’Œä¸€ä¸ªå›ºå®šçš„éšæœºæƒé‡çŸ©é˜µï¼Œå¯¹æ¯ä¸ªç¥ç»å…ƒè¿›è¡Œé‡å‚æ•°åŒ–ã€‚POETèƒ½å¤Ÿç¨³å®šåœ°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚å®éªŒç»“æœéªŒè¯äº†POETåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09007",
            "title": "Branched SchrÃ¶dinger Bridge Matching",
            "url": "https://huggingface.co/papers/2506.09007",
            "abstract": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.",
            "score": 0,
            "issue_id": 4252,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "8f3c4a6be505cd98",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Computer Science, Duke University",
                "Department of Computer and Information Science, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09007.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "BranchSBM: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "BranchSBM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ SchrÃ¶dinger Bridge Matching Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼. BranchSBM Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¾ÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ñ„ÑƒÑ€ĞºĞ°Ñ†Ğ¸Ğ¹ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑƒĞ´ĞµĞ± Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Branching Out: Modeling Multiple Outcomes with BranchSBM",
                    "desc": "BranchSBM is a new framework in generative modeling that enhances the traditional Schr\"odinger Bridge Matching by allowing for branched stochastic paths. Unlike previous methods that only model single paths between two distributions, BranchSBM can represent multiple outcomes from a single starting point. This is achieved by using multiple time-dependent velocity fields and growth processes, which capture the complexity of population-level divergence. The framework is particularly useful for applications like simulating cell fate decisions and navigating multi-path scenarios."
                },
                "zh": {
                    "title": "åˆ†æ”¯è–›å®šè°”æ¡¥åŒ¹é…ï¼šæ•æ‰å¤šè·¯å¾„æ¼”åŒ–çš„ç”Ÿæˆå»ºæ¨¡æ–°æ¡†æ¶",
                    "desc": "BranchSBMæ˜¯ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œæ‰©å±•äº†è–›å®šè°”æ¡¥åŒ¹é…æ–¹æ³•ï¼Œä»¥å»ºæ¨¡ä»å•ä¸€åˆå§‹åˆ†å¸ƒåˆ°å¤šä¸ªç»“æœçš„åˆ†æ”¯éšæœºè·¯å¾„å’Œå¤šè·¯å¾„æ¼”åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å‚æ•°åŒ–å¤šä¸ªæ—¶é—´ä¾èµ–çš„é€Ÿåº¦åœºå’Œç”Ÿé•¿è¿‡ç¨‹ï¼Œèƒ½å¤Ÿè¡¨ç¤ºä»å…±åŒèµ·æºåˆ°å¤šä¸ªä¸åŒç»“æœçš„äººå£çº§åˆ«çš„åˆ†æ­§ã€‚ä¸ç°æœ‰çš„å•æ¨¡æ€è¿‡æ¸¡æ–¹æ³•ç›¸æ¯”ï¼ŒBranchSBMåœ¨å¤„ç†å¤šè·¯å¾„è¡¨é¢å¯¼èˆªã€ç»†èƒå‘½è¿åˆ†å‰å»ºæ¨¡ä»¥åŠæ¨¡æ‹Ÿç»†èƒå¯¹æ‰°åŠ¨çš„ä¸åŒååº”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚æ€»ä¹‹ï¼ŒBranchSBMä¸ºç”Ÿæˆå»ºæ¨¡æä¾›äº†æ›´ä¸°å¯Œçš„å·¥å…·ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ¼”åŒ–è¿‡ç¨‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-11.html",
    "link_next": "2025-06-13.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "11.06",
        "en": "06/11",
        "zh": "6æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é‡Šå†å²äº‹ä»¶æ—¶çš„åœ°ç¼˜æ”¿æ²»åè§ã€‚ç ”ç©¶æ¶‰åŠç¾å›½ã€è‹±å›½ã€è‹è”å’Œä¸­å›½ç­‰å›½å®¶ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«ä¸­ç«‹çš„äº‹ä»¶æè¿°å’Œä¸åŒå›½å®¶çš„å¯¹ç«‹è§‚ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹å€¾å‘äºç‰¹å®šçš„å›½å®¶å™äº‹ï¼Œç®€å•çš„å»åè§æ–¹æ³•æ•ˆæœæœ‰é™ã€‚å®éªŒè¿˜æ˜¾ç¤ºï¼Œæ¨¡å‹å¯¹å‚ä¸è€…æ ‡ç­¾çš„æ›´æ”¹éå¸¸æ•æ„Ÿï¼Œæœ‰æ—¶ä¼šæ”¾å¤§åè§æˆ–è¯†åˆ«ä¸ä¸€è‡´ã€‚",
        "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
        "pinyin": "è¿™ç¯‡æ–‡ç« è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é‡Šå†å²äº‹ä»¶æ—¶çš„åœ°ç¼˜æ”¿æ²»åè§ã€‚ç ”ç©¶æ¶‰åŠç¾å›½ã€è‹±å›½ã€è‹è”å’Œä¸­å›½ç­‰å›½å®¶ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«ä¸­ç«‹çš„äº‹ä»¶æè¿°å’Œä¸åŒå›½å®¶çš„å¯¹ç«‹è§‚ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹å€¾å‘äºç‰¹å®šçš„å›½å®¶å™äº‹ï¼Œç®€å•çš„å»åè§æ–¹æ³•æ•ˆæœæœ‰é™ã€‚å®éªŒè¿˜æ˜¾ç¤ºï¼Œæ¨¡å‹å¯¹å‚ä¸è€…æ ‡ç­¾çš„æ›´æ”¹éå¸¸æ•æ„Ÿï¼Œæœ‰æ—¶ä¼šæ”¾å¤§åè§æˆ–è¯†åˆ«ä¸ä¸€è‡´ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng pÃ­nggÅ« le dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i jiÄ›shÃ¬ lÃ¬shÇ shÃ¬jiÃ n shÃ­ de dÃ¬yuÇn zhÃ¨ngzhÃ¬ piÄnjiÃ n. YÃ¡njiÅ« shÃ¨yÃº MÄ›iguÃ³, YÄ«ngguÃ³, SÅ«liÃ¡n hÃ© ZhÅngguÃ³ dÄ›ng guÃ³jiÄ. WÃ©nzhÄng yÇnrÃ¹ le yÄ«gÃ¨ xÄ«n de shÃ¹jÃ¹jÃ­, bÄohÃ¡n zhÅnglÃ¬ de shÃ¬jiÃ n miÃ¡oshÃ¹ hÃ© bÃ¹tÃ³ng guÃ³jiÄ de duÃ¬lÃ¬ guÄndiÇn. YÃ¡njiÅ« fÄxiÃ n, mÃ³xÃ­ng qÄ«ngxiÃ ng yÃº tÃ¨dÃ¬ng de guÃ³jiÄ xÃ¹shÃ¬, jiÇndÄn de qÃ¹ piÄnjiÃ n fÄngfÇ xiÃ oguÇ’ yÇ’uxiÃ n. ShÃ­yÃ n hÃ¡i xiÇnshÃ¬, mÃ³xÃ­ng duÃ¬ cÄnyÃ¹zhÄ› biÄoqiÄn de gÄ“nggÇi fÄ“ichÃ¡ng mÇngÇn, yÇ’ushÃ­ huÃ¬ fÃ ngdÃ  piÄnjiÃ n huÃ² shÃ­biÃ© bÃ¹ yÄ«zhÃ¬.",
        "vocab": "[\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"å¤§å‹è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language models\"},\n    {\"word\": \"åœ°ç¼˜æ”¿æ²»\", \"pinyin\": \"dÃ¬yuÃ¡n zhÃ¨ngzhÃ¬\", \"trans\": \"geopolitical\"},\n    {\"word\": \"åè§\", \"pinyin\": \"piÄnjiÃ n\", \"trans\": \"bias\"},\n    {\"word\": \"æ¶‰åŠ\", \"pinyin\": \"shÃ¨jÃ­\", \"trans\": \"involve\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"ä¸­ç«‹\", \"pinyin\": \"zhÅnglÃ¬\", \"trans\": \"neutral\"},\n    {\"word\": \"å¯¹ç«‹\", \"pinyin\": \"duÃ¬lÃ¬\", \"trans\": \"opposing\"},\n    {\"word\": \"è§‚ç‚¹\", \"pinyin\": \"guÄndiÇn\", \"trans\": \"viewpoint\"},\n    {\"word\": \"å€¾å‘äº\", \"pinyin\": \"qÄ«ngxiÃ ngyÃº\", \"trans\": \"tend towards\"},\n    {\"word\": \"å™äº‹\", \"pinyin\": \"xÃ¹shÃ¬\", \"trans\": \"narrative\"},\n    {\"word\": \"ç®€å•\", \"pinyin\": \"jiÇndÄn\", \"trans\": \"simple\"},\n    {\"word\": \"å»åè§\", \"pinyin\": \"qÃ¹ piÄnjiÃ n\", \"trans\": \"debias\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"æ•ˆæœ\", \"pinyin\": \"xiÃ oguÇ’\", \"trans\": \"effect\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’uxiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"å‚ä¸è€…\", \"pinyin\": \"cÄnyÃ¹zhÄ›\", \"trans\": \"participant\"},\n    {\"word\": \"æ ‡ç­¾\", \"pinyin\": \"biÄoqiÄn\", \"trans\": \"label\"},\n    {\"word\": \"æ›´æ”¹\", \"pinyin\": \"gÄ“nggÇi\", \"trans\": \"change\"},\n    {\"word\": \"æ•æ„Ÿ\", \"pinyin\": \"mÇngÇn\", \"trans\": \"sensitive\"},\n    {\"word\": \"æ”¾å¤§\", \"pinyin\": \"fÃ ngdÃ \", \"trans\": \"amplify\"},\n    {\"word\": \"è¯†åˆ«\", \"pinyin\": \"shÃ­biÃ©\", \"trans\": \"identify\"},\n    {\"word\": \"ä¸ä¸€è‡´\", \"pinyin\": \"bÃ¹ yÄ«zhÃ¬\", \"trans\": \"inconsistent\"}\n]",
        "trans": "This article evaluates the geopolitical biases of large language models (LLMs) when interpreting historical events. The study involves countries such as the United States, the United Kingdom, the Soviet Union, and China. The article introduces a new dataset that includes neutral event descriptions and contrasting viewpoints from different countries. The research finds that the models tend to favor specific national narratives, and simple debiasing methods have limited effectiveness. The experiments also show that the models are highly sensitive to changes in participant labels, sometimes amplifying biases or identifying inconsistencies.",
        "update_ts": "2025-06-11 15:13"
    }
}