{
    "date": {
        "ru": "12 –∏—é–Ω—è",
        "en": "June 12",
        "zh": "6Êúà12Êó•"
    },
    "time_utc": "2025-06-12 13:27",
    "weekday": 3,
    "issue_id": 4260,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.06395",
            "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
            "url": "https://huggingface.co/papers/2506.06395",
            "abstract": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.",
            "score": 52,
            "issue_id": 4258,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 –∏—é–Ω—è",
                "en": "June 5",
                "zh": "6Êúà5Êó•"
            },
            "hash": "0bd4f12f6c85c81f",
            "authors": [
                "Pengyi Li",
                "Matvey Skripkin",
                "Alexander Zubrey",
                "Andrey Kuznetsov",
                "Ivan Oseledets"
            ],
            "affiliations": [
                "AIRI, Skoltech Moscow"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06395.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#alignment"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ú–µ—Ç–æ–¥ Reinforcement Learning via Self-Confidence (RLSC) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. RLSC —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ –∏–ª–∏ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ RLSC –∫ –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏–ª–æ –µ–µ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, —Ç—Ä–µ–±—É—è –ª–∏—à—å –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞."
                },
                "en": {
                    "title": "Boosting Model Accuracy with Self-Confidence Rewards",
                    "desc": "Reinforcement Learning via Self-Confidence (RLSC) is a novel approach that enhances the accuracy of large language models (LLMs) by utilizing the model's own confidence as a reward signal. This method eliminates the reliance on human labels or complex reward engineering, making it more efficient and scalable. RLSC has been tested on the Qwen2.5-Math-7B model, showing significant accuracy improvements across various math benchmarks with minimal training data. By requiring only a few samples and no labeled data, RLSC offers a straightforward solution for post-training alignment of LLMs with task objectives."
                },
                "zh": {
                    "title": "Ëá™‰ø°È©±Âä®ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊèêÂçáÊ®°ÂûãÂáÜÁ°ÆÊÄßÔºÅ",
                    "desc": "Âº∫ÂåñÂ≠¶‰π†ÈÄöËøáËá™‰ø°ÔºàRLSCÔºâÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑËá™‰ø°Â∫¶‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑Ôºå‰ªéËÄåÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇËøôÁßçÊñπÊ≥ï‰∏çÂÜç‰æùËµñ‰∫é‰∫∫Â∑•Ê†áÁ≠æÊàñÂ§ñÈÉ®Â•ñÂä±Ê®°ÂûãÔºåÁÆÄÂåñ‰∫ÜËÆ≠ÁªÉËøáÁ®ã„ÄÇRLSCÂú®Â§ö‰∏™Êï∞Â≠¶‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá„ÄÇËØ•ÊñπÊ≥ïÁÆÄÂçï‰∏îÂèØÊâ©Â±ïÔºåÂè™ÈúÄÂ∞ëÈáèÊ†∑Êú¨ÂíåÊó†Ê†áÁ≠æÁöÑÁõëÁù£Âç≥ÂèØÂÆûÁé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09113",
            "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "url": "https://huggingface.co/papers/2506.09113",
            "abstract": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
            "score": 38,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "d44df125bd718f2f",
            "authors": [
                "Yu Gao",
                "Haoyuan Guo",
                "Tuyen Hoang",
                "Weilin Huang",
                "Lu Jiang",
                "Fangyuan Kong",
                "Huixia Li",
                "Jiashi Li",
                "Liang Li",
                "Xiaojie Li",
                "Xunsong Li",
                "Yifu Li",
                "Shanchuan Lin",
                "Zhijie Lin",
                "Jiawei Liu",
                "Shu Liu",
                "Xiaonan Nie",
                "Zhiwu Qing",
                "Yuxi Ren",
                "Li Sun",
                "Zhi Tian",
                "Rui Wang",
                "Sen Wang",
                "Guoqiang Wei",
                "Guohong Wu",
                "Jie Wu",
                "Ruiqi Xia",
                "Fei Xiao",
                "Xuefeng Xiao",
                "Jiangqiao Yan",
                "Ceyuan Yang",
                "Jianchao Yang",
                "Runkai Yang",
                "Tao Yang",
                "Yihang Yang",
                "Zilyu Ye",
                "Xuejiao Zeng",
                "Yan Zeng",
                "Heng Zhang",
                "Yang Zhao",
                "Xiaozheng Zheng",
                "Peihao Zhu",
                "Jiaxin Zou",
                "Feilong Zuo"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.09113.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#rlhf",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –±—ã—Å—Ç—Ä–æ, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ",
                    "desc": "Seedance 1.0 - —ç—Ç–æ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –º–Ω–æ–≥–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤—É—é –∫—É—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —Å —Ç–æ—á–Ω—ã–º–∏ –≤–∏–¥–µ–æ–ø–æ–¥–ø–∏—Å—è–º–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–æ–≤–º–µ—Å—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∑–∞–¥–∞—á–∞–º —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤-–≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –≤–∫–ª—é—á–∞—è —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ–¥ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º –∏ –≤–∏–¥–µ–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ RLHF. –ë–ª–∞–≥–æ–¥–∞—Ä—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º, Seedance 1.0 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç ~10-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Seedance 1.0: Fast and High-Quality Video Generation Revolutionized",
                    "desc": "Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds."
                },
                "zh": {
                    "title": "Seedance 1.0ÔºöÈ´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "Seedance 1.0 ÊòØ‰∏ÄÁßçÈ´òÊÄßËÉΩÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂÖàËøõÁöÑÊï∞ÊçÆÊï¥ÁêÜ„ÄÅÊúâÊïàÁöÑÊû∂ÊûÑËÆæËÆ°„ÄÅÂêéËÆ≠ÁªÉ‰ºòÂåñÂíåÊ®°ÂûãÂä†ÈÄüÊäÄÊúØÔºåÊèê‰æõ‰∫ÜÂçìË∂äÁöÑË¥®ÈáèÂíåÈÄüÂ∫¶„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ§öÊ∫êÊï∞ÊçÆÊï¥ÁêÜÂíåÁ≤æÂáÜÁöÑËßÜÈ¢ëÂ≠óÂπïÔºåÂ¢ûÂº∫‰∫ÜÂØπÂ§öÊ†∑Âú∫ÊôØÁöÑÂÖ®Èù¢Â≠¶‰π†ËÉΩÂäõ„ÄÇÂÆÉÁöÑÈ´òÊïàÊû∂ÊûÑÊîØÊåÅÂ§öÈïúÂ§¥ÁîüÊàêÔºåÂπ∂ÂêåÊó∂Â≠¶‰π†ÊñáÊú¨Âà∞ËßÜÈ¢ëÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁöÑ‰ªªÂä°„ÄÇSeedance 1.0 ÈÄöËøáÂ§öÈò∂ÊÆµËí∏È¶èÁ≠ñÁï•ÂÆûÁé∞‰∫ÜÁ∫¶10ÂÄçÁöÑÊé®ÁêÜÂä†ÈÄüÔºåËÉΩÂ§üÂú®41.4ÁßíÂÜÖÁîüÊàê5ÁßíÁöÑ1080pËßÜÈ¢ë„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09995",
            "title": "PlayerOne: Egocentric World Simulator",
            "url": "https://huggingface.co/papers/2506.09995",
            "abstract": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.",
            "score": 23,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "e7cba5eb3e340a0f",
            "authors": [
                "Yuanpeng Tu",
                "Hao Luo",
                "Xi Chen",
                "Xiang Bai",
                "Fan Wang",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "HKU",
                "HUST",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09995.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#games"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å: —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –º–∏—Ä–∞ —Å PlayerOne",
                    "desc": "PlayerOne - —ç—Ç–æ –ø–µ—Ä–≤—ã–π —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–∏–º—É–ª—è—Ç–æ—Ä —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –º–∏—Ä–∞, —Å–ø–æ—Å–æ–±–Ω—ã–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–Ω—è—Ç—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è, –Ω–∞—á–∏–Ω–∞—è —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. PlayerOne –ø—Ä–∏–º–µ–Ω—è–µ—Ç —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –¥–≤–∏–∂–µ–Ω–∏—è–º–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω—ã –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –∏ –∏–º–µ–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Revolutionizing Egocentric World Simulation with PlayerOne",
                    "desc": "PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments."
                },
                "zh": {
                    "title": "ÂºÄÂàõËá™Êàë‰∏≠ÂøÉÁé∞ÂÆû‰∏ñÁïåÊ®°ÊãüÁöÑÊñ∞Á∫™ÂÖÉ",
                    "desc": "PlayerOneÊòØ‰∏Ä‰∏™‰ª•Ëá™Êàë‰∏∫‰∏≠ÂøÉÁöÑÁé∞ÂÆû‰∏ñÁïåÊ®°ÊãüÂô®ÔºåËÉΩÂ§üÊ†πÊçÆÁî®Êà∑ÊçïÊçâÁöÑÂõæÂÉèÊûÑÂª∫ÂíåÁîüÊàêËßÜÈ¢ë„ÄÇÂÆÉÈááÁî®Á≤óÂà∞ÁªÜÁöÑËÆ≠ÁªÉÊµÅÁ®ãÔºåÈ¶ñÂÖàÂú®Â§ßËßÑÊ®°ÁöÑËá™Êàë‰∏≠ÂøÉÊñáÊú¨-ËßÜÈ¢ëÂØπ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂú®ÂêåÊ≠•ËøêÂä®-ËßÜÈ¢ëÊï∞ÊçÆ‰∏äËøõË°åÂæÆË∞É„ÄÇËØ•Á≥ªÁªüËÆæËÆ°‰∫ÜÈÉ®ÂàÜËß£ËÄ¶ÁöÑËøêÂä®Ê≥®ÂÖ•ÊñπÊ°àÔºå‰ª•ÂÆûÁé∞ÂØπÈÉ®ÂàÜËøêÂä®ÁöÑÁ≤æÁ°ÆÊéßÂà∂ÔºåÂπ∂ÈÄöËøáËÅîÂêàÈáçÂª∫Ê°ÜÊû∂Á°Æ‰øùÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÂú∫ÊôØ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPlayerOneÂú®ÊéßÂà∂‰∫∫Á±ªËøêÂä®ÂíåÂª∫Ê®°Â§öÊ†∑Âú∫ÊôØÊñπÈù¢ÂÖ∑ÊúâÂá∫Ëâ≤ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09350",
            "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
            "url": "https://huggingface.co/papers/2506.09350",
            "abstract": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2",
            "score": 23,
            "issue_id": 4256,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "f2ebb5225d0061fa",
            "authors": [
                "Shanchuan Lin",
                "Ceyuan Yang",
                "Hao He",
                "Jianwen Jiang",
                "Yuxi Ren",
                "Xin Xia",
                "Yang Zhao",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09350.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#video",
                    "#architecture"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é AAPT",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –∞–≤—Ç–æ—Ä–µgressiv–Ω–æ–≥–æ adversarial post-training (AAPT) –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –≤–∏–¥–µ–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–∞–¥—Ä—ã –∞–≤—Ç–æ—Ä–µgress–∏–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ (1NFE) –Ω–∞ –∫–∞–¥—Ä. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –ø–µ—Ä–µ–¥–∞—á—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ–ª—É—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –æ—Ç–∫–ª–∏–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 8 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å —á–∞—Å—Ç–æ—Ç–æ–π 24 –∫–∞–¥—Ä–∞ –≤ —Å–µ–∫—É–Ω–¥—É –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º –¥–æ 1280x720 –Ω–∞ 8 GPU H100."
                },
                "en": {
                    "title": "Real-Time Video Generation Made Easy!",
                    "desc": "This paper introduces a method called autoregressive adversarial post-training (AAPT) to enhance pre-trained latent video diffusion models for real-time video generation. The AAPT approach allows the model to generate video frames one at a time, using a single neural function evaluation, which significantly reduces computational demands. By incorporating adversarial training, the model improves its efficiency and reduces errors during the generation of longer videos. The results show that the model can produce high-quality video at 24 frames per second, making it suitable for interactive applications."
                },
                "zh": {
                    "title": "ÂÆûÊó∂‰∫§‰∫íËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÂõûÂΩíÂØπÊäóÂêéËÆ≠ÁªÉÔºàAAPTÔºâÊñπÊ≥ïÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÊΩúÂú®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãËΩ¨Âèò‰∏∫ÂÆûÊó∂‰∫§‰∫íÂºèËßÜÈ¢ëÁîüÊàêÂô®„ÄÇËØ•Ê®°ÂûãÈÄöËøáËá™ÂõûÂΩíÊñπÂºè‰∏ÄÊ¨°ÁîüÊàê‰∏Ä‰∏™ÊΩúÂú®Â∏ßÔºå‰ΩøÁî®ÂçïÊ¨°Á•ûÁªèÁΩëÁªúÂáΩÊï∞ËØÑ‰º∞Ôºà1NFEÔºâÔºåÂÆûÁé∞ÂÆûÊó∂ÊµÅÂºè‰º†Ëæì„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®ÂØπÊäóËÆ≠ÁªÉ‰Ωú‰∏∫Ëá™ÂõûÂΩíÁîüÊàêÁöÑÊúâÊïàËåÉÂºèÔºåËÆæËÆ°Âá∫Êõ¥È´òÊïàÁöÑÊû∂ÊûÑÔºåÂáèÂ∞ëÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËØØÂ∑ÆÁßØÁ¥Ø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑ8BÊ®°ÂûãÂú®Âçï‰∏™H100‰∏äÂÆûÁé∞‰∫Ü736x416ÂàÜËæ®ÁéáÁöÑÂÆûÊó∂24fpsËßÜÈ¢ëÁîüÊàêÔºåÊàñÂú®8‰∏™H100‰∏äÁîüÊàê1280x720ÂàÜËæ®ÁéáÁöÑËßÜÈ¢ëÔºåÊúÄÈïøÂèØËææ‰∏ÄÂàÜÈíüÔºà1440Â∏ßÔºâ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09790",
            "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
            "url": "https://huggingface.co/papers/2506.09790",
            "abstract": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.",
            "score": 21,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "6fb3fee31c3739a5",
            "authors": [
                "Zhenran Xu",
                "Yiyu Wang",
                "Xue Yang",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce, China",
                "Harbin Institute of Technology (Shenzhen), China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09790.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "ComfyUI-R1: –ò–ò-—Ö—É–¥–æ–∂–Ω–∏–∫ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è",
                    "desc": "ComfyUI-R1 - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤ —Å—Ñ–µ—Ä–µ –ò–ò-–∏—Å–∫—É—Å—Å—Ç–≤–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. ComfyUI-R1 –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 4000 —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞: —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ 7-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞—è –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º."
                },
                "en": {
                    "title": "Automating AI Art Workflows with ComfyUI-R1",
                    "desc": "ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities."
                },
                "zh": {
                    "title": "ComfyUI-R1ÔºöËá™Âä®ÂåñÂ∑•‰ΩúÊµÅÁîüÊàêÁöÑÊé®ÁêÜÊ®°Âûã",
                    "desc": "ComfyUI-R1 ÊòØ‰∏Ä‰∏™Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éËá™Âä®ÂåñÂ∑•‰ΩúÊµÅÁîüÊàêÔºåÁâπÂà´ÊòØÂú® AI Ëâ∫ÊúØÂàõ‰Ωú‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉÈÄöËøáÈïøÈìæÊé®ÁêÜÂíåÂº∫ÂåñÂ≠¶‰π†ÔºåÂ∏ÆÂä©Áî®Êà∑ÂàõÂª∫ÂÆöÂà∂ÂåñÁöÑÂ∑•‰ΩúÊµÅÔºåÈôç‰Ωé‰∫ÜÂ≠¶‰π†Êõ≤Á∫ø„ÄÇËØ•Ê®°Âûã‰ΩøÁî®‰∫Ü 4K Â∑•‰ΩúÊµÅÁöÑÊï∞ÊçÆÈõÜÔºåÁªèËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÔºåÁ°Æ‰øù‰∫ÜÂ∑•‰ΩúÊµÅÁöÑÊ†ºÂºèÊúâÊïàÊÄßÂíåÁªìÊûÑÂÆåÊï¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåComfyUI-R1 Âú®Ê†ºÂºèÊúâÊïàÊÄßÂíå F1 ÂàÜÊï∞‰∏äÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÈïøÈìæÊé®ÁêÜÂú® AI Ëâ∫ÊúØÂàõ‰Ωú‰∏≠ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08889",
            "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
            "url": "https://huggingface.co/papers/2506.08889",
            "abstract": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
            "score": 16,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "a6e46d58b91f0fad",
            "authors": [
                "Yizhao Gao",
                "Shuming Guo",
                "Shijie Cao",
                "Yuqing Xia",
                "Yu Cheng",
                "Lei Wang",
                "Lingxiao Ma",
                "Yutao Sun",
                "Tianzhu Ye",
                "Li Dong",
                "Hayden Kwok-Hay So",
                "Yu Hua",
                "Ting Cao",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Microsoft Research",
                "Peking University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08889.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "SeerAttention-R - —ç—Ç–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–¥–µ—Ä —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Å–µ–≥–æ 0,4 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ—á—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä—å —Å –±—é–¥–∂–µ—Ç–æ–º –≤ 4000 —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ—Å—Ç–µ AIME –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –±–ª–æ–∫–æ–≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—è TileLang, –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≤—ã—Å–æ–∫–æ–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —è–¥—Ä–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–æ—á—Ç–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 9 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention-3 –Ω–∞ GPU H100 –ø—Ä–∏ 90% —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏. SeerAttention-R –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
                },
                "en": {
                    "title": "SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models",
                    "desc": "SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware."
                },
                "zh": {
                    "title": "SeerAttention-RÔºöÈ´òÊïàÁ®ÄÁñèÊ≥®ÊÑèÂäõÊé®ÁêÜÊ°ÜÊû∂",
                    "desc": "SeerAttention-RÊòØ‰∏ÄÁßçÁ®ÄÁñèÊ≥®ÊÑèÂäõÊ°ÜÊû∂Ôºå‰∏ì‰∏∫Êé®ÁêÜÊ®°ÂûãÁöÑÈïøËß£Á†ÅËÄåËÆæËÆ°„ÄÇÂÆÉÈÄöËøáËá™Ëí∏È¶èÈó®ÊéßÊú∫Âà∂Â≠¶‰π†Ê≥®ÊÑèÂäõÁ®ÄÁñèÊÄßÔºåÂêåÊó∂ÂéªÈô§‰∫ÜÊü•ËØ¢Ê±†ÂåñÔºå‰ª•ÈÄÇÂ∫îËá™ÂõûÂΩíËß£Á†Å„ÄÇËØ•Ê°ÜÊû∂ËΩªÈáè‰∏îÁÅµÊ¥ªÔºåÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏≠ÔºåËÄåÊó†ÈúÄ‰øÆÊîπÂéüÂßãÂèÇÊï∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåSeerAttention-RÂú®AIMEÂü∫ÂáÜÊµãËØï‰∏≠‰ª•4K‰ª§ÁâåÈ¢ÑÁÆó‰øùÊåÅÊé•ËøëÊó†ÊçüÁöÑÊé®ÁêÜÂáÜÁ°ÆÊÄßÔºåÂπ∂Âú®H100 GPU‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ9ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08570",
            "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
            "url": "https://huggingface.co/papers/2506.08570",
            "abstract": "A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
            "score": 14,
            "issue_id": 4260,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "1ad6cb6752b933a6",
            "authors": [
                "Or Tal",
                "Felix Kreuk",
                "Yossi Adi"
            ],
            "affiliations": [
                "mail.huji.ac.il",
                "meta.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08570.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#synthetic",
                    "#architecture",
                    "#training",
                    "#audio"
                ],
                "emoji": "üéµ",
                "ru": {
                    "title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º: –∫–ª—é—á –∫ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–∞—Ä–∞–¥–∏–≥–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —É—Å–ª–æ–≤–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –≤–∫–ª—é—á–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º –≤—ã–≤–æ–¥–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤—ã—è–≤–ª—è—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –≤ –ø—Ä–∏–Ω—è—Ç–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏ –æ–±—É—á–∞—é—â–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –±—É–¥—É—â–µ–º."
                },
                "en": {
                    "title": "Decoding the Future of Music Generation: A Paradigm Comparison",
                    "desc": "This paper systematically compares two popular modeling paradigms in text-to-music generation: Auto-Regressive decoding and Conditional Flow-Matching. The authors focus on how these paradigms affect performance by conducting controlled experiments with identical datasets and training setups. They evaluate the models on various criteria, including generation quality, robustness, and editing capabilities. The findings reveal the unique strengths and weaknesses of each approach, providing valuable insights for future developments in music generation systems."
                },
                "zh": {
                    "title": "ÊØîËæÉËá™ÂõûÂΩí‰∏éÊù°‰ª∂ÊµÅÂåπÈÖçÔºöÊñáÊú¨Âà∞Èü≥‰πêÁîüÊàêÁöÑÊú™Êù•",
                    "desc": "Êú¨ÊñáÁ≥ªÁªüÊØîËæÉ‰∫ÜËá™ÂõûÂΩíËß£Á†ÅÂíåÊù°‰ª∂ÊµÅÂåπÈÖçÂú®ÊñáÊú¨Âà∞Èü≥‰πêÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÊè≠Á§∫‰∫ÜÂêÑËá™ÁöÑ‰ºòÂäøÂíåÂ±ÄÈôêÊÄß„ÄÇÁ†îÁ©∂ÈõÜ‰∏≠Âú®Âª∫Ê®°ËåÉÂºè‰∏äÔºåÈÄöËøáÁõ∏ÂêåÁöÑÊï∞ÊçÆÈõÜÂíåËÆ≠ÁªÉÈÖçÁΩÆÂØπÊ®°ÂûãËøõË°åÊéßÂà∂ÊØîËæÉ„ÄÇËØÑ‰º∞ÊåáÊ†áÂåÖÊã¨ÁîüÊàêË¥®Èáè„ÄÅÂØπÊé®ÁêÜÈÖçÁΩÆÁöÑÈ≤ÅÊ£íÊÄß„ÄÅÂèØÊâ©Â±ïÊÄß‰ª•ÂèäÂØπÊñáÊú¨ÂíåÊó∂Èó¥ÂØπÈΩêÊù°‰ª∂ÁöÑÈÅµÂæ™ËÉΩÂäõ„ÄÇÊ≠§Á†îÁ©∂‰∏∫Êú™Êù•ÊñáÊú¨Âà∞Èü≥‰πêÁîüÊàêÁ≥ªÁªüÁöÑÊû∂ÊûÑÂíåËÆ≠ÁªÉÂÜ≥Á≠ñÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09003",
            "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
            "url": "https://huggingface.co/papers/2506.09003",
            "abstract": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).",
            "score": 13,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "794ea1a282cfa727",
            "authors": [
                "Lei Zhang",
                "Jiaxi Yang",
                "Min Yang",
                "Jian Yang",
                "Mouxiang Chen",
                "Jiajun Zhang",
                "Zeyu Cui",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",
                "University of Science and Technology of China, Hefei, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09003.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "üß™",
                "ru": {
                    "title": "SWE-Flow: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è TDD –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏",
                    "desc": "SWE-Flow - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —á–µ—Ä–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (TDD). –û–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–≤–æ–¥–∏—Ç —ç—Ç–∞–ø—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –º–æ–¥—É–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å –ø–æ–º–æ—â—å—é –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (RDG). SWE-Flow –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–∞—Å—Ç–∏—á–Ω—É—é –∫–æ–¥–æ–≤—É—é –±–∞–∑—É, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ TDD-–∑–∞–¥–∞—á–∞—Ö."
                },
                "en": {
                    "title": "Automating TDD with SWE-Flow: Smarter Development Schedules",
                    "desc": "SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding."
                },
                "zh": {
                    "title": "SWE-FlowÔºöËá™Âä®ÂåñÊµãËØïÈ©±Âä®ÂºÄÂèëÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "SWE-FlowÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÂü∫‰∫éÊµãËØïÈ©±Âä®ÂºÄÂèëÔºàTDDÔºâÊñπÊ≥ï„ÄÇÂÆÉÈÄöËøáËá™Âä®Êé®Êñ≠ÂçïÂÖÉÊµãËØï‰∏≠ÁöÑÂºÄÂèëÊ≠•È™§ÔºåÁîüÊàêÁªìÊûÑÂåñÁöÑÂºÄÂèëËÆ°ÂàíÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÂú®ÁúüÂÆûÈ°πÁõÆ‰∏äÂæÆË∞ÉÂºÄÊîæÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇSWE-FlowÁöÑÊ†∏ÂøÉÊòØÊûÑÂª∫ËøêË°åÊó∂‰æùËµñÂõæÔºàRDGÔºâÔºåÂáÜÁ°ÆÊçïÊçâÂáΩÊï∞‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåÁ°Æ‰øùÊØè‰∏ÄÊ≠•ÁîüÊàêÈÉ®ÂàÜ‰ª£Á†ÅÂ∫ìÂèäÁõ∏Â∫îÁöÑÂçïÂÖÉÊµãËØï„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊàë‰ª¨‰ªéÁúüÂÆûÁöÑGitHubÈ°πÁõÆ‰∏≠ÁîüÊàê‰∫ÜÂ§ßÈáèÁöÑËÆ≠ÁªÉÂíåÊµãËØïÂÆû‰æãÔºåÊòæËëóÊèêÂçá‰∫ÜÂü∫‰∫éTDDÁöÑÁºñÁ†ÅÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09984",
            "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
            "url": "https://huggingface.co/papers/2506.09984",
            "abstract": "A novel framework for end-to-end human animation with multi-modal conditions enables high-quality video generation through explicit layout control and region-specific modality matching.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.",
            "score": 10,
            "issue_id": 4252,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "97a59a9be9ba0dc3",
            "authors": [
                "Zhenzhi Wang",
                "Jiaqi Yang",
                "Jianwen Jiang",
                "Chao Liang",
                "Gaojie Lin",
                "Zerong Zheng",
                "Ceyuan Yang",
                "Dahua Lin"
            ],
            "affiliations": [
                "ByteDance",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09984.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ª—é–¥–µ–π —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—é–¥–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –º–æ–∂–µ—Ç –∞–Ω–∏–º–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—É–±—ä–µ–∫—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, —É—á–∏—Ç—ã–≤–∞—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–∏–¥–µ–æ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏."
                },
                "en": {
                    "title": "Revolutionizing Human Animation with Multi-Modal Control",
                    "desc": "This paper presents a new framework for creating human animations that can incorporate multiple types of input, such as text, images, and audio. Unlike previous methods that only animate one subject at a time, this approach allows for complex interactions between multiple characters and objects in a video. The framework uses a mask predictor to accurately match visual elements to specific regions in the video, ensuring that each character's appearance is correctly represented. Additionally, it integrates audio cues in a way that aligns with the visual layout, resulting in high-quality, controllable animations that reflect rich human interactions."
                },
                "zh": {
                    "title": "Â§öÊ®°ÊÄÅÊù°‰ª∂‰∏ãÁöÑÈ´òË¥®Èáè‰∫∫Á±ªÂä®ÁîªÁîüÊàê",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ´ØÂà∞Á´Ø‰∫∫Á±ªÂä®ÁîªÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®Â§öÊ®°ÊÄÅÊù°‰ª∂‰∏ãÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ë„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåËØ•Ê°ÜÊû∂ÊîØÊåÅÂ§ö‰∏™Ê¶ÇÂøµÁöÑÁ≤æÁ°ÆÊéßÂà∂ÔºåÂÖÅËÆ∏‰∫∫Á±ª‰∏éÁâ©‰Ωì‰πãÈó¥ÁöÑ‰∏∞ÂØå‰∫§‰∫í„ÄÇÈÄöËøáÂå∫ÂüüÁâπÂÆöÁöÑÊù°‰ª∂ÁªëÂÆöÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üËá™Âä®Êé®Êñ≠Â∏ÉÂ±Ä‰ø°ÊÅØÔºåÂπ∂Á°Æ‰øù‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂåπÈÖç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Â§öÊ®°ÊÄÅÊù°‰ª∂‰∏ãÁöÑÊòæÂºèÂ∏ÉÂ±ÄÊéßÂà∂‰ºò‰∫éÁé∞ÊúâÁöÑÈöêÂºèÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09937",
            "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2506.09937",
            "abstract": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  \t\t\t\t\tAI-generated summary \t\t\t\t While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.",
            "score": 3,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 –∏—é–Ω—è",
                "en": "June 11",
                "zh": "6Êúà11Êó•"
            },
            "hash": "2a59fafef3ba118c",
            "authors": [
                "Qiao Gu",
                "Yuanliang Ju",
                "Shengxiang Sun",
                "Igor Gilitschenski",
                "Haruki Nishimura",
                "Masha Itkina",
                "Florian Shkurti"
            ],
            "affiliations": [
                "Toyota Research Institute (TRI)",
                "University of Toronto (UofT)",
                "UofT Robotics Institute",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09937.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#optimization",
                    "#agents",
                    "#robotics",
                    "#agi"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "SAFE: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤-–≥–µ–Ω–µ—Ä–∞–ª–∏—Å—Ç–æ–≤",
                    "desc": "SAFE - —ç—Ç–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA), –∫–æ—Ç–æ—Ä—ã–π –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏, –æ–±—É—á–∞—è—Å—å –Ω–∞ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ VLA –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç, —á—Ç–æ —ç—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ–± —É—Å–ø–µ—Ö–µ –∏ –Ω–µ—É–¥–∞—á–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ–±—â–∏–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. SAFE –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–∞—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ–≤–∏–¥–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –î–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –≤—Ä–µ–º–µ–Ω–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è."
                },
                "en": {
                    "title": "SAFE: Generalizing Failure Detection for Vision-Language-Action Models",
                    "desc": "SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods."
                },
                "zh": {
                    "title": "SAFEÔºöÊô∫ËÉΩÊú∫Âô®‰∫∫ÊïÖÈöúÊ£ÄÊµãÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "SAFEÊòØ‰∏Ä‰∏™Áî®‰∫éËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®Ê®°ÂûãÁöÑÊïÖÈöúÊ£ÄÊµãÂô®ÔºåÂÆÉËÉΩÂ§üÈÄöËøáÂ≠¶‰π†Ê®°ÂûãÁöÑÈ´òÂ±ÇÂÜÖÈÉ®ÁâπÂæÅÊù•Êé®ÂπøÂà∞Êú™ËßÅËøáÁöÑ‰ªªÂä°„ÄÇÂ∞ΩÁÆ°ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®Ê®°ÂûãÂú®Â§öÁßçÊìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êñ∞‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéáÊúâÈôê„ÄÇ‰∏∫‰∫ÜËÆ©Êú∫Âô®‰∫∫ÂÆâÂÖ®Âú∞‰∏éÁéØÂ¢É‰∫íÂä®ÔºåÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™ËÉΩÂ§üÂèäÊó∂ÂèëÂá∫Ë≠¶Êä•ÁöÑÊïÖÈöúÊ£ÄÊµãÂô®Ôºå‰ª•‰æøÊú∫Âô®‰∫∫ÂèØ‰ª•ÂÅúÊ≠¢„ÄÅÂõûÊ∫ØÊàñËØ∑Ê±ÇÂ∏ÆÂä©„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑSAFEËÉΩÂ§ü‰ªéVLAÁöÑÂÜÖÈÉ®ÁâπÂæÅ‰∏≠Â≠¶‰π†ÔºåÂπ∂È¢ÑÊµã‰ªªÂä°Â§±Ë¥•ÁöÑÂèØËÉΩÊÄßÔºåÁªèËøáÂπøÊ≥õÊµãËØïÔºåÊòæÁ§∫Âá∫‰ºòË∂äÁöÑÊïÖÈöúÊ£ÄÊµãÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08001",
            "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
            "url": "https://huggingface.co/papers/2506.08001",
            "abstract": "A new reParameterized training algorithm named POET uses Orthogonal Equivalence Transformation to optimize neurons, providing stable optimization and improved generalization for training large-scale neural networks including LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
            "score": 2,
            "issue_id": 4255,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 –∏—é–Ω—è",
                "en": "June 9",
                "zh": "6Êúà9Êó•"
            },
            "hash": "b47d919d8313c30a",
            "authors": [
                "Zeju Qiu",
                "Simon Buchholz",
                "Tim Z. Xiao",
                "Maximilian Dax",
                "Bernhard Sch√∂lkopf",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Max Planck Institute for Intelligent Systems, T√ºbingen",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08001.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "POET: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "POET - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤. –û–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. POET —Ä–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–µ—Ç –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö –æ–±—É—á–∞–µ–º—ã—Ö –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–ª—É—á–∞–π–Ω–æ–π –≤–µ—Å–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å POET –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "POET: Optimizing Neurons for Stable and Scalable Training",
                    "desc": "The paper introduces POET, a new training algorithm designed to enhance the optimization of neurons in large-scale neural networks, particularly large language models (LLMs). POET employs Orthogonal Equivalence Transformation, which reparameterizes neurons using two learnable orthogonal matrices alongside a fixed random weight matrix. This method ensures the stability of the optimization process and improves the generalization capabilities of the models. The authors also present efficient approximations that allow POET to be flexible and scalable, demonstrating its effectiveness through extensive experiments."
                },
                "zh": {
                    "title": "POETÔºö‰ºòÂåñÁ•ûÁªèÂÖÉÁöÑÈáçÂèÇÊï∞ÂåñËÆ≠ÁªÉÊñ∞ÁÆóÊ≥ï",
                    "desc": "POETÊòØ‰∏ÄÁßçÊñ∞ÁöÑÈáçÂèÇÊï∞ÂåñËÆ≠ÁªÉÁÆóÊ≥ïÔºåÂà©Áî®Ê≠£‰∫§Á≠â‰ª∑ÂèòÊç¢Êù•‰ºòÂåñÁ•ûÁªèÂÖÉ„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøá‰ΩøÁî®‰∏§‰∏™ÂèØÂ≠¶‰π†ÁöÑÊ≠£‰∫§Áü©ÈòµÂíå‰∏Ä‰∏™Âõ∫ÂÆöÁöÑÈöèÊú∫ÊùÉÈáçÁü©ÈòµÔºåÂØπÊØè‰∏™Á•ûÁªèÂÖÉËøõË°åÈáçÂèÇÊï∞Âåñ„ÄÇPOETËÉΩÂ§üÁ®≥ÂÆöÂú∞‰ºòÂåñÁõÆÊ†áÂáΩÊï∞ÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁâπÂà´ÈÄÇÁî®‰∫éÂ§ßËßÑÊ®°Á•ûÁªèÁΩëÁªúÁöÑËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫ÜPOETÂú®ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08900",
            "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
            "url": "https://huggingface.co/papers/2506.08900",
            "abstract": "MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE.",
            "score": 1,
            "issue_id": 4258,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "d3860686fd37639a",
            "authors": [
                "Jos√© Morano",
                "Botond Fazekas",
                "Emese S√ºkei",
                "Ronald Fecso",
                "Taha Emre",
                "Markus Gumpinger",
                "Georg Faustmann",
                "Marzieh Oghbaie",
                "Ursula Schmidt-Erfurth",
                "Hrvoje Bogunoviƒá"
            ],
            "affiliations": [
                "Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria",
                "Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria",
                "OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08900.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#science",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "üëÅÔ∏è",
                "ru": {
                    "title": "MIRAGE: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ò–ò-–∞–Ω–∞–ª–∏–∑–µ –æ—Ñ—Ç–∞–ª—å–º–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "MIRAGE - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–æ—Å–Ω–æ–≤–∞ (foundation model) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –û–ö–¢ –∏ –°–õ–û –≤ –æ—Ñ—Ç–∞–ª—å–º–æ–ª–æ–≥–∏–∏. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—â–∏–µ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. MIRAGE –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –ª—É—á—à–µ –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–æ—Å—Ç—É–ø–µ–Ω –ø—É–±–ª–∏—á–Ω–æ –≤–º–µ—Å—Ç–µ —Å —Å–∞–º–æ–π MIRAGE."
                },
                "en": {
                    "title": "MIRAGE: Revolutionizing Ophthalmic Image Analysis with Multimodal AI",
                    "desc": "MIRAGE is a multimodal foundation model designed to improve the classification and segmentation of ophthalmic images, specifically optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) images. It addresses the limitations of existing AI models that often require extensive labeled data and struggle with unseen datasets. By leveraging large amounts of unlabeled data, MIRAGE outperforms both general and specialized models in various tasks. The paper also introduces a new evaluation benchmark for OCT and SLO, demonstrating MIRAGE's effectiveness and potential for advancing AI in retinal image analysis."
                },
                "zh": {
                    "title": "MIRAGEÔºöÁúºÁßëÂõæÂÉèÂàÜÊûêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "MIRAGEÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÂÖâÂ≠¶Áõ∏Âπ≤Êñ≠Â±ÇÊâ´ÊèèÔºàOCTÔºâÂíåÊâ´ÊèèÊøÄÂÖâÁúºÂ∫ïÁÖßÁõ∏ÔºàSLOÔºâÂõæÂÉèÁöÑÂàÜÁ±ªÂíåÂàÜÂâ≤„ÄÇËØ•Ê®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÈÄöÁî®Âíå‰∏ì‰∏öÊ®°ÂûãÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÊ®°ÂûãÂú®Áã¨Á´ãÊú™ËßÅÊï∞ÊçÆ‰∏äÁöÑÊÄßËÉΩ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇMIRAGEÈÄöËøáÂú®Â§ßËßÑÊ®°Êú™Ê†áËÆ∞Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÂÖãÊúç‰∫ÜÂØπÂ§ßÈáèÊ†áÊ≥®ÁöÑ‰æùËµñÔºåÂπ∂‰∏îÂú®ÂàÜÂâ≤‰ªªÂä°‰∏äËøõË°å‰∫ÜÂπøÊ≥õÈ™åËØÅ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËØÑ‰º∞Âü∫ÂáÜÔºåËøõ‰∏ÄÊ≠•ËØÅÊòé‰∫ÜMIRAGEÂú®ÁúºÂ∫ïOCTÂõæÂÉèÂàÜÊûê‰∏≠ÁöÑÈÄÇÁî®ÊÄßÂíå‰ºòË∂äÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09007",
            "title": "Branched Schr√∂dinger Bridge Matching",
            "url": "https://huggingface.co/papers/2506.09007",
            "abstract": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.",
            "score": 0,
            "issue_id": 4252,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 –∏—é–Ω—è",
                "en": "June 10",
                "zh": "6Êúà10Êó•"
            },
            "hash": "8f3c4a6be505cd98",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Computer Science, Duke University",
                "Department of Computer and Information Science, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "Universit√© de Montr√©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09007.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#dataset",
                    "#data"
                ],
                "emoji": "üå≥",
                "ru": {
                    "title": "BranchSBM: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–≤–µ—Ç–≤–ª–µ–Ω–Ω—ã—Ö –ø—É—Ç–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "BranchSBM - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —Ä–∞—Å—à–∏—Ä—è—é—â–∞—è –º–µ—Ç–æ–¥ Schr√∂dinger Bridge Matching –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–≤–µ—Ç–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—É—Ç–µ–π. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —ç–≤–æ–ª—é—Ü–∏—é –æ—Ç –æ–¥–Ω–æ–≥–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–æ–Ω–µ—á–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º. BranchSBM –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≤–∏—Å—è—â–∏—Ö –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–ª–µ–π —Å–∫–æ—Ä–æ—Å—Ç–µ–π –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–æ—Å—Ç–∞, —á—Ç–æ –¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ø—É–ª—è—Ü–∏–∏ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω–µ—á–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π –ø–æ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø—É—Ç—è–º, –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –±–∏—Ñ—É—Ä–∫–∞—Ü–∏–π –∫–ª–µ—Ç–æ—á–Ω—ã—Ö —Å—É–¥–µ–± –∏ —Å–∏–º—É–ª—è—Ü–∏–µ–π —Ä–∞—Å—Ö–æ–¥—è—â–∏—Ö—Å—è –∫–ª–µ—Ç–æ—á–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏–π –Ω–∞ –≤–æ–∑–º—É—â–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Branching Out: Modeling Multiple Outcomes with BranchSBM",
                    "desc": "BranchSBM is a new framework in generative modeling that enhances the traditional Schr\"odinger Bridge Matching by allowing for branched stochastic paths. Unlike previous methods that only model single paths between two distributions, BranchSBM can represent multiple outcomes from a single starting point. This is achieved by using multiple time-dependent velocity fields and growth processes, which capture the complexity of population-level divergence. The framework is particularly useful for applications like simulating cell fate decisions and navigating multi-path scenarios."
                },
                "zh": {
                    "title": "ÂàÜÊîØËñõÂÆöË∞îÊ°•ÂåπÈÖçÔºöÊçïÊçâÂ§öË∑ØÂæÑÊºîÂåñÁöÑÁîüÊàêÂª∫Ê®°Êñ∞Ê°ÜÊû∂",
                    "desc": "BranchSBMÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÁîüÊàêÂª∫Ê®°Ê°ÜÊû∂ÔºåÊâ©Â±ï‰∫ÜËñõÂÆöË∞îÊ°•ÂåπÈÖçÊñπÊ≥ïÔºå‰ª•Âª∫Ê®°‰ªéÂçï‰∏ÄÂàùÂßãÂàÜÂ∏ÉÂà∞Â§ö‰∏™ÁªìÊûúÁöÑÂàÜÊîØÈöèÊú∫Ë∑ØÂæÑÂíåÂ§öË∑ØÂæÑÊºîÂåñ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂèÇÊï∞ÂåñÂ§ö‰∏™Êó∂Èó¥‰æùËµñÁöÑÈÄüÂ∫¶Âú∫ÂíåÁîüÈïøËøáÁ®ãÔºåËÉΩÂ§üË°®Á§∫‰ªéÂÖ±ÂêåËµ∑Ê∫êÂà∞Â§ö‰∏™‰∏çÂêåÁªìÊûúÁöÑ‰∫∫Âè£Á∫ßÂà´ÁöÑÂàÜÊ≠ß„ÄÇ‰∏éÁé∞ÊúâÁöÑÂçïÊ®°ÊÄÅËøáÊ∏°ÊñπÊ≥ïÁõ∏ÊØîÔºåBranchSBMÂú®Â§ÑÁêÜÂ§öË∑ØÂæÑË°®Èù¢ÂØºËà™„ÄÅÁªÜËÉûÂëΩËøêÂàÜÂèâÂª∫Ê®°‰ª•ÂèäÊ®°ÊãüÁªÜËÉûÂØπÊâ∞Âä®ÁöÑ‰∏çÂêåÂèçÂ∫îÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥Âº∫ÁöÑË°®ËææËÉΩÂäõ„ÄÇÊÄª‰πãÔºåBranchSBM‰∏∫ÁîüÊàêÂª∫Ê®°Êèê‰æõ‰∫ÜÊõ¥‰∏∞ÂØåÁöÑÂ∑•ÂÖ∑ÔºåËÉΩÂ§üÊçïÊçâÂ§çÊùÇÁöÑÊºîÂåñËøáÁ®ã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05309",
            "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
            "url": "https://huggingface.co/papers/2506.05309",
            "abstract": "An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.",
            "score": 0,
            "issue_id": 4260,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 –∏—é–Ω—è",
                "en": "June 5",
                "zh": "6Êúà5Êó•"
            },
            "hash": "474c2c6a688311bf",
            "authors": [
                "Niv Eckhaus",
                "Uri Berger",
                "Gabriel Stanovsky"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, The Hebrew University of Jerusalem",
                "School of Computing and Information Systems, University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05309.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–õ–õ–ú-–∞–≥–µ–Ω—Ç –æ—Å–≤–∞–∏–≤–∞–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è –≤ –ú–∞—Ñ–∏–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –õ–õ–ú-–∞–≥–µ–Ω—Ç–∞ –¥–ª—è —É—á–∞—Å—Ç–∏—è –≤ –æ–Ω–ª–∞–π–Ω-–∏–≥—Ä–∞—Ö –≤ –ú–∞—Ñ–∏—é. –ê–≥–µ–Ω—Ç –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–µ —Ç–æ–ª—å–∫–æ –æ —Ç–æ–º, —á—Ç–æ —Å–∫–∞–∑–∞—Ç—å, –Ω–æ –∏ –∫–æ–≥–¥–∞ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∞–≥–µ–Ω—Ç –≤—ã—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∏–≥—Ä–æ–∫–æ–≤ –∫–∞–∫ –ø–æ –∏–≥—Ä–æ–≤—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, —Ç–∞–∫ –∏ –ø–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–ø–∏—Å–∞—Ç—å—Å—è –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≥—Ä—É–ø–ø–æ–≤—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å–æ —Å–ª–æ–∂–Ω–æ–π —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π."
                },
                "en": {
                    "title": "Bridging AI and Human Interaction in Asynchronous Settings",
                    "desc": "This paper presents an adaptive asynchronous LLM-agent designed to participate in online Mafia games, showcasing its ability to mimic human players in complex social interactions. Unlike traditional LLMs that operate in synchronous settings, this agent can decide both what to say and when to say it, reflecting the nuances of real-world communication. The evaluation reveals that the agent performs comparably to human participants, effectively blending into the social dynamics of the game. The findings highlight the potential for LLMs to be integrated into various asynchronous environments, enhancing collaborative efforts in educational and professional contexts."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÂºÇÊ≠•‰ª£ÁêÜÔºöËÆ©AIËûçÂÖ•‰∫∫Á±ªÁ§æ‰∫§Ê∏∏Êàè",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËá™ÈÄÇÂ∫îÁöÑÂºÇÊ≠•Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÔºåÂÆÉÂú®Âú®Á∫ø Mafia Ê∏∏Êàè‰∏≠Ë°®Áé∞Âá∫‰∏é‰∫∫Á±ªÁé©ÂÆ∂Áõ∏‰ººÁöÑËÉΩÂäõ„ÄÇËøôÁßç‰ª£ÁêÜ‰∏ç‰ªÖÂÜ≥ÂÆöËØ¥‰ªÄ‰πàÔºåËøòÂÜ≥ÂÆö‰ΩïÊó∂ËØ¥ÔºåËøôÂú®ËÆ∏Â§öÁé∞ÂÆû‰∏ñÁïåÁöÑÁ§æ‰∫§Âú∫Âêà‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËØ•‰ª£ÁêÜÂú®Ê∏∏ÊàèË°®Áé∞Âíå‰∏é‰∫∫Á±ªÁé©ÂÆ∂ÁöÑ‰∫íÂä®‰∏≠ÈÉΩË°®Áé∞ËâØÂ•ΩÔºåËÉΩÂ§üÊúâÊïàËûçÂÖ•‰∫∫Á±ªÁ§æ‰∫§Âä®ÊÄÅ„ÄÇ‰ΩúËÄÖËøòÂèëÂ∏É‰∫ÜÁõ∏ÂÖ≥Êï∞ÊçÆÂíå‰ª£Á†ÅÔºå‰ª•‰øÉËøõÂØπÊõ¥ÁúüÂÆûÁöÑÂºÇÊ≠•Ê≤üÈÄöÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-06-11.html",
    "link_next": "2025-06-13.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "11.06",
        "en": "06/11",
        "zh": "6Êúà11Êó•"
    },
    "short_date_next": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6Êúà13Êó•"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü Seedance 1.0Ôºå‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑËßÜÈ¢ëÁîüÊàêÊ®°Âûã„ÄÇÂÆÉÁªìÂêà‰∫ÜÂÖàËøõÁöÑÊï∞ÊçÆÊï¥ÁêÜ„ÄÅÈ´òÊïàÁöÑÊû∂ÊûÑËÆæËÆ°„ÄÅËÆ≠ÁªÉÂêé‰ºòÂåñÂíåÊ®°ÂûãÂä†ÈÄü„ÄÇSeedance 1.0 ËÉΩÂ§üÂú®1080pÂàÜËæ®Áéá‰∏ãÁîüÊàê5ÁßíÁöÑËßÜÈ¢ëÔºåÂè™ÈúÄ41.4Áßí„ÄÇ‰∏éÂÖ∂‰ªñÈ°∂Â∞ñÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁõ∏ÊØîÔºåSeedance 1.0 Âú®Ë¥®ÈáèÂíåÈÄüÂ∫¶‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑Êúâ‰ºòË∂äÁöÑÊó∂Á©∫ÊµÅÁïÖÊÄßÂíåÁªìÊûÑÁ®≥ÂÆöÊÄß„ÄÇ",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü Seedance 1.0Ôºå‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑËßÜÈ¢ëÁîüÊàêÊ®°Âûã„ÄÇÂÆÉÁªìÂêà‰∫ÜÂÖàËøõÁöÑÊï∞ÊçÆÊï¥ÁêÜ„ÄÅÈ´òÊïàÁöÑÊû∂ÊûÑËÆæËÆ°„ÄÅËÆ≠ÁªÉÂêé‰ºòÂåñÂíåÊ®°ÂûãÂä†ÈÄü„ÄÇSeedance 1.0 ËÉΩÂ§üÂú®1080pÂàÜËæ®Áéá‰∏ãÁîüÊàê5ÁßíÁöÑËßÜÈ¢ëÔºåÂè™ÈúÄ41.4Áßí„ÄÇ‰∏éÂÖ∂‰ªñÈ°∂Â∞ñÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁõ∏ÊØîÔºåSeedance 1.0 Âú®Ë¥®ÈáèÂíåÈÄüÂ∫¶‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∑Êúâ‰ºòË∂äÁöÑÊó∂Á©∫ÊµÅÁïÖÊÄßÂíåÁªìÊûÑÁ®≥ÂÆöÊÄß„ÄÇ\n\nZh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le Seedance 1.0, yƒ´g√® gƒÅo x√¨ngn√©ng de sh√¨p√≠n shƒìngch√©ng m√≥x√≠ng. TƒÅ ji√©h√© le xiƒÅnj√¨n de sh√πj√π zhƒõngl«ê, gƒÅoxi√†o de ji√†g√≤u sh√®j√¨, x√πnli√†n h√≤u y≈çuhu√† h√© m√≥x√≠ng jiƒÅs√π. Seedance 1.0 n√©ngg√≤u z√†i 1080p fƒìnbiƒÅnl«ú xi√† shƒìngch√©ng 5 mi«éo de sh√¨p√≠n, zh«ê x≈´ 41.4 mi«éo. Y«î q√≠tƒÅ d«êngjiƒÅn de sh√¨p√≠n shƒìngch√©ng m√≥x√≠ng xiƒÅngb«ê, Seedance 1.0 z√†i zh√¨li√†ng h√© s√πd√π sh√†ng bi«éoxi√†n ch≈´s√®, j√πy«íu y≈çuyu√® de sh√≠k≈çng li√∫ch√†ngx√¨ng h√© ji√©g√≤u wƒõnd√¨ngx√¨ng.",
        "vocab": "[\n    {\"word\": \"Seedance\", \"pinyin\": \"Sƒ´d√†ns√¨\", \"trans\": \"Seedance\"},\n    {\"word\": \"È´òÊÄßËÉΩ\", \"pinyin\": \"gƒÅo x√¨ngn√©ng\", \"trans\": \"high performance\"},\n    {\"word\": \"ËßÜÈ¢ëÁîüÊàêÊ®°Âûã\", \"pinyin\": \"sh√¨p√≠n shƒìngch√©ng m√≥x√≠ng\", \"trans\": \"video generation model\"},\n    {\"word\": \"ÁªìÂêà\", \"pinyin\": \"ji√©h√©\", \"trans\": \"combine\"},\n    {\"word\": \"ÂÖàËøõ\", \"pinyin\": \"xiƒÅnj√¨n\", \"trans\": \"advanced\"},\n    {\"word\": \"Êï∞ÊçÆÊï¥ÁêÜ\", \"pinyin\": \"sh√πj√π zhƒõngl«ê\", \"trans\": \"data processing\"},\n    {\"word\": \"È´òÊïà\", \"pinyin\": \"gƒÅoxi√†o\", \"trans\": \"efficient\"},\n    {\"word\": \"Êû∂ÊûÑËÆæËÆ°\", \"pinyin\": \"ji√†g√≤u sh√®j√¨\", \"trans\": \"architecture design\"},\n    {\"word\": \"ËÆ≠ÁªÉÂêé‰ºòÂåñ\", \"pinyin\": \"x√πnli√†n h√≤u y≈çuhu√†\", \"trans\": \"post-training optimization\"},\n    {\"word\": \"Ê®°ÂûãÂä†ÈÄü\", \"pinyin\": \"m√≥x√≠ng jiƒÅs√π\", \"trans\": \"model acceleration\"},\n    {\"word\": \"1080pÂàÜËæ®Áéá\", \"pinyin\": \"1080p fƒìnbiƒÅnl«ú\", \"trans\": \"1080p resolution\"},\n    {\"word\": \"ÁîüÊàê\", \"pinyin\": \"shƒìngch√©ng\", \"trans\": \"generate\"},\n    {\"word\": \"Âè™ÈúÄ\", \"pinyin\": \"zh«ê x≈´\", \"trans\": \"only need\"},\n    {\"word\": \"È°∂Â∞ñ\", \"pinyin\": \"d«êngjiƒÅn\", \"trans\": \"top-notch\"},\n    {\"word\": \"Ë¥®Èáè\", \"pinyin\": \"zh√¨li√†ng\", \"trans\": \"quality\"},\n    {\"word\": \"ÈÄüÂ∫¶\", \"pinyin\": \"s√πd√π\", \"trans\": \"speed\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éoxi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"Âá∫Ëâ≤\", \"pinyin\": \"ch≈´s√®\", \"trans\": \"outstanding\"},\n    {\"word\": \"‰ºòË∂ä\", \"pinyin\": \"y≈çuyu√®\", \"trans\": \"superior\"},\n    {\"word\": \"Êó∂Á©∫ÊµÅÁïÖÊÄß\", \"pinyin\": \"sh√≠k≈çng li√∫ch√†ngx√¨ng\", \"trans\": \"spatiotemporal smoothness\"},\n    {\"word\": \"ÁªìÊûÑÁ®≥ÂÆöÊÄß\", \"pinyin\": \"ji√©g√≤u wƒõnd√¨ngx√¨ng\", \"trans\": \"structural stability\"}\n]",
        "trans": "This article introduces Seedance 1.0, a high-performance video generation model. It combines advanced data curation, efficient architectural design, post-training optimization, and model acceleration. Seedance 1.0 can generate a 5-second video at 1080p resolution in just 41.4 seconds. Compared to other leading video generation models, Seedance 1.0 excels in both quality and speed, demonstrating superior spatiotemporal smoothness and structural stability.",
        "update_ts": "2025-06-12 11:10"
    }
}