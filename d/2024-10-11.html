
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. October 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 октября</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-10.html">⬅️ <span id="prev-date">10.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-14.html">➡️ <span id="next-date">14.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'};
        let feedDateNext = {'ru': '14.10', 'en': '10/14', 'zh': '10月14日'};
        let feedDatePrev = {'ru': '10.10', 'en': '10/10', 'zh': '10月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.07484', 'title': 'WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents', 'url': 'https://huggingface.co/papers/2410.07484', 'abstract': 'Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment\'s dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such "world alignment" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent "WALL-E" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E\'s reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.', 'score': 48, 'issue_id': 59, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '6e6aca942958f302', 'data': {'categories': ['#reasoning', '#agi', '#rl', '#agents', '#alignment', '#games', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'LLM как модель мира: эффективное обучение правилам для согласования с окружающей средой', 'desc': 'Статья исследует возможность использования больших языковых моделей (LLM) в качестве мощных моделей мира для агентов на основе модели. Авторы предлагают нейросимволический подход для обучения правил, позволяющих согласовать предсказания LLM с динамикой конкретной среды. Разработанный агент WALL-E использует модельно-прогнозирующее управление (MPC) для оптимизации действий. В экспериментах в Minecraft и ALFWorld WALL-E демонстрирует более высокую эффективность по сравнению с существующими методами.'}, 'en': {'title': 'Aligning LLMs with the World: A New Era of Efficient Exploration', 'desc': 'The paper explores the potential of large language models (LLMs) to serve as effective world models for model-based agents by aligning them with specific environment dynamics through rule learning. This alignment is achieved using a neurosymbolic approach that involves inducing, updating, and pruning rules based on comparisons between agent-explored trajectories and LLM predictions. The resulting model, which combines the LLM with learned rules, enhances exploration and learning efficiency, as demonstrated by the WALL-E agent in open-world challenges like Minecraft and ALFWorld. WALL-E outperforms existing methods by achieving higher success rates with fewer resources, such as replanning time and tokens, required for reasoning.'}, 'zh': {'title': '大型语言模型：智能体世界模型的新可能', 'desc': '这篇论文探讨了大型语言模型（LLM）是否可以直接作为基于模型的智能体的强大世界模型。研究发现，通过规则学习，可以有效地将LLM与其部署环境对齐，从而弥合知识差距。作者提出了一种神经符号方法，通过比较智能体探索轨迹和世界模型预测来学习规则。最终的世界模型由LLM和学习到的规则组成，在Minecraft和ALFWorld等开放世界挑战中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.08196', 'title': 'MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code', 'url': 'https://huggingface.co/papers/2410.08196', 'abstract': 'Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. The code is released at https://github.com/mathllm/MathCoder2 .', 'score': 44, 'issue_id': 52, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '0ef94d1fd5147144', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#math', '#data', '#plp', '#open_source', '#synthetic'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических способностей языковых моделей с помощью кода', 'desc': 'Статья представляет новый метод генерации математического кода с соответствующими шагами рассуждений для дальнейшего предобучения языковых моделей. Авторы создали высококачественный датасет MathCode-Pile объемом 19.2 млрд токенов, включающий математические веб-данные, код с использованием математических библиотек, учебники и синтетические данные. Метод включает извлечение выражений LaTeX, необходимых условий и результатов из собранных данных, а затем генерацию соответствующего кода. Обучение популярных базовых моделей на этом корпусе значительно улучшило их математические способности.'}, 'en': {'title': '"Code Your Way to Better Math Reasoning!"', 'desc': 'This paper presents a new method for improving the mathematical reasoning skills of large language models by generating mathematical code with reasoning steps. The authors create a high-quality dataset called MathCode-Pile, which includes math-related web data, code, textbooks, and synthetic data. They extract LaTeX expressions and conditions from this dataset to generate code that accurately reflects mathematical reasoning. Training models with this dataset significantly enhances their mathematical abilities, resulting in the MathCoder2 family of models.'}, 'zh': {'title': '代码与推理：提升语言模型的数学能力', 'desc': '这篇论文介绍了一种新的方法，通过生成数学代码和相应的推理步骤来增强大语言模型的数学推理能力。研究人员构建了一个高质量的数学预训练数据集，结合了数学相关的网络数据、使用数学包的代码、数学教科书和合成数据。通过提取LaTeX表达式、条件和结果，生成相应的代码来准确捕捉数学推理过程。最终，使用这个数据集训练的模型显著提高了数学能力，形成了MathCoder2模型家族。'}}}, {'id': 'https://huggingface.co/papers/2410.03450', 'title': 'MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents', 'url': 'https://huggingface.co/papers/2410.03450', 'abstract': "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.", 'score': 32, 'issue_id': 51, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'db11faae830e2807', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#optimization', '#agents', '#benchmark', '#games', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'MART: Улучшение воплощенных агентов через тонкую настройку мультимодальных ретриверов', 'desc': 'Статья представляет новый метод MART для улучшения работы воплощенных агентов с использованием мультимодальных языковых моделей (MLLM). MART использует данные взаимодействия для тонкой настройки MLLM-ретривера на основе обучения с предпочтениями, что позволяет эффективно оценивать и приоритезировать траектории для новых задач. Также вводится механизм абстракции траекторий, использующий возможности MLLM по суммаризации для лучшего понимания ключевых этапов. Эксперименты показывают значительное улучшение успешности выполнения задач в новых сценах по сравнению с базовыми методами.'}, 'en': {'title': "Revolutionizing Task Success: MART's Smart Trajectory Retrieval", 'desc': 'This paper introduces a new method called MLLM as ReTriever (MART) to improve the performance of embodied agents in complex tasks. MART fine-tunes a machine learning language model (MLLM) to retrieve task-relevant trajectory data by focusing on the effectiveness of the trajectories rather than just surface-level similarities. The method also uses Trajectory Abstraction to summarize trajectories with fewer tokens, helping agents understand key milestones. Experimental results show that MART significantly enhances task success rates in new environments compared to existing methods.'}, 'zh': {'title': '通过MART方法提升具身智能体的任务成功率', 'desc': '这篇论文提出了一种新方法，称为MLLM作为检索器（MART），通过利用交互数据来微调MLLM检索器，以提高具身智能体在复杂任务中的表现。传统的检索方法主要关注轨迹的表面相似性，而忽略了其在特定任务中的有效性。MART方法通过偏好学习来优化检索器，使其能够优先考虑未见任务中轨迹的有效性。实验结果表明，该方法在不同环境中显著提高了任务成功率。'}}}, {'id': 'https://huggingface.co/papers/2410.05265', 'title': 'PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs', 'url': 'https://huggingface.co/papers/2410.05265', 'abstract': 'Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at https://github.com/ChenMnZ/PrefixQuant.', 'score': 29, 'issue_id': 51, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'a083aa646dcae575', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'PrefixQuant: Эффективное статическое квантование LLM без потери качества', 'desc': 'PrefixQuant - это новый метод квантования больших языковых моделей (LLM), который изолирует выбросы на уровне токенов офлайн без переобучения. Он префиксирует часто встречающиеся токены-выбросы в KV-кэше, что позволяет применять эффективное статическое квантование на уровне тензоров. PrefixQuant превосходит существующие методы динамического квантования по производительности и скорости вывода. Метод демонстрирует значительные улучшения в перплексии и точности на различных задачах по сравнению с предыдущими подходами.'}, 'en': {'title': 'PrefixQuant: Revolutionizing LLM Quantization with Token-Wise Precision', 'desc': 'The paper introduces PrefixQuant, a new method for quantizing large language models that focuses on token-wise outliers, which are often overlooked by existing methods. PrefixQuant works by identifying and managing high-frequency outlier tokens offline, thus avoiding the need for expensive per-token dynamic quantization during inference. This approach allows for efficient per-tensor static quantization, which not only improves memory efficiency and inference speed but also enhances model performance. The results show that PrefixQuant significantly outperforms previous methods in terms of perplexity and accuracy while being faster than traditional FP16 models.'}, 'zh': {'title': 'PrefixQuant：高效静态量化的新突破', 'desc': '量化对于部署大型语言模型至关重要，因为它可以提高内存效率和推理速度。现有的激活量化方法主要解决通道级异常值的问题，往往忽略了令牌级异常值，从而依赖于昂贵的每令牌动态量化。PrefixQuant是一种新技术，可以在不重新训练的情况下离线隔离异常令牌。它通过在KV缓存中前缀高频异常令牌，防止推理过程中生成异常令牌，从而简化量化过程。'}}}, {'id': 'https://huggingface.co/papers/2410.07869', 'title': 'Benchmarking Agentic Workflow Generation', 'url': 'https://huggingface.co/papers/2410.07869', 'abstract': "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset will be available at https://github.com/zjunlp/WorFBench.", 'score': 25, 'issue_id': 54, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ce1da2a3c48bcb17', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#graphs', '#inference', '#agents', '#benchmark', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'WorFBench: новый стандарт оценки способностей ЯМ в генерации рабочих процессов', 'desc': 'Статья представляет WorFBench - новый бенчмарк для оценки способности языковых моделей генерировать рабочие процессы. Авторы также предлагают WorFEval - протокол оценки, использующий алгоритмы сопоставления подпоследовательностей и подграфов. Исследование выявило значительный разрыв между способностями моделей в планировании последовательностей и графов. Результаты показывают, что сгенерированные рабочие процессы могут улучшить выполнение последующих задач.'}, 'en': {'title': 'Bridging the Workflow Gap in Large Language Models', 'desc': "The paper introduces WorFBench, a benchmark designed to evaluate the workflow generation capabilities of Large Language Models (LLMs) across diverse scenarios and complex graph structures. It also presents WorFEval, an evaluation protocol that uses subsequence and subgraph matching to assess the accuracy of these workflows. The study reveals a significant gap in the sequence and graph planning abilities of LLMs, including a 15% gap in GPT-4's performance. Additionally, the research shows that the generated workflows can improve the efficiency and effectiveness of downstream tasks."}, 'zh': {'title': 'WorFBench：提升LLM工作流程生成能力的基准', 'desc': '大型语言模型（LLMs）在处理推理和规划任务方面表现出色，能够将复杂问题分解为可执行的工作流程。现有的工作流程评估框架存在场景覆盖有限、结构简单和评估标准松散等问题。为此，我们引入了WorFBench，一个统一的工作流程生成基准，具有多方面的场景和复杂的图形工作流程结构。通过对不同类型的LLMs进行评估，我们发现序列规划能力和图形规划能力之间存在显著差距，甚至GPT-4也存在约15%的差距。'}}}, {'id': 'https://huggingface.co/papers/2410.08164', 'title': 'Agent S: An Open Agentic Framework that Uses Computers Like a Human', 'url': 'https://huggingface.co/papers/2410.08164', 'abstract': 'We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.', 'score': 24, 'issue_id': 52, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ccbad559d2898387', 'data': {'categories': ['#reasoning', '#graphs', '#agents', '#benchmark', '#transfer_learning', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Agent S: Автономный помощник для управления компьютером', 'desc': 'Agent S - это открытая агентная система, предназначенная для автономного взаимодействия с компьютерами через графический интерфейс пользователя. Система решает три ключевые проблемы автоматизации компьютерных задач: получение специфических знаний, планирование долгосрочных задач и работа с динамическими интерфейсами. Agent S использует иерархическое планирование, усиленное опытом, и специальный интерфейс Agent-Computer Interface для улучшения взаимодействия с GUI на основе мультимодальных больших языковых моделей.'}, 'en': {'title': 'Agent S: Revolutionizing Task Automation with Intelligent GUI Interaction', 'desc': 'Agent S is a framework designed to automate complex tasks on computers by interacting with their graphical interfaces. It tackles challenges like acquiring specific knowledge, planning long tasks, and managing changing interfaces using experience-augmented hierarchical planning. This approach combines learning from external searches and internal experiences to improve task execution. The framework shows significant improvements in task success rates and generalizes well across different operating systems.'}, 'zh': {'title': 'Agent S：自动化人机交互的未来', 'desc': 'Agent S 是一个开放的代理框架，旨在通过图形用户界面实现计算机的自主交互，自动化复杂的多步骤任务。它通过引入经验增强的层次规划，解决了获取领域知识、长任务规划和处理动态界面等挑战。Agent S 使用多模态大语言模型来提升 GUI 代理的推理和控制能力。在 OSWorld 基准测试中，Agent S 的成功率比基线高出 9.37%，并在 WindowsAgentArena 基准中展示了广泛的通用性。'}}}, {'id': 'https://huggingface.co/papers/2410.08159', 'title': 'DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.08159', 'abstract': 'Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model with the same architecture as standard language models. DART does not rely on image quantization, enabling more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.', 'score': 23, 'issue_id': 57, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '40e6937b715fa538', 'data': {'categories': ['#cv', '#optimization', '#transfer_learning', '#benchmark', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'DART: Объединение авторегрессии и диффузии для эффективной генерации изображений', 'desc': 'DART - это новый подход к генерации изображений, объединяющий авторегрессионные модели и диффузию в рамках немарковского процесса. Модель использует трансформер для итеративного шумоподавления патчей изображения как в пространственном, так и в спектральном измерении. DART не требует квантизации изображений и может обучаться одновременно на текстовых и визуальных данных. Эксперименты показывают конкурентоспособность DART в задачах генерации изображений по классу и текстовому описанию.'}, 'en': {'title': 'DART: Revolutionizing Image Generation with Unified AR and Diffusion', 'desc': 'The paper introduces DART, a new model for generating images that combines autoregressive and diffusion techniques in a non-Markovian framework. Unlike traditional diffusion models, DART does not rely on a step-by-step noise addition process, allowing it to use the generation path more efficiently. By using a transformer-based architecture similar to language models, DART can denoise image patches without needing image quantization, improving image quality and flexibility. The model also supports training with both text and image data, achieving strong results in generating images from text descriptions.'}, 'zh': {'title': 'DART：突破扩散模型的高效图像生成新标杆', 'desc': '扩散模型是目前视觉生成的主流方法，但其马尔可夫性质限制了模型充分利用生成轨迹，导致训练和推理效率低下。本文提出了一种名为DART的模型，它结合了自回归和扩散方法，采用非马尔可夫框架。DART通过自回归模型在空间和光谱上迭代去噪图像块，不依赖图像量化，从而提高了图像建模的效果。DART还可以在统一模型中同时训练文本和图像数据，展示了在类条件和文本到图像生成任务中的竞争力。'}}}, {'id': 'https://huggingface.co/papers/2410.08207', 'title': 'DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models', 'url': 'https://huggingface.co/papers/2410.08207', 'abstract': 'Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.', 'score': 18, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '49494862b22f09dc', 'data': {'categories': ['#cv', '#training', '#diffusion', '#architecture', '#synthetic'], 'emoji': '🎛️', 'ru': {'title': 'DICE: Точная инверсия для гибкого редактирования дискретных данных', 'desc': 'DICE (Discrete Inversion for Controllable Editing) - это новый подход к точной инверсии для дискретных диффузионных моделей. Он позволяет осуществлять точную реконструкцию и гибкое редактирование дискретных данных без необходимости в предопределенных масках или манипуляциях с вниманием. DICE работает с моделями как в области изображений, так и текста, включая VQ-Diffusion, Paella и RoBERTa. Метод сохраняет высокую точность данных при улучшении возможностей редактирования.'}, 'en': {'title': 'DICE: Precision Editing in Discrete Diffusion Models', 'desc': 'The paper introduces DICE, a novel method for precise content editing in discrete diffusion models, which are used in tasks like image generation and language modeling. DICE records noise sequences and masking patterns during the reverse diffusion process, allowing for accurate data reconstruction and flexible editing without predefined masks. This approach is tested on models like VQ-Diffusion and RoBERTa, showing that it maintains high data fidelity while improving editing capabilities. DICE opens up new possibilities for detailed content manipulation in discrete data spaces.'}, 'zh': {'title': 'DICE：离散扩散模型的精确可控编辑', 'desc': '这篇论文介绍了一种名为DICE的新方法，用于在离散扩散模型中实现可控编辑。DICE通过记录噪声序列和掩码模式，实现了对离散数据的精确重建和灵活编辑。与传统方法不同，DICE不需要预定义的掩码或注意力操作。实验表明，DICE在图像和文本领域都能保持高数据保真度，同时增强编辑能力。'}}}, {'id': 'https://huggingface.co/papers/2410.04751', 'title': 'Intriguing Properties of Large Language and Vision Models', 'url': 'https://huggingface.co/papers/2410.04751', 'abstract': "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.", 'score': 16, 'issue_id': 52, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '4712ab5ada7bb4c9', 'data': {'categories': ['#reasoning', '#cv', '#math', '#interpretability', '#benchmark', '#alignment', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая тайны восприятия в крупных языково-визуальных моделях', 'desc': 'В статье исследуются крупные языковые и визуальные модели (LLVM) с точки зрения их восприятия изображений. Авторы проводят систематический анализ различных аспектов работы LLVM, включая инвариантность к перестановкам, устойчивость, математические рассуждения и сохранение выравнивания. Эксперименты показывают, что LLVM обрабатывают изображения глобально, могут решать математические задачи без полного восприятия числовой информации, и что нижние слои модели играют ключевую роль в определении производительности. На основе полученных результатов предлагаются направления для улучшения LLVM и создания более сложных тестовых наборов.'}, 'en': {'title': 'Bridging the Gap: Enhancing Perception in Large Language and Vision Models', 'desc': 'The paper explores the performance of large language and vision models (LLVMs) in perception-related tasks, revealing a gap between their advanced reasoning abilities and basic perceptual skills. It highlights that LLVMs process images globally, sometimes solving math problems without detailed perception, and that their cross-modal alignment may compromise original perceptual capabilities. The study finds that the lower layers of these models are crucial for visual understanding, suggesting that improvements in these areas could enhance performance. The authors propose future research directions to develop more effective LLVMs and create more challenging evaluation benchmarks.'}, 'zh': {'title': '探索大型语言与视觉模型的感知奥秘', 'desc': '大型语言与视觉模型（LLVMs）在许多需要感知和认知能力的任务中表现出色，但在基本感知任务上的表现却不尽如人意。研究发现，这些模型在处理图像时具有全局处理的特性，即使视觉片段顺序被打乱，它们仍能保持一定的理解能力。此外，它们在解决数学问题时，有时不需要完全感知详细的数据信息。研究还指出，模型的跨模态对齐过度适应复杂推理任务，导致其视觉编码器的原始感知能力有所下降。'}}}, {'id': 'https://huggingface.co/papers/2410.07303', 'title': 'Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow', 'url': 'https://huggingface.co/papers/2410.07303', 'abstract': 'Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing boldsymbol v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.', 'score': 16, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '08c053399c56f831', 'data': {'categories': ['#cv', '#training', '#optimization', '#open_source', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение диффузионных моделей: новый взгляд на ректификацию', 'desc': 'Статья представляет новый метод под названием Rectified Diffusion, который улучшает скорость генерации изображений в диффузионных моделях. Авторы утверждают, что успех ректификации основан на использовании предобученной диффузионной модели для получения сопоставленных пар шума и образцов. Метод обобщает пространство проектирования и область применения ректификации на более широкую категорию диффузионных моделей. Rectified Diffusion упрощает процедуру обучения и достигает лучшей производительности при меньших затратах на обучение.'}, 'en': {'title': 'Speed Up and Simplify: Revolutionizing Image Generation with Rectified Diffusion', 'desc': "The paper discusses how diffusion models, which are used for generating images, can be slow because they require solving complex equations. The authors propose a new method called Rectified Diffusion, which simplifies the process by using a pretrained model to match noise with samples, making the training faster and more efficient. They argue that previous methods focused too much on making the path straight, but the real goal should be to create a path that naturally fits the model's needs. Their approach not only speeds up the process but also improves the quality of the generated images while reducing training costs."}, 'zh': {'title': '校正扩散：简化训练，提升性能', 'desc': '扩散模型在视觉生成方面有很大提升，但生成速度较慢。本文提出了一种称为“校正扩散”的方法，通过使用预训练的扩散模型来匹配噪声和样本对，从而简化训练过程。我们发现，直线化并不是校正的必要目标，关键在于实现一阶近似的ODE路径。我们的方法在稳定扩散模型上验证，表现优异且训练成本更低。'}}}, {'id': 'https://huggingface.co/papers/2410.06154', 'title': 'GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models', 'url': 'https://huggingface.co/papers/2410.06154', 'abstract': 'In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.', 'score': 15, 'issue_id': 54, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '1b375d9257a67241', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#benchmark', '#vision', '#architecture'], 'emoji': '🔮', 'ru': {'title': 'GLOV: LLM как оптимизатор для улучшения задач компьютерного зрения', 'desc': 'Предложен новый метод GLOV, позволяющий использовать большие языковые модели (LLM) в качестве неявных оптимизаторов для визуально-языковых моделей (VLM) с целью улучшения задач компьютерного зрения. GLOV использует мета-промпты для LLM с описанием целевой задачи, запрашивая подходящие промпты для VLM. Эти промпты ранжируются по мере чистоты, полученной с помощью функции пригодности. Метод также включает явное управление процессом генерации LLM путем добавления вектора смещения, основанного на предыдущих решениях.'}, 'en': {'title': 'GLOV: Boosting Vision-Language Models with Smart Language Prompts', 'desc': "The paper introduces GLOV, a method that uses Large Language Models (LLMs) as implicit optimizers to improve Vision-Language Models (VLMs) for vision tasks. GLOV works by generating and ranking prompts for VLMs, using a fitness function to measure their effectiveness. The method also adjusts the LLM's output by incorporating an offset vector, which guides the language generation towards what the VLM prefers. This approach significantly boosts the performance of VLMs on various datasets, achieving up to 57.5% improvement in recognition tasks."}, 'zh': {'title': 'GLOV：提升视觉任务性能的隐式优化器', 'desc': '这篇论文提出了一种新方法GLOV，使大型语言模型（LLM）可以作为视觉语言模型（VLM）的隐式优化器，以增强视觉任务。GLOV通过元提示向LLM提供任务描述，并查询适合的VLM提示，这些提示根据纯度测量进行排名。在每个优化步骤中，排名的提示作为上下文示例输入LLM，以帮助其了解VLM偏好的文本提示类型。此外，通过在每个优化步骤中添加偏移向量来引导LLM生成过程，从而提高下游视觉任务的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.08151', 'title': 'Progressive Autoregressive Video Diffusion Models', 'url': 'https://huggingface.co/papers/2410.08151', 'abstract': 'Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/.', 'score': 15, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '599024cc2a308da1', 'data': {'categories': ['#video', '#long_context', '#training', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Прорыв в генерации длинных видео: минута качественного контента', 'desc': 'Статья представляет новый подход к генерации длинных видео с использованием авторегрессивных моделей диффузии. Авторы предлагают применять прогрессивно увеличивающиеся уровни шума к латентным кадрам, что позволяет создавать более тонкие связи между ними. Этот метод позволяет генерировать видео длительностью до 1 минуты (1440 кадров при 24 FPS) без ухудшения качества или резких изменений сцены. Результаты демонстрируют передовой уровень в области генерации длинных видео.'}, 'en': {'title': 'Breaking Barriers: Extending Video Diffusion to One Minute', 'desc': 'The paper introduces a method to extend current video diffusion models to generate longer videos by using an autoregressive approach. By assigning progressively increasing noise levels to latent frames, the model can maintain high-quality video generation without abrupt scene changes. This technique allows for fine-grained conditioning among latent frames and large overlaps in attention windows, enabling the generation of videos up to one minute long. The results demonstrate state-of-the-art performance in long video generation, overcoming previous limitations of short clip production.'}, 'zh': {'title': '逐步去噪：长视频生成的新突破', 'desc': '这篇论文探讨了如何将现有的视频扩散模型扩展为自回归视频扩散模型，而无需改变其架构。作者提出了一种关键方法，即为潜在帧分配逐步增加的噪声水平，而不是单一的噪声水平。这种逐步的视频去噪方法使得模型能够自回归地生成视频帧，而不会出现质量下降或场景突变。实验结果表明，该方法在长视频生成方面达到了最先进的水平，能够生成长达一分钟的视频。'}}}, {'id': 'https://huggingface.co/papers/2410.05603', 'title': 'Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition', 'url': 'https://huggingface.co/papers/2410.05603', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution.', 'score': 11, 'issue_id': 57, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'a6c2dccffed8ffb4', 'data': {'categories': ['#reasoning', '#training', '#inference', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Суперпозиция задач: скрытая сила языковых моделей', 'desc': "Исследование демонстрирует удивительную способность больших языковых моделей (LLM) выполнять несколько вычислительно различных задач обучения в контексте одновременно, названную 'суперпозицией задач'. Это явление наблюдается в различных семействах LLM и масштабах, даже если модель обучена решать задачи по одной. Теоретически показано, что эта способность находится в пределах выразительной мощности трансформеров. Исследование также раскрывает, как LLM внутренне составляют векторы задач во время суперпозиции."}, 'en': {'title': 'Unleashing the Power of Task Superposition in LLMs', 'desc': "This paper investigates a unique ability of Large Language Models (LLMs) called 'task superposition', where they can handle multiple distinct in-context learning tasks at once during a single inference. The authors provide evidence that this capability is present across different LLM families and sizes, even when models are trained to learn one task at a time. They explain that this ability is possible due to the expressive power of transformers, and they explore how LLMs internally manage task vectors during superposition. The study also finds that larger models can perform more tasks simultaneously and better adjust their output, offering new insights into the potential of LLMs."}, 'zh': {'title': '探索大型语言模型的任务叠加能力', 'desc': '大型语言模型（LLMs）在上下文学习中表现出色，能够在一次推理中同时执行多种不同的任务，这种能力被称为“任务叠加”。研究表明，即使模型被训练为一次只学习一个任务，这种现象仍然会出现。理论上，这种能力在变压器的表达能力范围内。更大的模型可以同时解决更多的任务，并更好地校准其输出分布。'}}}, {'id': 'https://huggingface.co/papers/2410.05210', 'title': 'Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality', 'url': 'https://huggingface.co/papers/2410.05210', 'abstract': "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.", 'score': 10, 'issue_id': 50, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'aec4ac6f174a5a13', 'data': {'categories': ['#reasoning', '#cv', '#training', '#optimization', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Улучшение композиционного понимания без потери мультимодальных возможностей', 'desc': 'В статье предлагается новый метод улучшения композиционного понимания в предобученных моделях зрения и языка (VLM) без ущерба для производительности в мультимодальных задачах с нулевым обучением. Авторы представляют Fine-grained Selective Calibrated CLIP (FSC-CLIP), который интегрирует локальную потерю жестких негативных примеров и селективную калиброванную регуляризацию. FSC-CLIP обеспечивает детальный негативный надзор, сохраняя целостность представлений модели. Результаты оценки на различных бенчмарках показывают, что FSC-CLIP достигает композиционности на уровне современных моделей, сохраняя при этом сильные мультимодальные возможности.'}, 'en': {'title': 'Balancing Act: Enhancing Compositional Understanding Without Compromise', 'desc': "The paper introduces FSC-CLIP, a method to improve how vision and language models understand compositions without losing their ability to handle multiple types of data at once. Traditional methods often harm the model's ability to work with both images and text by using a global hard negative loss, which can confuse the model. FSC-CLIP uses a local hard negative loss and selective calibrated regularization to provide more precise guidance, helping the model learn better without losing its multi-modal skills. Tests show that FSC-CLIP matches top models in understanding compositions while keeping its multi-modal strengths intact."}, 'zh': {'title': 'FSC-CLIP：提升组合理解，不损多模态表现', 'desc': '这篇论文提出了一种新方法，增强预训练视觉和语言模型的组合理解能力，同时不影响零样本多模态任务的表现。传统的微调方法通常通过使用全局硬负样本损失来提高组合推理，但这会损害多模态能力。为了解决这个问题，我们提出了细粒度选择性校准CLIP（FSC-CLIP），结合了局部硬负样本损失和选择性校准正则化。这些创新提供了细粒度的负监督，同时保持了模型的表示完整性。'}}}, {'id': 'https://huggingface.co/papers/2410.06508', 'title': 'Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning', 'url': 'https://huggingface.co/papers/2410.06508', 'abstract': 'Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.', 'score': 9, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'e3c203504e6848e3', 'data': {'categories': ['#reasoning', '#training', '#math', '#rl', '#optimization', '#games'], 'emoji': '🧠', 'ru': {'title': 'AlphaLLM-CPL: Усиление рассуждений LLM через дистилляцию MCTS', 'desc': 'AlphaLLM-CPL - это новый метод обучения языковых моделей (LLM) с использованием поиска по дереву Монте-Карло (MCTS). Он улучшает рассуждения LLM, эффективно извлекая информацию из траекторий MCTS. AlphaLLM-CPL использует попарное обучение на основе узлов дерева поиска и куррикулярное обучение предпочтениям. Эксперименты показывают значительное улучшение способностей LLM к математическим рассуждениям по сравнению с предыдущими методами.'}, 'en': {'title': 'Boosting LLM Reasoning with Smart MCTS Learning', 'desc': 'This paper introduces AlphaLLM-CPL, a new training framework that helps large language models (LLMs) improve their reasoning skills using Monte Carlo Tree Search (MCTS). Unlike previous methods, AlphaLLM-CPL makes better use of the detailed path information from MCTS by creating stepwise trajectory pairs, which helps in more effective learning. It also uses a technique called curriculum preference learning to adjust the order of learning steps, focusing on the most important ones and avoiding overfitting. Tests on math problems show that AlphaLLM-CPL greatly enhances the reasoning abilities of LLMs compared to older methods.'}, 'zh': {'title': 'AlphaLLM-CPL：通过MCTS行为蒸馏自我提升LLM推理能力', 'desc': '这篇论文介绍了一种新的训练框架AlphaLLM-CPL，用于通过蒙特卡罗树搜索（MCTS）行为蒸馏来提升大型语言模型（LLM）的推理能力。AlphaLLM-CPL通过构建来自同一父节点的子节点的逐步轨迹对，提供了更有效的MCTS行为蒸馏。它还引入了课程偏好学习，动态调整轨迹对的训练顺序，以优先考虑关键学习步骤并减少过拟合。实验结果表明，AlphaLLM-CPL在数学推理任务中显著优于现有的MCTS行为蒸馏方法。'}}}, {'id': 'https://huggingface.co/papers/2410.05248', 'title': 'SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe', 'url': 'https://huggingface.co/papers/2410.05248', 'abstract': "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.", 'score': 8, 'issue_id': 52, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '960aef6ca0d9e59b', 'data': {'categories': ['#dataset', '#training', '#healthcare', '#optimization', '#data', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'SFTMix: улучшение инструктивной настройки LLM без дорогостоящей фильтрации данных', 'desc': 'SFTMix - это новый метод для улучшения процесса инструктивной настройки больших языковых моделей (LLM). Он использует динамику обучения для идентификации примеров с разными уровнями уверенности модели. Затем применяется регуляризация на основе Mixup для снижения переобучения на уверенных примерах и улучшения обучения на менее уверенных. SFTMix превосходит стандартный метод предсказания следующего токена на различных задачах без необходимости в тщательно отобранных наборах данных.'}, 'en': {'title': 'SFTMix: Elevating Instruction-Tuning Without Expensive Datasets', 'desc': 'The paper introduces SFTMix, a new method to improve instruction-tuning in large language models without relying on expensive, high-quality datasets. SFTMix uses training dynamics to identify examples with varying confidence levels and applies a Mixup-based regularization to balance learning. This approach helps prevent overfitting on confident examples and enhances learning on less confident ones, leading to better performance across various tasks. The method is shown to be adaptable to different models and scalable to any dataset size, making it a versatile tool in natural language processing.'}, 'zh': {'title': 'SFTMix：无需精心策划数据集的指令微调新方法', 'desc': '这篇论文提出了一种名为SFTMix的新方法，用于提升大语言模型的指令微调性能。传统方法依赖高质量的数据集，而SFTMix通过利用数据集的内在特性，减少了对精心策划数据集的依赖。SFTMix通过识别不同信心水平的例子，并应用一种基于Mixup的正则化方法，来改善模型在不确定例子上的学习。实验表明，SFTMix在多种任务中表现优异，具有良好的适应性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2410.08049', 'title': 'Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations', 'url': 'https://huggingface.co/papers/2410.08049', 'abstract': 'This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet promoting further research and development in the community.', 'score': 8, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '8c8b1af09bf67bbe', 'data': {'categories': ['#video', '#audio', '#cv', '#inference', '#optimization', '#open_source', '#architecture', '#3d'], 'emoji': '🔍', 'ru': {'title': 'Большие сверточные ядра - ключ к универсальным и эффективным ConvNets', 'desc': 'Статья предлагает новую парадигму использования больших сверточных ядер в современных сверточных нейронных сетях (ConvNets). Авторы утверждают, что применение нескольких больших ядер вместо стека из множества мелких может быть более эффективной стратегией проектирования. Они представляют архитектуру UniRepLKNet, которая обеспечивает систематические принципы проектирования для ConvNets с большими ядрами, подчеркивая их уникальную способность захватывать обширную пространственную информацию без глубокого наслоения. Результаты показывают превосходство этого подхода в различных задачах компьютерного зрения и других модальностях, демонстрируя универсальные возможности моделирования ConvNets с большими ядрами при более высокой скорости вывода по сравнению с vision transformers.'}, 'en': {'title': '"Think Big: Revolutionizing ConvNets with Large Kernels"', 'desc': 'This paper introduces a new approach to designing Convolutional Neural Networks (ConvNets) by using large convolutional kernels instead of many smaller ones. The authors present the UniRepLKNet architecture, which is optimized for capturing extensive spatial information efficiently. Their model achieves high accuracy and performance across various tasks, outperforming traditional models and vision transformers. The research highlights the advantages of large-kernel ConvNets, such as larger receptive fields and a shift from texture to shape bias, offering faster inference speeds.'}, 'zh': {'title': '大卷积核：现代卷积神经网络的新方向', 'desc': '这篇论文提出了在设计现代卷积神经网络时使用大卷积核的范式。研究表明，使用少量大卷积核比堆叠多个小卷积核更优。我们介绍了一套针对大卷积核网络的架构设计指南，优化其效率和性能。UniRepLKNet架构展示了大卷积核网络在捕捉广泛空间信息方面的独特能力，并在多个领域表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.07041', 'title': 'Emergent properties with repeated examples', 'url': 'https://huggingface.co/papers/2410.07041', 'abstract': 'We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.', 'score': 8, 'issue_id': 50, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'eac06638db21722d', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#math', '#optimization'], 'emoji': '🔁', 'ru': {'title': 'Повторение - мать учения для трансформеров', 'desc': 'Исследование показывает, что трансформеры лучше обучаются на меньших наборах данных с повторяющимися примерами, чем на больших наборах с уникальными примерами. Эксперименты проводились на трех математических задачах: нахождение наибольшего общего делителя, модульное умножение и собственные значения матриц. Авторы демонстрируют эффективность метода двухэтапного обучения, сочетающего повторное использование малого подмножества примеров с обычной выборкой из остального набора данных. Результаты подчеркивают, что преимущества повторения могут превосходить преимущества разнообразия данных в глубоком обучении.'}, 'en': {'title': 'Repetition Over Diversity: A New Path to Better Learning', 'desc': 'This paper explores how transformers perform when trained with repeated examples versus a larger variety of single-use examples. It finds that using a smaller set of repeated examples can lead to better performance than using a larger, more diverse dataset. The study introduces a two-set training method, which combines repeated examples with normal sampling, resulting in faster learning and improved outcomes. This research provides insights into the balance between memorization and generalization in deep learning.'}, 'zh': {'title': '重复训练：超越数据多样性的力量', 'desc': '这篇论文研究了在算法生成的数据集上，Transformer模型的性能与训练样本重复次数之间的关系。在最大公约数、模乘法和矩阵特征值三个数学问题上，研究表明在固定训练步数下，使用较小重复样本集训练的模型优于使用较大单次样本集的模型。研究还展示了双集训练法，即在正常采样的基础上重复使用一小部分随机样本，可以加快学习速度并提高性能。这表明重复的好处可能超过数据多样性的好处。'}}}, {'id': 'https://huggingface.co/papers/2410.08115', 'title': 'Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System', 'url': 'https://huggingface.co/papers/2410.08115', 'abstract': "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).", 'score': 7, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'b7cacda3f030e1bd', 'data': {'categories': ['#reasoning', '#training', '#inference', '#rl', '#optimization', '#agents', '#games', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Optima: революция в эффективности многоагентных систем на основе LLM', 'desc': 'Optima - новая система, улучшающая эффективность коммуникации и решения задач в многоагентных системах на основе больших языковых моделей. Она использует итеративный подход генерации, ранжирования, отбора и обучения с функцией вознаграждения, балансирующей производительность, токен-эффективность и читаемость. Система интегрирует техники, вдохновленные Monte Carlo Tree Search, для генерации данных обучения. Optima демонстрирует значительные улучшения по сравнению с одноагентными базовыми линиями и обычными многоагентными системами на основе Llama 3 8B.'}, 'en': {'title': 'Optima: Revolutionizing Multi-Agent Communication and Efficiency', 'desc': 'The paper introduces Optima, a framework designed to improve communication efficiency and task effectiveness in multi-agent systems using large language models. Optima uses a generate, rank, select, and train approach with a reward function to balance task performance and communication readability. It explores reinforcement learning techniques like Supervised Fine-Tuning and Direct Preference Optimization, integrating Monte Carlo Tree Search methods to enhance data generation. The framework demonstrates significant performance improvements in multi-agent tasks, achieving better results with fewer resources compared to traditional methods.'}, 'zh': {'title': 'Optima：提升多智能体系统效率的新框架', 'desc': '这篇论文介绍了一种名为Optima的新框架，旨在提高基于大型语言模型的多智能体系统的通信效率和任务效果。Optima通过生成、排序、选择和训练的迭代过程，结合奖励函数来平衡任务表现、令牌效率和通信可读性。研究中使用了多种强化学习算法，包括监督微调和直接偏好优化，并结合蒙特卡罗树搜索技术来生成数据。实验结果表明，Optima在信息不对称问答和复杂推理等任务中表现出显著的性能提升，展示了其在大规模、多智能体系统中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.07137', 'title': 'Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates', 'url': 'https://huggingface.co/papers/2410.07137', 'abstract': 'Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a "null model" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.', 'score': 6, 'issue_id': 56, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'fdf529020c19324c', 'data': {'categories': ['#security', '#inference', '#ethics', '#benchmark', '#open_source'], 'emoji': '🕵️', 'ru': {'title': "Осторожно: даже 'нулевая модель' может обмануть бенчмарки ЯМ!", 'desc': "Статья рассматривает проблему обмана автоматических бенчмарков для оценки языковых моделей. Авторы демонстрируют, как даже 'нулевая модель', всегда выдающая один и тот же ответ, может достичь высоких результатов на популярных бенчмарках. Эксперименты показывают, что такие трюки могут быть использованы для неэтичного повышения рейтинга моделей. Исследование подчеркивает необходимость разработки механизмов защиты от обмана для обеспечения надежности автоматических бенчмарков."}, 'en': {'title': 'Unmasking the Illusion: Ensuring Fair Play in LLM Benchmarks', 'desc': "The paper discusses how automatic benchmarks for evaluating language models, like AlpacaEval 2.0, can be manipulated to falsely boost a model's performance. It reveals that even a simple model that outputs the same response regardless of input can achieve high scores on these benchmarks. This highlights the vulnerability of current evaluation systems to gaming tactics, which can lead to misleading promotional claims about a model's capabilities. The authors emphasize the need for developing robust anti-cheating mechanisms to ensure the reliability of automatic benchmarks."}, 'zh': {'title': '揭示自动化评估基准的漏洞：呼唤反作弊机制', 'desc': '这篇论文讨论了自动化语言模型评估基准的漏洞，这些基准如AlpacaEval 2.0等，虽然成本低且可扩展，但容易被操控。研究发现，即使是简单的“空模型”也能通过固定输出来欺骗这些基准，获得高胜率。论文强调了需要开发反作弊机制，以确保评估的可靠性。研究结果表明，现有的基准测试可能无法准确反映模型的真实性能。'}}}, {'id': 'https://huggingface.co/papers/2410.06293', 'title': 'Accelerated Preference Optimization for Large Language Model Alignment', 'url': 'https://huggingface.co/papers/2410.06293', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark.", 'score': 4, 'issue_id': 105, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'b5313b248ee2e065', 'data': {'categories': ['#training', '#math', '#rl', '#optimization', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🚀', 'ru': {'title': 'Ускоряем обучение языковых моделей с помощью импульса', 'desc': 'Статья представляет новый метод Accelerated Preference Optimization (APO) для обучения больших языковых моделей с учетом предпочтений человека. APO использует технику импульса Нестерова для ускорения процесса выравнивания моделей. Теоретически доказано, что APO имеет более быструю скорость сходимости по сравнению со стандартными методами итеративной оптимизации предпочтений. Эмпирические эксперименты на бенчмарке AlpacaEval 2.0 подтверждают превосходство APO над существующими подходами.'}, 'en': {'title': 'Accelerating Alignment: Momentum in RLHF', 'desc': "This paper introduces Accelerated Preference Optimization (APO), a new framework that enhances Reinforcement Learning from Human Feedback (RLHF) for aligning large language models with human preferences. It builds on Direct Preference Optimization (DPO) by applying Nesterov's momentum technique to improve the speed and efficiency of the optimization process. The authors demonstrate that APO achieves a faster convergence rate compared to traditional methods like DPO and Self-Play Preference Optimization (SPPO). Empirical results show that APO outperforms these existing approaches on the AlpacaEval 2.0 benchmark, confirming its effectiveness in optimizing language model alignment."}, 'zh': {'title': '加速偏好优化：提升RLHF效率的关键', 'desc': '强化学习中的人类反馈（RLHF）是对齐大型语言模型（LLM）与人类偏好的重要工具。直接偏好优化（DPO）是一种流行的方法，它将RLHF视为一个策略优化问题，而不需要明确估计奖励函数。本文提出了一种加速偏好优化（APO）框架，利用Nesterov动量技术加速LLM的对齐过程，并证明其收敛速度优于传统的迭代偏好优化方法。通过实验证明，APO在AlpacaEval 2.0基准测试中表现优于DPO和其他强基线。'}}}, {'id': 'https://huggingface.co/papers/2410.05269', 'title': 'Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models', 'url': 'https://huggingface.co/papers/2410.05269', 'abstract': 'Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.', 'score': 3, 'issue_id': 105, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '583137a3293a5c8c', 'data': {'categories': ['#training', '#data', '#synthetic', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Data Advisor: умный помощник для создания качественных данных в обучении ИИ', 'desc': 'Статья представляет метод Data Advisor для улучшения качества данных, генерируемых большими языковыми моделями (LLM) для их настройки. Data Advisor анализирует текущий набор данных, выявляет слабые места и даёт рекомендации для следующей итерации генерации. Этот метод легко интегрируется в существующие подходы к генерации данных. Эксперименты показали эффективность Data Advisor в повышении безопасности моделей без ущерба для их полезности.'}, 'en': {'title': 'Enhancing Data Quality in LLMs with Data Advisor', 'desc': 'This paper introduces Data Advisor, a novel method designed to improve the quality of data generated by large language models (LLMs). It addresses common issues such as low-quality data points and gaps in representation by monitoring the generated data and identifying its weaknesses. By following a set of predefined principles, Data Advisor provides guidance for subsequent data generation iterations, ensuring better alignment with the desired dataset characteristics. Experiments show that integrating Data Advisor enhances the safety of LLMs while maintaining their overall utility.'}, 'zh': {'title': '数据顾问：提升LLM数据质量的利器', 'desc': '数据在大型语言模型（LLM）对齐中至关重要。最近的研究探讨了使用LLM进行高效数据收集，但LLM生成的数据常常存在质量问题，如某些方面的代表性不足或缺失。为了解决这些问题，我们提出了数据顾问（Data Advisor），这是一种基于LLM的增强方法，能够根据所需数据集的特征生成数据。数据顾问可以轻松集成到现有的数据生成方法中，以提高数据质量和覆盖率，并在实验中证明了其在增强模型安全性方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.05629', 'title': 'Vector-ICL: In-context Learning with Continuous Vector Representations', 'url': 'https://huggingface.co/papers/2410.05629', 'abstract': "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.", 'score': 3, 'issue_id': 58, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '0f95824fcb35b2e3', 'data': {'categories': ['#science', '#video', '#audio', '#cv', '#training', '#graphs', '#data', '#optimization', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'LLM осваивают векторный мир: от текста к универсальным данным', 'desc': 'В статье исследуется возможность расширения способностей больших языковых моделей (LLM) к обучению в контексте на векторные данные из различных доменов. Авторы предлагают метод Vector-ICL, который позволяет LLM эффективно обрабатывать и учиться на проецированных векторах. Эксперименты показывают, что Vector-ICL часто превосходит как few-shot ICL, так и специализированные модели в различных задачах и модальностях. Результаты указывают на потенциал LLM в обработке векторных представлений за пределами традиционных токен-ориентированных парадигм.'}, 'en': {'title': 'Expanding LLM Horizons: From Text to Vectors', 'desc': "This paper investigates the ability of large language models (LLMs) to perform in-context learning (ICL) on continuous vectors from various domains. By using lightweight projectors to align input data with the LLM's embedding space, the study introduces Vector-ICL, where LLMs can effectively learn from these projected vectors. The research shows that pretraining projectors with general language modeling objectives is crucial for enabling Vector-ICL, and task-specific finetuning further improves its performance. Experiments demonstrate that Vector-ICL often outperforms few-shot ICL and domain-specific models across multiple tasks and modalities, suggesting LLMs' potential to handle vector data beyond traditional text tokens."}, 'zh': {'title': '向量上下文学习：超越传统的语言模型', 'desc': '大型语言模型（LLMs）在文本数据上展示了出色的上下文学习能力。我们研究这些能力是否可以扩展到来自不同领域的连续向量，这些向量是通过黑箱预训练编码器获得的。通过轻量级投影器将输入数据与LLM的嵌入空间对齐，我们发现LLM可以有效地处理和学习这些投影向量，这被称为向量上下文学习（Vector-ICL）。实验表明，预训练投影器与一般语言建模目标可以实现Vector-ICL，而特定任务的微调进一步提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2410.07707', 'title': 'MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.07707', 'abstract': 'Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page', 'score': 3, 'issue_id': 53, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'f7341783586984de', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'MotionGS: Реконструкция динамических сцен с помощью деформируемых 3D-гауссианов', 'desc': 'MotionGS - это новый фреймворк для деформируемого 3D-сплаттинга гауссианов, который использует явные ограничения движения объектов для улучшения реконструкции динамических сцен. Система включает модуль декомпозиции оптического потока, разделяющий его на поток камеры и поток движения объектов. Также предложен модуль уточнения положения камеры для оптимизации 3D-гауссианов и поз камеры. Эксперименты показывают превосходство MotionGS над современными методами в задаче реконструкции монокулярных динамических сцен.'}, 'en': {'title': 'MotionGS: Mastering Dynamic 3D Scenes with Motion-Aware Gaussians', 'desc': 'The paper introduces MotionGS, a new framework for reconstructing dynamic 3D scenes using Gaussian splatting. It addresses the challenge of optimizing object motion by incorporating explicit motion priors through an optical flow decoupling module. This module separates camera movement from object motion, allowing for more accurate deformation of 3D Gaussians. The framework also includes a camera pose refinement module to improve the accuracy of camera poses, resulting in superior performance compared to existing methods.'}, 'zh': {'title': 'MotionGS：动态场景重建的新突破', 'desc': '动态场景重建一直是3D视觉领域的长期挑战。最近，3D高斯点的出现为这个问题提供了新的见解。我们提出了一种新的可变形3D高斯点框架MotionGS，通过引入光流解耦模块来指导3D高斯的变形。实验表明，MotionGS在单目动态场景中优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2410.03437', 'title': 'Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs', 'url': 'https://huggingface.co/papers/2410.03437', 'abstract': 'Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.', 'score': 2, 'issue_id': 58, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '49b78243ef220e9c', 'data': {'categories': ['#reasoning', '#math', '#inference', '#optimization', '#transfer_learning', '#diffusion', '#architecture'], 'emoji': '🦓', 'ru': {'title': 'Zebra: решение ДУЧП с помощью обучения в контексте', 'desc': 'Zebra - это новый генеративный авторегрессионный трансформер для решения параметрических дифференциальных уравнений в частных производных (ДУЧП). В отличие от существующих подходов, Zebra не требует градиентной адаптации во время вывода, а использует обучение в контексте, вдохновленное возможностями больших языковых моделей. Модель динамически адаптируется к новым задачам, обусловливая входные последовательности, включающие контекстные траектории или предыдущие состояния. Zebra демонстрирует превосходную производительность по сравнению с существующими подходами в различных сценариях ДУЧП.'}, 'en': {'title': 'Zebra: Transforming PDE Solutions with In-Context Learning', 'desc': 'The paper introduces Zebra, a new generative auto-regressive transformer model designed to solve time-dependent parametric partial differential equations (PDEs) without needing gradient adaptation during inference. Zebra leverages in-context learning, inspired by large language models, to dynamically adapt to new tasks by conditioning on input sequences that include context trajectories. This approach allows Zebra to handle varying context sizes and provides uncertainty quantification by sampling multiple solution trajectories. The model is evaluated on various challenging PDE scenarios, showing its adaptability, robustness, and superior performance compared to existing methods.'}, 'zh': {'title': 'Zebra：无需梯度调整的PDE解决方案', 'desc': '这篇论文介绍了一种名为Zebra的新型生成自回归Transformer，用于解决参数化偏微分方程（PDEs）。Zebra通过在预训练和推理时利用上下文信息，动态适应新任务，而无需在推理时进行梯度调整。它能够灵活处理任意大小的上下文输入，并通过采样多个解轨迹来支持不确定性量化。实验表明，Zebra在各种复杂的PDE场景中表现出色，具有很强的适应性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2410.04808', 'title': 'LPZero: Language Model Zero-cost Proxy Search from Zero', 'url': 'https://huggingface.co/papers/2410.04808', 'abstract': "In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily rely on expert knowledge and incur significant trial-and-error costs. Particularly in NLP tasks, most existing ZC proxies fail to surpass the performance of the naive baseline. To address these challenges, we introduce a novel framework, LPZero, which is the first to automatically design ZC proxies for various tasks, achieving higher ranking consistency than human-designed proxies. Specifically, we model the ZC proxy as a symbolic equation and incorporate a unified proxy search space that encompasses existing ZC proxies, which are composed of a predefined set of mathematical symbols. To heuristically search for the best ZC proxy, LPZero incorporates genetic programming to find the optimal symbolic composition. We propose a Rule-based Pruning Strategy (RPS), which preemptively eliminates unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's superior ranking ability and performance on downstream tasks compared to current approaches.", 'score': 2, 'issue_id': 56, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '58e4dff9a00bb4f7', 'data': {'categories': ['#nlp', '#training', '#math', '#optimization', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'LPZero: Автоматическое проектирование прокси с нулевой стоимостью для эффективного нейроархитектурного поиска', 'desc': 'LPZero - это новый фреймворк для автоматического проектирования прокси с нулевой стоимостью (ZC) для различных задач нейроархитектурного поиска (NAS). Он моделирует ZC-прокси как символическое уравнение и включает единое пространство поиска прокси. LPZero использует генетическое программирование для поиска оптимальной символической композиции и предлагает стратегию правил-основанной обрезки (RPS) для устранения бесперспективных прокси. Эксперименты на FlexiBERT, GPT-2 и LLaMA-7B показывают превосходные способности ранжирования и производительность LPZero по сравнению с существующими подходами.'}, 'en': {'title': '"LPZero: Automating Efficiency in Neural Architecture Search"', 'desc': 'The paper introduces LPZero, a novel framework for Zero-shot Neural Architecture Search (NAS) that reduces computational demands by automatically designing Zero-cost (ZC) proxies. Unlike existing methods that rely heavily on expert knowledge, LPZero uses genetic programming to explore a unified search space of symbolic equations, improving ranking consistency across tasks. The framework includes a Rule-based Pruning Strategy (RPS) to eliminate ineffective proxies early, enhancing efficiency and performance. Experiments show that LPZero outperforms current methods in ranking and downstream tasks, particularly in natural language processing applications.'}, 'zh': {'title': 'LPZero：自动化零成本代理设计的突破', 'desc': '这篇论文介绍了一种新的框架LPZero，用于自动设计零成本代理（ZC proxies），以减少神经架构搜索（NAS）的计算需求。LPZero通过将ZC代理建模为符号方程，并结合统一的代理搜索空间，来提高代理的排名一致性。该框架使用遗传编程来寻找最佳的符号组合，并通过规则剪枝策略（RPS）提前淘汰不理想的代理。实验表明，LPZero在FlexiBERT、GPT-2和LLaMA-7B等任务中表现优于现有方法。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (5)', '#agi (1)', '#alignment (5)', '#architecture (17)', '#audio (2)', '#benchmark (9)', '#cv (9)', '#data (4)', '#dataset (5)', '#diffusion (5)', '#ethics (1)', '#games (4)', '#graphs (4)', '#hallucinations', '#healthcare (1)', '#inference (7)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (7)', '#multilingual', '#multimodal (6)', '#open_source (9)', '#optimization (16)', '#plp (1)', '#rag', '#reasoning (12)', '#rl (4)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (3)', '#training (16)', '#transfer_learning (5)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-11 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-11 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-11 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    