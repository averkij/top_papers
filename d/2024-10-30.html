
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. October 30.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">30 октября</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-29.html">⬅️ <span id="prev-date">29.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-31.html">➡️ <span id="next-date">31.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'};
        let feedDateNext = {'ru': '31.10', 'en': '10/31', 'zh': '10月31日'};
        let feedDatePrev = {'ru': '29.10', 'en': '10/29', 'zh': '10月29日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.18057', 'title': 'CLEAR: Character Unlearning in Textual and Visual Modalities', 'url': 'https://huggingface.co/papers/2410.18057', 'abstract': 'Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark. To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple ell_1 regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR', 'score': 194, 'issue_id': 337, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'ce7b8092b3ce7ef9', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'CLEAR: Новый стандарт для мультимодального разобучения в ИИ', 'desc': 'Статья представляет новый бенчмарк CLEAR для оценки методов мультимодального разобучения (MMU) в больших мультимодальных языковых моделях (MLLM). CLEAR содержит данные о 200 вымышленных личностях, включая 3700 изображений с соответствующими парами вопрос-ответ. Авторы адаптировали и оценили 10 методов машинного разобучения (MU) для мультимодальных задач. Исследование показало, что простая L1-регуляризация весов LoRA значительно снижает катастрофическое забывание, сохраняя производительность модели на оставшихся данных.'}, 'en': {'title': 'Enhancing Privacy with Multimodal Unlearning: Introducing CLEAR', 'desc': "This paper focuses on Machine Unlearning (MU), which is important for protecting privacy in deep learning models, especially in large multimodal language models (MLLMs). The authors identify a gap in multimodal unlearning (MMU) research and introduce CLEAR, a new benchmark that includes 200 fictitious individuals and 3,700 images with question-answer pairs for evaluating MMU methods. They assess 10 existing MU methods adapted for MMU and reveal unique challenges related to forgetting in multimodal contexts. Additionally, they find that applying simple ell_1 regularization on LoRA weights can help reduce catastrophic forgetting, thus maintaining the model's performance on the data that remains."}, 'zh': {'title': 'CLEAR：多模态遗忘的新基准', 'desc': '机器遗忘（MU）在深度学习模型中对于增强隐私和安全性至关重要，尤其是在大型多模态语言模型（MLLMs）中。尽管在文本和视觉模态上，MU已经取得了显著进展，但多模态遗忘（MMU）仍然未被充分探索，部分原因是缺乏合适的开源基准。为了解决这个问题，我们引入了CLEAR，一个新的基准，用于评估MMU方法。CLEAR包含200个虚构个体和3700张与相应问答对相关的图像，能够全面评估不同模态的遗忘效果。'}}}, {'id': 'https://huggingface.co/papers/2410.20424', 'title': 'AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions', 'url': 'https://huggingface.co/papers/2410.20424', 'abstract': 'Data science tasks involving tabular data present complex challenges that require sophisticated problem-solving approaches. We propose AutoKaggle, a powerful and user-centric framework that assists data scientists in completing daily data pipelines through a collaborative multi-agent system. AutoKaggle implements an iterative development process that combines code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. The framework offers highly customizable workflows, allowing users to intervene at each phase, thus integrating automated intelligence with human expertise. Our universal data science toolkit, comprising validated functions for data cleaning, feature engineering, and modeling, forms the foundation of this solution, enhancing productivity by streamlining common tasks. We selected 8 Kaggle competitions to simulate data processing workflows in real-world application scenarios. Evaluation results demonstrate that AutoKaggle achieves a validation submission rate of 0.85 and a comprehensive score of 0.82 in typical data science pipelines, fully proving its effectiveness and practicality in handling complex data science tasks.', 'score': 35, 'issue_id': 335, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'd86977fc81e85089', 'data': {'categories': ['#agents', '#data', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'AutoKaggle: ИИ-помощник для ускорения работы с данными', 'desc': 'AutoKaggle - это мощный фреймворк для решения задач с табличными данными с помощью мультиагентной системы. Он использует итеративный процесс разработки, включающий выполнение кода, отладку и модульное тестирование. Фреймворк предлагает настраиваемые рабочие процессы и универсальный набор инструментов для обработки данных. Оценка на 8 соревнованиях Kaggle показала высокую эффективность AutoKaggle в типичных задачах анализа данных.'}, 'en': {'title': 'Empowering Data Science with AutoKaggle: Automation Meets Expertise', 'desc': 'AutoKaggle is a framework designed to help data scientists manage complex tasks involving tabular data. It uses a collaborative multi-agent system to streamline data pipelines through an iterative process that includes code execution, debugging, and unit testing. The framework allows for customizable workflows, enabling users to integrate their expertise with automated processes. Evaluation on real-world Kaggle competitions shows that AutoKaggle significantly improves productivity and achieves high validation rates in data science tasks.'}, 'zh': {'title': 'AutoKaggle：智能与人类的完美结合', 'desc': '本文提出了一个名为AutoKaggle的框架，旨在帮助数据科学家处理复杂的表格数据任务。该框架通过协作的多智能体系统，支持数据管道的自动化和用户干预，结合了代码执行、调试和单元测试的迭代开发过程。AutoKaggle提供了高度可定制的工作流程，允许用户在每个阶段进行干预，从而将自动智能与人类专业知识相结合。通过在8个Kaggle竞赛中进行模拟，评估结果显示AutoKaggle在数据科学管道中具有良好的有效性和实用性。'}}}, {'id': 'https://huggingface.co/papers/2410.21411', 'title': 'SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization', 'url': 'https://huggingface.co/papers/2410.21411', 'abstract': 'Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named {\\name}, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. {\\name} introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.', 'score': 19, 'issue_id': 335, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'ad99b3e3b4ef165c', 'data': {'categories': ['#cv', '#interpretability', '#long_context', '#multimodal'], 'emoji': '👥', 'ru': {'title': 'SocialGPT: объединение зрения и языка для интерпретируемого распознавания социальных отношений', 'desc': 'Эта статья представляет новый подход к распознаванию социальных отношений на изображениях, названный SocialGPT. Метод объединяет возможности восприятия Визуальных Фундаментальных Моделей (VFM) и способности рассуждения Больших Языковых Моделей (LLM) в модульную структуру. SocialGPT переводит содержание изображения в текстовую социальную историю с помощью VFM, а затем использует LLM для рассуждений на основе текста. Авторы также предлагают метод оптимизации промптов под названием Greedy Segment Prompt Optimization (GSPO) для улучшения производительности.'}, 'en': {'title': 'Bridging Vision and Language for Social Relation Recognition', 'desc': 'This paper introduces a new framework called SocialGPT for social relation reasoning, which identifies relationships like friends or colleagues from images. It combines Vision Foundation Models (VFMs) for image analysis with Large Language Models (LLMs) for reasoning, allowing for better generalization and interpretability. The framework translates visual content into textual narratives and uses LLMs to reason about these narratives, achieving competitive results without additional training. Additionally, it proposes a method called Greedy Segment Prompt Optimization (GSPO) to enhance the performance of LLMs by optimizing prompts effectively, leading to improved outcomes across various image styles.'}, 'zh': {'title': '社交关系推理的新方法：结合视觉与语言的力量', 'desc': '社交关系推理旨在从图像中识别关系类别，如朋友、配偶和同事。当前的方法通常采用端到端的专用网络训练方式，但在泛化能力和可解释性方面存在局限。为了解决这些问题，我们提出了一个名为SocialGPT的框架，结合了视觉基础模型（VFM）和大型语言模型（LLM）的感知能力与推理能力。该方法在不额外训练模型的情况下，在两个数据库上实现了竞争性的零样本结果，并提供了可解释的答案。'}}}, {'id': 'https://huggingface.co/papers/2410.19609', 'title': 'OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization', 'url': 'https://huggingface.co/papers/2410.19609', 'abstract': 'The rapid development of large language and multimodal models has sparked significant interest in using proprietary models, such as GPT-4o, to develop autonomous agents capable of handling real-world scenarios like web navigation. Although recent open-source efforts have tried to equip agents with the ability to explore environments and continuously improve over time, they are building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents struggle to generalize to realistic settings that require multimodal perception abilities and lack ground-truth signals. In this paper, we introduce an open-source framework designed to facilitate the development of multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets.', 'score': 14, 'issue_id': 336, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'c9775abc4b4ddd0d', 'data': {'categories': ['#agents', '#multimodal', '#rl'], 'emoji': '🕸️', 'ru': {'title': 'Самообучающиеся мультимодальные веб-агенты для реального мира', 'desc': 'Эта статья представляет открытую платформу для разработки мультимодальных веб-агентов, способных к автономному исследованию реального мира и самосовершенствованию. Базовая модель обучается с помощью имитационного обучения, затем агент исследует открытый веб и собирает обратную связь. Далее агент улучшает свою стратегию, обучаясь на успешных траекториях, оцененных другой универсальной моделью. Экспериментальные результаты показывают, что веб-агент успешно улучшается после каждой итерации этого цикла.'}, 'en': {'title': 'Empowering Web Agents with Multimodal Learning and Self-Improvement', 'desc': 'This paper presents an open-source framework for developing multimodal web agents that can autonomously explore real-world environments. Unlike previous text-only agents that operate in synthetic settings, this framework allows agents to learn from real web interactions and improve their performance over time. The approach utilizes imitation learning to establish a baseline capability, followed by an exploration-feedback-optimization cycle where the agent refines its policy based on feedback from its own experiences. Experimental results indicate that the agent effectively enhances its skills through iterative learning, showcasing its ability to adapt and perform well in diverse scenarios.'}, 'zh': {'title': '自主多模态网络代理的探索与优化', 'desc': '本文介绍了一种开源框架，旨在开发能够自主进行真实世界探索的多模态网络代理。我们首先通过模仿学习训练基础模型，使其获得基本能力。然后，代理在开放网络中探索并收集其轨迹的反馈，进一步通过学习表现良好的轨迹来优化其策略。实验结果表明，该网络代理在每次迭代后成功自我改进，在多个测试集上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.22304', 'title': 'Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning', 'url': 'https://huggingface.co/papers/2410.22304', 'abstract': 'Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning Flows. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.', 'score': 14, 'issue_id': 334, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'ffb1cb8e8855bf5d', 'data': {'categories': ['#math', '#reasoning', '#rlhf'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических рассуждений LLM через онлайн-обучение потоков', 'desc': 'Эта статья представляет новый подход к созданию качественных цепочек рассуждений для дообучения больших языковых моделей (LLM) в задачах математических рассуждений. Метод использует инкрементальный процесс построения решения, где компонентные LLM совместно конструируют решение через итеративное взаимодействие. Обучение происходит с помощью онлайн-оптимизации прямых предпочтений (DPO) с использованием развёртываний, генерируя пары для DPO для каждого обучающего примера и обновляя модели в реальном времени. Авторы напрямую сравнивают качество цепочек рассуждений, созданных их методом, с теми, что получены прямым выводом модели, демонстрируя эффективность подхода в улучшении производительности LLM в задачах математических рассуждений.'}, 'en': {'title': 'Enhancing Mathematical Reasoning in LLMs with Collaborative Learning Flows', 'desc': 'This paper addresses the challenge of generating accurate reasoning traces for Large Language Models (LLMs) in mathematical tasks. It presents a new method that uses online learning Flows, where multiple LLMs work together to create solutions through iterative communication. The training process involves Direct Preference Optimization (DPO) with rollouts, allowing real-time updates and improvements. The results show that this approach significantly enhances the quality of reasoning traces compared to traditional model inference methods.'}, 'zh': {'title': '提升大型语言模型的数学推理能力', 'desc': '本文探讨了大型语言模型（LLMs）在数学推理中的能力，尤其是生成详细和准确的推理过程的挑战。我们提出了一种新方法，通过在线学习流（Flows）来生成高质量的推理过程，以便对LLM进行微调。该方法采用增量输出生成流，多个组件LLM通过迭代通信共同构建解决方案。我们使用在线直接偏好优化（DPO）学习进行训练，实时更新模型，从而提高LLM在数学推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.21647', 'title': "Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'", 'url': 'https://huggingface.co/papers/2410.21647', 'abstract': 'Large language models (LLMs) have shown remarkable ability in code generation with more than 90 pass@1 in solving Python coding problems in HumanEval and MBPP. Such high accuracy leads to the question: can LLMs replace human programmers? Existing manual crafted, simple, or single-line code generation benchmarks cannot answer this question due to their gap with real-world software development. To answer this question, we propose REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. In our evaluations on ten LLMs, none of the models can achieve more than 30 pass@1 on REPOCOD, disclosing the necessity of building stronger LLMs that can help developers in real-world software development.', 'score': 11, 'issue_id': 345, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '998f04d017622288', 'data': {'categories': ['#dataset', '#benchmark', '#plp'], 'emoji': '🧪', 'ru': {'title': 'REPOCOD: Реалистичная оценка возможностей LLM в генерации кода', 'desc': 'Статья представляет новый бенчмарк REPOCOD для оценки способностей больших языковых моделей (LLM) в генерации кода. В отличие от существующих простых бенчмарков, REPOCOD содержит 980 задач из реальных проектов, требующих понимания контекста на уровне файлов и репозиториев. Бенчмарк характеризуется наибольшей средней длиной канонического решения и высокой цикломатической сложностью. Тестирование 10 современных LLM на REPOCOD показало, что ни одна модель не достигает точности выше 30% pass@1, что указывает на необходимость разработки более мощных LLM для реальных задач разработки ПО.'}, 'en': {'title': 'REPOCOD: Bridging the Gap in Code Generation for Real-World Software Development', 'desc': 'This paper introduces REPOCOD, a new benchmark for evaluating code generation capabilities of large language models (LLMs) in real-world software development. Unlike previous benchmarks, REPOCOD includes 980 problems from popular projects, emphasizing the need for context beyond simple code snippets. The benchmark features complex problems with longer solutions and higher cyclomatic complexity, making it more representative of actual coding tasks. Evaluation results show that current LLMs struggle with REPOCOD, indicating a gap in their ability to assist human programmers effectively.'}, 'zh': {'title': 'REPOCOD：评估LLMs在真实软件开发中的能力', 'desc': '大型语言模型（LLMs）在代码生成方面表现出色，在解决Python编码问题时的通过率超过90%。然而，现有的简单或单行代码生成基准无法有效评估LLMs是否能替代人类程序员。为了解决这个问题，我们提出了REPOCOD，这是一个包含980个来自11个流行真实项目的问题的代码生成基准，其中超过58%的问题需要文件级或仓库级的上下文信息。我们的评估显示，十个LLMs在REPOCOD上的通过率都未超过30%，这表明需要构建更强大的LLMs来支持真实软件开发中的开发者。'}}}, {'id': 'https://huggingface.co/papers/2410.22330', 'title': 'Task Vectors are Cross-Modal', 'url': 'https://huggingface.co/papers/2410.22330', 'abstract': 'We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io.', 'score': 10, 'issue_id': 345, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'c6b834102135a2a9', 'data': {'categories': ['#multimodal', '#transfer_learning', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Универсальные векторные представления задач в мультимодальных моделях', 'desc': 'Исследование посвящено анализу внутренних представлений в мультимодальных моделях, объединяющих зрение и язык (VLM). Авторы обнаружили, что концептуально схожие задачи отображаются на похожие векторные представления независимо от способа их спецификации. В процессе генерации ответа токены в VLM проходят три фазы: ввод, задача и ответ. Векторные представления задач оказались достаточно общими, чтобы их можно было переносить между модальностями.'}, 'en': {'title': 'Unified Task Representation Across Modalities in VLMs', 'desc': 'This paper explores how vision-and-language models (VLMs) represent tasks using different types of inputs, such as text and images. It reveals that similar tasks are represented by similar vectors, regardless of their input format. The authors identify a three-phase process in VLMs where inputs are transformed into task representations and then into answers. Furthermore, they demonstrate that task vectors can be effectively transferred between modalities, and combining different types of task vectors enhances performance.'}, 'zh': {'title': '跨模态任务表示的统一性', 'desc': '我们研究了视觉与语言模型（VLMs）的内部表示，以及它们如何编码任务表示。我们发现，无论任务是通过示例还是指令指定，概念上相似的任务会映射到相似的任务向量表示。VLMs 输出答案的过程分为三个阶段：输入、任务和答案，这一过程在不同的模态和规格中是一致的。我们的研究表明，任务向量可以在一种模态（例如文本）中生成，并转移到另一种模态（例如图像），并且结合示例和指令的任务向量可以产生更好的任务表示。'}}}, {'id': 'https://huggingface.co/papers/2410.21333', 'title': 'Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse', 'url': 'https://huggingface.co/papers/2410.21333', 'abstract': 'Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.', 'score': 9, 'issue_id': 341, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'c5caddd15a467732', 'data': {'categories': ['#interpretability', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Когда размышления вредят: ограничения CoT в языковых моделях', 'desc': 'Статья исследует влияние промптинга с цепочкой рассуждений (CoT) на производительность языковых моделей. Авторы выявляют задачи, где CoT снижает эффективность, опираясь на примеры из когнитивной психологии. Эксперименты показывают значительное падение точности современных моделей при использовании CoT в определенных сценариях. Исследование устанавливает связь между литературой о человеческих рассуждениях и оценкой CoT в контексте языковых моделей.'}, 'en': {'title': 'Understanding When Chain-of-Thought Prompting Fails in AI Models', 'desc': 'This paper investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language and multimodal models, particularly focusing on when it may hinder performance. By drawing parallels from cognitive psychology, the authors explore scenarios where verbal reasoning negatively impacts human performance and whether these scenarios apply to language models. Through extensive experiments, they demonstrate that certain tasks, such as implicit statistical learning and visual recognition, can lead to significant performance drops in models when using CoT prompting. The findings suggest that understanding human cognitive limitations can provide insights into optimizing model performance and prompt design.'}, 'zh': {'title': '链式思维的影响：何时有益，何时有害', 'desc': '链式思维（CoT）提示是一种在大型语言和多模态模型中广泛使用的策略。本文探讨了在何种情况下链式思维会降低模型性能，借鉴了认知心理学的研究。通过对隐性统计学习、视觉识别和包含例外的模式分类等任务的实验，我们发现使用推理时的链式思维会导致模型性能显著下降。我们的研究表明，理解人类思维对模型性能的影响，可以帮助我们更好地选择提示和推理策略。'}}}, {'id': 'https://huggingface.co/papers/2410.22325', 'title': 'Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset', 'url': 'https://huggingface.co/papers/2410.22325', 'abstract': 'The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the "manipulation centricity" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot\'s proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project website: https://robots-pretrain-robots.github.io/.', 'score': 9, 'issue_id': 335, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'b8a6dec29ce881c9', 'data': {'categories': ['#agents', '#dataset', '#robotics'], 'emoji': '🦾', 'ru': {'title': 'Улучшение роботизированных манипуляций через предобучение с учетом динамики', 'desc': 'Эта статья представляет новый подход к предобучению визуальных представлений для робототехники под названием Manipulation Centric Representation (MCR). MCR использует датасет DROID и информацию о движениях робота для улучшения эффективности манипуляций. Авторы вводят новую контрастивную функцию потерь, которая сопоставляет визуальные наблюдения с динамикой состояний и действий робота. Эмпирические результаты показывают, что MCR превосходит базовые методы на 14.8% в симуляциях и на 76.9% в реальных задачах.'}, 'en': {'title': 'Enhancing Robot Learning with Manipulation Centric Representation', 'desc': 'This paper discusses the importance of pre-training visual representations for improving robot learning efficiency. It highlights the challenges posed by using human videos for training, which can lead to distribution shifts and a lack of essential dynamic information. The authors introduce a new framework called Manipulation Centric Representation (MCR) that integrates visual features with dynamic data from robotic tasks to enhance performance. Their empirical results show that MCR significantly outperforms existing methods in both simulation and real-world tasks, demonstrating its effectiveness in robotic manipulation.'}, 'zh': {'title': '操作中心表示：提升机器人学习效率的关键', 'desc': '本论文探讨了视觉表示的预训练如何提高机器人学习的效率。由于缺乏大规模的领域内机器人数据集，之前的研究利用人类视频进行预训练，但这些视频的表示存在分布偏移，并缺乏完成任务所需的动态信息。我们提出了一种新的表示学习框架，称为操作中心表示（MCR），它同时捕捉视觉特征和操作任务的动态信息。实验结果表明，MCR在多个模拟领域和真实任务中显著提高了机器人的操作性能。'}}}, {'id': 'https://huggingface.co/papers/2410.21465', 'title': 'ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference', 'url': 'https://huggingface.co/papers/2410.21465', 'abstract': 'With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6times larger batch sizes and boost throughput by up to 3.04times on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.', 'score': 9, 'issue_id': 334, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'f954b9ea6eb1a3ff', 'data': {'categories': ['#benchmark', '#inference', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'ShadowKV: Ускорение вывода длинноконтекстных LLM без компромиссов', 'desc': 'Статья представляет ShadowKV - систему для высокопроизводительного вывода длинноконтекстных больших языковых моделей (LLM). ShadowKV хранит кэш ключей низкого ранга и выгружает кэш значений для уменьшения объема памяти, что позволяет обрабатывать большие пакеты и длинные последовательности. Система использует точную стратегию выбора KV, восстанавливая минимальные разреженные пары KV на лету для минимизации задержки декодирования. Эксперименты показали, что ShadowKV может увеличить размер пакета до 6 раз и повысить производительность до 3,04 раз на GPU A100 без потери точности.'}, 'en': {'title': 'Boosting Long-Context LLM Inference with ShadowKV', 'desc': 'The paper introduces ShadowKV, a system designed to enhance the efficiency of long-context large language model (LLM) inference. It addresses the challenges of high memory usage and low throughput caused by the expanding key-value (KV) cache during token generation. ShadowKV reduces memory consumption by storing a low-rank key cache while offloading the value cache, allowing for larger batch sizes and longer sequences. The system also implements a KV selection strategy that dynamically reconstructs sparse KV pairs, significantly improving throughput without compromising accuracy.'}, 'zh': {'title': '高效推理，提升长上下文模型性能', 'desc': '随着长上下文大语言模型（LLMs）的广泛应用，对高吞吐量推理的需求不断增加。本文提出了ShadowKV，一个高吞吐量的长上下文LLM推理系统，通过存储低秩键缓存并将值缓存卸载，从而减少内存占用。ShadowKV采用准确的KV选择策略，实时重构最小稀疏KV对，以降低解码延迟。实验结果表明，ShadowKV在多个基准测试中表现优异，支持更大的批量大小并显著提高吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2410.21845', 'title': 'Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.21845', 'abstract': 'Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. Videos and code are available at our project website https://hil-serl.github.io/.', 'score': 8, 'issue_id': 334, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'b8302dbf79e25f7d', 'data': {'categories': ['#rl', '#rlhf', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Человек и ИИ: совместное обучение роботов сложным манипуляциям', 'desc': 'В этой статье представлена система обучения с подкреплением (RL) для роботизированной манипуляции с участием человека. Система демонстрирует впечатляющие результаты в различных задачах ловкой манипуляции, включая динамическую манипуляцию, точную сборку и координацию двух рук. Подход интегрирует демонстрации и коррекции человека, эффективные алгоритмы RL и другие системные решения для обучения политик с почти идеальным уровнем успеха за 1-2,5 часа тренировки. Результаты показывают значительное превосходство над базовыми методами имитационного обучения и предыдущими подходами RL.'}, 'en': {'title': 'Empowering Robots with Human-guided Reinforcement Learning for Complex Manipulation', 'desc': 'This paper presents a novel human-in-the-loop reinforcement learning (RL) system designed for robotic manipulation tasks. By combining human demonstrations and corrections with efficient RL algorithms, the system achieves high success rates and quick training times for complex tasks like dynamic manipulation and dual-arm coordination. The results show a significant improvement over traditional imitation learning and previous RL methods, with a twofold increase in success rates and faster execution times. The findings indicate that RL can effectively learn complex manipulation skills in real-world scenarios, paving the way for advancements in robotic applications.'}, 'zh': {'title': '人机协作强化学习：实现复杂机器人操作的突破', 'desc': '强化学习（RL）在自主获取复杂机器人操作技能方面具有很大潜力，但在现实环境中实现这一潜力面临挑战。我们提出了一种人机协作的基于视觉的RL系统，在多种灵巧操作任务中表现出色，包括动态操作、精密组装和双臂协调。该方法结合了演示和人类纠正、有效的RL算法以及其他系统设计选择，使得在仅1到2.5小时的训练内学习到接近完美的成功率和快速的循环时间。我们的实验结果表明，该方法在成功率和执行速度上显著优于模仿学习基线和之前的RL方法，展示了RL在现实世界中学习复杂视觉操作策略的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.21242', 'title': 'Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback', 'url': 'https://huggingface.co/papers/2410.21242', 'abstract': 'Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Our experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query.', 'score': 6, 'issue_id': 343, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '4bd9d5661581a6a8', 'data': {'categories': ['#benchmark', '#dataset', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Эффективный поиск без обучения: от гипотетических документов к оценке релевантности', 'desc': 'Статья представляет новый метод под названием ReDE-RF для эффективного поиска информации без размеченных данных. Вместо генерации гипотетических документов с помощью большой языковой модели (LLM), ReDE-RF использует LLM для оценки релевантности существующих документов. Это позволяет избежать необходимости в доменных знаниях у LLM и значительно ускоряет процесс поиска. Эксперименты показывают, что ReDE-RF превосходит современные методы поиска без обучения на различных наборах данных с ограниченными ресурсами.'}, 'en': {'title': 'ReDE-RF: Efficient Relevance Estimation for Dense Retrieval', 'desc': 'This paper addresses the challenges of building effective dense retrieval systems without relevance supervision. It introduces a method called Real Document Embeddings from Relevance Feedback (ReDE-RF), which reframes the generation of hypothetical documents as a relevance estimation task. By using a Large Language Model (LLM) to select relevant documents for nearest neighbor search, the method reduces the need for domain-specific knowledge. The approach not only improves the efficiency of the retrieval process by requiring the LLM to output a single token but also enhances performance on low-resource datasets compared to existing methods.'}, 'zh': {'title': '通过相关反馈提升检索效率', 'desc': '在缺乏相关性监督的情况下，构建有效的密集检索系统仍然很困难。最近的研究尝试使用大型语言模型（LLM）生成假设文档，以找到最接近的真实文档。然而，这种方法完全依赖于LLM具备与查询相关的领域知识，这在实际中可能不切实际。为了解决这些问题，我们提出了基于相关反馈的真实文档嵌入（ReDE-RF），通过将假设文档生成重新框定为相关性估计任务，显著提高了检索效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2410.20305', 'title': 'Accelerating Direct Preference Optimization with Prefix Sharing', 'url': 'https://huggingface.co/papers/2410.20305', 'abstract': 'Offline paired preference optimization algorithms have become a popular approach for fine-tuning on preference data, outperforming traditional supervised fine-tuning in various tasks. However, traditional implementations often involve redundant computations, especially for tasks with long shared prompts. We introduce prefix sharing for preference tuning, a novel technique that processes chosen and rejected responses as one sequence with a shared prefix. To prevent cross-response contamination, we use a custom block-sparse attention mask. Our method achieves 1.1-1.5times improvement in training throughput on popular DPO datasets, without any effect on convergence. When combined with sequence packing, we observe consistent 1.3-1.6times speedups, benefiting even datasets with smaller sequence lengths. While we focus on Direct Preference Optimization (DPO), our approach is applicable to other paired preference tuning methods. By enhancing computational efficiency, our work contributes to making preference-based fine-tuning more accessible for a wider range of applications and model sizes. We open-source our code at https://github.com/frankxwang/dpo-prefix-sharing.', 'score': 5, 'issue_id': 343, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'e6a34a0109321884', 'data': {'categories': ['#optimization', '#rlhf'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения языковых моделей с помощью разделения префиксов', 'desc': 'Статья представляет новый метод оптимизации обучения языковых моделей на основе предпочтений пользователей. Авторы предлагают технику разделения префиксов для обработки выбранных и отвергнутых ответов как единой последовательности. Использование специальной маски внимания предотвращает перекрестное загрязнение между ответами. Метод демонстрирует значительное увеличение скорости обучения на популярных наборах данных для DPO без ущерба для сходимости.'}, 'en': {'title': 'Boosting Efficiency in Preference Tuning with Prefix Sharing', 'desc': 'This paper presents a new technique called prefix sharing for optimizing preference tuning in machine learning. It addresses the inefficiencies in traditional methods that lead to redundant computations, especially with long prompts. By processing chosen and rejected responses together with a shared prefix and using a custom block-sparse attention mask, the authors improve training throughput significantly. Their method not only enhances efficiency for Direct Preference Optimization (DPO) but is also applicable to other paired preference tuning methods, making fine-tuning more accessible.'}, 'zh': {'title': '提升偏好微调效率的新方法', 'desc': '离线配对偏好优化算法在偏好数据的微调中变得越来越流行，超越了传统的监督微调方法。我们提出了一种新技术——前缀共享，用于偏好调优，它将选择和拒绝的响应作为一个序列处理，并使用共享前缀。为了防止响应之间的交叉污染，我们采用了自定义的块稀疏注意力掩码。我们的研究提高了训练吞吐量，尤其是在流行的DPO数据集上，提升了1.1到1.5倍，且不影响收敛性。'}}}, {'id': 'https://huggingface.co/papers/2410.20088', 'title': 'RARe: Retrieval Augmented Retrieval with In-Context Examples', 'url': 'https://huggingface.co/papers/2410.20088', 'abstract': 'We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.', 'score': 5, 'issue_id': 342, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '03c6d910a07fe4c7', 'data': {'categories': ['#benchmark', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Улучшение поисковых систем с помощью обучения на примерах в контексте', 'desc': 'Исследователи изучают возможность улучшения работы моделей встраивания в задачах поиска с помощью примеров в контексте, широко используемых в языковых моделях. Предложен метод RARe, который дообучает предварительно обученную модель с использованием семантически схожих примеров в контексте. Этот подход демонстрирует улучшение производительности до 2.72% nDCG на различных наборах данных для поиска в открытом домене. RARe также показывает более сильную обобщающую способность вне домена по сравнению с моделями, не использующими примеры в контексте.'}, 'en': {'title': 'Enhancing Retrieval with In-Context Examples: The RARe Approach', 'desc': 'This paper explores the use of in-context examples to enhance the performance of embedding models in retrieval tasks. Unlike decoder-only language models, simply adding these examples to queries does not yield immediate benefits. The authors propose a method called RARe, which fine-tunes a pre-trained model using semantically similar in-context examples to the target query. Their approach shows consistent performance improvements, achieving up to +2.72% nDCG across various datasets, and demonstrates better out-of-domain generalization compared to traditional methods.'}, 'zh': {'title': '利用上下文示例提升检索模型性能', 'desc': '本文研究了在检索任务中，使用上下文示例是否能提高嵌入模型的性能。与语言模型不同，简单地将上下文示例添加到目标查询并不能直接奏效。我们提出了一种简单的方法RARe，通过对预训练模型进行微调，使其能够使用与目标查询语义相似的上下文示例。实验结果表明，RARe在多个开放域检索数据集上实现了最高+2.72%的nDCG性能提升，并展现出更强的领域外泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2410.19482', 'title': 'Measuring memorization through probabilistic discoverable extraction', 'url': 'https://huggingface.co/papers/2410.19482', 'abstract': 'Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Our experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. We further investigate the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. Our contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions.', 'score': 4, 'issue_id': 343, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'd39237a67eee0b18', 'data': {'categories': ['#alignment', '#interpretability', '#security'], 'emoji': '🧠', 'ru': {'title': 'Вероятностный подход к измерению запоминания в языковых моделях', 'desc': 'Эта статья предлагает новый метод оценки уровня запоминания данных большими языковыми моделями (LLM). Авторы вводят вероятностное расширение концепции извлекаемого извлечения, которое учитывает различные схемы семплирования и множественные попытки. Эксперименты показывают, что этот подход может выявить случаи более высокого уровня запоминания по сравнению с традиционными методами. Исследование также анализирует влияние различных схем семплирования на извлекаемость данных, предоставляя более полную оценку рисков, связанных с запоминанием в LLM.'}, 'en': {'title': 'Unveiling the Hidden Memorization of Language Models', 'desc': "This paper addresses the issue of large language models (LLMs) memorizing sensitive training data, which can lead to privacy concerns. It critiques existing methods for measuring memorization rates, particularly the single-sequence greedy sampling approach, which may not accurately reflect true memorization levels. The authors propose a new probabilistic method that assesses the likelihood of extracting specific sequences from generated samples, taking into account various sampling strategies. Their findings indicate that this new approach can uncover higher memorization rates than previously reported, offering a more nuanced understanding of LLMs' memorization capabilities and associated risks."}, 'zh': {'title': '提升LLM记忆评估的概率性方法', 'desc': '大型语言模型（LLMs）可能会记住训练数据，这引发了对敏感信息提取的担忧。现有的测量LLMs记忆率的方法主要依赖于单序列贪婪采样，可能低估了真实的记忆程度。本文提出了一种可扩展的概率性提取方法，量化在生成样本集中提取目标序列的概率，考虑了不同的采样方案和多次尝试。我们的实验表明，这种概率性测量能够揭示出比现有方法更高的记忆率，从而提供了对LLM记忆及其相关风险的更全面评估。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (3)', '#agi', '#alignment (1)', '#architecture', '#audio', '#benchmark (5)', '#cv (2)', '#data (1)', '#dataset (5)', '#diffusion', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference (1)', '#interpretability (3)', '#long_context (2)', '#math (1)', '#medicine', '#multilingual', '#multimodal (5)', '#optimization (1)', '#plp (1)', '#rag (2)', '#reasoning (2)', '#rl (2)', '#rlhf (3)', '#robotics (2)', '#security (1)', '#story_generation', '#survey', '#synthetic', '#training', '#transfer_learning (1)', '#translation', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-30 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-30 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-30 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    