{
    "date": {
        "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 9",
        "zh": "9æœˆ9æ—¥"
    },
    "time_utc": "2025-09-09 02:20",
    "weekday": 1,
    "issue_id": 5783,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.01656",
            "title": "Reinforced Visual Perception with Tools",
            "url": "https://huggingface.co/papers/2509.01656",
            "abstract": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
            "score": 10,
            "issue_id": 5783,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "08ec116a2ed1dc2c",
            "authors": [
                "Zetong Zhou",
                "Dongping Chen",
                "Zixian Ma",
                "Zhihan Hu",
                "Mingyang Fu",
                "Sinan Wang",
                "Yao Wan",
                "Zhou Zhao",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "ONE Lab, HUST",
                "University of Maryland",
                "University of Washington",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01656.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReVPT, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ReVPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ RL Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SAT, CV-Bench, BLINK Ğ¸ MMStar. ReVPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs",
                    "desc": "ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods."
                },
                "zh": {
                    "title": "ReVPTï¼šæå‡å¤šæ¨¡æ€LLMçš„è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "ReVPTæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè¾¾åˆ°äº†è§†è§‰åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ¶‰åŠå¤æ‚çš„æ„ŸçŸ¥å’Œé€»è¾‘è¿‡ç¨‹ï¼Œä½†å°†è®¡ç®—æœºè§†è§‰æ¨¡å‹åº”ç”¨äºä¸€èˆ¬è§†è§‰æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå°†è§†è§‰æ¨¡å‹ä¸LLMç»“åˆå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®ç”Ÿæˆæˆæœ¬é«˜ã€æ•°æ®è¿‡æ»¤ä¾èµ–æ€§å¼ºå’Œæ³›åŒ–èƒ½åŠ›å·®ç­‰å…³é”®é™åˆ¶ã€‚ReVPTé€šè¿‡å¼•å…¥åŸºäºGRPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè®­ç»ƒæ¨¡å‹ä½¿ç”¨å››ç§è§†è§‰å·¥å…·è¿›è¡Œæ¨ç†ï¼Œä»è€Œæœ‰æ•ˆè§£å†³äº†è¿™äº›é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06949",
            "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
            "url": "https://huggingface.co/papers/2509.06949",
            "abstract": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
            "score": 5,
            "issue_id": 5783,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "f6f75969a4d93684",
            "authors": [
                "Yinjie Wang",
                "Ling Yang",
                "Bowen Li",
                "Ye Tian",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06949.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "TraceRL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ TraceRL Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TraDo, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Language Models with Trajectory-Aware Reinforcement Learning",
                    "desc": "TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks."
                },
                "zh": {
                    "title": "TraceRLï¼šæå‡æ¨ç†æ€§èƒ½çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ",
                    "desc": "TraceRLæ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨åæœŸè®­ç»ƒä¸­èå…¥ä¼˜å…ˆæ¨ç†è½¨è¿¹ï¼Œé€‚ç”¨äºä¸åŒçš„æ¨¡å‹æ¶æ„ã€‚è¯¥æ¡†æ¶é…å¤‡äº†åŸºäºæ‰©æ•£çš„ä»·å€¼æ¨¡å‹ï¼Œæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶åœ¨å¤æ‚çš„æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šå±•ç¤ºäº†æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTraceRLè¿˜å¯ä»¥å°†ç‰¹å®šå—çš„æ¨¡å‹é€‚åº”åˆ°æ›´å¤§çš„å—ï¼Œä»è€Œæé«˜é‡‡æ ·çš„çµæ´»æ€§ã€‚é€šè¿‡ä½¿ç”¨TraceRLï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹TraDoï¼Œå°½ç®¡å…¶è§„æ¨¡å°äº7Bçš„è‡ªå›å½’æ¨¡å‹ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä»ç„¶è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06155",
            "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
            "url": "https://huggingface.co/papers/2509.06155",
            "abstract": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.",
            "score": 4,
            "issue_id": 5783,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 7",
                "zh": "9æœˆ7æ—¥"
            },
            "hash": "9f7107d4dbe89f53",
            "authors": [
                "Duomin Wang",
                "Wei Zuo",
                "Aojie Li",
                "Ling-Hao Chen",
                "Xinyao Liao",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Daxin Jiang",
                "Gang Yu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology(GuangZhou)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06155.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "UniVerse-1: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UniVerse-1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ \"stitching of experts\". Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 7600 Ñ‡Ğ°ÑĞ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "UniVerse-1: Harmonizing Audio and Video Generation",
                    "desc": "UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field."
                },
                "zh": {
                    "title": "ç»Ÿä¸€éŸ³è§†é¢‘ç”Ÿæˆçš„æœªæ¥",
                    "desc": "UniVerse-1 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨ä¸“å®¶æ‹¼æ¥æŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘å’ŒéŸ³ä¹æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ï¼Œç¡®ä¿æ—¶é—´ä¸Šçš„å‡†ç¡®å¯¹é½ï¼Œç”Ÿæˆé«˜è´¨é‡çš„éŸ³è§†é¢‘è¾“å‡ºã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦èåˆé¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„ä½æ•ˆï¼Œå……åˆ†åˆ©ç”¨äº†å·²æœ‰çš„åŸºç¡€èƒ½åŠ›ã€‚ä¸ºäº†ç¡®ä¿éŸ³é¢‘å’Œè§†é¢‘å†…å®¹çš„å‡†ç¡®æ³¨é‡Šå’Œæ—¶é—´å¯¹é½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æ³¨é‡Šç®¡é“ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†æ‰€éœ€çš„æ•°æ®å¹¶ç”Ÿæˆæ ‡ç­¾ã€‚ç»è¿‡çº¦7600å°æ—¶çš„éŸ³è§†é¢‘æ•°æ®å¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆåè°ƒè‰¯å¥½çš„éŸ³è§†é¢‘å†…å®¹ï¼Œå¹¶åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢å®ç°å¼ºå¯¹é½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-08.html",
    "link_next": "2025-09-10.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "08.09",
        "en": "09/08",
        "zh": "9æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}