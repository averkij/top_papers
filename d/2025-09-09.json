{
    "date": {
        "ru": "9 сентября",
        "en": "September 9",
        "zh": "9月9日"
    },
    "time_utc": "2025-09-09 02:20",
    "weekday": 1,
    "issue_id": 5783,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.01656",
            "title": "Reinforced Visual Perception with Tools",
            "url": "https://huggingface.co/papers/2509.01656",
            "abstract": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
            "score": 10,
            "issue_id": 5783,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "08ec116a2ed1dc2c",
            "authors": [
                "Zetong Zhou",
                "Dongping Chen",
                "Zixian Ma",
                "Zhihan Hu",
                "Mingyang Fu",
                "Sinan Wang",
                "Yao Wan",
                "Zhou Zhao",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "ONE Lab, HUST",
                "University of Maryland",
                "University of Washington",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01656.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в визуальном мышлении ИИ через обучение с подкреплением",
                    "desc": "Статья представляет метод ReVPT, улучшающий способности мультимодальных языковых моделей к визуальному рассуждению с помощью обучения с подкреплением. ReVPT использует новый алгоритм RL на основе GRPO для обучения моделей рассуждать с помощью набора из четырех визуальных инструментов. Эксперименты показывают, что метод достигает лучших результатов на нескольких бенчмарках, включая SAT, CV-Bench, BLINK и MMStar. ReVPT значительно превосходит базовые модели, обученные с учителем и на текстовых данных."
                },
                "en": {
                    "title": "ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs",
                    "desc": "ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods."
                },
                "zh": {
                    "title": "ReVPT：提升多模态LLM的视觉推理能力",
                    "desc": "ReVPT是一种增强多模态大语言模型（LLM）视觉推理能力的方法，采用强化学习技术，达到了视觉基准测试的最先进性能。视觉推理是人类智能的核心，涉及复杂的感知和逻辑过程，但将计算机视觉模型应用于一般视觉推理仍然具有挑战性。以往的研究表明，通过监督微调将视觉模型与LLM结合可以提高性能，但存在数据生成成本高、数据过滤依赖性强和泛化能力差等关键限制。ReVPT通过引入基于GRPO的新型强化学习算法，训练模型使用四种视觉工具进行推理，从而有效解决了这些问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06949",
            "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
            "url": "https://huggingface.co/papers/2509.06949",
            "abstract": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
            "score": 5,
            "issue_id": 5783,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "f6f75969a4d93684",
            "authors": [
                "Yinjie Wang",
                "Ling Yang",
                "Bowen Li",
                "Ye Tian",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06949.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений языковых моделей с помощью траекторного обучения с подкреплением",
                    "desc": "TraceRL - это новый метод обучения с подкреплением для диффузионных языковых моделей, который улучшает их способность рассуждать над сложными задачами. Он использует траекторно-ориентированный подход и диффузионную модель ценности для повышения стабильности обучения. Метод позволяет адаптировать модели к работе с более длинными блоками текста, повышая гибкость сэмплирования. С помощью TraceRL авторы создали серию современных диффузионных языковых моделей TraDo, превосходящих более крупные авторегрессионные модели в задачах математических рассуждений."
                },
                "en": {
                    "title": "Enhancing Language Models with Trajectory-Aware Reinforcement Learning",
                    "desc": "TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks."
                },
                "zh": {
                    "title": "TraceRL：提升推理性能的轨迹感知强化学习",
                    "desc": "TraceRL是一种针对扩散语言模型的轨迹感知强化学习框架，能够在后期训练中融入优先推理轨迹，适用于不同的模型架构。该框架配备了基于扩散的价值模型，提升了训练的稳定性，并在复杂的数学和编程任务上展示了更好的推理性能。此外，TraceRL还可以将特定块的模型适应到更大的块，从而提高采样的灵活性。通过使用TraceRL，我们开发了一系列最先进的扩散语言模型TraDo，尽管其规模小于7B的自回归模型，但在复杂的数学推理任务中仍然表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06155",
            "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
            "url": "https://huggingface.co/papers/2509.06155",
            "abstract": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.",
            "score": 4,
            "issue_id": 5783,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 сентября",
                "en": "September 7",
                "zh": "9月7日"
            },
            "hash": "9f7107d4dbe89f53",
            "authors": [
                "Duomin Wang",
                "Wei Zuo",
                "Aojie Li",
                "Ling-Hao Chen",
                "Xinyao Liao",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Daxin Jiang",
                "Gang Yu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology(GuangZhou)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06155.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "UniVerse-1: синергия аудио и видео в одном модели",
                    "desc": "В статье представлена модель UniVerse-1, которая объединяет генерацию аудио и видео, используя технику \"stitching of experts\". Это позволяет эффективно использовать уже обученные модели для видео и музыки, обеспечивая точное временное согласование. Для улучшения аннотаций и временного выравнивания звуков и речи с видео, разработан онлайн-пайплайн аннотаций. Модель была дообучена на 7600 часах данных и показала высокое качество координации аудио-визуальных элементов."
                },
                "en": {
                    "title": "UniVerse-1: Harmonizing Audio and Video Generation",
                    "desc": "UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field."
                },
                "zh": {
                    "title": "统一音视频生成的未来",
                    "desc": "UniVerse-1 是一个统一的音频视频生成模型，采用专家拼接技术，将预训练的视频和音乐模型结合在一起，确保时间上的准确对齐，生成高质量的音视频输出。该模型通过深度融合预训练的专家模型，避免了从头开始训练的低效，充分利用了已有的基础能力。为了确保音频和视频内容的准确注释和时间对齐，我们开发了一个在线注释管道，在训练过程中处理所需的数据并生成标签。经过约7600小时的音视频数据微调后，我们的模型能够生成协调良好的音视频内容，并在语音生成方面实现强对齐。"
                }
            }
        }
    ],
    "link_prev": "2025-09-08.html",
    "link_next": "2025-09-10.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "08.09",
        "en": "09/08",
        "zh": "9月8日"
    },
    "short_date_next": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9月10日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}