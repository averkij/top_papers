{
    "date": {
        "ru": "9 сентября",
        "en": "September 9",
        "zh": "9月9日"
    },
    "time_utc": "2025-09-09 07:12",
    "weekday": 1,
    "issue_id": 5788,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.06160",
            "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
            "url": "https://huggingface.co/papers/2509.06160",
            "abstract": "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.",
            "score": 57,
            "issue_id": 5784,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 сентября",
                "en": "September 7",
                "zh": "9月7日"
            },
            "hash": "4df21d6c69df73d5",
            "authors": [
                "Haozhe Wang",
                "Haoran Que",
                "Qixin Xu",
                "Minghao Liu",
                "Wangchunshu Zhou",
                "Jiazhan Feng",
                "Wanjun Zhong",
                "Wei Ye",
                "Tong Yang",
                "Wenhao Huang",
                "Ge Zhang",
                "Fangzhen Lin"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University of Science and Technology",
                "M-A-P",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06160.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#architecture",
                    "#training",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Обратная инженерия рассуждений: новый подход к глубокому обучению",
                    "desc": "REER (Reverse-Engineered Reasoning) - это новая парадигма в глубоком обучении, которая использует обратную инженерию для обнаружения пошаговых процессов рассуждения. Этот подход позволяет модели эффективно выполнять открытые задачи, преодолевая ограничения традиционных методов, таких как обучение с подкреплением и дистилляция инструкций. На основе REER был создан набор данных DeepWriting-20K и обучена модель DeepWriter-8B, которая показывает результаты, конкурентоспособные с ведущими проприетарными моделями."
                },
                "en": {
                    "title": "Unlocking Creativity with Reverse Engineering in Deep Reasoning",
                    "desc": "REER introduces a novel approach to deep reasoning by utilizing reverse engineering to uncover the step-by-step processes behind successful solutions. This method addresses the limitations of traditional reinforcement learning and instruction distillation, which struggle with open-ended tasks due to unclear rewards and high costs. By working backwards from known solutions, REER enables the discovery of effective reasoning pathways without the need for extensive trial-and-error. The resulting model, DeepWriter-8B, demonstrates competitive performance against both open-source and proprietary models, showcasing the potential of this new paradigm in creative generation tasks."
                },
                "zh": {
                    "title": "逆向推理，开启深度推理新纪元",
                    "desc": "REER是一种新的深度推理范式，通过逆向工程发现逐步推理过程，使模型能够在开放性任务中表现出色。传统的推理方法，如强化学习和指令蒸馏，在处理开放性生成时面临挑战，前者缺乏明确的奖励信号，后者成本高昂且受限于教师模型的能力。REER通过从已知的优秀解决方案向后推导，计算性地发现潜在的逐步深度推理过程，从而克服了这些限制。我们还开源了DeepWriting-20K数据集，包含20,000个深度推理轨迹，训练的DeepWriter-8B模型在开放性任务中超越了强大的开源基线，并在某些情况下与领先的专有模型如GPT-4o和Claude 3.5的表现相当或更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06501",
            "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
            "url": "https://huggingface.co/papers/2509.06501",
            "abstract": "WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.",
            "score": 37,
            "issue_id": 5787,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "a4011c0eeba062d1",
            "authors": [
                "Junteng Liu",
                "Yunji Li",
                "Chi Zhang",
                "Jingyang Li",
                "Aili Chen",
                "Ke Ji",
                "Weiyu Cheng",
                "Zijia Wu",
                "Chengyu Du",
                "Qidi Xu",
                "Jiayuan Song",
                "Zhengmao Zhu",
                "Wenhu Chen",
                "Pengyu Zhao",
                "Junxian He"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06501.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training",
                    "#reasoning",
                    "#agi",
                    "#dataset",
                    "#rl",
                    "#long_context"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "WebExplorer: Маленький агент с большими возможностями",
                    "desc": "WebExplorer - это новый подход к разработке продвинутых веб-агентов, основанный на данных. Он использует систематическую генерацию данных и обучение с подкреплением для достижения передовых результатов в задачах поиска информации. Модель WebExplorer-8B поддерживает контекст до 128K токенов и до 100 вызовов инструментов, что позволяет решать сложные многоэтапные задачи. Несмотря на размер всего 8 миллиардов параметров, WebExplorer-8B превосходит более крупные модели на различных бенчмарках поиска информации."
                },
                "en": {
                    "title": "WebExplorer: Pioneering Advanced Web Agents for Information Seeking",
                    "desc": "WebExplorer is a novel approach that enhances web agents' ability to seek information effectively by generating challenging data through model-based exploration and iterative query evolution. It addresses the limitations of existing web agents, which often struggle with complex tasks due to a lack of robust data. By employing reinforcement learning and supervised fine-tuning, WebExplorer-8B demonstrates superior performance in information-seeking benchmarks, outperforming larger models in accuracy. This work paves the way for advanced web agents capable of long-horizon problem solving and complex web navigation."
                },
                "zh": {
                    "title": "WebExplorer：信息检索的先进网络代理",
                    "desc": "WebExplorer是一种基于数据驱动的方法，用于开发先进的网络代理，能够在信息检索任务中实现最先进的性能。该方法通过系统的数据生成和强化学习，解决了现有开源网络代理在复杂任务中的信息检索能力不足的问题。WebExplorer通过模型驱动的探索和迭代的查询演变，生成需要多步推理和复杂网页导航的挑战性查询-答案对。最终，WebExplorer-8B模型在多种信息检索基准测试中表现出色，展示了其在长时间问题解决中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06949",
            "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
            "url": "https://huggingface.co/papers/2509.06949",
            "abstract": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
            "score": 19,
            "issue_id": 5783,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "f6f75969a4d93684",
            "authors": [
                "Yinjie Wang",
                "Ling Yang",
                "Bowen Li",
                "Ye Tian",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06949.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#math",
                    "#rl",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений языковых моделей с помощью траекторного обучения с подкреплением",
                    "desc": "TraceRL - это новый метод обучения с подкреплением для диффузионных языковых моделей, который улучшает их способность рассуждать над сложными задачами. Он использует траекторно-ориентированный подход и диффузионную модель ценности для повышения стабильности обучения. Метод позволяет адаптировать модели к работе с более длинными блоками текста, повышая гибкость сэмплирования. С помощью TraceRL авторы создали серию современных диффузионных языковых моделей TraDo, превосходящих более крупные авторегрессионные модели в задачах математических рассуждений."
                },
                "en": {
                    "title": "Enhancing Language Models with Trajectory-Aware Reinforcement Learning",
                    "desc": "TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks."
                },
                "zh": {
                    "title": "TraceRL：提升推理性能的轨迹感知强化学习",
                    "desc": "TraceRL是一种针对扩散语言模型的轨迹感知强化学习框架，能够在后期训练中融入优先推理轨迹，适用于不同的模型架构。该框架配备了基于扩散的价值模型，提升了训练的稳定性，并在复杂的数学和编程任务上展示了更好的推理性能。此外，TraceRL还可以将特定块的模型适应到更大的块，从而提高采样的灵活性。通过使用TraceRL，我们开发了一系列最先进的扩散语言模型TraDo，尽管其规模小于7B的自回归模型，但在复杂的数学推理任务中仍然表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01656",
            "title": "Reinforced Visual Perception with Tools",
            "url": "https://huggingface.co/papers/2509.01656",
            "abstract": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
            "score": 15,
            "issue_id": 5783,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "08ec116a2ed1dc2c",
            "authors": [
                "Zetong Zhou",
                "Dongping Chen",
                "Zixian Ma",
                "Zhihan Hu",
                "Mingyang Fu",
                "Sinan Wang",
                "Yao Wan",
                "Zhou Zhao",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "ONE Lab, HUST",
                "University of Maryland",
                "University of Washington",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01656.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#cv"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в визуальном мышлении ИИ через обучение с подкреплением",
                    "desc": "Статья представляет метод ReVPT, улучшающий способности мультимодальных языковых моделей к визуальному рассуждению с помощью обучения с подкреплением. ReVPT использует новый алгоритм RL на основе GRPO для обучения моделей рассуждать с помощью набора из четырех визуальных инструментов. Эксперименты показывают, что метод достигает лучших результатов на нескольких бенчмарках, включая SAT, CV-Bench, BLINK и MMStar. ReVPT значительно превосходит базовые модели, обученные с учителем и на текстовых данных."
                },
                "en": {
                    "title": "ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs",
                    "desc": "ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods."
                },
                "zh": {
                    "title": "ReVPT：提升多模态LLM的视觉推理能力",
                    "desc": "ReVPT是一种增强多模态大语言模型（LLM）视觉推理能力的方法，采用强化学习技术，达到了视觉基准测试的最先进性能。视觉推理是人类智能的核心，涉及复杂的感知和逻辑过程，但将计算机视觉模型应用于一般视觉推理仍然具有挑战性。以往的研究表明，通过监督微调将视觉模型与LLM结合可以提高性能，但存在数据生成成本高、数据过滤依赖性强和泛化能力差等关键限制。ReVPT通过引入基于GRPO的新型强化学习算法，训练模型使用四种视觉工具进行推理，从而有效解决了这些问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06461",
            "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
            "url": "https://huggingface.co/papers/2509.06461",
            "abstract": "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.",
            "score": 12,
            "issue_id": 5785,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "f239789cf3c0a63d",
            "authors": [
                "Yuyao Ge",
                "Shenghua Liu",
                "Yiwei Wang",
                "Lingrui Mei",
                "Baolong Bi",
                "Xuanshan Zhou",
                "Jiayu Yao",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "affiliations": [
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06461.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#training",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности визуального анализа через контрастное уточнение внимания",
                    "desc": "Статья представляет метод CARVE для улучшения работы визуально-языковых моделей (VLM) в сложных визуальных средах. CARVE использует контрастирование внимания для извлечения релевантных визуальных сигналов на уровне пикселей. Исследование выявило связь между визуальной сложностью и энтропией внимания, а также показало, как внимание модели уточняется от общего сканирования к фокусированной конвергенции. Метод CARVE не требует дополнительного обучения и значительно повышает производительность открытых VLM-моделей."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Contrastive Attention",
                    "desc": "The paper introduces Contrastive Attention Refinement for Visual Enhancement (CARVE), a method designed to improve the performance of Vision-Language Models (VLMs) in complex visual environments. It identifies that visual complexity affects attention patterns, leading to decreased reasoning performance. By contrasting attention maps from general and task-specific queries, CARVE effectively separates useful visual signals from noise without requiring additional training or external tools. The results show significant performance improvements, highlighting the importance of attention mechanisms in visual reasoning tasks."
                },
                "zh": {
                    "title": "对比注意力精炼，提升视觉推理能力！",
                    "desc": "对比注意力精炼（CARVE）是一种提高视觉语言模型（VLM）性能的方法，通过对比注意力机制提取与任务相关的视觉信号。该方法解决了在复杂视觉环境中，现有增强方法需要额外训练或依赖外部分割工具的问题。研究发现，视觉复杂性与注意力熵密切相关，影响推理性能，而注意力在不同层次上逐渐从全局扫描转向深层的聚焦收敛。CARVE通过对比一般查询和任务特定查询的注意力图，能够有效分解视觉信号，提升视觉推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06733",
            "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
            "url": "https://huggingface.co/papers/2509.06733",
            "abstract": "Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
            "score": 11,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "039bfe4e964d5aad",
            "authors": [
                "Wenjun Li",
                "Zhi Chen",
                "Jingru Lin",
                "Hannan Cao",
                "Wei Han",
                "Sheng Liang",
                "Zhi Zhang",
                "Kuicai Dong",
                "Dexun Li",
                "Chen Zhang",
                "Yong Liu"
            ],
            "affiliations": [
                "Huawei Technologies Co., Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06733.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#long_context",
                    "#benchmark",
                    "#survey",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение с подкреплением как основа для создания интеллектуальных исследовательских систем",
                    "desc": "Статья исследует применение обучения с подкреплением для тренировки глубоких исследовательских систем искусственного интеллекта. Авторы рассматривают ограничения методов обучения с учителем и выравнивания предпочтений, предлагая использовать обучение с подкреплением для оптимизации политик взаимодействия с инструментами и исследования. Обсуждаются три ключевых аспекта: синтез и курирование данных, методы обучения с подкреплением для агентных исследований и системы обучения агентов. Статья также затрагивает вопросы архитектуры агентов, координации и методов оценки."
                },
                "en": {
                    "title": "Empowering AI with Reinforcement Learning for Complex Task Mastery",
                    "desc": "This paper discusses the use of reinforcement learning (RL) as a key method for training advanced AI systems that can perform complex tasks. It highlights the limitations of traditional supervised learning and preference alignment methods, which often rely on human-defined rules and can lead to biases. The authors propose that RL can improve the training of these systems by optimizing their interactions with tools and environments, allowing for better exploration and decision-making. The paper also provides a comprehensive overview of RL techniques and frameworks that can enhance the development of deep research agents."
                },
                "zh": {
                    "title": "强化学习：深度研究系统的基础",
                    "desc": "本论文探讨了强化学习作为训练深度研究系统的基础方法，旨在解决监督学习和偏好对齐方法的局限性。通过优化工具交互和探索的策略，强化学习能够更好地处理复杂的多步骤任务。我们系统化了与深度研究系统相关的强化学习方法，包括数据合成、样本效率和多目标优化等方面。最后，论文提供了关于如何训练稳健、透明的深度研究代理的实用指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06467",
            "title": "Does DINOv3 Set a New Medical Vision Standard?",
            "url": "https://huggingface.co/papers/2509.06467",
            "abstract": "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.",
            "score": 11,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "05928cb3cc52644d",
            "authors": [
                "Che Liu",
                "Yinda Chen",
                "Haoyuan Shi",
                "Jinpeng Lu",
                "Bailiang Jian",
                "Jiazhen Pan",
                "Linghan Cai",
                "Jiayi Wang",
                "Yundi Zhang",
                "Jun Li",
                "Cosmin I. Bercea",
                "Cheng Ouyang",
                "Chen Chen",
                "Zhiwei Xiong",
                "Benedikt Wiestler",
                "Christian Wachinger",
                "Daniel Rueckert",
                "Wenjia Bai",
                "Rossella Arcucci"
            ],
            "affiliations": [
                "Dresden University of Technology",
                "Helmholtz AI and Helmholtz Munich",
                "Imperial College London",
                "Munich Center for Machine Learning",
                "Technical University of Munich (TUM)",
                "University of Erlangen-Nuremberg",
                "University of Oxford",
                "University of Science and Technology of China",
                "University of Sheffield"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06467.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#3d",
                    "#cv",
                    "#healthcare",
                    "#benchmark",
                    "#optimization",
                    "#science"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "DINOv3: Универсальный кодировщик для медицинской визуализации",
                    "desc": "Модель DINOv3, основанная на архитектуре Vision Transformer, показывает высокую эффективность в различных задачах медицинской визуализации без специфической предварительной подготовки для медицинской области. Исследование выявило, что DINOv3 может превзойти специализированные медицинские модели на некоторых задачах, несмотря на обучение только на естественных изображениях. Однако модель имеет ограничения в глубоко специализированных доменах, таких как патологические изображения и электронная микроскопия. Кроме того, DINOv3 не всегда следует законам масштабирования в медицинской области, показывая разное поведение при увеличении размера модели или разрешения входных данных."
                },
                "en": {
                    "title": "DINOv3: A Powerful Vision Transformer for Medical Imaging Tasks",
                    "desc": "DINOv3 is a self-supervised vision transformer that excels in various medical imaging tasks without needing pre-training on medical data. It has been benchmarked against common tasks like classification and segmentation, showing strong performance and even surpassing some specialized medical models. However, it struggles in highly specialized areas, such as Whole-Slide Pathological Images and Electron Microscopy, where deep domain knowledge is crucial. Additionally, DINOv3 does not consistently follow scaling laws in the medical domain, indicating that larger models or higher resolutions do not always lead to better performance."
                },
                "zh": {
                    "title": "DINOv3：医学视觉任务的新基准",
                    "desc": "DINOv3是一种自监督的视觉变换器，在各种医学视觉任务中表现出色，无需特定领域的预训练。尽管它在许多任务中超越了医学特定的基础模型，但在需要深度专业化的领域中表现有限。研究表明，DINOv3的性能在不同模型大小和输入图像分辨率下并不总是遵循扩展规律。总的来说，DINOv3为复杂的医学任务提供了强大的视觉特征基础，开启了未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06155",
            "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
            "url": "https://huggingface.co/papers/2509.06155",
            "abstract": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.",
            "score": 7,
            "issue_id": 5783,
            "pub_date": "2025-09-07",
            "pub_date_card": {
                "ru": "7 сентября",
                "en": "September 7",
                "zh": "9月7日"
            },
            "hash": "9f7107d4dbe89f53",
            "authors": [
                "Duomin Wang",
                "Wei Zuo",
                "Aojie Li",
                "Ling-Hao Chen",
                "Xinyao Liao",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Daxin Jiang",
                "Gang Yu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology",
                "The Hong Kong University of Science and Technology(GuangZhou)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06155.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "UniVerse-1: синергия аудио и видео в одном модели",
                    "desc": "В статье представлена модель UniVerse-1, которая объединяет генерацию аудио и видео, используя технику \"stitching of experts\". Это позволяет эффективно использовать уже обученные модели для видео и музыки, обеспечивая точное временное согласование. Для улучшения аннотаций и временного выравнивания звуков и речи с видео, разработан онлайн-пайплайн аннотаций. Модель была дообучена на 7600 часах данных и показала высокое качество координации аудио-визуальных элементов."
                },
                "en": {
                    "title": "UniVerse-1: Harmonizing Audio and Video Generation",
                    "desc": "UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field."
                },
                "zh": {
                    "title": "统一音视频生成的未来",
                    "desc": "UniVerse-1 是一个统一的音频视频生成模型，采用专家拼接技术，将预训练的视频和音乐模型结合在一起，确保时间上的准确对齐，生成高质量的音视频输出。该模型通过深度融合预训练的专家模型，避免了从头开始训练的低效，充分利用了已有的基础能力。为了确保音频和视频内容的准确注释和时间对齐，我们开发了一个在线注释管道，在训练过程中处理所需的数据并生成标签。经过约7600小时的音视频数据微调后，我们的模型能够生成协调良好的音视频内容，并在语音生成方面实现强对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03516",
            "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
            "url": "https://huggingface.co/papers/2509.03516",
            "abstract": "T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/.",
            "score": 7,
            "issue_id": 5784,
            "pub_date": "2025-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "8f5ee8fae69242b1",
            "authors": [
                "Ouxiang Li",
                "Yuan Wang",
                "Xinting Hu",
                "Huijuan Huang",
                "Rui Chen",
                "Jiarong Ou",
                "Xin Tao",
                "Pengfei Wan",
                "Fuli Feng"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Nanyang Technological University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03516.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "T2I-CoReBench: комплексная оценка генерации изображений по тексту",
                    "desc": "T2I-CoReBench - это новый бенчмарк для оценки способностей моделей преобразования текста в изображение к композиции и рассуждению. Он включает 1080 сложных промптов и около 13500 контрольных вопросов, структурированных по 12-мерной таксономии. Бенчмарк оценивает способности моделей к композиции на основе элементов графа сцены и к рассуждению на основе философских типов вывода. Эксперименты показали, что современные модели text-to-image все еще ограничены в сложных сценариях с высокой плотностью, а способность к рассуждению значительно отстает."
                },
                "en": {
                    "title": "Elevating Text-to-Image Models: A New Benchmark for Composition and Reasoning",
                    "desc": "T2I-CoReBench is a new benchmark designed to assess the composition and reasoning abilities of text-to-image (T2I) models. It introduces a detailed evaluation framework that includes a 12-dimensional taxonomy focusing on scene graph elements and various types of reasoning. The benchmark features 1,080 complex prompts and approximately 13,500 checklist questions to ensure a thorough evaluation of model performance. Results indicate that while T2I models can handle basic tasks, they struggle significantly with complex scenarios and implicit reasoning, highlighting areas for improvement."
                },
                "zh": {
                    "title": "评估文本到图像模型的组合与推理能力",
                    "desc": "T2I-CoReBench是一个基准测试，旨在评估文本到图像模型的组合和推理能力。该基准通过复杂的提示和检查问题，全面考察模型在高场景密度下的表现。研究发现，现有模型在复杂场景中的组合能力仍然有限，而推理能力更是成为关键瓶颈。我们提出的基准包含1080个挑战性提示和约13500个检查问题，以实现细致可靠的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06945",
            "title": "Interleaving Reasoning for Better Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2509.06945",
            "abstract": "Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
            "score": 5,
            "issue_id": 5786,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "0f828dd1d00b0a90",
            "authors": [
                "Wenxuan Huang",
                "Shuang Chen",
                "Zheyong Xie",
                "Shaosheng Cao",
                "Shixiang Tang",
                "Yufan Shen",
                "Qingyu Yin",
                "Wenbo Hu",
                "Xiaoman Wang",
                "Yuntian Tang",
                "Junbo Qiao",
                "Yue Guo",
                "Yao Hu",
                "Zhenfei Yin",
                "Philip Torr",
                "Yu Cheng",
                "Wanli Ouyang",
                "Shaohui Lin"
            ],
            "affiliations": [
                "East China Normal University",
                "The Chinese University of Hong Kong",
                "University of California, Los Angeles",
                "University of Oxford",
                "Xiaohongshu Inc.",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06945.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#optimization",
                    "#reasoning",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Чередование рассуждений и генерации для улучшения Text-to-Image моделей",
                    "desc": "Статья представляет новый подход к генерации изображений по текстовому описанию - Interleaving Reasoning Generation (IRG). Эта методика чередует текстовое рассуждение и синтез изображений, что позволяет улучшить качество и детализацию генерируемых изображений. Авторы предлагают специальный фреймворк обучения IRGL и датасет IRGL-300K для эффективного обучения модели. Эксперименты показывают, что IRG достигает state-of-the-art результатов на нескольких бенчмарках, значительно улучшая визуальное качество и точность деталей."
                },
                "en": {
                    "title": "Enhancing Image Generation through Interleaved Reasoning",
                    "desc": "The Interleaving Reasoning Generation (IRG) framework enhances Text-to-Image (T2I) generation by alternating between text-based reasoning and image synthesis. This approach allows the model to first generate an initial image based on textual input, then refine it by reflecting on the generated output to improve details and visual quality. The training process, called Interleaving Reasoning Generation Learning (IRGL), focuses on establishing a strong initial image and ensuring high-quality textual feedback for further refinements. The results demonstrate significant advancements in performance metrics and visual fidelity, showcasing the effectiveness of this interleaved reasoning approach."
                },
                "zh": {
                    "title": "交错推理生成：提升文本到图像的质量",
                    "desc": "本文提出了一种名为交错推理生成（IRG）的框架，旨在提高文本到图像生成的效果。该框架通过交替进行基于文本的思考和图像合成，来增强生成图像的视觉质量和细节保留。IRG的训练过程包括两个阶段：首先建立核心内容和基础质量，然后在后续图像中实现高质量的文本反思和细致的改进。实验结果表明，IRG在多个评估指标上达到了最先进的性能，显著提升了生成图像的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06493",
            "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
            "url": "https://huggingface.co/papers/2509.06493",
            "abstract": "BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces BFS-Prover-V2, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.",
            "score": 4,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "e9de5406241d634e",
            "authors": [
                "Ran Xin",
                "Zeyu Zheng",
                "Yanchen Nie",
                "Kun Yuan",
                "Xia Xiao"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06493.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабируемое доказательство теорем с помощью ИИ",
                    "desc": "BFS-Prover-V2 представляет собой систему автоматического доказательства теорем, решающую проблемы масштабирования в этой области. Она использует инновационную структуру обучения с подкреплением и архитектуру многоагентного поиска с планировщиком. Система применяет многоэтапный конвейер экспертных итераций с адаптивной фильтрацией данных на уровне тактик. BFS-Prover-V2 достигает лучших результатов на эталонных тестах по формальной математике."
                },
                "en": {
                    "title": "Scaling Theorem Proving with Smart Collaboration",
                    "desc": "BFS-Prover-V2 is a system that enhances automated theorem proving by tackling the challenges of scaling in both training and inference. It introduces a multi-turn off-policy reinforcement learning (RL) framework that improves the performance of large language models (LLMs) during training, inspired by AlphaZero's expert iteration approach. Additionally, it features a planner-enhanced multi-agent search architecture that breaks down complex theorems into simpler subgoals, allowing multiple agents to work together efficiently. This innovative dual approach has achieved state-of-the-art results on formal mathematics benchmarks, demonstrating its potential for broader applications in complex reasoning tasks."
                },
                "zh": {
                    "title": "双重扩展，突破定理证明的极限",
                    "desc": "BFS-Prover-V2 是一个针对自动定理证明中的扩展挑战的系统，结合了多轮离线强化学习框架和规划增强的多智能体搜索架构。该系统通过创新的多轮离线强化学习方法，持续提升大语言模型（LLM）在训练时的表现，并克服了长期强化学习中的性能瓶颈。其次，它采用了一个高层次的规划模型，将复杂定理分解为一系列简单的子目标，从而在推理时有效缩小搜索空间。通过这种双重扩展方法，BFS-Prover-V2 在正式数学基准测试中取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06917",
            "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
            "url": "https://huggingface.co/papers/2509.06917",
            "abstract": "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.",
            "score": 3,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "2a482f9a59e68451",
            "authors": [
                "Jiacheng Miao",
                "Joe R. Davis",
                "Jonathan K. Pritchard",
                "James Zou"
            ],
            "affiliations": [
                "Department of Biology, Stanford University",
                "Department of Biomedical Data Science, Stanford University",
                "Department of Computer Science, Stanford University",
                "Department of Genetics, Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06917.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Превращение научных статей в интерактивных ИИ-ассистентов",
                    "desc": "Paper2Agent - это автоматизированная система, которая преобразует научные статьи в интерактивных ИИ-агентов. Она анализирует текст и код статьи, создавая Model Context Protocol (MCP) сервер, который затем подключается к чат-агенту для выполнения сложных научных запросов на естественном языке. Система тестирует и улучшает созданных агентов, чтобы обеспечить их надежность и функциональность. Paper2Agent демонстрирует эффективность на примерах агентов для интерпретации геномных вариантов и анализа транскриптомики."
                },
                "en": {
                    "title": "Transforming Research Papers into Interactive AI Agents",
                    "desc": "Paper2Agent is a framework that transforms traditional research papers into interactive AI agents, making it easier for users to access and utilize scientific knowledge. By analyzing the paper and its associated code, it creates a Model Context Protocol (MCP) server that allows the AI agent to answer complex queries in natural language. This approach reduces the effort required for researchers to understand and apply the findings, thus enhancing the dissemination and reuse of research outputs. The effectiveness of Paper2Agent is demonstrated through case studies where it successfully reproduces original results and handles novel queries, paving the way for a new collaborative ecosystem in scientific research."
                },
                "zh": {
                    "title": "将静态论文转变为动态 AI 代理的创新之路",
                    "desc": "Paper2Agent 是一个自动化框架，可以将研究论文转化为互动的 AI 代理，以促进知识传播和复杂科学查询。它通过分析论文及其相关代码，构建模型上下文协议（MCP）服务器，使得研究成果从被动的文献变为主动的系统。这样，用户可以通过自然语言与 AI 代理进行交流，轻松获取论文中的信息和工具。Paper2Agent 的有效性通过案例研究得到了验证，能够重现原论文的结果并处理新的用户查询。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06786",
            "title": "R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World",
            "url": "https://huggingface.co/papers/2509.06786",
            "abstract": "A new framework, R²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose safe-by-coevolution as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce R^2AI -- Resistant and Resilient AI -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. R^2AI integrates fast and slow safe models, adversarial simulation and verification through a safety wind tunnel, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI.",
            "score": 3,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "bdf421cc1d868635",
            "authors": [
                "Youbang Sun",
                "Xiang Wang",
                "Jie Fu",
                "Chaochao Lu",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06786.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#security",
                    "#ethics",
                    "#training",
                    "#agi"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Коэволюция безопасности и возможностей ИИ",
                    "desc": "Статья представляет новую концепцию R²AI для повышения безопасности искусственного интеллекта через коэволюцию. Подход объединяет устойчивость к известным угрозам и адаптивность к непредвиденным рискам, используя быстрые и медленные безопасные модели. R²AI включает в себя adversarial simulation и верификацию через 'safety wind tunnel', а также непрерывные циклы обратной связи. Авторы утверждают, что эта концепция предлагает масштабируемый и проактивный путь к поддержанию постоянной безопасности ИИ в динамичных средах."
                },
                "en": {
                    "title": "R²AI: Evolving Safety for Advanced AI Systems",
                    "desc": "The paper introduces R²AI, a new framework aimed at improving AI safety by combining two key strategies: resistance to known threats and resilience to unexpected risks. It critiques existing safety approaches that either reactively apply safety measures or struggle with unforeseen challenges in complex environments. R²AI proposes a coevolutionary approach to safety, inspired by biological immunity, where safety is treated as an ongoing learning process. This framework utilizes fast and slow safe models, adversarial simulations, and continuous feedback to ensure that AI systems can adapt and remain safe as they evolve."
                },
                "zh": {
                    "title": "R²AI：共进化提升人工智能安全性",
                    "desc": "本文提出了一种新的框架R²AI，旨在通过共进化增强人工智能的安全性。该框架结合了对已知威胁的抵抗力和对未知风险的韧性，使用快速和慢速安全模型以及对抗性模拟。R²AI的设计灵感来自生物免疫，强调安全性是一个动态的、对抗性的持续学习过程。通过这一框架，我们可以在动态环境中保持持续的安全性，解决短期脆弱性和长期存在风险的问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06477",
            "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents",
            "url": "https://huggingface.co/papers/2509.06477",
            "abstract": "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  \t\t\t\t\tAI-generated summary \t\t\t\t To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.",
            "score": 2,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "4c5d5e5da6b90a95",
            "authors": [
                "Pengxiang Zhao",
                "Guangyi Liu",
                "Yaozhen Liang",
                "Weiqing He",
                "Zhengxi Lu",
                "Yuehao Huang",
                "Yaxuan Guo",
                "Kexin Zhang",
                "Hao Wang",
                "Liang Liu",
                "Yong Liu"
            ],
            "affiliations": [
                "Huzhou Institute of Zhejiang University",
                "Zhejiang University",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06477.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#games",
                    "#optimization"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Гибридные агенты - будущее мобильной автоматизации",
                    "desc": "MAS-Bench - это новая система оценки гибридных агентов, сочетающих графический интерфейс и ярлыки, для мобильных устройств. Она включает 139 сложных задач в 11 реальных приложениях и базу знаний из 88 предопределенных ярлыков. Эксперименты показывают, что гибридные агенты значительно превосходят агентов, использующих только графический интерфейс, по успешности и эффективности. MAS-Bench заполняет важный пробел в оценке и создает основу для будущих разработок более эффективных интеллектуальных агентов."
                },
                "en": {
                    "title": "Unlocking Efficiency: Evaluating Hybrid Agents with MAS-Bench",
                    "desc": "MAS-Bench is a benchmarking framework designed to evaluate hybrid agents that combine graphical user interface (GUI) operations with shortcut methods on mobile devices. It addresses the need for a systematic way to assess these agents, which can autonomously generate shortcuts to improve task efficiency. The framework includes 139 complex tasks from real-world applications and evaluates agents based on their ability to utilize predefined shortcuts and create new, efficient workflows. Results indicate that hybrid agents outperform traditional GUI-only agents in both success rates and efficiency, highlighting the potential of this approach in enhancing intelligent agent performance."
                },
                "zh": {
                    "title": "混合代理：提升移动设备操作效率的未来",
                    "desc": "MAS-Bench是一个评估移动设备上GUI-快捷键混合代理的基准框架，展示了其在性能和效率上优于仅使用GUI的代理。该框架不仅使用预定义的快捷键，还评估代理自主生成快捷键的能力，发现和创建可重用的低成本工作流程。MAS-Bench包含139个复杂任务，涵盖11个真实应用程序，并提供88个预定义快捷键的知识库和7个评估指标。实验结果表明，混合代理的成功率和效率显著高于仅使用GUI的代理，证明了该方法在评估代理快捷键生成能力方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05668",
            "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian",
            "url": "https://huggingface.co/papers/2509.05668",
            "abstract": "Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.",
            "score": 2,
            "issue_id": 5784,
            "pub_date": "2025-09-06",
            "pub_date_card": {
                "ru": "6 сентября",
                "en": "September 6",
                "zh": "9月6日"
            },
            "hash": "8fe2894e1bb529f0",
            "authors": [
                "Michael Hoffmann",
                "Jophin John",
                "Stefan Schweter",
                "Gokul Ramakrishnan",
                "Hoi-Fong Mak",
                "Alice Zhang",
                "Dmitry Gaynullin",
                "Nicolay J. Hammer"
            ],
            "affiliations": [
                "Cerebras Systems Sunnyvale, USA",
                "Independent Researcher Holzkirchen, Germany",
                "Leibniz Supercomputing Centre (LRZ) Garching, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05668.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#machine_translation",
                    "#architecture",
                    "#training",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Преодоление языкового неравенства в ИИ: трехъязычная модель с акцентом на малоресурсные языки",
                    "desc": "Llama-GENBA-10B - это трехъязычная языковая модель, созданная для решения проблемы англоцентричности в крупных языковых моделях. Модель обучена на сбалансированном корпусе из английского, немецкого и баварского языков, что позволяет ей достигать высоких результатов в кросс-языковых задачах. Особое внимание уделено интеграции баварского как малоресурсного языка. Модель превосходит существующие аналоги по работе с баварским языком, демонстрируя при этом сильные результаты на английском и немецком."
                },
                "en": {
                    "title": "Balancing Languages: Llama-GENBA-10B Redefines Multilingual AI",
                    "desc": "Llama-GENBA-10B is a trilingual foundation model designed to reduce English-centric bias in language processing. It is trained on a balanced dataset of English, German, and Bavarian, ensuring that no single language dominates the training process. The model addresses challenges such as the scarcity of Bavarian data and the need for a unified tokenizer across languages. Evaluations indicate that Llama-GENBA-10B excels in cross-lingual tasks, particularly in Bavarian, setting new performance benchmarks and demonstrating effective multilingual pretraining techniques."
                },
                "zh": {
                    "title": "打破语言偏见，促进多语言平衡",
                    "desc": "Llama-GENBA-10B 是一个三语基础模型，旨在解决大型语言模型中的英语偏见问题。该模型在训练过程中平衡了英语、德语和巴伐利亚语的资源，使用了 1640 亿个标记，确保了各语言的公平性。通过优化架构和语言比例超参数，Llama-GENBA-10B 在跨语言迁移方面表现出色，尤其是在巴伐利亚语的评估中设立了新的基准。该模型的开发为低资源语言的整合提供了有效的解决方案，展示了多语言预训练的高效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06771",
            "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
            "url": "https://huggingface.co/papers/2509.06771",
            "abstract": "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
            "score": 1,
            "issue_id": 5784,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "48d37925f403181d",
            "authors": [
                "Sai Kartheek Reddy Kasu",
                "Mohammad Zia Ur Rehman",
                "Shahid Shafi Dar",
                "Rishi Bharat Junghare",
                "Dhanvin Sanjay Namboodiri",
                "Nagendra Kumar"
            ],
            "affiliations": [
                "Indian Institute of Information Technology Dharwad, India",
                "Indian Institute of Technology Indore, India",
                "Malaviya National Institute of Technology Jaipur, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06771.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Искусственный интеллект распознает черный юмор в мемах",
                    "desc": "Статья представляет новый подход к обнаружению черного юмора в мультимодальных мемах с использованием большой визуально-языковой модели (VLM) и трехпоточной сети кросс-рассуждений (TCRNet). Авторы создали датасет из 4,379 мемов Reddit с аннотациями по наличию черного юмора, категории цели и уровню интенсивности. VLM генерирует структурированные объяснения для каждого мема, а TCRNet объединяет текстовые, визуальные и логические признаки для классификации. Результаты экспериментов показывают превосходство предложенного метода над базовыми подходами в задачах обнаружения черного юмора, идентификации цели и предсказания интенсивности."
                },
                "en": {
                    "title": "Unraveling Dark Humor in Memes with Advanced Reasoning Techniques",
                    "desc": "This paper presents a new framework for detecting dark humor in memes using a combination of a Large Vision-Language Model (VLM) and a Tri-stream Cross-Reasoning Network (TCRNet). The authors created a dataset of 4,379 Reddit memes, annotated for dark humor, target categories, and intensity levels. The framework generates structured explanations for memes and refines them through a self-loop mechanism, enhancing the model's understanding. By integrating textual and visual features with reasoning, the model achieves superior performance in identifying dark humor, targets, and intensity compared to existing methods."
                },
                "zh": {
                    "title": "增强推理，精准识别黑色幽默",
                    "desc": "本研究提出了一种增强推理的框架，利用大型视觉语言模型和三流交叉推理网络，旨在提高对黑色幽默的检测能力。我们创建了一个包含4379个Reddit表情包的新数据集，标注了黑色幽默、目标类别和强度等级。该框架通过生成结构化解释，结合文本、图像和推理特征，进行有效的分类。实验结果表明，我们的方法在黑色幽默检测、目标识别和强度预测等任务上优于现有的强基线。"
                }
            }
        }
    ],
    "link_prev": "2025-09-08.html",
    "link_next": "2025-09-10.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "08.09",
        "en": "09/08",
        "zh": "9月8日"
    },
    "short_date_next": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9月10日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 7,
        "#agents": 6,
        "#cv": 5,
        "#rl": 6,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 7,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 9,
        "#robotics": 0,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 10,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}