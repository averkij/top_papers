{
    "date": {
        "ru": "22 июля",
        "en": "July 22",
        "zh": "7月22日"
    },
    "time_utc": "2025-07-22 09:17",
    "weekday": 1,
    "issue_id": 4943,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.14683",
            "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
            "url": "https://huggingface.co/papers/2507.14683",
            "abstract": "The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.",
            "score": 64,
            "issue_id": 4937,
            "pub_date": "2025-07-19",
            "pub_date_card": {
                "ru": "19 июля",
                "en": "July 19",
                "zh": "7月19日"
            },
            "hash": "47799c3d5002f685",
            "authors": [
                "Xingxuan Li",
                "Yao Xiao",
                "Dianwen Ng",
                "Hai Ye",
                "Yue Deng",
                "Xiang Lin",
                "Bin Wang",
                "Zhanfeng Mo",
                "Chong Zhang",
                "Yueyi Zhang",
                "Zonglin Yang",
                "Ruilin Li",
                "Lei Lei",
                "Shihao Xu",
                "Han Zhao",
                "Weiling Chen",
                "Feng Ji",
                "Lidong Bing"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.14683.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Открытые модели для математических рассуждений на новом уровне",
                    "desc": "MiroMind-M1 - это серия открытых языковых моделей для математических рассуждений, достигающих передовых результатов на соответствующих бенчмарках. Модели обучаются в два этапа: сначала на корпусе из 719 тысяч математических задач с верифицированными решениями, затем с помощью обучения с подкреплением на 62 тысячах сложных задач. Авторы представляют новый алгоритм Context-Aware Multi-Stage Policy Optimization для повышения эффективности обучения с подкреплением. Все ресурсы, включая модели, датасеты и конфигурации, открыто опубликованы для воспроизводимости результатов."
                },
                "en": {
                    "title": "Open-Source Models for Superior Mathematical Reasoning",
                    "desc": "The MiroMind-M1 series introduces open-source reasoning language models that excel in mathematical reasoning tasks through a two-stage training approach. The first stage involves supervised fine-tuning (SFT) on a large dataset of math problems, while the second stage employs reinforcement learning with verified responses (RLVR) to refine the model's reasoning capabilities. To improve training efficiency, the authors propose a novel Context-Aware Multi-Stage Policy Optimization algorithm that adapts training based on context and problem complexity. By providing complete access to models, datasets, and training configurations, this work aims to enhance transparency and reproducibility in the development of reasoning language models."
                },
                "zh": {
                    "title": "开源推理模型的透明性与先进性",
                    "desc": "MiroMind-M1系列是一个开源推理语言模型，通过两阶段训练过程和上下文感知多阶段策略优化，在数学推理基准测试中取得了最先进的表现。这些模型首先在经过精心挑选的719K数学推理问题上进行监督微调，然后在62K具有挑战性的问题上进行强化学习验证。为了提高强化学习验证过程的鲁棒性和效率，提出了一种新的算法，结合了长度渐进训练和自适应重复惩罚。我们希望通过发布完整的模型、数据集和训练配置，促进研究的可重复性和社区的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15846",
            "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
            "url": "https://huggingface.co/papers/2507.15846",
            "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.",
            "score": 33,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "d36bacfa3f66add9",
            "authors": [
                "Fei Tang",
                "Zhangxuan Gu",
                "Zhengxi Lu",
                "Xuyang Liu",
                "Shuheng Shen",
                "Changhua Meng",
                "Wen Wang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15846.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🖱️",
                "ru": {
                    "title": "Гауссово моделирование для точного взаимодействия с GUI",
                    "desc": "Статья представляет новый подход к обучению моделей машинного обучения для взаимодействия с графическим пользовательским интерфейсом (GUI). Авторы предлагают метод GUI Gaussian Grounding Rewards (GUI-G^2), который моделирует элементы интерфейса как непрерывные гауссовы распределения на плоскости интерфейса. Этот метод включает в себя гауссовы точечные награды для точной локализации и награды за покрытие для оценки пространственного выравнивания. Эксперименты показывают, что GUI-G^2 значительно превосходит современные методы на нескольких бенчмарках, демонстрируя улучшение до 24.7% на ScreenSpot-Pro."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with Continuous Gaussian Rewards",
                    "desc": "This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks."
                },
                "zh": {
                    "title": "高斯奖励框架提升GUI交互精度",
                    "desc": "本论文提出了一种新的奖励框架，称为GUI Gaussian Grounding Rewards（GUI-G^2），用于将自然语言指令映射到图形用户界面（GUI）的精确位置。与传统的二元奖励方法不同，GUI-G^2通过将GUI元素建模为连续的高斯分布，提供了更丰富的梯度信号，促进了模型的优化。该框架结合了高斯点奖励和覆盖奖励，能够更好地处理不同元素的尺度，并提高了模型在界面变化中的鲁棒性。实验结果表明，GUI-G^2在多个基准测试中显著优于现有的最先进方法，展示了其在GUI交互任务中的新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14843",
            "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin",
            "url": "https://huggingface.co/papers/2507.14843",
            "abstract": "Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions.",
            "score": 33,
            "issue_id": 4940,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 июля",
                "en": "July 20",
                "zh": "7月20日"
            },
            "hash": "bb8fd850ce625ea5",
            "authors": [
                "Fang Wu",
                "Weihao Xuan",
                "Ximing Lu",
                "Zaid Harchaoui",
                "Yejin Choi"
            ],
            "affiliations": [
                "RIKEN AIP",
                "Stanford University",
                "University of Tokyo",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14843.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "RLVR: повышение точности ценой ограничения исследования",
                    "desc": "Исследование анализирует метод обучения с подкреплением с проверяемыми вознаграждениями (RLVR) в контексте решения сложных логических задач. Авторы обнаружили, что RLVR повышает точность модели, но ограничивает ее способность находить новые решения. Теоретический анализ показывает, что RLVR ограничен возможностями базовой модели и действует как механизм консервативного перевзвешивания. Эмпирические эксперименты подтверждают, что RLVR улучшает показатель pass@1, но сужает область исследования, потенциально упуская правильные, но недопредставленные решения."
                },
                "en": {
                    "title": "Balancing Precision and Exploration in RLVR",
                    "desc": "This paper investigates Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on AI's problem-solving abilities. It finds that while RLVR improves precision in generating high-reward outputs, it limits exploration, which can hinder the discovery of novel solutions. The study introduces a theoretical framework showing that RLVR acts as a conservative mechanism, unable to sample solutions with zero initial probability. Empirical results indicate that although RLVR enhances performance metrics like pass@1, it often reduces the diversity of solutions, suggesting a need for new strategies to encourage exploration."
                },
                "zh": {
                    "title": "强化学习与可验证奖励的探索限制",
                    "desc": "强化学习与可验证奖励（RLVR）在提高精度方面表现出色，但却限制了探索能力，可能导致无法发现新颖的解决方案。研究表明，RLVR的效果受到基础模型的支持限制，无法采样初始概率为零的解决方案。虽然RLVR在提高精度方面表现稳定，但其对探索的压缩可能会忽视一些正确但代表性不足的解决方案。未来的算法创新可能需要引入显式探索机制或混合策略，以便在未被充分代表的解决方案区域中注入概率质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15061",
            "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
            "url": "https://huggingface.co/papers/2507.15061",
            "abstract": "A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.",
            "score": 23,
            "issue_id": 4936,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 июля",
                "en": "July 20",
                "zh": "7月20日"
            },
            "hash": "16ab84cfe7ace89e",
            "authors": [
                "Zhengwei Tao",
                "Jialong Wu",
                "Wenbiao Yin",
                "Junkai Zhang",
                "Baixuan Li",
                "Haiyang Shen",
                "Kuan Li",
                "Liwen Zhang",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15061.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Формализация для синтеза данных: новый подход к обучению ИИ-агентов поиску информации",
                    "desc": "WebShaper - это фреймворк для синтеза наборов данных для задач поиска информации, основанный на формализации с использованием теории множеств и Проекций Знаний. Он позволяет улучшить производительность агентов на основе больших языковых моделей (LLM) в открытых задачах. WebShaper систематически формализует задачи поиска информации и использует многоэтапный процесс расширения для создания сложных вопросов. Эксперименты показывают, что WebShaper достигает наилучших результатов среди открытых агентов поиска информации на бенчмарках GAIA и WebWalkerQA."
                },
                "en": {
                    "title": "Enhancing LLM Agents with Structured Data Synthesis",
                    "desc": "WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks."
                },
                "zh": {
                    "title": "WebShaper：提升信息检索智能体性能的创新框架",
                    "desc": "WebShaper是一个基于形式化驱动的框架，利用集合论和知识投影技术合成信息检索数据集，从而提升大型语言模型（LLM）驱动的智能体在开放式任务中的表现。该框架通过系统化的形式化过程，确保信息结构与推理结构的一致性，解决了现有方法中常见的数据不一致问题。WebShaper的核心是知识投影（KP）概念，通过KP操作组合实现对推理结构的精确控制。实验结果表明，WebShaper在GAIA和WebWalkerQA基准测试中，达到了开源信息检索智能体的最先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11061",
            "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
            "url": "https://huggingface.co/papers/2507.11061",
            "abstract": "A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.",
            "score": 23,
            "issue_id": 4939,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "4c8104d951622fec",
            "authors": [
                "Hayeon Kim",
                "Ji Ha Jang",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
                "INMC & IPAI Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11061.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "Точное локальное 3D-редактирование с помощью робастных масок и улучшенной регуляризации",
                    "desc": "RoMaP - это новая система для точного локального 3D-редактирования, использующая генерацию робастных 3D-масок и улучшенную регуляризацию функции потерь SDS. Она включает модуль 3D-GALP для создания согласованных сегментаций частей объекта с разных ракурсов. RoMaP также применяет регуляризованную функцию потерь SDS с дополнительными регуляризаторами, включая L1-якорную потерю через метод SLaMP. Эксперименты показывают, что RoMaP достигает наилучших результатов в локальном 3D-редактировании как реконструированных, так и сгенерированных гауссовых сцен и объектов."
                },
                "en": {
                    "title": "RoMaP: Revolutionizing Local 3D Editing with Precision and Flexibility",
                    "desc": "The paper introduces RoMaP, a new framework designed to enhance local 3D editing by generating robust 3D masks and improving the Score Distillation Sampling (SDS) loss regularization. It addresses challenges in achieving precise edits in 3D content, particularly with Gaussian Splatting, by utilizing a 3D-Geometry Aware Label Prediction (3D-GALP) module for accurate part segmentations. The framework also incorporates a regularized SDS loss that includes an L1 anchor loss to ensure modifications are confined to specific areas while maintaining overall coherence. Experimental results show that RoMaP outperforms existing methods in local 3D editing, providing a more flexible and effective approach for part-level modifications."
                },
                "zh": {
                    "title": "RoMaP：精确局部3D编辑的新框架",
                    "desc": "RoMaP是一个新颖的局部3D编辑框架，旨在通过强大的3D掩模生成和增强的SDS损失正则化来提高精确的局部3D编辑能力。该框架引入了3D几何感知标签预测模块，利用球谐系数建模视角依赖的标签变化，从而实现准确一致的部分分割。通过结合标准SDS损失和额外的正则化项，RoMaP能够在目标区域内进行高质量的部分编辑，同时保持上下文的一致性。实验结果表明，RoMaP在重建和生成的高斯场景及物体上实现了最先进的局部3D编辑效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15493",
            "title": "GR-3 Technical Report",
            "url": "https://huggingface.co/papers/2507.15493",
            "abstract": "A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.",
            "score": 19,
            "issue_id": 4938,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "5e91567240893b65",
            "authors": [
                "Chilam Cheang",
                "Sijin Chen",
                "Zhongren Cui",
                "Yingdong Hu",
                "Liqun Huang",
                "Tao Kong",
                "Hang Li",
                "Yifeng Li",
                "Yuxiao Liu",
                "Xiao Ma",
                "Hao Niu",
                "Wenxuan Ou",
                "Wanli Peng",
                "Zeyu Ren",
                "Haixin Shi",
                "Jiawen Tian",
                "Hongtao Wu",
                "Xin Xiao",
                "Yuyang Xiao",
                "Jiafeng Xu",
                "Yichu Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.15493.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#agents",
                    "#training",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "GR-3: Шаг к универсальным роботам-помощникам",
                    "desc": "Статья представляет GR-3 - крупномасштабную модель визуально-языкового действия (VLA) для робототехники. Модель демонстрирует исключительные способности к обобщению на новые объекты, среды и инструкции, включая абстрактные концепции. GR-3 может эффективно дообучаться на минимальном количестве человеческих траекторий, что позволяет быстро адаптироваться к новым условиям. Модель превосходит существующие базовые методы в широком спектре сложных задач, включая длительные и требующие ловкости операции."
                },
                "en": {
                    "title": "GR-3: A Leap Towards Generalist Robots for Everyday Tasks",
                    "desc": "The paper presents GR-3, a large-scale vision-language-action model that excels in generalizing across various robotic tasks. It can adapt quickly to new environments and instructions with minimal human input, making it efficient for fine-tuning. GR-3 is particularly effective in performing complex tasks that require dexterity and coordination, such as bi-manual manipulation. The model's training combines web-scale data and imitation learning, leading to superior performance compared to existing methods."
                },
                "zh": {
                    "title": "GR-3：通用机器人政策的未来",
                    "desc": "GR-3是一个大型的视觉-语言-动作模型，能够在复杂的机器人任务中表现出色。它具有很强的泛化能力，能够适应新物体、新环境和抽象概念的指令。该模型可以通过少量的人类轨迹数据进行高效的微调，快速适应新环境。通过与网络规模的视觉-语言数据共同训练，GR-3在长时间和灵巧任务中表现出强大的性能，展示了其在日常生活中辅助人类的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15852",
            "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction",
            "url": "https://huggingface.co/papers/2507.15852",
            "abstract": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.",
            "score": 18,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "8d4e431fe003417f",
            "authors": [
                "Zhixiong Zhang",
                "Shuangrui Ding",
                "Xiaoyi Dong",
                "Songxin He",
                "Jianfan Lin",
                "Junsong Tang",
                "Yuhang Zang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15852.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Концептуальное понимание для улучшения сегментации объектов в видео",
                    "desc": "Статья представляет новый подход к сегментации объектов в видео под названием Segment Concept (SeC). SeC использует большие мультимодальные модели для создания концептуальных представлений объектов, что позволяет лучше справляться со сложными сценариями. Авторы также представляют новый датасет SeCVOS для оценки методов сегментации в сложных семантических сценариях. SeC показывает значительное улучшение результатов по сравнению с существующими методами на этом датасете."
                },
                "en": {
                    "title": "Revolutionizing Video Object Segmentation with Conceptual Understanding",
                    "desc": "This paper introduces Segment Concept (SeC), a new framework for Video Object Segmentation (VOS) that enhances object tracking and segmentation in videos. Unlike traditional methods that rely heavily on appearance matching, SeC focuses on building high-level, object-centric representations using Large Vision-Language Models (LVLMs). This approach allows the model to better understand and adapt to complex visual changes and occlusions, leading to improved segmentation accuracy. The authors also present a new benchmark, SeCVOS, to evaluate VOS methods in challenging scenarios, where SeC demonstrates significant performance improvements over existing techniques."
                },
                "zh": {
                    "title": "概念驱动的视频目标分割新突破",
                    "desc": "视频目标分割（VOS）是计算机视觉中的一项核心任务，要求模型在视频帧中跟踪和分割目标物体。尽管近年来取得了一些进展，但现有技术在处理剧烈的视觉变化、遮挡和复杂场景变化时仍然落后于人类能力。为了解决这一问题，我们提出了Segment Concept（SeC），它通过构建和利用高层次的以对象为中心的表示，转变了传统的特征匹配方法。SeC结合了大型视觉语言模型（LVLMs），在推理过程中形成全面的语义表示，从而实现对后续帧的稳健分割。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15778",
            "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
            "url": "https://huggingface.co/papers/2507.15778",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.",
            "score": 14,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "8e0f7bdfedf50691",
            "authors": [
                "Jiakang Wang",
                "Runze Liu",
                "Fuzheng Zhang",
                "Xiu Li",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15778.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точное обучение с подкреплением: улучшение рассуждений ИИ с помощью энтропийно-адаптивного подхода",
                    "desc": "Статья представляет новый метод обучения с подкреплением для улучшения рассуждающих способностей больших языковых моделей, называемый Archer. Этот подход учитывает энтропию токенов и применяет различные ограничения к токенам знаний и рассуждений. Archer использует более слабую KL-регуляризацию и более высокие пороги отсечения для токенов рассуждений, чтобы стимулировать исследование, сохраняя при этом фактические знания. Экспериментальные результаты показывают, что Archer превосходит предыдущие методы RLVR на нескольких бенчмарках математических рассуждений и генерации кода."
                },
                "en": {
                    "title": "Archer: Smart Token Training for Better Reasoning in LLMs",
                    "desc": "This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size."
                },
                "zh": {
                    "title": "提升推理能力的双重令牌强化学习",
                    "desc": "本文提出了一种新的强化学习方法，称为Archer，旨在提高大型语言模型的推理能力。Archer通过双重令牌约束和同步更新，分别对知识相关的低熵令牌和推理相关的高熵令牌施加不同的训练信号。与以往的算法不同，Archer在推理令牌上使用较弱的KL正则化，以鼓励探索，同时对知识令牌施加更强的约束，以保持事实知识的准确性。实验结果表明，Archer在多个数学推理和代码生成基准测试中显著优于之前的RLVR方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15629",
            "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
            "url": "https://huggingface.co/papers/2507.15629",
            "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
            "score": 13,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "f2fc3e4b855b88d5",
            "authors": [
                "Zuo-Liang Zhu",
                "Jian Yang",
                "Beibei Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15629.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Дискретизированное SDF для улучшенного обратного рендеринга с гауссовским сплаттингом",
                    "desc": "Статья представляет новый подход к обратному рендерингу с использованием дискретизированного поля расстояний со знаком (SDF) в контексте 3D гауссовского сплаттинга. Авторы кодируют SDF в каждом гауссиане с помощью дискретных значений, что позволяет связать SDF с прозрачностью гауссианов через специальное преобразование. Для регуляризации дискретных выборок вводится проекционная функция потерь, обеспечивающая согласованность с базовым SDF. Эксперименты показывают, что предложенный метод превосходит существующие подходы к обратному рендерингу на основе гауссианов по качеству перерисовки освещения."
                },
                "en": {
                    "title": "Efficient Inverse Rendering with Discretized SDF and Gaussian Splatting",
                    "desc": "This paper presents a novel approach to inverse rendering using a discretized signed distance field (SDF) integrated with 3D Gaussian splatting (3DGS). By encoding the continuous SDF within each Gaussian, the method links SDF with Gaussian opacity, allowing for efficient rendering without the heavy computational costs of ray marching. The authors introduce a projection-based consistency loss to ensure that the discrete samples align with the underlying SDF, improving the quality of relighting. Overall, this approach enhances the performance of Gaussian-based inverse rendering while maintaining low memory usage and simplifying the optimization process."
                },
                "zh": {
                    "title": "离散化SDF提升逆向渲染质量",
                    "desc": "3D高斯点云（3DGS）在新视图合成（NVS）任务中表现出色，但在逆向渲染中仍面临挑战。我们提出了一种离散化的有符号距离场（SDF），通过在每个高斯中编码采样值来表示连续的SDF，从而简化了几何约束的应用。该方法通过SDF与高斯不透明度的转换，避免了光线行进的计算成本，同时提高了重光照质量。实验结果表明，我们的方法在性能上优于现有的基于高斯的逆向渲染方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15597",
            "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos",
            "url": "https://huggingface.co/papers/2507.15597",
            "abstract": "Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.",
            "score": 13,
            "issue_id": 4942,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "c48aaae53a1f9330",
            "authors": [
                "Hao Luo",
                "Yicheng Feng",
                "Wanpeng Zhang",
                "Sipeng Zheng",
                "Ye Wang",
                "Haoqi Yuan",
                "Jiazheng Liu",
                "Chaoyi Xu",
                "Qin Jin",
                "Zongqing Lu"
            ],
            "affiliations": [
                "BeingBeyond",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15597.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#robotics",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение роботов человеческим движениям через видео",
                    "desc": "Being-H0 - это модель зрения-языка-действия (VLA), обученная на видео с людьми для решения задач манипуляции. Модель использует физическое обучение по инструкциям и токенизацию движений на уровне частей тела для улучшения точности и обобщаемости. Being-H0 демонстрирует превосходные результаты в генерации движений рук и реальной робототехнической манипуляции. Модель обучается на масштабном наборе данных, включающем захват движений, VR и RGB-видео."
                },
                "en": {
                    "title": "Empowering Robots with Human-Like Dexterity through Vision-Language-Action!",
                    "desc": "Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications."
                },
                "zh": {
                    "title": "Being-H0：灵巧的视觉-语言-动作模型",
                    "desc": "Being-H0 是一种视觉-语言-动作模型，专注于从人类视频中学习，以解决灵巧性和泛化能力的问题。该模型通过物理指令调优和部件级运动标记化，能够生成精确的手部动作并在真实世界中进行机器人操作。与传统模型相比，Being-H0 更好地处理复杂的操作任务，并能有效适应新场景。我们的研究表明，Being-H0 在手部动作生成和指令跟随方面表现出色，且在实际机器人操作中也取得了显著的进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15028",
            "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding",
            "url": "https://huggingface.co/papers/2507.15028",
            "abstract": "Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.",
            "score": 12,
            "issue_id": 4938,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 июля",
                "en": "July 20",
                "zh": "7月20日"
            },
            "hash": "7f71d09a9b276de8",
            "authors": [
                "Yuanhan Zhang",
                "Yunice Chew",
                "Yuhao Dong",
                "Aria Leo",
                "Bo Hu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Independent Researcher",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15028.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#security",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый рубеж в оценке видео-LLM: человекоподобное понимание реального мира",
                    "desc": "Статья представляет новый тест Video Thinking Test (Video-TT) для оценки способности видео-LLM интерпретировать реальные видео на уровне человека. Video-TT состоит из 1000 коротких YouTube-видео с открытыми и провокационными вопросами, оценивающими понимание сложных визуальных нарративов. Тест выявляет существенный разрыв между производительностью видео-LLM и человеческим интеллектом в корректности и устойчивости интерпретации видео. Video-TT направлен на оценку способности моделей точно и устойчиво интерпретировать визуальный контент в сложных условиях."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Video LLMs with the Video Thinking Test",
                    "desc": "This paper discusses the importance of correctness and robustness in video understanding, which are essential for mimicking human intelligence. It highlights that current benchmarks do not adequately measure how well video large language models (LLMs) interpret videos compared to humans. To address this, the authors introduce the Video Thinking Test (Video-TT), designed to evaluate the performance of video LLMs on real-world videos. The test includes 1,000 YouTube Shorts videos with questions that challenge the models' understanding of complex visual narratives, revealing a significant performance gap between the models and human interpreters."
                },
                "zh": {
                    "title": "视频理解的挑战：人类与模型的差距",
                    "desc": "本论文探讨了视频理解中的正确性和鲁棒性问题。尽管视频大型语言模型（视频LLMs）取得了一定进展，但现有基准测试未能充分反映这些模型与人类智能在视频解释中的差距。我们提出了视频思维测试（Video-TT），旨在评估视频LLMs是否能像人类一样有效地理解现实世界的视频。测试包含1000个YouTube Shorts视频，每个视频配有一个开放性问题和四个针对视觉和叙事复杂性的对抗性问题，评估结果显示视频LLMs与人类表现之间存在显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15375",
            "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models",
            "url": "https://huggingface.co/papers/2507.15375",
            "abstract": "Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.",
            "score": 5,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "2a7d1e1e1882f002",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15375.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Stitch: Думай и говори одновременно",
                    "desc": "Статья представляет новый метод генерации речи для разговорных языковых моделей под названием Stitch. Этот метод позволяет моделям осуществлять внутренний процесс мышления, чередуя генерацию невысказанных рассуждений и произносимых ответов. Stitch использует свободное время во время воспроизведения аудио для генерации следующего фрагмента невысказанных рассуждений, что позволяет модели одновременно думать и говорить. Результаты показывают, что Stitch превосходит базовые модели на 15% в задачах математических рассуждений, сохраняя при этом такую же задержку и производительность на других наборах данных."
                },
                "en": {
                    "title": "Stitch: Simultaneous Thinking and Talking for Enhanced Spoken Language Models",
                    "desc": "This paper introduces Stitch, a new method for Spoken Language Models (SLMs) that allows them to think internally while responding to speech. Unlike traditional SLMs that generate responses without prior reasoning, Stitch alternates between generating unspoken reasoning chunks and spoken responses. This approach minimizes latency by utilizing the time taken to play audio responses to continue generating reasoning. As a result, Stitch not only matches the response time of existing models but also improves performance on math reasoning tasks by 15%."
                },
                "zh": {
                    "title": "同步思考与表达的口语模型",
                    "desc": "本论文提出了一种新的口语语言模型生成方法，名为Stitch。该方法通过交替生成无声推理片段和口语响应片段，解决了传统模型在回应前缺乏内在思考过程的问题。Stitch利用口语响应的音频持续时间，充分利用剩余时间生成推理内容，从而实现思考与表达的同步进行。实验结果表明，Stitch在数学推理数据集上比基线模型提高了15%的性能，同时在非推理数据集上表现也与基线模型相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11539",
            "title": "Streaming 4D Visual Geometry Transformer",
            "url": "https://huggingface.co/papers/2507.11539",
            "abstract": "A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.",
            "score": 5,
            "issue_id": 4937,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "03e472d31e5edcaf",
            "authors": [
                "Dong Zhuo",
                "Wenzhao Zheng",
                "Jiahe Guo",
                "Yuqi Wu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11539.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Реконструкция 4D-геометрии в реальном времени с помощью потокового трансформера",
                    "desc": "Статья представляет потоковый 4D-трансформер визуальной геометрии для реконструкции пространственно-временной геометрии из видео в реальном времени. Модель использует каузальную архитектуру трансформера и временное каузальное внимание для обработки входной последовательности в онлайн-режиме. Для эффективного обучения применяется дистилляция знаний от более плотной двунаправленной модели. Эксперименты показывают, что предложенный подход увеличивает скорость вывода, сохраняя конкурентоспособную производительность."
                },
                "en": {
                    "title": "Real-Time 4D Reconstruction with Streaming Transformers",
                    "desc": "This paper presents a streaming 4D visual geometry transformer that utilizes causal attention and knowledge distillation for real-time 4D reconstruction from video data. The model processes input sequences in an online manner, leveraging a causal transformer architecture to maintain high spatial consistency while integrating historical information. By employing temporal causal attention and caching past data, the system achieves efficient long-term reconstruction. The approach is validated through extensive experiments, showing improved inference speed and competitive performance, making it suitable for interactive 4D vision applications."
                },
                "zh": {
                    "title": "实时4D重建的创新变换器",
                    "desc": "本文提出了一种流式4D视觉几何变换器，利用因果注意力和知识蒸馏技术，实现实时的4D重建。该模型采用因果变换器架构，能够在线处理输入序列，并通过缓存历史信息来提高重建效率。通过从密集双向视觉几何变换器中蒸馏知识，模型在训练过程中得以优化。实验结果表明，该模型在保持高空间一致性的同时，显著提高了在线推理速度，适用于可扩展的交互式4D视觉系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15815",
            "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
            "url": "https://huggingface.co/papers/2507.15815",
            "abstract": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.",
            "score": 4,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "ad03ed3ae6e4256b",
            "authors": [
                "Seth Karten",
                "Wenzhe Li",
                "Zihan Ding",
                "Samuel Kleiner",
                "Yu Bai",
                "Chi Jin"
            ],
            "affiliations": [
                "Princeton University",
                "Salesforce Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15815.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#multimodal",
                    "#rl",
                    "#science"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Искусственный интеллект как экономист: моделирование и оптимизация экономической политики",
                    "desc": "Статья представляет новую концепцию под названием 'LLM Economist', которая использует агентное моделирование для разработки и оценки экономической политики в стратегических средах с иерархическим принятием решений. На нижнем уровне ограниченно рациональные агенты-работники выбирают предложение труда для максимизации текстовых функций полезности, изученных в контексте. На верхнем уровне агент-планировщик использует обучение с подкреплением для предложения кусочно-линейных графиков предельных налоговых ставок. Эксперименты показывают, что планировщик сходится к равновесиям, улучшающим совокупное общественное благосостояние по сравнению с решениями Саеза."
                },
                "en": {
                    "title": "Harnessing AI for Smarter Economic Policy Design",
                    "desc": "The LLM Economist is a new framework that combines agent-based modeling with large language models to evaluate economic policies in complex decision-making environments. It features two levels of agents: lower-level worker agents that optimize their labor supply based on learned utility functions, and an upper-level planner agent that uses reinforcement learning to create tax schedules. This approach allows for realistic simulations of diverse populations and effective mechanism design, all expressed in natural language. The framework shows promising results in improving social welfare through strategic interactions among agents, making it a valuable tool for testing economic policies."
                },
                "zh": {
                    "title": "利用大语言模型优化经济政策",
                    "desc": "本文介绍了一种名为LLM Economist的新框架，利用基于代理的建模来设计和评估具有层级决策的经济政策。在低层次，有限理性的工人代理根据美国人口普查的收入和人口统计数据选择劳动供给，以最大化基于文本的效用函数。在高层次，规划者代理使用上下文强化学习提出与当前美国联邦税率相结合的分段线性边际税率。这种构建使经济模拟具备了优化异质效用、生成大规模人口和机制设计等三种能力，能够在自然语言中进行有效的财政实验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15856",
            "title": "Latent Denoising Makes Good Visual Tokenizers",
            "url": "https://huggingface.co/papers/2507.15856",
            "abstract": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.",
            "score": 3,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "60a696cb47720198",
            "authors": [
                "Jiawei Yang",
                "Tianhong Li",
                "Lijie Fan",
                "Yonglong Tian",
                "Yue Wang"
            ],
            "affiliations": [
                "Google DeepMind",
                "MIT CSAIL",
                "OpenAI",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15856.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "🧹",
                "ru": {
                    "title": "Шумоподавление как ключ к эффективным визуальным токенизаторам",
                    "desc": "Статья представляет новый подход к разработке визуальных токенизаторов для генеративных моделей. Авторы предлагают метод Latent Denoising Tokenizer (l-DeTok), который обучается восстанавливать чистые изображения из зашумленных латентных представлений. Эксперименты на ImageNet показывают превосходство l-DeTok над стандартными токенизаторами для шести различных генеративных моделей. Исследование подчеркивает важность принципа шумоподавления в разработке токенизаторов для генеративного моделирования."
                },
                "en": {
                    "title": "Enhancing Generative Models with Denoising Tokenizers",
                    "desc": "This paper explores how visual tokenizers can be improved for generative modeling by focusing on a process called denoising. The authors propose a new tokenizer, the Latent Denoising Tokenizer (l-DeTok), which aligns its embeddings with the goal of reconstructing clean images from corrupted inputs. By training this tokenizer to handle noise and masking, it becomes more effective at generating high-quality outputs. The results show that l-DeTok outperforms traditional tokenizers in various generative models, suggesting that denoising should be a key consideration in future tokenizer designs."
                },
                "zh": {
                    "title": "去噪：分词器设计的新原则",
                    "desc": "本论文探讨了视觉分词器在生成建模中的有效性，提出了对分词器嵌入与去噪目标进行对齐的概念。我们引入了潜在去噪分词器（l-DeTok），该分词器旨在从受到干扰的潜在嵌入中重建干净图像。实验结果表明，l-DeTok在多个生成模型上优于传统分词器，验证了去噪作为分词器设计的重要原则。我们希望这一发现能够为未来的分词器设计提供新的视角。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15640",
            "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training",
            "url": "https://huggingface.co/papers/2507.15640",
            "abstract": "Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.",
            "score": 2,
            "issue_id": 4942,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 июля",
                "en": "July 21",
                "zh": "7月21日"
            },
            "hash": "2ce6b05c03e1226b",
            "authors": [
                "Kailai Yang",
                "Xiao Liu",
                "Lei Ji",
                "Hao Li",
                "Yeyun Gong",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15640.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Умное смешивание данных для адаптивного обучения языковых моделей",
                    "desc": "Data Mixing Agent - это фреймворк на основе обучения с подкреплением для переобучения больших языковых моделей. Он эффективно перевзвешивает обучающие данные для сбалансансировки производительности между исходными и целевыми областями. Этот подход превосходит сильные базовые модели в достижении сбалансированной производительности в задачах математических рассуждений. Data Mixing Agent хорошо обобщается на новые исходные области, целевые модели и пространства доменов без переобучения."
                },
                "en": {
                    "title": "Reinforcement Learning for Balanced Data Mixing in Language Models",
                    "desc": "The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data."
                },
                "zh": {
                    "title": "数据混合代理：平衡源与目标领域的智能学习",
                    "desc": "数据混合代理是一种基于模型的框架，利用强化学习有效地重新加权训练数据，以平衡在持续预训练中源领域和目标领域的性能。该方法解决了在小规模特定任务数据上持续预训练时可能出现的灾难性遗忘问题。通过提出数据混合代理，研究者证明了更通用的启发式方法可以被参数化，从而实现端到端的学习。实验结果表明，该代理在数学推理的持续预训练中表现优于强基线，并且在未见过的源领域和目标模型上具有良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13428",
            "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models",
            "url": "https://huggingface.co/papers/2507.13428",
            "abstract": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.",
            "score": 1,
            "issue_id": 4943,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 июля",
                "en": "July 17",
                "zh": "7月17日"
            },
            "hash": "0bcb2373e179ecb8",
            "authors": [
                "Jing Gu",
                "Xian Liu",
                "Yu Zeng",
                "Ashwin Nagarajan",
                "Fangrui Zhu",
                "Daniel Hong",
                "Yue Fan",
                "Qianqi Yan",
                "Kaiwen Zhou",
                "Ming-Yu Liu",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Northeastern University",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13428.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#interpretability",
                    "#optimization",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Физика в виртуальном мире: новый бенчмарк для оценки реалистичности видеогенерации",
                    "desc": "Статья представляет PhyWorldBench - комплексный бенчмарк для оценки моделей генерации видео на основе их соответствия законам физики. Бенчмарк охватывает различные уровни физических явлений, от базовых принципов до сложных сценариев, включая категорию 'Анти-физика'. Авторы оценили 12 современных моделей text-to-video на 1050 специально подобранных промптах. В результате были выявлены ключевые проблемы, с которыми сталкиваются модели при соблюдении реальной физики, и даны рекомендации по составлению промптов для повышения физической достоверности."
                },
                "en": {
                    "title": "Evaluating Video Generation with Physics: PhyWorldBench",
                    "desc": "This paper introduces PhyWorldBench, a benchmark for evaluating video generation models based on their ability to simulate physical laws accurately. It assesses models across various physical phenomena, from basic principles like motion and energy conservation to complex interactions involving living beings. A unique 'Anti-Physics' category is included to test models' responses to prompts that contradict real-world physics, ensuring logical consistency in their outputs. The study evaluates 12 leading text-to-video models, revealing significant challenges in maintaining physical realism and providing insights for improving prompt design to enhance adherence to physical principles."
                },
                "zh": {
                    "title": "评估视频生成模型的物理真实性",
                    "desc": "视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。本文提出了PhyWorldBench，这是一个全面的基准，用于评估视频生成模型在遵循物理法则方面的表现。我们评估了12个最先进的文本到视频生成模型，并识别出这些模型在遵循现实物理方面面临的主要挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.10935",
            "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization",
            "url": "https://huggingface.co/papers/2507.10935",
            "abstract": "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.",
            "score": 1,
            "issue_id": 4942,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 июля",
                "en": "July 15",
                "zh": "7月15日"
            },
            "hash": "8d8109c5462763ac",
            "authors": [
                "Shaowen Tong",
                "Zimin Xia",
                "Alexandre Alahi",
                "Xuming He",
                "Yujiao Shi"
            ],
            "affiliations": [
                "Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10935.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#cv"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "GeoDistill: геометрия на службе кросс-видовой локализации",
                    "desc": "GeoDistill - это новый подход к кросс-видовой локализации, использующий слабо контролируемое самообучение на основе геометрии. Метод применяет обучение по схеме учитель-ученик с маскированием по полю зрения для улучшения извлечения локальных признаков. Учитель локализует панорамное изображение, а ученик предсказывает положение по ограниченному полю зрения. Такой подход позволяет модели фокусироваться на ключевых особенностях и игнорировать бестекстурные области, повышая точность локализации."
                },
                "en": {
                    "title": "GeoDistill: Enhancing Cross-View Localization with Weak Supervision",
                    "desc": "This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation."
                },
                "zh": {
                    "title": "GeoDistill：高效的跨视角定位解决方案",
                    "desc": "跨视角定位是通过将地面图像与卫星图像对齐来估计相机的三自由度姿态，这在自动导航和增强现实等大规模户外应用中至关重要。现有方法通常依赖于完全监督学习，这需要昂贵的真实姿态标注。我们提出了GeoDistill，一个几何引导的弱监督自蒸馏框架，利用教师-学生学习和基于视场(FoV)的掩蔽来增强局部特征学习，从而实现稳健的跨视角定位。GeoDistill通过对齐学生模型和教师模型的预测，帮助学生模型专注于关键特征，提高了定位精度并减少了不确定性。"
                }
            }
        }
    ],
    "link_prev": "2025-07-21.html",
    "link_next": "2025-07-23.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7月21日"
    },
    "short_date_next": {
        "ru": "23.07",
        "en": "07/23",
        "zh": "7月23日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 5,
        "#cv": 4,
        "#rl": 6,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 2,
        "#agi": 3,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 11,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}