
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. December 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-25.html">â¬…ï¸ <span id="prev-date">25.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-27.html">â¡ï¸ <span id="next-date">27.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 26', 'zh': '12æœˆ26æ—¥'};
        let feedDateNext = {'ru': '27.12', 'en': '12/27', 'zh': '12æœˆ27æ—¥'};
        let feedDatePrev = {'ru': '25.12', 'en': '12/25', 'zh': '12æœˆ25æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.18450', 'title': '3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.18450', 'abstract': 'A 3D scene graph represents a compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with a user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose a method 3DGraphLLM for constructing a learnable representation of a 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM.', 'score': 21, 'issue_id': 1311, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 24', 'zh': '12æœˆ24æ—¥'}, 'hash': '3d80e95d793a8b5e', 'authors': ['Tatiana Zemskova', 'Dmitry Yudin'], 'affiliations': ['Artificial Intelligence Research Institute', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.18450.jpg', 'data': {'categories': ['#3d', '#multimodal', '#reasoning', '#open_source', '#agents', '#games', '#graphs'], 'emoji': 'ğŸ¤–', 'ru': {'title': '3D-Ğ³Ñ€Ğ°Ñ„Ñ‹ ÑÑ†ĞµĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 3DGraphLLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ScanRefer, RIORefer Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Robot Understanding with 3D Scene Graphs and Language Models', 'desc': 'This paper introduces 3DGraphLLM, a method for creating a learnable representation of 3D scene graphs that captures both object information and their semantic relationships. By integrating this representation with Large Language Models (LLMs), the approach enhances user-robot interactions, allowing robots to better understand and respond to natural language queries about 3D scenes. The authors demonstrate that their method outperforms existing techniques that only consider object coordinates, highlighting the importance of semantic relationships in improving LLM responses. Experiments conducted on various datasets show the effectiveness of 3DGraphLLM in performing 3D vision-language tasks.'}, 'zh': {'title': 'æå‡æœºå™¨äººäº¤äº’çš„3Dåœºæ™¯ç†è§£', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º3DGraphLLMçš„æ–¹æ³•ï¼Œç”¨äºæ„å»º3Dåœºæ™¯å›¾çš„å¯å­¦ä¹ è¡¨ç¤ºã€‚è¯¥è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨3Dè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è¯­ä¹‰å…³ç³»çš„ä¿¡æ¯å¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹çš„å“åº”è´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ºæœºå™¨äººä¸ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€äº¤äº’æä¾›äº†æ›´å¼ºå¤§çš„æ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.18153', 'title': 'DepthLab: From Partial to Complete', 'url': 'https://huggingface.co/papers/2412.18153', 'abstract': 'Missing values remain a common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, a foundation depth inpainting model powered by image diffusion priors. Our model features two notable strengths: (1) it demonstrates resilience to depth-deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https://johanan528.github.io/depthlab_web/.', 'score': 21, 'issue_id': 1305, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 24', 'zh': '12æœˆ24æ—¥'}, 'hash': 'c319c831137b3ce6', 'authors': ['Zhiheng Liu', 'Ka Leong Cheng', 'Qiuyu Wang', 'Shuzhe Wang', 'Hao Ouyang', 'Bin Tan', 'Kai Zhu', 'Yujun Shen', 'Qifeng Chen', 'Ping Luo'], 'affiliations': ['Aalto University', 'Ant Group', 'HKU', 'HKUST', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.18153.jpg', 'data': {'categories': ['#diffusion', '#3d', '#open_source', '#dataset'], 'emoji': 'ğŸ•³ï¸', 'ru': {'title': 'DepthLab: Ğ’Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DepthLab - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ². DepthLab Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LiDAR.'}, 'en': {'title': 'DepthLab: Bridging the Gap in Depth Data Completion', 'desc': 'This paper presents DepthLab, a novel model designed to address the issue of missing values in depth data, which often occurs due to incomplete data collection or changes in perspective. DepthLab utilizes image diffusion priors to effectively inpaint depth information, ensuring that both continuous and isolated missing regions are filled accurately. The model maintains scale consistency with known depth values, which is crucial for realistic depth completion. DepthLab outperforms existing methods in various applications, such as 3D scene inpainting and LiDAR depth completion, demonstrating superior numerical and visual results.'}, 'zh': {'title': 'æ·±åº¦ä¿®å¤æ–°çªç ´ï¼šDepthLabæ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDepthLabçš„æ·±åº¦å›¾åƒä¿®å¤æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦æ•°æ®ä¸­çš„ç¼ºå¤±å€¼é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å›¾åƒæ‰©æ•£å…ˆéªŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¡«è¡¥æ·±åº¦ä¸è¶³çš„åŒºåŸŸï¼Œç¡®ä¿è¿ç»­åŒºåŸŸå’Œå­¤ç«‹ç‚¹çš„å¯é ä¿®å¤ã€‚DepthLabåœ¨å¡«è¡¥ç¼ºå¤±å€¼æ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸å·²çŸ¥æ·±åº¦çš„ä¸€è‡´æ€§ï¼Œç¡®ä¿å°ºåº¦çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›ä¼˜åŠ¿ï¼Œè¯¥æ¨¡å‹åœ¨3Dåœºæ™¯ä¿®å¤ã€æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆã€ç¨€ç–è§†å›¾é‡å»ºå’ŒLiDARæ·±åº¦è¡¥å…¨ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.17739', 'title': "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", 'url': 'https://huggingface.co/papers/2412.17739', 'abstract': "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.", 'score': 17, 'issue_id': 1306, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 23', 'zh': '12æœˆ23æ—¥'}, 'hash': '1ce9c827a32ec3c5', 'authors': ['Ermo Hua', 'Che Jiang', 'Xingtai Lv', 'Kaiyan Zhang', 'Ning Ding', 'Youbang Sun', 'Biqing Qi', 'Yuchen Fan', 'Xue Kai Zhu', 'Bowen Zhou'], 'affiliations': ['Northeastern University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.17739.jpg', 'data': {'categories': ['#training', '#optimization', '#long_context', '#architecture'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Fourier Position Embedding (FoPE). FoPE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Rotary Position Embedding (RoPE) Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FoPE ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ñ€ÑĞ´Ñ‹ Ğ¤ÑƒÑ€ÑŒĞµ Ğ¸ Ğ¾Ğ±Ğ½ÑƒĞ»ÑĞµÑ‚ Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FoPE Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ RoPE Ğ¸ ALiBi.'}, 'en': {'title': 'Enhancing Language Models with Fourier Position Embedding for Better Context Handling', 'desc': 'This paper explores the limitations of Rotary Position Embedding (RoPE) in Language Models (LMs) and its impact on length generalization. It reveals that while RoPE allows for periodic attention through Non-Uniform Discrete Fourier Transform, this capability is compromised by linear layers and insufficient training of frequency components. The authors introduce Fourier Position Embedding (FoPE), which improves the frequency-domain characteristics of attention by eliminating harmful frequency components. Experimental results demonstrate that FoPE outperforms RoPE and ALiBi in maintaining stability in perplexity and accuracy across different context lengths.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡æ”¹è¿›æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥æ‰©å±•è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬åˆ†æäº†RoPEåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¹‹å¤–çš„å„ä¸ªéƒ¨åˆ†çš„å½±å“ï¼Œå‘ç°å…¶å¯¹é•¿åº¦æ³›åŒ–çš„è´Ÿé¢æ•ˆåº”ã€‚åŸºäºç¦»æ•£ä¿¡å·å¤„ç†ç†è®ºï¼Œæˆ‘ä»¬æå‡ºäº†å‚…é‡Œå¶ä½ç½®åµŒå…¥ï¼ˆFoPEï¼‰ï¼Œå®ƒé€šè¿‡æ„å»ºå‚…é‡Œå¶çº§æ•°æ¥å¢å¼ºæ³¨æ„åŠ›çš„é¢‘åŸŸç‰¹æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFoPEåœ¨ä¸åŒä¸Šä¸‹æ–‡çª—å£ä¸‹èƒ½å¤Ÿä¿æŒæ›´ç¨³å®šçš„å›°æƒ‘åº¦å’Œä¸€è‡´çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.18597', 'title': 'DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation', 'url': 'https://huggingface.co/papers/2412.18597', 'abstract': "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.", 'score': 10, 'issue_id': 1307, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 24', 'zh': '12æœˆ24æ—¥'}, 'hash': '210ce3ba0e7e45d2', 'authors': ['Minghong Cai', 'Xiaodong Cun', 'Xiaoyu Li', 'Wenze Liu', 'Zhaoyang Zhang', 'Yong Zhang', 'Ying Shan', 'Xiangyu Yue'], 'affiliations': ['ARC Lab, Tencent PCG', 'GVC Lab, Great Bay University', 'MMLab, The Chinese University of Hong Kong', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.18597.jpg', 'data': {'categories': ['#video', '#multimodal', '#games', '#diffusion', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiTCtrl Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Multi-Modal Diffusion Transformer (MM-DiT) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MPVBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing Video Generation with Multi-Prompt Control', 'desc': 'This paper introduces DiTCtrl, a novel method for generating videos using multiple prompts without the need for additional training. It leverages the Multi-Modal Diffusion Transformer (MM-DiT) architecture to facilitate smooth transitions and coherent object motion across sequential prompts. By analyzing the attention mechanism of MM-DiT, the authors enable precise semantic control, allowing for effective multi-prompt video generation. The proposed method outperforms existing techniques and is evaluated using a new benchmark called MPVBench, specifically designed for this purpose.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„å¤šæç¤ºè§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæç¤ºè§†é¢‘ç”Ÿæˆæ–¹æ³•DiTCtrlï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šä¸ªé¡ºåºæç¤ºæ—¶çš„å›°éš¾ã€‚æˆ‘ä»¬åˆ©ç”¨MM-DiTæ¶æ„ï¼Œé€šè¿‡åˆ†æå…¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨å¤šæç¤ºè§†é¢‘ç”Ÿæˆä¸­å¹³æ»‘è¿‡æ¸¡å’Œä¸€è‡´çš„ç‰©ä½“è¿åŠ¨ã€‚DiTCtrlä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæç¤ºä¸‹ç”Ÿæˆè‡ªç„¶æµç•…çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MPVBenchåŸºå‡†ï¼Œä»¥è¯„ä¼°å¤šæç¤ºç”Ÿæˆçš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ— é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.17758', 'title': "In Case You Missed It: ARC 'Challenge' Is Not That Challenging", 'url': 'https://huggingface.co/papers/2412.17758', 'abstract': 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.', 'score': 7, 'issue_id': 1311, 'pub_date': '2024-12-23', 'pub_date_card': {'ru': '23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 23', 'zh': '12æœˆ23æ—¥'}, 'hash': '38b823b470857f90', 'authors': ['Åukasz Borchmann'], 'affiliations': ['Snowflake AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.17758.jpg', 'data': {'categories': ['#training', '#reasoning', '#interpretability', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ñ‚ĞµÑÑ‚Ğ¾Ğ²: Ñ€Ğ¾Ğ»ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞ¹ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ° ARC Challenge Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ½Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Reevaluating Evaluation: Uncovering True Model Capabilities', 'desc': 'The paper discusses the challenges faced by modern language models (LLMs) when evaluating their performance on the ARC Challenge compared to ARC Easy. It argues that the evaluation setup, which does not allow for direct comparison of answer choices, is the main reason for the perceived difficulty, rather than the tasks themselves being inherently harder. The authors highlight a recent shift in evaluation practices that has not been widely recognized, showing that adopting fairer methods can significantly improve performance metrics. They emphasize the importance of accurate evaluation methods to truly reflect the capabilities of models and avoid misleading conclusions about their reasoning abilities.'}, 'zh': {'title': 'è¯„ä¼°æ–¹æ³•å½±å“æ¨¡å‹è¡¨ç°çš„è®¤çŸ¥', 'desc': 'ARCæŒ‘æˆ˜å¯¹ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä¼¼ä¹æ¯”ARCç®€å•æ›´å›°éš¾ï¼Œä¸»è¦æ˜¯å› ä¸ºè¯„ä¼°è®¾ç½®é˜»æ­¢äº†å¯¹ç­”æ¡ˆé€‰æ‹©çš„ç›´æ¥æ¯”è¾ƒï¼Œè€Œä¸æ˜¯å›ºæœ‰çš„å¤æ‚æ€§ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶äººå‘˜åœ¨è¿‡å»ä¸€å¹´ä¸­æ‚„ç„¶è½¬å‘æ›´åˆé€‚çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä½†è¿™ä¸€å˜åŒ–çš„å½±å“å°šæœªè¢«å¹¿æ³›è®¤å¯ã€‚æˆ‘ä»¬å¼ºè°ƒäº†è¿™ä¸€è¢«å¿½è§†çš„è½¬å˜ï¼Œå±•ç¤ºäº†ç±»ä¼¼çš„è¯„ä¼°å®è·µå¦‚ä½•é”™è¯¯åœ°æš—ç¤ºå…¶ä»–åŸºå‡†ä¸­çš„æ¨ç†ç¼ºé™·ï¼Œå¹¶è¯æ˜æ›´å…¬å¹³çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ€§èƒ½å·®è·ï¼ˆä¾‹å¦‚åœ¨SIQAä¸Šï¼‰ï¼Œç”šè‡³äº§ç”Ÿè¶…äººç±»çš„ç»“æœï¼ˆOpenBookQAï¼‰ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¯„ä¼°å¦‚ä½•å½±å“æ„ŸçŸ¥çš„éš¾åº¦ï¼Œå¹¶æä¾›äº†ç¡®ä¿å¤šé¡¹é€‰æ‹©è¯„ä¼°å‡†ç¡®åæ˜ æ¨¡å‹å®é™…èƒ½åŠ›çš„æŒ‡å¯¼æ–¹é’ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.14711', 'title': 'ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing', 'url': 'https://huggingface.co/papers/2412.14711', 'abstract': "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.", 'score': 7, 'issue_id': 1305, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '0b43c3f140601a96', 'authors': ['Ziteng Wang', 'Jianfei Chen', 'Jun Zhu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.14711.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ReMoE: Ğ”Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ReMoE Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE). ReMoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReLU Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ TopK+Softmax. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReMoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ MoE Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'ReMoE: Revolutionizing Mixture-of-Experts with Differentiable Routing', 'desc': 'This paper introduces ReMoE, a new architecture for Mixture-of-Experts (MoE) models that improves upon traditional TopK routers by making them fully differentiable. By using ReLU as the routing mechanism, ReMoE allows for continuous optimization, which enhances performance and scalability. The authors also present techniques to manage the sparsity of the router and ensure an even distribution of workload among experts. Experimental results show that ReMoE outperforms conventional MoE models in various scenarios, demonstrating better scalability with an increasing number of experts.'}, 'zh': {'title': 'ReMoEï¼šæå‡æ··åˆä¸“å®¶æ¨¡å‹çš„æ€§èƒ½ä¸å¯æ‰©å±•æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ¿€æ´»æ··åˆä¸“å®¶æ¨¡å‹ReMoEï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿçš„TopKè·¯ç”±å™¨ä¸åŒï¼ŒReMoEé‡‡ç”¨äº†å®Œå…¨å¯å¾®åˆ†çš„æ¶æ„ï¼Œä½¿ç”¨ReLUä½œä¸ºè·¯ç”±å™¨ï¼Œä»è€Œå…‹æœäº†éè¿ç»­æ€§å¸¦æ¥çš„é™åˆ¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è°ƒèŠ‚è·¯ç”±å™¨ç¨€ç–æ€§çš„æ–¹æ³•ï¼Œä»¥å¹³è¡¡ä¸“å®¶ä¹‹é—´çš„è´Ÿè½½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMoEåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œä¸“å®¶æ•°é‡ä¸‹ï¼Œå‡ä¼˜äºä¼ ç»Ÿçš„TopKè·¯ç”±æ··åˆä¸“å®¶æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.15443', 'title': 'SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval', 'url': 'https://huggingface.co/papers/2412.15443', 'abstract': "Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer_relevancy, faithfulness, context_precision and context_recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH's capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems.", 'score': 6, 'issue_id': 1305, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': '2d16e57527037cb7', 'authors': ['Aakash Mahalingam', 'Vinesh Kumar Gande', 'Aman Chadha', 'Vinija Jain', 'Divya Chaudhary'], 'affiliations': ['Amazon AI', 'Meta', 'Northeastern University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2412.15443.jpg', 'data': {'categories': ['#graphs', '#dataset', '#hallucinations', '#benchmark', '#rag', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SKETCH: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SKETCH, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ (RAG). SKETCH Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. SKETCH Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ RAGAS Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'SKETCH: Elevating RAG with Semantic and Structured Data Integration', 'desc': 'This paper presents SKETCH, a new method that improves Retrieval-Augmented Generation (RAG) systems by combining semantic text retrieval with knowledge graphs. This integration allows for better processing of large datasets while ensuring a deeper understanding of context. SKETCH shows significant enhancements in retrieval performance and context integrity compared to traditional RAG methods. The results from various datasets demonstrate that SKETCH achieves high scores in answer relevancy and context precision, establishing new standards for retrieval systems.'}, 'zh': {'title': 'SKETCHï¼šæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSKETCHçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚SKETCHé€šè¿‡å°†è¯­ä¹‰æ–‡æœ¬æ£€ç´¢ä¸çŸ¥è¯†å›¾è°±ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å’Œæ£€ç´¢å¤§æ•°æ®é›†ä¸­çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸Šä¸‹æ–‡çš„å…¨é¢ç†è§£ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSKETCHåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ„å¤§åˆ©ç¾é£Ÿæ•°æ®é›†ä¸Šï¼Œè¾¾åˆ°äº†0.94çš„ç­”æ¡ˆç›¸å…³æ€§å’Œ0.99çš„ä¸Šä¸‹æ–‡ç²¾åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSKETCHèƒ½å¤Ÿæä¾›æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”ï¼Œä¸ºæœªæ¥çš„æ£€ç´¢ç³»ç»Ÿè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.18608', 'title': 'PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models', 'url': 'https://huggingface.co/papers/2412.18608', 'abstract': 'Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.', 'score': 5, 'issue_id': 1308, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 24', 'zh': '12æœˆ24æ—¥'}, 'hash': '7f6d99dea7ea25bc', 'authors': ['Minghao Chen', 'Roman Shapovalov', 'Iro Laina', 'Tom Monnier', 'Jianyuan Wang', 'David Novotny', 'Andrea Vedaldi'], 'affiliations': ['Meta AI', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2412.18608.jpg', 'data': {'categories': ['#3d', '#hallucinations', '#diffusion'], 'emoji': 'ğŸ§©', 'ru': {'title': 'PartGen: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'PartGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¸Ğ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸. PartGen Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'PartGen: Transforming 3D Generation with Meaningful Parts', 'desc': 'This paper presents PartGen, a new method for generating 3D objects that are composed of meaningful, manipulable parts from various inputs like text, images, or unstructured 3D objects. It utilizes a multi-view diffusion model to segment the 3D object into plausible parts based on multiple views, ensuring consistency across different perspectives. A second diffusion model then reconstructs each part by filling in occlusions and integrating them into a cohesive whole, even generating parts that are not visible in the input. The results demonstrate that PartGen significantly outperforms existing methods for segmentation and part extraction, enabling advanced applications like 3D part editing.'}, 'zh': {'title': 'PartGenï¼šç”Ÿæˆå¯æ“ä½œçš„3Dç‰©ä½“éƒ¨åˆ†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPartGençš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»æ–‡æœ¬ã€å›¾åƒæˆ–éç»“æ„åŒ–3Då¯¹è±¡ç”Ÿæˆç”±æœ‰æ„ä¹‰éƒ¨åˆ†ç»„æˆçš„3Dç‰©ä½“ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹æå–3Då¯¹è±¡çš„éƒ¨åˆ†åˆ†å‰²ï¼Œå°†å¯¹è±¡åˆ’åˆ†ä¸ºå¤šä¸ªå¯ç‹¬ç«‹æ“ä½œçš„éƒ¨åˆ†ã€‚æ¥ç€ï¼Œç¬¬äºŒä¸ªå¤šè§†è§’æ‰©æ•£æ¨¡å‹å¯¹æ¯ä¸ªéƒ¨åˆ†è¿›è¡Œå¡«å……å’Œ3Dé‡å»ºï¼Œç¡®ä¿å„éƒ¨åˆ†åœ¨æ•´ä½“ä¸Šä¸‹æ–‡ä¸­å’Œè°èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartGenåœ¨ç”Ÿæˆå’ŒçœŸå®3Dèµ„äº§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åˆ†å‰²å’Œéƒ¨åˆ†æå–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.17847', 'title': 'Bridging the Data Provenance Gap Across Text, Speech and Video', 'url': 'https://huggingface.co/papers/2412.17847', 'abstract': 'Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.', 'score': 2, 'issue_id': 1322, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 19', 'zh': '12æœˆ19æ—¥'}, 'hash': 'a67cb86f2c58f7e0', 'authors': ['Shayne Longpre', 'Nikhil Singh', 'Manuel Cherep', 'Kushagra Tiwary', 'Joanna Materzynska', 'William Brannon', 'Robert Mahari', 'Manan Dey', 'Mohammed Hamdy', 'Nayan Saxena', 'Ahmad Mustafa Anis', 'Emad A. Alghamdi', 'Vu Minh Chien', 'Naana Obeng-Marnu', 'Da Yin', 'Kun Qian', 'Yizhi Li', 'Minnie Liang', 'An Dinh', 'Shrestha Mohanty', 'Deividas Mataciunas', 'Tobin South', 'Jianguo Zhang', 'Ariel N. Lee', 'Campbell S. Lund', 'Christopher Klamm', 'Damien Sileo', 'Diganta Misra', 'Enrico Shippole', 'Kevin Klyman', 'Lester JV Miranda', 'Niklas Muennighoff', 'Seonghyeon Ye', 'Seungone Kim', 'Vipul Gupta', 'Vivek Sharma', 'Xuhui Zhou', 'Caiming Xiong', 'Luis Villa', 'Stella Biderman', 'Alex Pentland', 'Sara Hooker', 'Jad Kabbara'], 'affiliations': ['The Data Provenance Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2412.17847.jpg', 'data': {'categories': ['#multimodal', '#data', '#ethics', '#dataset', '#synthetic', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…: Ñ‚ĞµĞºÑÑ‚, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 4000 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ 1990 Ğ¿Ğ¾ 2024 Ğ³Ğ¾Ğ´, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 608 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ 67 ÑÑ‚Ñ€Ğ°Ğ½. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²ĞµĞ±-ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³Ğ°, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ YouTube, Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ 2013 Ğ³Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unveiling the Landscape of AI Training Datasets', 'desc': 'This paper presents a comprehensive analysis of nearly 4000 public datasets used in machine learning, focusing on text, speech, and video modalities. The study reveals that since 2019, most datasets have been sourced from web-crawled content, synthetic data, and social media, particularly YouTube. It also highlights that while the number of languages and regions represented in datasets has increased, the actual geographical and multilingual coverage has not improved significantly since 2013. The authors emphasize the importance of dataset transparency and responsible AI practices, providing their findings to help practitioners understand data sourcing and restrictions better.'}, 'zh': {'title': 'æ•°æ®é€æ˜åº¦ä¸è´Ÿè´£ä»»çš„AIè¿›æ­¥', 'desc': 'è¿™ç¯‡è®ºæ–‡åˆ†æäº†äººå·¥æ™ºèƒ½è®­ç»ƒæ•°æ®é›†çš„å±æ€§ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘æ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œè‡ª2019å¹´ä»¥æ¥ï¼Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ åº”ç”¨ä¸»è¦ä¾èµ–äºç½‘ç»œçˆ¬è™«ã€åˆæˆæ•°æ®å’Œç¤¾äº¤åª’ä½“å¹³å°ï¼ˆå¦‚YouTubeï¼‰ä½œä¸ºè®­ç»ƒé›†æ¥æºã€‚å°½ç®¡å…¬å…±AIè®­ç»ƒæ•°æ®é›†ä¸­è¯­è¨€å’Œåœ°ç†è¦†ç›–é¢æœ‰æ‰€å¢åŠ ï¼Œä½†ç›¸å¯¹çš„åœ°ç†å’Œå¤šè¯­è¨€ä»£è¡¨æ€§å¹¶æ²¡æœ‰æ˜¾è‘—æ”¹å–„ã€‚è®ºæ–‡çš„å®¡è®¡ç»“æœä¸ºæ•°æ®æ¥æºã€ä½¿ç”¨é™åˆ¶å’Œè¥¿æ–¹ä¸­å¿ƒä¸»ä¹‰æä¾›äº†å®è¯åˆ†æï¼Œå¼ºè°ƒäº†æ•°æ®é€æ˜åº¦å’Œè´Ÿè´£ä»»ä½¿ç”¨çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.16153', 'title': 'MotiF: Making Text Count in Image Animation with Motion Focal Loss', 'url': 'https://huggingface.co/papers/2412.16153', 'abstract': "Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in https://wang-sj16.github.io/motif/.", 'score': 2, 'issue_id': 1319, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 20', 'zh': '12æœˆ20æ—¥'}, 'hash': '48c7cc6d9e1fa27b', 'authors': ['Shijie Wang', 'Samaneh Azadi', 'Rohit Girdhar', 'Saketh Rambhatla', 'Chen Sun', 'Xi Yin'], 'affiliations': ['Brown University', 'GenAI, Meta'], 'pdf_title_img': 'assets/pdf/title_img/2412.16153.jpg', 'data': {'categories': ['#video', '#dataset', '#open_source', '#optimization', '#games', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'MotiF: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ MotiF Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TI2V Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 320 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚.'}, 'en': {'title': 'MotiF: Enhancing Video Generation with Motion Awareness', 'desc': "The paper presents MotiF, a novel approach for generating videos from images based on text descriptions, addressing the challenge of aligning generated videos with specified motions. By utilizing optical flow to create a motion heatmap, MotiF enhances the model's focus on areas with significant motion, leading to better text alignment and motion generation. The authors also introduce TI2V Bench, a new dataset with 320 image-text pairs designed for evaluating text-image-to-video generation methods. Comprehensive evaluations show that MotiF significantly outperforms existing models, achieving a 72% preference rate in human assessments."}, 'zh': {'title': 'MotiFï¼šæå‡æ–‡æœ¬å¼•å¯¼è§†é¢‘ç”Ÿæˆçš„è¿åŠ¨å¯¹é½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬å¼•å¯¼å›¾åƒåŠ¨ç”»æ–¹æ³•ï¼Œç§°ä¸ºMotiFï¼Œæ—¨åœ¨ä»å›¾åƒç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºä¸€è‡´çš„è§†é¢‘æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨è¿åŠ¨æŒ‡å®šæ–¹é¢å­˜åœ¨å›°éš¾ã€‚MotiFé€šè¿‡å…³æ³¨è¿åŠ¨åŒºåŸŸæ¥æ”¹å–„æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨ç”Ÿæˆï¼Œä½¿ç”¨å…‰æµç”Ÿæˆè¿åŠ¨çƒ­å›¾å¹¶æ ¹æ®è¿åŠ¨å¼ºåº¦åŠ æƒæŸå¤±ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TI2V Benchæ•°æ®é›†ï¼Œä»¥ä¾¿å¯¹TI2Vç”Ÿæˆè¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†MotiFåœ¨å¤šä¸ªæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.15797', 'title': 'Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning', 'url': 'https://huggingface.co/papers/2412.15797', 'abstract': 'Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov decision process. In this framework, states represent intermediate reasoning paths, while actions consist of generating the next reasoning step using one of the language models selected from a predefined pool. Guided by a process-based reward model, LE-MCTS performs a tree search over the reasoning steps generated by different language models, identifying the most accurate reasoning chain. Experimental results on five mathematical reasoning benchmarks demonstrate that our approach outperforms both single language model decoding algorithms and language model ensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the MATH and MQA datasets, respectively, highlighting its effectiveness in solving complex reasoning problems.', 'score': 1, 'issue_id': 1315, 'pub_date': '2024-12-20', 'pub_date_card': {'ru': '20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 20', 'zh': '12æœˆ20æ—¥'}, 'hash': 'ed8cd715177d35d0', 'authors': ['Sungjin Park', 'Xiao Liu', 'Yeyun Gong', 'Edward Choi'], 'affiliations': ['KAIST AI', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.15797.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#training', '#math', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LE-MCTS: ĞĞ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LE-MCTS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LE-MCTS Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LE-MCTS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with LE-MCTS', 'desc': 'This paper introduces a new method called Language model Ensemble with Monte Carlo Tree Search (LE-MCTS) to improve the reasoning abilities of open-source language models. LE-MCTS treats the reasoning process as a Markov decision process, where different states represent various reasoning paths and actions involve selecting a language model to generate the next step. By using a reward model to guide the search for the best reasoning chain, LE-MCTS effectively combines the strengths of multiple models. The results show that this approach significantly enhances performance on complex reasoning tasks compared to traditional methods.'}, 'zh': {'title': 'é€šè¿‡LE-MCTSæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œå¼€æºæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„é›†æˆæ–¹æ³•æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè¯­è¨€æ¨¡å‹é›†æˆä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆLE-MCTSï¼‰ï¼Œç”¨äºè¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹çº§é›†æˆã€‚LE-MCTSå°†é€æ­¥æ¨ç†å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ ‘æœç´¢è¯†åˆ«æœ€å‡†ç¡®çš„æ¨ç†é“¾ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi', '#alignment', '#architecture (2)', '#audio', '#benchmark (5)', '#cv', '#data (1)', '#dataset (5)', '#diffusion (3)', '#ethics (1)', '#games (3)', '#graphs (2)', '#hallucinations (2)', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (3)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (3)', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (4)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-26 00:45',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-26 00:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-26 00:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    