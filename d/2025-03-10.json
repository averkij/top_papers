{
    "date": {
        "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 10",
        "zh": "3æœˆ10æ—¥"
    },
    "time_utc": "2025-03-10 03:11",
    "weekday": 0,
    "issue_id": 2608,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.05236",
            "title": "Unified Reward Model for Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2503.05236",
            "abstract": "Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.",
            "score": 37,
            "issue_id": 2607,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 7",
                "zh": "3æœˆ7æ—¥"
            },
            "hash": "6ebf61a6777b8e4d",
            "authors": [
                "Yibin Wang",
                "Yuhang Zang",
                "Hao Li",
                "Cheng Jin",
                "Jiaqi Wang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.05236.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#cv",
                    "#alignment",
                    "#dataset",
                    "#rlhf",
                    "#rag"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifiedReward - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. UnifiedReward Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO)."
                },
                "en": {
                    "title": "UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment",
                    "desc": "This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ",
                    "desc": "æœ€è¿‘åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£çš„èƒ½åŠ›ã€‚å…³é”®æ–¹æ³•æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼åå¥½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒè§†è§‰åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºäº†UnifiedRewardï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæˆå¯¹æ’åå’Œé€ç‚¹è¯„åˆ†ï¼Œä»è€Œå®ç°è§†è§‰æ¨¡å‹çš„åå¥½å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05179",
            "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
            "url": "https://huggingface.co/papers/2503.05179",
            "abstract": "Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.",
            "score": 18,
            "issue_id": 2607,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 7",
                "zh": "3æœˆ7æ—¥"
            },
            "hash": "e02cb6f62715b753",
            "authors": [
                "Simon A. Aytes",
                "Jinheon Baek",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05179.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#multilingual",
                    "#optimization",
                    "#open_source",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Sketch-of-Thought (SoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. SoT Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Conceptual Chaining, Chunked Symbolism Ğ¸ Expert Lexicons, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SoT ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 76% Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞµĞµ."
                },
                "en": {
                    "title": "Efficient Reasoning with Sketch-of-Thought",
                    "desc": "This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks."
                },
                "zh": {
                    "title": "æ€ç»´è‰å›¾ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºæ¡†æ¶ï¼Œç§°ä¸ºæ€ç»´è‰å›¾ï¼ˆSketch-of-Thoughtï¼ŒSoTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘ä¸­é—´è¾“å‡ºçš„å†—é•¿æ€§ã€‚SoTç»“åˆäº†è®¤çŸ¥ç§‘å­¦çš„æ¨ç†èŒƒå¼å’Œè¯­è¨€çº¦æŸï¼Œä»¥æœ€å°åŒ–ä»¤ç‰Œä½¿ç”¨é‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶çµæ´»ï¼Œå¯ä»¥æ ¹æ®è®¤çŸ¥ç§‘å­¦çš„ä¸åŒæ¨ç†èŒƒå¼è¿›è¡Œå®šåˆ¶ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±æ¨¡å‹åŠ¨æ€é€‰æ‹©ã€‚é€šè¿‡åœ¨15ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°ï¼ŒSoTå®ç°äº†76%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸”å¯¹å‡†ç¡®æ€§å½±å“å¾®ä¹å…¶å¾®ï¼Œç”šè‡³åœ¨æŸäº›é¢†åŸŸæé«˜äº†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05652",
            "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
            "url": "https://huggingface.co/papers/2503.05652",
            "abstract": "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/",
            "score": 1,
            "issue_id": 2608,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 7",
                "zh": "3æœˆ7æ—¥"
            },
            "hash": "52a7efb2f40f020a",
            "authors": [
                "Yunfan Jiang",
                "Ruohan Zhang",
                "Josiah Wong",
                "Chen Wang",
                "Yanjie Ze",
                "Hang Yin",
                "Cem Gokmen",
                "Shuran Song",
                "Jiajun Wu",
                "Li Fei-Fei"
            ],
            "affiliations": [
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05652.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#open_source",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BEHAVIOR Robot Suite (BRS) - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. BRS Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¾Ğ¼ ĞºĞ¾Ğ»ĞµÑĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ñ 4-Ğ¾ÑĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ€ÑĞ¾Ğ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑĞ³Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². BRS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Empowering Robots for Everyday Household Tasks with BRS",
                    "desc": "This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments."
                },
                "zh": {
                    "title": "å®ç°å®¶åº­ä»»åŠ¡çš„å…¨èº«æ“æ§æœºå™¨äºº",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†BEHAVIORæœºå™¨äººå¥—ä»¶ï¼ˆBRSï¼‰ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨æ“ä½œæœºå™¨äººåœ¨å®¶åº­ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸå®Œæˆä»»åŠ¡ä¾èµ–äºä¸‰é¡¹å…³é”®çš„å…¨èº«æ§åˆ¶èƒ½åŠ›ï¼šåŒæ‰‹åè°ƒã€ç¨³å®šç²¾ç¡®çš„å¯¼èˆªå’Œå¹¿æ³›çš„æœ«ç«¯æ‰§è¡Œå™¨å¯è¾¾æ€§ã€‚BRSæ¡†æ¶ç»“åˆäº†ä¸€ä¸ªåŒæ‰‹è½®å¼æœºå™¨äººå’Œ4è‡ªç”±åº¦çš„èº¯å¹²ï¼Œæä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„å…¨èº«é¥æ“ä½œæ¥å£ç”¨äºæ•°æ®æ”¶é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ç”¨äºå­¦ä¹ å…¨èº«è§†è§‰è¿åŠ¨ç­–ç•¥ã€‚é€šè¿‡åœ¨äº”ä¸ªå¤æ‚çš„å®¶åº­ä»»åŠ¡ä¸Šè¯„ä¼°BRSï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿è·ç¦»å¯¼èˆªã€ä¸å¯åŠ¨å’Œå¯å˜å½¢ç‰©ä½“çš„äº¤äº’ä»¥åŠåœ¨ç‹­å°ç©ºé—´ä¸­çš„æ“ä½œèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02130",
            "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
            "url": "https://huggingface.co/papers/2503.02130",
            "abstract": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
            "score": 1,
            "issue_id": 2607,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 3",
                "zh": "3æœˆ3æ—¥"
            },
            "hash": "4c39f334b6c4ed28",
            "authors": [
                "Zhixuan Lin",
                "Evgenii Nikishin",
                "Xu Owen He",
                "Aaron Courville"
            ],
            "affiliations": [
                "MakerMaker AI",
                "Mila & Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02130.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Forgetting Transformer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Forgetting Transformer (FoX), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ' Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. FoX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ FlashAttention Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ FoX ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Transformers with Forgetting Attention for Superior Performance",
                    "desc": "This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness."
                },
                "zh": {
                    "title": "é—å¿˜å˜æ¢å™¨ï¼šæå‡é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºé—å¿˜æ³¨æ„åŠ›ï¼ˆForgetting Attentionï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†é—å¿˜é—¨é›†æˆåˆ°Transformeræ¨¡å‹ä¸­ã€‚é€šè¿‡ä»¥æ•°æ®ä¸ºä¾èµ–çš„æ–¹å¼é™ä½æœªå½’ä¸€åŒ–æ³¨æ„åŠ›åˆ†æ•°ï¼Œé—å¿˜æ³¨æ„åŠ›ä½¿å¾—Transformeråœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡å’Œé•¿åº¦å¤–æ¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„Transformerã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªâ€œProâ€æ¨¡å—ï¼Œç»“åˆäº†é€’å½’åºåˆ—æ¨¡å‹ä¸­çš„ä¸€äº›å¸¸è§æ¶æ„ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFoXåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¿æŒäº†Transformerçš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†å…¶ä»–é€’å½’åºåˆ—æ¨¡å‹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-07.html",
    "link_next": "2025-03-11.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "07.03",
        "en": "03/07",
        "zh": "3æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "11.03",
        "en": "03/11",
        "zh": "3æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå«åš STARTã€‚å®ƒç»“åˆäº†å¤–éƒ¨å·¥å…·æ¥å¢å¼ºå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚START é€šè¿‡æ‰§è¡Œä»£ç ï¼Œè¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œè§£å†³äº†ä¼ ç»Ÿå¤§å‹æ¨ç†æ¨¡å‹çš„å±€é™æ€§ã€‚å…¶åˆ›æ–°ç‚¹åœ¨äºè‡ªå­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ç§æŠ€æœ¯ï¼šHint-infer å’Œ Hint-RFTã€‚è¿™äº›æŠ€æœ¯ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¹¶åœ¨å¤šä¸ªé«˜éš¾åº¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚",
        "title": "START: Self-taught Reasoner with Tools",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå«åš STARTã€‚å®ƒç»“åˆäº†å¤–éƒ¨å·¥å…·æ¥å¢å¼ºå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚START é€šè¿‡æ‰§è¡Œä»£ç ï¼Œè¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œè§£å†³äº†ä¼ ç»Ÿå¤§å‹æ¨ç†æ¨¡å‹çš„å±€é™æ€§ã€‚å…¶åˆ›æ–°ç‚¹åœ¨äºè‡ªå­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ç§æŠ€æœ¯ï¼šHint-infer å’Œ Hint-RFTã€‚è¿™äº›æŠ€æœ¯ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¹¶åœ¨å¤šä¸ªé«˜éš¾åº¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng, jiÃ ozuÃ² START. TÄ jiÃ©hÃ© le wÃ ibÃ¹ gÅngjÃ¹ lÃ¡i zÄ“ngqiÃ¡ng fÃ¹zÃ¡ pÄ«nglÇ rÃ¨nwÃ¹ de nÃ©nglÃ¬. START tÅngguÃ² zhÃ­xÃ­ng dÃ imÇ, jÃ¬nxÃ­ng fÃ¹zÃ¡ jÃ¬suÃ n, zÃ¬wÇ’ jiÇnchÃ¡, tÃ nsuÇ’ duÅzhÇ’ng fÄngfÇ hÃ© zÃ¬wÇ’ tiÃ¡oshÃ¬, jiÄ›juÃ© le chuÃ¡ntÇ’ng dÃ xÃ­ng tuÄ«lÇ mÃ³xÃ­ng de jÃºxÃ¬anxÃ¬ng. QÃ­ chuÃ ngxÄ«n diÇn zÃ i yÃº zÃ¬xuÃ©xÃ­ kuÃ ngjiÃ , bÄokuÃ² liÇng zhÇ’ng jÃ¬shÃ¹: Hint-infer hÃ© Hint-RFT. ZhÃ¨xiÄ“ jÃ¬shÃ¹ shÇ mÃ³xÃ­ng nÃ©nggÃ²u gÄoxiÃ o lÃ¬yÃ²ng wÃ ibÃ¹ gÅngjÃ¹, bÃ¬ng zÃ i duÅgÃ¨ gÄo nÃ¡ndÃ¹ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨shÃ o', 'trans': 'introduce'},\n{'word': 'å¤§å‹', 'pinyin': 'dÃ xÃ­ng', 'trans': 'large-scale'},\n{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'language model'},\n{'word': 'ç»“åˆ', 'pinyin': 'jiÃ©hÃ©', 'trans': 'combine'},\n{'word': 'å¤–éƒ¨', 'pinyin': 'wÃ ibÃ¹', 'trans': 'external'},\n{'word': 'å·¥å…·', 'pinyin': 'gÅngjÃ¹', 'trans': 'tool'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'reasoning'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'},\n{'word': 'æ‰§è¡Œ', 'pinyin': 'zhÃ­xÃ­ng', 'trans': 'execute'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ imÇ', 'trans': 'code'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'calculation'},\n{'word': 'è‡ªæˆ‘æ£€æŸ¥', 'pinyin': 'zÃ¬wÇ’ jiÇnchÃ¡', 'trans': 'self-check'},\n{'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'},\n{'word': 'è‡ªæˆ‘è°ƒè¯•', 'pinyin': 'zÃ¬wÇ’ tiÃ¡oshÃ¬', 'trans': 'self-debug'},\n{'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solve'},\n{'word': 'å±€é™æ€§', 'pinyin': 'jÃºxiÃ nxÃ¬ng', 'trans': 'limitation'},\n{'word': 'åˆ›æ–°ç‚¹', 'pinyin': 'chuÃ ngxÄ«n diÇn', 'trans': 'innovation'},\n{'word': 'è‡ªå­¦ä¹ ', 'pinyin': 'zÃ¬ xuÃ©xÃ­', 'trans': 'self-learning'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬shÃ¹', 'trans': 'technology'},\n{'word': 'é«˜æ•ˆ', 'pinyin': 'gÄoxiÃ o', 'trans': 'efficient'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬yÃ²ng', 'trans': 'utilize'},\n{'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ«zhÇ”n cÃ¨shÃ¬', 'trans': 'benchmark test'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}]",
        "trans": "This article introduces a new large language model called START, which combines external tools to enhance its capabilities in complex reasoning tasks. START addresses the limitations of traditional large reasoning models by executing code, performing complex calculations, self-checking, exploring multiple methods, and self-debugging. Its innovative aspect lies in the self-learning framework, which includes two techniques: Hint-infer and Hint-RFT. These techniques enable the model to efficiently utilize external tools and perform exceptionally well on multiple high-difficulty benchmark tests.",
        "update_ts": "2025-03-09 18:23"
    }
}