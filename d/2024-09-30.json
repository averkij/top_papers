{
    "date": {
        "ru": "30 сентября",
        "en": "September 30",
        "zh": "9月30日"
    },
    "time_utc": "2024-09-30 09:00",
    "weekday": 0,
    "issue_id": 2,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-30",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.18869",
            "title": "Emu3: Next-Token Prediction is All You Need",
            "url": "https://huggingface.co/papers/2409.18869",
            "abstract": "While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.",
            "score": 91,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "924e1dbc713d3bd9",
            "authors": [
                "Xinlong Wang",
                "Xiaosong Zhang",
                "Zhengxiong Luo",
                "Quan Sun",
                "Yufeng Cui",
                "Jinsheng Wang",
                "Fan Zhang",
                "Yueze Wang",
                "Zhen Li",
                "Qiying Yu",
                "Yingli Zhao",
                "Yulong Ao",
                "Xuebin Min",
                "Tao Li",
                "Boya Wu",
                "Bo Zhao",
                "Bowen Zhang",
                "Liangdong Wang",
                "Guang Liu",
                "Zheqi He",
                "Xi Yang",
                "Jingjing Liu",
                "Yonghua Lin",
                "Tiejun Huang",
                "Zhongyuan Wang"
            ],
            "affiliations": [
                "BAAI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18869.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#agi",
                    "#inference",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "Единый подход к мультимодальному ИИ через предсказание токенов",
                    "desc": "Статья представляет Emu3 - набор мультимодальных моделей, обученных исключительно на предсказании следующего токена. Модель токенизирует изображения, текст и видео в дискретное пространство и обучается на смеси мультимодальных последовательностей. Emu3 превосходит специализированные модели в задачах генерации и восприятия, включая SDXL и LLaVA-1.6. Результаты показывают, что предсказание следующего токена - перспективный путь к созданию общего мультимодального искусственного интеллекта."
                },
                "en": {
                    "title": "Unlocking Multimodal Intelligence with Next-Token Prediction",
                    "desc": "This paper presents Emu3, a novel suite of multimodal models that utilize next-token prediction for tasks involving images, text, and videos. By converting these modalities into a discrete token space, Emu3 is trained on a diverse set of multimodal sequences using a single transformer architecture. The results show that Emu3 outperforms existing models like SDXL and LLaVA-1.6 in both generation and perception tasks, demonstrating the effectiveness of next-token prediction in multimodal contexts. This approach simplifies the design of multimodal models and highlights the potential for developing general multimodal intelligence without relying on diffusion or compositional methods."
                },
                "zh": {
                    "title": "Emu3：下一个标记预测的多模态智能新路径",
                    "desc": "本论文介绍了一种新的多模态模型Emu3，该模型仅通过下一个标记预测进行训练。我们将图像、文本和视频标记化为离散空间，并在多模态序列的混合上从零开始训练一个单一的变换器。Emu3在生成和感知任务中超越了多种传统的特定任务模型，展示了下一个标记预测在构建通用多模态智能方面的潜力。我们还开源了关键技术和模型，以支持进一步的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17692",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "url": "https://huggingface.co/papers/2409.17692",
            "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "score": 51,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "07b5003a2a69dd9e",
            "authors": [
                "Zekun Wang",
                "King Zhu",
                "Chunpu Xu",
                "Wangchunshu Zhou",
                "Jiaheng Liu",
                "Yibo Zhang",
                "Jiashuo Wang",
                "Ning Shi",
                "Siyu Li",
                "Yizhi Li",
                "Haoran Que",
                "Zhaoxiang Zhang",
                "Yuanxing Zhang",
                "Ge Zhang",
                "Ke Xu",
                "Jie Fu",
                "Wenhao Huang"
            ],
            "affiliations": [
                "201.AI",
                "AIWaves",
                "Beihang University",
                "Institute of Automation, Chinese Academy of Sciences",
                "M-A-P",
                "Peking University",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "University of Alberta",
                "University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17692.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#audio",
                    "#cv",
                    "#training",
                    "#agi",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "MIO: Универсальная мультимодальная модель для понимания и генерации любых типов данных",
                    "desc": "В статье представлена модель MIO - новая мультимодальная основополагающая модель, способная понимать и генерировать речь, текст, изображения и видео. MIO обучается на дискретных токенах четырех модальностей с использованием каузального мультимодального моделирования. Модель проходит четырехэтапный процесс обучения, включающий предварительное обучение и тонкую настройку на разнообразных задачах. Результаты экспериментов показывают, что MIO демонстрирует конкурентоспособную и в некоторых случаях превосходящую производительность по сравнению с предыдущими базовыми моделями."
                },
                "en": {
                    "title": "MIO: Unifying Speech, Text, Images, and Videos in One Model",
                    "desc": "This paper presents MIO, a new foundation model that can process and create speech, text, images, and videos all at once. Unlike existing models, MIO achieves true any-to-any understanding and generation, allowing it to handle complex tasks across different types of data. The model is trained using a unique four-stage process that enhances its ability to work with multimodal inputs and outputs. Experimental results show that MIO outperforms previous models in various tasks, showcasing its advanced capabilities in generating interleaved sequences and reasoning across modalities."
                },
                "zh": {
                    "title": "MIO：实现任意模态的理解与生成",
                    "desc": "本文介绍了一种新型基础模型MIO，它基于多模态令牌，能够以端到端的自回归方式理解和生成语音、文本、图像和视频。尽管大型语言模型（LLMs）和多模态大型语言模型（MM-LLMs）在人工通用智能方面取得了进展，但它们仍然缺乏真正的任意到任意的理解和生成能力。MIO通过因果多模态建模，使用四种模态的离散令牌混合进行训练，经过四个阶段的训练过程，最终在多样的文本、视觉和语音任务上进行全面的监督微调。实验结果表明，MIO在性能上与之前的双模态基线、任意到任意模型基线，甚至特定模态基线相比，表现出竞争力，甚至在某些情况下表现更优。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18786",
            "title": "A Survey on the Honesty of Large Language Models",
            "url": "https://huggingface.co/papers/2409.18786",
            "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current LLMs still exhibit significant dishonest behaviors, such as confidently presenting wrong answers or failing to express what they know. In addition, research on the honesty of LLMs also faces challenges, including varying definitions of honesty, difficulties in distinguishing between known and unknown knowledge, and a lack of comprehensive understanding of related research. To address these issues, we provide a survey on the honesty of LLMs, covering its clarification, evaluation approaches, and strategies for improvement. Moreover, we offer insights for future research, aiming to inspire further exploration in this important area.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "8803ff973921a3c5",
            "authors": [
                "Siheng Li",
                "Cheng Yang",
                "Taiqiang Wu",
                "Chufan Shi",
                "Yuji Zhang",
                "Xinyu Zhu",
                "Zesen Cheng",
                "Deng Cai",
                "Mo Yu",
                "Lemao Liu",
                "Jie Zhou",
                "Yujiu Yang",
                "Ngai Wong",
                "Xixin Wu",
                "Wai Lam"
            ],
            "affiliations": [
                "Peking University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of Illinois at Urbana-Champaign",
                "University of Virginia",
                "WeChat AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18786.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#hallucinations",
                    "#training",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🤥",
                "ru": {
                    "title": "Честность больших языковых моделей: проблемы, оценка и перспективы",
                    "desc": "Это обзорная статья, посвященная вопросу честности больших языковых моделей (LLM). Авторы рассматривают проблему способности моделей распознавать и правдиво выражать свои знания и незнания. В работе анализируются существующие подходы к оценке честности LLM и стратегии её улучшения. Статья также затрагивает проблемы, связанные с исследованиями в этой области, такие как различные определения честности и сложности в различении известных и неизвестных знаний."
                },
                "en": {
                    "title": "Enhancing Honesty in Language Models for Better Alignment with Human Values",
                    "desc": "This paper discusses the importance of honesty in large language models (LLMs) to ensure they align with human values. It highlights the current shortcomings of LLMs, which often present incorrect information confidently and fail to acknowledge their limitations. The authors identify challenges in defining honesty, recognizing known versus unknown knowledge, and the need for a deeper understanding of existing research. They provide a comprehensive survey on the topic, including evaluation methods and strategies for enhancing the honesty of LLMs, while also suggesting directions for future research."
                },
                "zh": {
                    "title": "提升大型语言模型的诚实性",
                    "desc": "本文探讨了大型语言模型（LLMs）与人类价值观对齐的基本原则——诚实。尽管现有的LLMs在某些方面表现良好，但仍然存在显著的不诚实行为，例如自信地给出错误答案或未能表达其所知。研究LLMs的诚实性面临多重挑战，包括诚实性的定义不一、区分已知与未知知识的困难，以及对相关研究缺乏全面理解。为了解决这些问题，本文提供了关于LLMs诚实性的综述，涵盖了其澄清、评估方法和改进策略，并为未来的研究提供了见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17066",
            "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
            "url": "https://huggingface.co/papers/2409.17066",
            "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.   In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8times increase in inference throughput compared to SOTA.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "03a8eca32256fbfa",
            "authors": [
                "Yifei Liu",
                "Jicheng Wen",
                "Yang Wang",
                "Shengyu Ye",
                "Li Lyna Zhang",
                "Ting Cao",
                "Cheng Li",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17066.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VPTQ: Прорыв в экстремально низкобитной квантизации языковых моделей",
                    "desc": "Статья представляет новый метод под названием Vector Post-Training Quantization (VPTQ) для экстремально низкобитной квантизации больших языковых моделей (LLM). VPTQ использует оптимизацию второго порядка для формулирования проблемы векторной квантизации LLM и руководства разработкой алгоритма квантизации. Метод включает в себя инициализацию кодовой книги и поддерживает остаточную квантизацию и квантизацию выбросов для повышения точности модели. Экспериментальные результаты показывают значительное улучшение перплексии и точности по сравнению с современными методами при 2-битной квантизации на различных моделях LLM."
                },
                "en": {
                    "title": "Efficiently Shrinking Large Language Models with VPTQ",
                    "desc": "This paper presents a new method called Vector Post-Training Quantization (VPTQ) aimed at reducing the size of Large Language Models (LLMs) through extremely low-bit quantization. By utilizing Vector Quantization (VQ) and Second-Order Optimization, the authors improve the efficiency of weight representation, allowing for better compression and faster inference. The method also includes enhancements for handling residuals and outliers, which helps maintain model accuracy while achieving significant size reduction. Experimental results demonstrate that VPTQ outperforms state-of-the-art techniques, achieving lower perplexity and higher accuracy on various QA tasks while increasing inference throughput."
                },
                "zh": {
                    "title": "极低位数量化，提升语言模型性能",
                    "desc": "本文介绍了一种新的向量后训练量化方法（VPTQ），旨在实现极低位数的语言模型量化。通过使用二阶优化，我们设计了量化算法，并通过解决优化问题来指导其实现。VPTQ不仅提高了模型的准确性，还通过支持残差和异常值量化进一步压缩了模型。实验结果表明，VPTQ在多个模型上显著降低了量化困惑度，并提高了推理吞吐量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18839",
            "title": "MinerU: An Open-Source Solution for Precise Document Content Extraction",
            "url": "https://huggingface.co/papers/2409.18839",
            "abstract": "Document content analysis has been a crucial research area in computer vision. Despite significant advancements in methods such as OCR, layout detection, and formula recognition, existing open-source solutions struggle to consistently deliver high-quality content extraction due to the diversity in document types and content. To address these challenges, we present MinerU, an open-source solution for high-precision document content extraction. MinerU leverages the sophisticated PDF-Extract-Kit models to extract content from diverse documents effectively and employs finely-tuned preprocessing and postprocessing rules to ensure the accuracy of the final results. Experimental results demonstrate that MinerU consistently achieves high performance across various document types, significantly enhancing the quality and consistency of content extraction. The MinerU open-source project is available at https://github.com/opendatalab/MinerU.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "9d0b2cc695cb8d9b",
            "authors": [
                "Bin Wang",
                "Chao Xu",
                "Xiaomeng Zhao",
                "Linke Ouyang",
                "Fan Wu",
                "Zhiyuan Zhao",
                "Rui Xu",
                "Kaiwen Liu",
                "Yuan Qu",
                "Fukai Shang",
                "Bo Zhang",
                "Liqun Wei",
                "Zhihao Sui",
                "Wei Li",
                "Botian Shi",
                "Yu Qiao",
                "Dahua Lin",
                "Conghui He"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18839.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#data",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "MinerU: Точное извлечение контента из любых документов",
                    "desc": "MinerU - это открытое решение для высокоточного извлечения содержимого документов. Оно использует сложные модели PDF-Extract-Kit для эффективного извлечения контента из различных типов документов. MinerU применяет тонко настроенные правила предобработки и постобработки для обеспечения точности конечных результатов. Экспериментальные результаты показывают, что MinerU стабильно демонстрирует высокую производительность для различных типов документов, значительно повышая качество и согласованность извлечения контента."
                },
                "en": {
                    "title": "MinerU: Elevating Document Content Extraction to New Heights",
                    "desc": "This paper introduces MinerU, an open-source tool designed to improve the extraction of content from various document types in computer vision. It addresses the limitations of existing solutions by utilizing advanced PDF-Extract-Kit models for effective content extraction. Additionally, MinerU incorporates carefully designed preprocessing and postprocessing techniques to enhance the accuracy of the extracted data. Experimental results show that MinerU outperforms other methods, providing high-quality and consistent content extraction across diverse documents."
                },
                "zh": {
                    "title": "MinerU：高精度文档内容提取的开源解决方案",
                    "desc": "本文介绍了MinerU，一个开源解决方案，旨在高精度提取文档内容。尽管现有的OCR、布局检测和公式识别方法取得了显著进展，但由于文档类型和内容的多样性，现有的开源解决方案在内容提取上仍然存在困难。MinerU利用先进的PDF-Extract-Kit模型，有效地从各种文档中提取内容，并通过精细调整的预处理和后处理规则确保最终结果的准确性。实验结果表明，MinerU在不同文档类型上始终表现出色，显著提高了内容提取的质量和一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18964",
            "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2409.18964",
            "abstract": "We present PhysGen, a novel image-to-video generation method that converts a single image and an input condition (e.g., force and torque applied to an object in the image) to produce a realistic, physically plausible, and temporally consistent video. Our key insight is to integrate model-based physical simulation with a data-driven video generation process, enabling plausible image-space dynamics. At the heart of our system are three core components: (i) an image understanding module that effectively captures the geometry, materials, and physical parameters of the image; (ii) an image-space dynamics simulation model that utilizes rigid-body physics and inferred parameters to simulate realistic behaviors; and (iii) an image-based rendering and refinement module that leverages generative video diffusion to produce realistic video footage featuring the simulated motion. The resulting videos are realistic in both physics and appearance and are even precisely controllable, showcasing superior results over existing data-driven image-to-video generation works through quantitative comparison and comprehensive user study. PhysGen's resulting videos can be used for various downstream applications, such as turning an image into a realistic animation or allowing users to interact with the image and create various dynamics. Project page: https://stevenlsw.github.io/physgen/",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "db9593061ca856f4",
            "authors": [
                "Shaowei Liu",
                "Zhongzheng Ren",
                "Saurabh Gupta",
                "Shenlong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18964.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Физически достоверная генерация видео из одного изображения",
                    "desc": "PhysGen - это новый метод генерации видео из изображения, интегрирующий физическое моделирование с процессом генерации видео на основе данных. Система состоит из трех ключевых компонентов: модуля понимания изображения, модели симуляции динамики в пространстве изображения и модуля рендеринга и уточнения изображения. PhysGen позволяет создавать реалистичные видео с физически достоверным движением объектов, превосходя существующие методы генерации видео из изображений. Результаты могут применяться для создания анимаций из статичных изображений и интерактивного взаимодействия с ними."
                },
                "en": {
                    "title": "Transforming Images into Realistic Videos with PhysGen",
                    "desc": "PhysGen is a new method for generating videos from a single image by applying specific conditions like force and torque. It combines physical simulations with data-driven techniques to create videos that look realistic and behave according to physical laws. The system has three main parts: understanding the image's details, simulating realistic movements using physics, and refining the video output with advanced rendering techniques. This approach allows for precise control over the generated videos, making them useful for applications like animations and interactive experiences."
                },
                "zh": {
                    "title": "PhysGen：将图像转化为真实视频的创新方法",
                    "desc": "PhysGen是一种新颖的图像到视频生成方法，它可以将单张图像和输入条件（例如施加在图像对象上的力和扭矩）转换为逼真且物理上合理的动态视频。该方法的核心在于将基于模型的物理模拟与数据驱动的视频生成过程相结合，从而实现可信的图像空间动态。系统的三个核心组件包括：图像理解模块、图像空间动态模拟模型和图像基础的渲染与优化模块。PhysGen生成的视频在物理和外观上都非常真实，并且可以精确控制，展示了优于现有数据驱动图像到视频生成方法的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17545",
            "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
            "url": "https://huggingface.co/papers/2409.17545",
            "abstract": "Preference optimization methods typically begin training with a well-trained SFT model as a reference model. In RLHF and DPO, a regularization term is used during the preference optimization process to prevent the policy model from deviating too far from the reference model's distribution, thereby avoiding the generation of anomalous responses. When the reference model is already well-aligned with the given data or only requires slight adjustments, this approach can produce a well-aligned model. However, if the reference model is not aligned with the given data and requires significant deviation from its current state, a regularization term may actually hinder the model alignment. In this study, we propose Modulated Intervention Preference Optimization (MIPO) to address this issue. MIPO modulates the degree of intervention from the reference model based on how well the given data is aligned with it. If the data is well-aligned, the intervention is increased to prevent the policy model from diverging significantly from reference model. Conversely, if the alignment is poor, the interference is reduced to facilitate more extensive training. We compare the performance of MIPO and DPO using Mistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental results demonstrate that MIPO consistently outperforms DPO across various evaluation scenarios.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "935b6b85f16f7519",
            "authors": [
                "Cheolhun Jang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17545.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "MIPO: Умное вмешательство для лучшей оптимизации языковых моделей",
                    "desc": "В статье представлен новый метод оптимизации предпочтений - Modulated Intervention Preference Optimization (MIPO). MIPO модулирует степень вмешательства эталонной модели в зависимости от того, насколько хорошо данные согласуются с ней. Это позволяет избежать ограничений, связанных с регуляризацией в традиционных методах, таких как RLHF и DPO. Эксперименты с моделями Mistral-7B и Llama3-8B показали, что MIPO превосходит DPO в различных сценариях оценки."
                },
                "en": {
                    "title": "MIPO: Smartly Balancing Model Alignment and Flexibility",
                    "desc": "This paper discusses a new method called Modulated Intervention Preference Optimization (MIPO) for improving machine learning models. Traditional methods like RLHF and DPO use a regularization term to keep the model close to a well-trained reference model. However, if the reference model is not well-aligned with the data, this can limit the model's ability to learn effectively. MIPO adjusts the level of intervention based on the alignment of the data, allowing for better training when the reference model is misaligned and maintaining stability when it is well-aligned."
                },
                "zh": {
                    "title": "调制干预，优化偏好模型",
                    "desc": "本研究提出了一种新的偏好优化方法，称为调制干预偏好优化（MIPO）。MIPO根据参考模型与给定数据的对齐程度来调节干预的强度，以优化模型的训练过程。当数据与参考模型对齐良好时，增加干预以防止策略模型的显著偏离；而当对齐较差时，减少干预以促进更广泛的训练。实验结果表明，MIPO在多个评估场景中始终优于传统的DPO方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18957",
            "title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction",
            "url": "https://huggingface.co/papers/2409.18957",
            "abstract": "This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called \"Language Model Learning (LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words \"Act as an Explainable Machine Learning Model\" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "bffc41d0d62057ea",
            "authors": [
                "Praneeth Vadlapati"
            ],
            "affiliations": [
                "University of Arizona, Tucson, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18957.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#interpretability",
                    "#data",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Языковые модели как интерпретируемые классификаторы данных",
                    "desc": "Статья представляет новый подход к использованию больших языковых моделей (LLM) для задач классификации, которые обычно решаются с помощью моделей машинного обучения. Предложен метод 'Обучение языковой модели' (LML), основанный на 'Предсказании с дополнением данными' (DAP). В процессе DAP система использует сводку данных для автоматического создания запроса и извлечения релевантных строк из набора данных. LLM генерирует классификацию, используя сводку данных и релевантные строки, обеспечивая высокую точность даже для сложных данных."
                },
                "en": {
                    "title": "Revolutionizing Classification with Language Model Learning",
                    "desc": "This paper presents a novel approach called Language Model Learning (LML) that utilizes Large Language Models (LLMs) for classification tasks, reducing the need for extensive data cleaning and feature engineering typical in traditional Machine Learning (ML) models. The method introduces Data-Augmented Prediction (DAP), where LLMs classify data by mimicking human-like exploration and understanding of the dataset. By summarizing training data and generating queries to retrieve relevant information, the LLM can make context-aware decisions that enhance classification accuracy. The approach also emphasizes interpretability by allowing users to understand the reasoning behind predictions, achieving over 90% accuracy in some tests, showcasing its potential to surpass conventional ML methods."
                },
                "zh": {
                    "title": "利用大型语言模型提升分类任务的效率",
                    "desc": "本文介绍了一种新的方法，利用大型语言模型（LLMs）进行分类任务，这通常由机器学习（ML）模型处理。与依赖数据清理和特征工程的传统ML模型不同，这种方法通过LLMs简化了流程。提出的“语言模型学习（LML）”概念结合了“数据增强预测（DAP）”方法，使得LLMs能够像人类一样探索和理解数据，从而进行分类。通过数据摘要和相关数据的使用，确保了在复杂数据下的准确性，并提高了预测的可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17433",
            "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows",
            "url": "https://huggingface.co/papers/2409.17433",
            "abstract": "Despite recent advancements in large language models (LLMs), their performance on complex reasoning problems requiring multi-step thinking and combining various skills is still limited. To address this, we propose a novel framework HDFlow for complex reasoning with LLMs that combines fast and slow thinking modes in an adaptive manner. Our approach consists of two key components: 1) a new approach for slow, deliberate reasoning called Dynamic Workflow, which automatically decomposes complex problems into more manageable sub-tasks and dynamically designs a workflow to assemble specialized LLM or symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general framework that dynamically combines fast and slow thinking based on problem complexity. Finally, we propose an easy-to-scale method for automatically synthesizing a large-scale dataset of 27K challenging reasoning problems for complex reasoning and a hybrid thinking tuning method that trains smaller LLMs on this dataset to internalize the fast/slow hybrid reasoning strategies. Experiments on four reasoning benchmark datasets demonstrate that our slow thinking with dynamic workflows significantly outperforms Chain-of-Thought, and hybrid thinking achieves the highest accuracy while providing an effective balance between computational efficiency and performance. Fine-tuning using our hybrid thinking approach also significantly boosts the complex reasoning capabilities of open-source language models. The results showcase the promise of slow thinking, dynamic workflows, and hybrid thinking in expanding the frontier of complex problem-solving with LLMsCode and data will be released at \\url{https://github.com/wenlinyao/HDFlow.}.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "750db1b173f71245",
            "authors": [
                "Wenlin Yao",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab Bellevue, WA 98004, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17433.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#rag",
                    "#rl",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "HDFlow: Гибридное мышление для решения сложных задач языковыми моделями",
                    "desc": "Статья представляет новый подход HDFlow для улучшения способностей крупных языковых моделей (LLM) решать сложные задачи, требующие многоступенчатого мышления. Метод сочетает режимы быстрого и медленного мышления, используя динамический рабочий процесс для декомпозиции задач и гибридное мышление для адаптивного переключения между режимами. Авторы также разработали масштабируемый метод для автоматического создания крупного набора данных из 27 тысяч сложных задач на рассуждение. Эксперименты показали, что предложенный подход значительно превосходит существующие методы на нескольких эталонных наборах данных."
                },
                "en": {
                    "title": "Enhancing Complex Reasoning in LLMs with HDFlow",
                    "desc": "This paper introduces HDFlow, a new framework designed to enhance the reasoning capabilities of large language models (LLMs) for complex problems. It features two main components: Dynamic Workflow, which breaks down complex tasks into simpler sub-tasks and organizes the use of specialized reasoning tools, and Hybrid Thinking, which adapts the reasoning approach based on the complexity of the problem. The authors also present a method for creating a large dataset of challenging reasoning problems and a tuning technique to improve LLMs' performance on these tasks. Experimental results show that HDFlow significantly outperforms existing methods and improves the reasoning abilities of smaller LLMs."
                },
                "zh": {
                    "title": "HDFlow：提升复杂推理能力的新框架",
                    "desc": "尽管大型语言模型（LLMs）在许多任务上取得了进展，但在复杂推理问题上仍然存在局限。为了解决这个问题，我们提出了一种新框架HDFlow，结合了快速和慢速思维模式。该框架包括动态工作流和混合思维两大核心组件，能够自动将复杂问题分解为可管理的子任务，并根据问题复杂性动态调整思维方式。实验结果表明，我们的方法在复杂推理能力上显著优于传统方法，并有效提升了开源语言模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16686",
            "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
            "url": "https://huggingface.co/papers/2409.16686",
            "abstract": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "a06420da3aa04bb8",
            "authors": [
                "Dayuan Fu",
                "Biqing Qi",
                "Yihuai Gao",
                "Che Jiang",
                "Guanting Dong",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "Department of Electronic Engineering, Tsinghua University",
                "Shanghai AI Laboratory",
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.16686.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#agents",
                    "#architecture",
                    "#robotics"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Многоуровневые инсайты для улучшения планирования языковых моделей",
                    "desc": "Статья представляет Multi-Scale Insight Agent (MSI-Agent) - воплощенного агента, улучшающего способности языковых моделей к планированию и принятию решений. MSI-Agent использует трехчастный конвейер для генерации, хранения и применения инсайтов на разных уровнях. Эксперименты показывают, что MSI превосходит другие стратегии инсайтов при планировании с помощью GPT-3.5. Исследование также рассматривает стратегии отбора исходного опыта и инсайтов для более эффективного принятия решений."
                },
                "en": {
                    "title": "Enhancing Decision-Making with Multi-Scale Insights",
                    "desc": "This paper presents the Multi-Scale Insight Agent (MSI-Agent), which enhances the planning and decision-making capabilities of large language models (LLMs) by effectively managing insights. The MSI-Agent employs a three-part pipeline consisting of an experience selector, an insight generator, and an insight selector to generate and utilize task-specific insights. By summarizing insights across different scales and storing them in a database, MSI can retrieve relevant information to support decision-making processes. Experimental results demonstrate that MSI outperforms existing insight strategies, particularly in adapting to domain shifts, thereby improving the robustness of LLMs."
                },
                "zh": {
                    "title": "多尺度洞察智能体：提升决策能力的关键",
                    "desc": "长期记忆对智能体非常重要，其中洞察力起着关键作用。然而，无关的洞察力和缺乏通用洞察力会严重影响洞察力的有效性。为了解决这个问题，本文提出了多尺度洞察智能体（MSI-Agent），旨在通过有效总结和利用不同尺度的洞察力来提高大型语言模型（LLM）的规划和决策能力。MSI通过经验选择器、洞察生成器和洞察选择器的三部分管道，生成特定任务和高层次的洞察力，并将其存储在数据库中，以便在决策时使用相关的洞察力。"
                }
            }
        }
    ],
    "link_prev": "2024-09-27.html",
    "link_next": "2024-10-01.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "27.09",
        "en": "09/27",
        "zh": "9月27日"
    },
    "short_date_next": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10月1日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}