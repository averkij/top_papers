{
    "date": {
        "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 30",
        "zh": "9æœˆ30æ—¥"
    },
    "time_utc": "2024-09-30 09:00",
    "weekday": 0,
    "issue_id": 2,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-30",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.18869",
            "title": "Emu3: Next-Token Prediction is All You Need",
            "url": "https://huggingface.co/papers/2409.18869",
            "abstract": "While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a new suite of state-of-the-art multimodal models trained solely with next-token prediction. By tokenizing images, text, and videos into a discrete space, we train a single transformer from scratch on a mixture of multimodal sequences. Emu3 outperforms several well-established task-specific models in both generation and perception tasks, surpassing flagship models such as SDXL and LLaVA-1.6, while eliminating the need for diffusion or compositional architectures. Emu3 is also capable of generating high-fidelity video via predicting the next token in a video sequence. We simplify complex multimodal model designs by converging on a singular focus: tokens, unlocking great potential for scaling both during training and inference. Our results demonstrate that next-token prediction is a promising path towards building general multimodal intelligence beyond language. We open-source key techniques and models to support further research in this direction.",
            "score": 91,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "924e1dbc713d3bd9",
            "authors": [
                "Xinlong Wang",
                "Xiaosong Zhang",
                "Zhengxiong Luo",
                "Quan Sun",
                "Yufeng Cui",
                "Jinsheng Wang",
                "Fan Zhang",
                "Yueze Wang",
                "Zhen Li",
                "Qiying Yu",
                "Yingli Zhao",
                "Yulong Ao",
                "Xuebin Min",
                "Tao Li",
                "Boya Wu",
                "Bo Zhao",
                "Bowen Zhang",
                "Liangdong Wang",
                "Guang Liu",
                "Zheqi He",
                "Xi Yang",
                "Jingjing Liu",
                "Yonghua Lin",
                "Tiejun Huang",
                "Zhongyuan Wang"
            ],
            "affiliations": [
                "BAAI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18869.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#agi",
                    "#inference",
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Emu3 - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Emu3 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SDXL Ğ¸ LLaVA-1.6. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° - Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Multimodal Intelligence with Next-Token Prediction",
                    "desc": "This paper presents Emu3, a novel suite of multimodal models that utilize next-token prediction for tasks involving images, text, and videos. By converting these modalities into a discrete token space, Emu3 is trained on a diverse set of multimodal sequences using a single transformer architecture. The results show that Emu3 outperforms existing models like SDXL and LLaVA-1.6 in both generation and perception tasks, demonstrating the effectiveness of next-token prediction in multimodal contexts. This approach simplifies the design of multimodal models and highlights the potential for developing general multimodal intelligence without relying on diffusion or compositional methods."
                },
                "zh": {
                    "title": "Emu3ï¼šä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å¤šæ¨¡æ€æ™ºèƒ½æ–°è·¯å¾„",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨¡å‹Emu3ï¼Œè¯¥æ¨¡å‹ä»…é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†å›¾åƒã€æ–‡æœ¬å’Œè§†é¢‘æ ‡è®°åŒ–ä¸ºç¦»æ•£ç©ºé—´ï¼Œå¹¶åœ¨å¤šæ¨¡æ€åºåˆ—çš„æ··åˆä¸Šä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªå•ä¸€çš„å˜æ¢å™¨ã€‚Emu3åœ¨ç”Ÿæˆå’Œæ„ŸçŸ¥ä»»åŠ¡ä¸­è¶…è¶Šäº†å¤šç§ä¼ ç»Ÿçš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Œå±•ç¤ºäº†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹åœ¨æ„å»ºé€šç”¨å¤šæ¨¡æ€æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†å…³é”®æŠ€æœ¯å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17692",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "url": "https://huggingface.co/papers/2409.17692",
            "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "score": 51,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "07b5003a2a69dd9e",
            "authors": [
                "Zekun Wang",
                "King Zhu",
                "Chunpu Xu",
                "Wangchunshu Zhou",
                "Jiaheng Liu",
                "Yibo Zhang",
                "Jiashuo Wang",
                "Ning Shi",
                "Siyu Li",
                "Yizhi Li",
                "Haoran Que",
                "Zhaoxiang Zhang",
                "Yuanxing Zhang",
                "Ge Zhang",
                "Ke Xu",
                "Jie Fu",
                "Wenhao Huang"
            ],
            "affiliations": [
                "201.AI",
                "AIWaves",
                "Beihang University",
                "Institute of Automation, Chinese Academy of Sciences",
                "M-A-P",
                "Peking University",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "University of Alberta",
                "University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17692.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#audio",
                    "#cv",
                    "#training",
                    "#agi",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MIO: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MIO - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ, Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. MIO Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MIO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¸ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "MIO: Unifying Speech, Text, Images, and Videos in One Model",
                    "desc": "This paper presents MIO, a new foundation model that can process and create speech, text, images, and videos all at once. Unlike existing models, MIO achieves true any-to-any understanding and generation, allowing it to handle complex tasks across different types of data. The model is trained using a unique four-stage process that enhances its ability to work with multimodal inputs and outputs. Experimental results show that MIO outperforms previous models in various tasks, showcasing its advanced capabilities in generating interleaved sequences and reasoning across modalities."
                },
                "zh": {
                    "title": "MIOï¼šå®ç°ä»»æ„æ¨¡æ€çš„ç†è§£ä¸ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹åŸºç¡€æ¨¡å‹MIOï¼Œå®ƒåŸºäºå¤šæ¨¡æ€ä»¤ç‰Œï¼Œèƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„è‡ªå›å½’æ–¹å¼ç†è§£å’Œç”Ÿæˆè¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMsï¼‰åœ¨äººå·¥é€šç”¨æ™ºèƒ½æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹çœŸæ­£çš„ä»»æ„åˆ°ä»»æ„çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚MIOé€šè¿‡å› æœå¤šæ¨¡æ€å»ºæ¨¡ï¼Œä½¿ç”¨å››ç§æ¨¡æ€çš„ç¦»æ•£ä»¤ç‰Œæ··åˆè¿›è¡Œè®­ç»ƒï¼Œç»è¿‡å››ä¸ªé˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼Œæœ€ç»ˆåœ¨å¤šæ ·çš„æ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸Šè¿›è¡Œå…¨é¢çš„ç›‘ç£å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIOåœ¨æ€§èƒ½ä¸Šä¸ä¹‹å‰çš„åŒæ¨¡æ€åŸºçº¿ã€ä»»æ„åˆ°ä»»æ„æ¨¡å‹åŸºçº¿ï¼Œç”šè‡³ç‰¹å®šæ¨¡æ€åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18786",
            "title": "A Survey on the Honesty of Large Language Models",
            "url": "https://huggingface.co/papers/2409.18786",
            "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current LLMs still exhibit significant dishonest behaviors, such as confidently presenting wrong answers or failing to express what they know. In addition, research on the honesty of LLMs also faces challenges, including varying definitions of honesty, difficulties in distinguishing between known and unknown knowledge, and a lack of comprehensive understanding of related research. To address these issues, we provide a survey on the honesty of LLMs, covering its clarification, evaluation approaches, and strategies for improvement. Moreover, we offer insights for future research, aiming to inspire further exploration in this important area.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "8803ff973921a3c5",
            "authors": [
                "Siheng Li",
                "Cheng Yang",
                "Taiqiang Wu",
                "Chufan Shi",
                "Yuji Zhang",
                "Xinyu Zhu",
                "Zesen Cheng",
                "Deng Cai",
                "Mo Yu",
                "Lemao Liu",
                "Jie Zhou",
                "Yujiu Yang",
                "Ngai Wong",
                "Xixin Wu",
                "Wai Lam"
            ],
            "affiliations": [
                "Peking University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "Tsinghua University",
                "University of Illinois at Urbana-Champaign",
                "University of Virginia",
                "WeChat AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18786.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#hallucinations",
                    "#training",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "ğŸ¤¥",
                "ru": {
                    "title": "Ğ§ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ, Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑƒ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞµÑ‘ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Honesty in Language Models for Better Alignment with Human Values",
                    "desc": "This paper discusses the importance of honesty in large language models (LLMs) to ensure they align with human values. It highlights the current shortcomings of LLMs, which often present incorrect information confidently and fail to acknowledge their limitations. The authors identify challenges in defining honesty, recognizing known versus unknown knowledge, and the need for a deeper understanding of existing research. They provide a comprehensive survey on the topic, including evaluation methods and strategies for enhancing the honesty of LLMs, while also suggesting directions for future research."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯šå®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„åŸºæœ¬åŸåˆ™â€”â€”è¯šå®ã€‚å°½ç®¡ç°æœ‰çš„LLMsåœ¨æŸäº›æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„ä¸è¯šå®è¡Œä¸ºï¼Œä¾‹å¦‚è‡ªä¿¡åœ°ç»™å‡ºé”™è¯¯ç­”æ¡ˆæˆ–æœªèƒ½è¡¨è¾¾å…¶æ‰€çŸ¥ã€‚ç ”ç©¶LLMsçš„è¯šå®æ€§é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯šå®æ€§çš„å®šä¹‰ä¸ä¸€ã€åŒºåˆ†å·²çŸ¥ä¸æœªçŸ¥çŸ¥è¯†çš„å›°éš¾ï¼Œä»¥åŠå¯¹ç›¸å…³ç ”ç©¶ç¼ºä¹å…¨é¢ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æä¾›äº†å…³äºLLMsè¯šå®æ€§çš„ç»¼è¿°ï¼Œæ¶µç›–äº†å…¶æ¾„æ¸…ã€è¯„ä¼°æ–¹æ³•å’Œæ”¹è¿›ç­–ç•¥ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17066",
            "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models",
            "url": "https://huggingface.co/papers/2409.17066",
            "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.   In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8times increase in inference throughput compared to SOTA.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "03a8eca32256fbfa",
            "authors": [
                "Yifei Liu",
                "Jicheng Wen",
                "Yang Wang",
                "Shengyu Ye",
                "Li Lyna Zhang",
                "Ting Cao",
                "Cheng Li",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17066.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VPTQ: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vector Post-Training Quantization (VPTQ) Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). VPTQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLM."
                },
                "en": {
                    "title": "Efficiently Shrinking Large Language Models with VPTQ",
                    "desc": "This paper presents a new method called Vector Post-Training Quantization (VPTQ) aimed at reducing the size of Large Language Models (LLMs) through extremely low-bit quantization. By utilizing Vector Quantization (VQ) and Second-Order Optimization, the authors improve the efficiency of weight representation, allowing for better compression and faster inference. The method also includes enhancements for handling residuals and outliers, which helps maintain model accuracy while achieving significant size reduction. Experimental results demonstrate that VPTQ outperforms state-of-the-art techniques, achieving lower perplexity and higher accuracy on various QA tasks while increasing inference throughput."
                },
                "zh": {
                    "title": "æä½ä½æ•°é‡åŒ–ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å‘é‡åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼ˆVPTQï¼‰ï¼Œæ—¨åœ¨å®ç°æä½ä½æ•°çš„è¯­è¨€æ¨¡å‹é‡åŒ–ã€‚é€šè¿‡ä½¿ç”¨äºŒé˜¶ä¼˜åŒ–ï¼Œæˆ‘ä»¬è®¾è®¡äº†é‡åŒ–ç®—æ³•ï¼Œå¹¶é€šè¿‡è§£å†³ä¼˜åŒ–é—®é¢˜æ¥æŒ‡å¯¼å…¶å®ç°ã€‚VPTQä¸ä»…æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè¿˜é€šè¿‡æ”¯æŒæ®‹å·®å’Œå¼‚å¸¸å€¼é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©äº†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVPTQåœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—é™ä½äº†é‡åŒ–å›°æƒ‘åº¦ï¼Œå¹¶æé«˜äº†æ¨ç†ååé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18839",
            "title": "MinerU: An Open-Source Solution for Precise Document Content Extraction",
            "url": "https://huggingface.co/papers/2409.18839",
            "abstract": "Document content analysis has been a crucial research area in computer vision. Despite significant advancements in methods such as OCR, layout detection, and formula recognition, existing open-source solutions struggle to consistently deliver high-quality content extraction due to the diversity in document types and content. To address these challenges, we present MinerU, an open-source solution for high-precision document content extraction. MinerU leverages the sophisticated PDF-Extract-Kit models to extract content from diverse documents effectively and employs finely-tuned preprocessing and postprocessing rules to ensure the accuracy of the final results. Experimental results demonstrate that MinerU consistently achieves high performance across various document types, significantly enhancing the quality and consistency of content extraction. The MinerU open-source project is available at https://github.com/opendatalab/MinerU.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "9d0b2cc695cb8d9b",
            "authors": [
                "Bin Wang",
                "Chao Xu",
                "Xiaomeng Zhao",
                "Linke Ouyang",
                "Fan Wu",
                "Zhiyuan Zhao",
                "Rui Xu",
                "Kaiwen Liu",
                "Yuan Qu",
                "Fukai Shang",
                "Bo Zhang",
                "Liqun Wei",
                "Zhihao Sui",
                "Wei Li",
                "Botian Shi",
                "Yu Qiao",
                "Dahua Lin",
                "Conghui He"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18839.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#data",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "MinerU: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ»ÑĞ±Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "MinerU - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PDF-Extract-Kit Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². MinerU Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MinerU ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "MinerU: Elevating Document Content Extraction to New Heights",
                    "desc": "This paper introduces MinerU, an open-source tool designed to improve the extraction of content from various document types in computer vision. It addresses the limitations of existing solutions by utilizing advanced PDF-Extract-Kit models for effective content extraction. Additionally, MinerU incorporates carefully designed preprocessing and postprocessing techniques to enhance the accuracy of the extracted data. Experimental results show that MinerU outperforms other methods, providing high-quality and consistent content extraction across diverse documents."
                },
                "zh": {
                    "title": "MinerUï¼šé«˜ç²¾åº¦æ–‡æ¡£å†…å®¹æå–çš„å¼€æºè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MinerUï¼Œä¸€ä¸ªå¼€æºè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é«˜ç²¾åº¦æå–æ–‡æ¡£å†…å®¹ã€‚å°½ç®¡ç°æœ‰çš„OCRã€å¸ƒå±€æ£€æµ‹å’Œå…¬å¼è¯†åˆ«æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºæ–‡æ¡£ç±»å‹å’Œå†…å®¹çš„å¤šæ ·æ€§ï¼Œç°æœ‰çš„å¼€æºè§£å†³æ–¹æ¡ˆåœ¨å†…å®¹æå–ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚MinerUåˆ©ç”¨å…ˆè¿›çš„PDF-Extract-Kitæ¨¡å‹ï¼Œæœ‰æ•ˆåœ°ä»å„ç§æ–‡æ¡£ä¸­æå–å†…å®¹ï¼Œå¹¶é€šè¿‡ç²¾ç»†è°ƒæ•´çš„é¢„å¤„ç†å’Œåå¤„ç†è§„åˆ™ç¡®ä¿æœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinerUåœ¨ä¸åŒæ–‡æ¡£ç±»å‹ä¸Šå§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†å†…å®¹æå–çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18964",
            "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
            "url": "https://huggingface.co/papers/2409.18964",
            "abstract": "We present PhysGen, a novel image-to-video generation method that converts a single image and an input condition (e.g., force and torque applied to an object in the image) to produce a realistic, physically plausible, and temporally consistent video. Our key insight is to integrate model-based physical simulation with a data-driven video generation process, enabling plausible image-space dynamics. At the heart of our system are three core components: (i) an image understanding module that effectively captures the geometry, materials, and physical parameters of the image; (ii) an image-space dynamics simulation model that utilizes rigid-body physics and inferred parameters to simulate realistic behaviors; and (iii) an image-based rendering and refinement module that leverages generative video diffusion to produce realistic video footage featuring the simulated motion. The resulting videos are realistic in both physics and appearance and are even precisely controllable, showcasing superior results over existing data-driven image-to-video generation works through quantitative comparison and comprehensive user study. PhysGen's resulting videos can be used for various downstream applications, such as turning an image into a realistic animation or allowing users to interact with the image and create various dynamics. Project page: https://stevenlsw.github.io/physgen/",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "db9593061ca856f4",
            "authors": [
                "Shaowei Liu",
                "Zhongzheng Ren",
                "Saurabh Gupta",
                "Shenlong Wang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18964.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#multimodal",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "PhysGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. PhysGen Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ½Ğ¸Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Images into Realistic Videos with PhysGen",
                    "desc": "PhysGen is a new method for generating videos from a single image by applying specific conditions like force and torque. It combines physical simulations with data-driven techniques to create videos that look realistic and behave according to physical laws. The system has three main parts: understanding the image's details, simulating realistic movements using physics, and refining the video output with advanced rendering techniques. This approach allows for precise control over the generated videos, making them useful for applications like animations and interactive experiences."
                },
                "zh": {
                    "title": "PhysGenï¼šå°†å›¾åƒè½¬åŒ–ä¸ºçœŸå®è§†é¢‘çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "PhysGenæ˜¯ä¸€ç§æ–°é¢–çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒå¯ä»¥å°†å•å¼ å›¾åƒå’Œè¾“å…¥æ¡ä»¶ï¼ˆä¾‹å¦‚æ–½åŠ åœ¨å›¾åƒå¯¹è±¡ä¸Šçš„åŠ›å’Œæ‰­çŸ©ï¼‰è½¬æ¢ä¸ºé€¼çœŸä¸”ç‰©ç†ä¸Šåˆç†çš„åŠ¨æ€è§†é¢‘ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå°†åŸºäºæ¨¡å‹çš„ç‰©ç†æ¨¡æ‹Ÿä¸æ•°æ®é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ç›¸ç»“åˆï¼Œä»è€Œå®ç°å¯ä¿¡çš„å›¾åƒç©ºé—´åŠ¨æ€ã€‚ç³»ç»Ÿçš„ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼šå›¾åƒç†è§£æ¨¡å—ã€å›¾åƒç©ºé—´åŠ¨æ€æ¨¡æ‹Ÿæ¨¡å‹å’Œå›¾åƒåŸºç¡€çš„æ¸²æŸ“ä¸ä¼˜åŒ–æ¨¡å—ã€‚PhysGenç”Ÿæˆçš„è§†é¢‘åœ¨ç‰©ç†å’Œå¤–è§‚ä¸Šéƒ½éå¸¸çœŸå®ï¼Œå¹¶ä¸”å¯ä»¥ç²¾ç¡®æ§åˆ¶ï¼Œå±•ç¤ºäº†ä¼˜äºç°æœ‰æ•°æ®é©±åŠ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17545",
            "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
            "url": "https://huggingface.co/papers/2409.17545",
            "abstract": "Preference optimization methods typically begin training with a well-trained SFT model as a reference model. In RLHF and DPO, a regularization term is used during the preference optimization process to prevent the policy model from deviating too far from the reference model's distribution, thereby avoiding the generation of anomalous responses. When the reference model is already well-aligned with the given data or only requires slight adjustments, this approach can produce a well-aligned model. However, if the reference model is not aligned with the given data and requires significant deviation from its current state, a regularization term may actually hinder the model alignment. In this study, we propose Modulated Intervention Preference Optimization (MIPO) to address this issue. MIPO modulates the degree of intervention from the reference model based on how well the given data is aligned with it. If the data is well-aligned, the intervention is increased to prevent the policy model from diverging significantly from reference model. Conversely, if the alignment is poor, the interference is reduced to facilitate more extensive training. We compare the performance of MIPO and DPO using Mistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental results demonstrate that MIPO consistently outperforms DPO across various evaluation scenarios.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "935b6b85f16f7519",
            "authors": [
                "Cheolhun Jang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17545.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "MIPO: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ - Modulated Intervention Preference Optimization (MIPO). MIPO Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ğ½ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº RLHF Ğ¸ DPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Mistral-7B Ğ¸ Llama3-8B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MIPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DPO Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "MIPO: Smartly Balancing Model Alignment and Flexibility",
                    "desc": "This paper discusses a new method called Modulated Intervention Preference Optimization (MIPO) for improving machine learning models. Traditional methods like RLHF and DPO use a regularization term to keep the model close to a well-trained reference model. However, if the reference model is not well-aligned with the data, this can limit the model's ability to learn effectively. MIPO adjusts the level of intervention based on the alignment of the data, allowing for better training when the reference model is misaligned and maintaining stability when it is well-aligned."
                },
                "zh": {
                    "title": "è°ƒåˆ¶å¹²é¢„ï¼Œä¼˜åŒ–åå¥½æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºè°ƒåˆ¶å¹²é¢„åå¥½ä¼˜åŒ–ï¼ˆMIPOï¼‰ã€‚MIPOæ ¹æ®å‚è€ƒæ¨¡å‹ä¸ç»™å®šæ•°æ®çš„å¯¹é½ç¨‹åº¦æ¥è°ƒèŠ‚å¹²é¢„çš„å¼ºåº¦ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚å½“æ•°æ®ä¸å‚è€ƒæ¨¡å‹å¯¹é½è‰¯å¥½æ—¶ï¼Œå¢åŠ å¹²é¢„ä»¥é˜²æ­¢ç­–ç•¥æ¨¡å‹çš„æ˜¾è‘—åç¦»ï¼›è€Œå½“å¯¹é½è¾ƒå·®æ—¶ï¼Œå‡å°‘å¹²é¢„ä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIPOåœ¨å¤šä¸ªè¯„ä¼°åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„DPOæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.18957",
            "title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction",
            "url": "https://huggingface.co/papers/2409.18957",
            "abstract": "This paper introduces a new approach to using Large Language Models (LLMs) for classification tasks, which are typically handled using Machine Learning (ML) models. Unlike ML models that rely heavily on data cleaning and feature engineering, this method streamlines the process using LLMs. This paper proposes a new concept called \"Language Model Learning (LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The classification is performed by LLMs using a method similar to humans manually exploring and understanding the data and deciding classifications using data as a reference. Training data is summarized and evaluated to determine the features that lead to the classification of each label the most. In the process of DAP, the system uses the data summary to automatically create a query, which is used to retrieve relevant rows from the dataset. A classification is generated by the LLM using data summary and relevant rows, ensuring satisfactory accuracy even with complex data. Usage of data summary and similar data in DAP ensures context-aware decision-making. The proposed method uses the words \"Act as an Explainable Machine Learning Model\" in the prompt to enhance the interpretability of the predictions by allowing users to review the logic behind each prediction. In some test cases, the system scored an accuracy above 90%, proving the effectiveness of the system and its potential to outperform conventional ML models in various scenarios. The code is available at https://github.com/Pro-GenAI/LML-DAP",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "bffc41d0d62057ea",
            "authors": [
                "Praneeth Vadlapati"
            ],
            "affiliations": [
                "University of Arizona, Tucson, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.18957.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rag",
                    "#interpretability",
                    "#data",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸' (LML), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸' (DAP). Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ DAP ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ´ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ²Ğ¾Ğ´ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Classification with Language Model Learning",
                    "desc": "This paper presents a novel approach called Language Model Learning (LML) that utilizes Large Language Models (LLMs) for classification tasks, reducing the need for extensive data cleaning and feature engineering typical in traditional Machine Learning (ML) models. The method introduces Data-Augmented Prediction (DAP), where LLMs classify data by mimicking human-like exploration and understanding of the dataset. By summarizing training data and generating queries to retrieve relevant information, the LLM can make context-aware decisions that enhance classification accuracy. The approach also emphasizes interpretability by allowing users to understand the reasoning behind predictions, achieving over 90% accuracy in some tests, showcasing its potential to surpass conventional ML methods."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡åˆ†ç±»ä»»åŠ¡çš„æ•ˆç‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œè¿™é€šå¸¸ç”±æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹å¤„ç†ã€‚ä¸ä¾èµ–æ•°æ®æ¸…ç†å’Œç‰¹å¾å·¥ç¨‹çš„ä¼ ç»ŸMLæ¨¡å‹ä¸åŒï¼Œè¿™ç§æ–¹æ³•é€šè¿‡LLMsç®€åŒ–äº†æµç¨‹ã€‚æå‡ºçš„â€œè¯­è¨€æ¨¡å‹å­¦ä¹ ï¼ˆLMLï¼‰â€æ¦‚å¿µç»“åˆäº†â€œæ•°æ®å¢å¼ºé¢„æµ‹ï¼ˆDAPï¼‰â€æ–¹æ³•ï¼Œä½¿å¾—LLMsèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ¢ç´¢å’Œç†è§£æ•°æ®ï¼Œä»è€Œè¿›è¡Œåˆ†ç±»ã€‚é€šè¿‡æ•°æ®æ‘˜è¦å’Œç›¸å…³æ•°æ®çš„ä½¿ç”¨ï¼Œç¡®ä¿äº†åœ¨å¤æ‚æ•°æ®ä¸‹çš„å‡†ç¡®æ€§ï¼Œå¹¶æé«˜äº†é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17433",
            "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows",
            "url": "https://huggingface.co/papers/2409.17433",
            "abstract": "Despite recent advancements in large language models (LLMs), their performance on complex reasoning problems requiring multi-step thinking and combining various skills is still limited. To address this, we propose a novel framework HDFlow for complex reasoning with LLMs that combines fast and slow thinking modes in an adaptive manner. Our approach consists of two key components: 1) a new approach for slow, deliberate reasoning called Dynamic Workflow, which automatically decomposes complex problems into more manageable sub-tasks and dynamically designs a workflow to assemble specialized LLM or symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general framework that dynamically combines fast and slow thinking based on problem complexity. Finally, we propose an easy-to-scale method for automatically synthesizing a large-scale dataset of 27K challenging reasoning problems for complex reasoning and a hybrid thinking tuning method that trains smaller LLMs on this dataset to internalize the fast/slow hybrid reasoning strategies. Experiments on four reasoning benchmark datasets demonstrate that our slow thinking with dynamic workflows significantly outperforms Chain-of-Thought, and hybrid thinking achieves the highest accuracy while providing an effective balance between computational efficiency and performance. Fine-tuning using our hybrid thinking approach also significantly boosts the complex reasoning capabilities of open-source language models. The results showcase the promise of slow thinking, dynamic workflows, and hybrid thinking in expanding the frontier of complex problem-solving with LLMsCode and data will be released at \\url{https://github.com/wenlinyao/HDFlow.}.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "750db1b173f71245",
            "authors": [
                "Wenlin Yao",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab Bellevue, WA 98004, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.17433.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#rag",
                    "#rl",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "HDFlow: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ HDFlow Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 27 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Complex Reasoning in LLMs with HDFlow",
                    "desc": "This paper introduces HDFlow, a new framework designed to enhance the reasoning capabilities of large language models (LLMs) for complex problems. It features two main components: Dynamic Workflow, which breaks down complex tasks into simpler sub-tasks and organizes the use of specialized reasoning tools, and Hybrid Thinking, which adapts the reasoning approach based on the complexity of the problem. The authors also present a method for creating a large dataset of challenging reasoning problems and a tuning technique to improve LLMs' performance on these tasks. Experimental results show that HDFlow significantly outperforms existing methods and improves the reasoning abilities of smaller LLMs."
                },
                "zh": {
                    "title": "HDFlowï¼šæå‡å¤æ‚æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†åœ¨å¤æ‚æ¨ç†é—®é¢˜ä¸Šä»ç„¶å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶HDFlowï¼Œç»“åˆäº†å¿«é€Ÿå’Œæ…¢é€Ÿæ€ç»´æ¨¡å¼ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŠ¨æ€å·¥ä½œæµå’Œæ··åˆæ€ç»´ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œå¹¶æ ¹æ®é—®é¢˜å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ€ç»´æ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆæå‡äº†å¼€æºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16686",
            "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
            "url": "https://huggingface.co/papers/2409.16686",
            "abstract": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "a06420da3aa04bb8",
            "authors": [
                "Dayuan Fu",
                "Biqing Qi",
                "Yihuai Gao",
                "Che Jiang",
                "Guanting Dong",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "Department of Electronic Engineering, Tsinghua University",
                "Shanghai AI Laboratory",
                "Stanford University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2409.16686.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#agents",
                    "#architecture",
                    "#robotics"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multi-Scale Insight Agent (MSI-Agent) - Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. MSI-Agent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MSI Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-3.5. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Decision-Making with Multi-Scale Insights",
                    "desc": "This paper presents the Multi-Scale Insight Agent (MSI-Agent), which enhances the planning and decision-making capabilities of large language models (LLMs) by effectively managing insights. The MSI-Agent employs a three-part pipeline consisting of an experience selector, an insight generator, and an insight selector to generate and utilize task-specific insights. By summarizing insights across different scales and storing them in a database, MSI can retrieve relevant information to support decision-making processes. Experimental results demonstrate that MSI outperforms existing insight strategies, particularly in adapting to domain shifts, thereby improving the robustness of LLMs."
                },
                "zh": {
                    "title": "å¤šå°ºåº¦æ´å¯Ÿæ™ºèƒ½ä½“ï¼šæå‡å†³ç­–èƒ½åŠ›çš„å…³é”®",
                    "desc": "é•¿æœŸè®°å¿†å¯¹æ™ºèƒ½ä½“éå¸¸é‡è¦ï¼Œå…¶ä¸­æ´å¯ŸåŠ›èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œæ— å…³çš„æ´å¯ŸåŠ›å’Œç¼ºä¹é€šç”¨æ´å¯ŸåŠ›ä¼šä¸¥é‡å½±å“æ´å¯ŸåŠ›çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¤šå°ºåº¦æ´å¯Ÿæ™ºèƒ½ä½“ï¼ˆMSI-Agentï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æœ‰æ•ˆæ€»ç»“å’Œåˆ©ç”¨ä¸åŒå°ºåº¦çš„æ´å¯ŸåŠ›æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚MSIé€šè¿‡ç»éªŒé€‰æ‹©å™¨ã€æ´å¯Ÿç”Ÿæˆå™¨å’Œæ´å¯Ÿé€‰æ‹©å™¨çš„ä¸‰éƒ¨åˆ†ç®¡é“ï¼Œç”Ÿæˆç‰¹å®šä»»åŠ¡å’Œé«˜å±‚æ¬¡çš„æ´å¯ŸåŠ›ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œä»¥ä¾¿åœ¨å†³ç­–æ—¶ä½¿ç”¨ç›¸å…³çš„æ´å¯ŸåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-27.html",
    "link_next": "2024-10-01.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "27.09",
        "en": "09/27",
        "zh": "9æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}