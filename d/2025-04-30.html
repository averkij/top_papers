
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. April 30.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">30 апреля</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-29.html">⬅️ <span id="prev-date">29.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-01.html">➡️ <span id="next-date">01.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'};
        let feedDateNext = {'ru': '01.05', 'en': '05/01', 'zh': '5月1日'};
        let feedDatePrev = {'ru': '29.04', 'en': '04/29', 'zh': '4月29日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.20734', 'title': 'UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities', 'url': 'https://huggingface.co/papers/2504.20734', 'abstract': 'Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.', 'score': 42, 'issue_id': 3503, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '53427aff6a4d7ed5', 'authors': ['Woongyeong Yeo', 'Kangsan Kim', 'Soyeong Jeong', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2504.20734.jpg', 'data': {'categories': ['#rag', '#reasoning', '#interpretability', '#benchmark', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'UniversalRAG: Универсальное извлечение знаний из разнородных источников', 'desc': 'UniversalRAG - это новая система извлечения и интеграции знаний из разнородных источников с различными модальностями и уровнями детализации. В отличие от существующих подходов RAG, ограниченных одномодальными корпусами, UniversalRAG использует механизм маршрутизации с учетом модальности для динамического выбора наиболее подходящего корпуса. Система также организует каждую модальность на несколько уровней детализации, позволяя точно настраивать извлечение информации в соответствии со сложностью и объемом запроса. Эксперименты на 8 тестовых наборах данных показали превосходство UniversalRAG над одномодальными и унифицированными базовыми моделями.'}, 'en': {'title': 'UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval', 'desc': "The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations."}, 'zh': {'title': '多模态知识整合的全新框架', 'desc': '本文介绍了一种新的检索增强生成框架，称为UniversalRAG，旨在从多种异构知识源中检索和整合信息，以提高模型的事实准确性。现有的检索增强生成方法通常仅限于文本数据，而UniversalRAG能够处理多种模态的信息，如图像和视频。该框架采用了一种模态感知的路由机制，能够动态识别最合适的模态特定语料库，从而进行针对性的检索。此外，UniversalRAG还将每种模态组织为多个粒度级别，以便根据查询的复杂性和范围进行精细化检索。'}}}, {'id': 'https://huggingface.co/papers/2504.20571', 'title': 'Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example', 'url': 'https://huggingface.co/papers/2504.20571', 'abstract': 'We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B\'s performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR', 'score': 38, 'issue_id': 3502, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '5392cdfe5ab1de59', 'authors': ['Yiping Wang', 'Qing Yang', 'Zhiyuan Zeng', 'Liliang Ren', 'Lucas Liu', 'Baolin Peng', 'Hao Cheng', 'Xuehai He', 'Kuan Wang', 'Jianfeng Gao', 'Weizhu Chen', 'Shuohang Wang', 'Simon Shaolei Du', 'Yelong Shen'], 'affiliations': ['Georgia Institute of Technology', 'Microsoft', 'University of California, Santa Cruz', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.20571.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#math', '#open_source', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Один пример - огромный скачок в математических способностях ИИ', 'desc': "Исследование показывает эффективность обучения с подкреплением с верифицируемым вознаграждением (RLVR) на одном примере для улучшения математических рассуждений больших языковых моделей. Применение этого метода к базовой модели Qwen2.5-Math-1.5B значительно повысило ее производительность на различных математических тестах. Авторы наблюдали интересные явления, такие как межобластная генерализация и улучшение тестовых показателей после насыщения точности обучения. Исследование также подчеркивает важность поощрения исследования в процессе обучения и отличие этого метода от феномена 'грокинга'."}, 'en': {'title': 'Boosting Math Skills in LLMs with 1-Shot RLVR', 'desc': 'This paper presents a method called 1-shot Reinforcement Learning with Verifiable Reward (RLVR) that significantly enhances the mathematical reasoning abilities of large language models (LLMs). By using just one training example, the authors demonstrate a remarkable increase in performance on the MATH500 benchmark, achieving a score of 73.6% compared to 36.0% before. The study also reveals that this approach leads to improvements across various models and algorithms, highlighting the importance of exploration in training. Additionally, the authors introduce the concept of post-saturation generalization, where performance continues to improve even after training accuracy levels off, suggesting new avenues for research in RLVR.'}, 'zh': {'title': '一例强化学习，提升数学推理能力！', 'desc': '本文展示了使用可验证奖励的强化学习（1-shot RLVR）在提升大型语言模型（LLMs）数学推理能力方面的有效性。通过将RLVR应用于基础模型Qwen2.5-Math-1.5B，我们发现一个单一示例可以将模型在MATH500上的表现从36.0%提升至73.6%。此外，本文还观察到在1-shot RLVR过程中出现了一些有趣现象，如跨领域泛化和自我反思频率增加。我们验证了1-shot RLVR的有效性主要源于策略梯度损失，并强调了促进探索在训练中的关键作用。'}}}, {'id': 'https://huggingface.co/papers/2504.20595', 'title': 'ReasonIR: Training Retrievers for Reasoning Tasks', 'url': 'https://huggingface.co/papers/2504.20595', 'abstract': 'We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.', 'score': 30, 'issue_id': 3503, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '244cff2e64afeaa0', 'authors': ['Rulin Shao', 'Rui Qiao', 'Varsha Kishore', 'Niklas Muennighoff', 'Xi Victoria Lin', 'Daniela Rus', 'Bryan Kian Hsiang Low', 'Sewon Min', 'Wen-tau Yih', 'Pang Wei Koh', 'Luke Zettlemoyer'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'FAIR at Meta', 'Massachusetts Institute of Technology', 'National University of Singapore', 'Singapore-MIT Alliance for Research and Technology', 'Stanford University', 'University of California, Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.20595.jpg', 'data': {'categories': ['#data', '#dataset', '#rag', '#reasoning', '#synthetic', '#open_source', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ReasonIR-8B: Прорыв в информационном поиске для задач рассуждения', 'desc': 'ReasonIR-8B - это первая модель для поиска информации, специально обученная для задач рассуждения. Авторы разработали конвейер генерации синтетических данных, создающий сложные и релевантные запросы для каждого документа. Модель, обученная на смеси синтетических и существующих публичных данных, достигает нового уровня производительности на бенчмарке BRIGHT. При применении к задачам RAG, ReasonIR-8B значительно улучшает результаты на MMLU и GPQA по сравнению с baseline без доступа к внешней информации.'}, 'en': {'title': 'Revolutionizing Reasoning with ReasonIR-8B', 'desc': 'ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications.'}, 'zh': {'title': '推理任务的专属检索器：ReasonIR-8B', 'desc': '我们提出了ReasonIR-8B，这是第一个专门为一般推理任务训练的检索器。现有的检索器在推理任务上的表现有限，部分原因是现有的训练数据集主要集中在与文档直接相关的短小事实查询上。我们开发了一种合成数据生成管道，为每个文档创建具有挑战性和相关性的查询，以及一个看似相关但实际上无用的困难负样本。通过在合成数据和现有公共数据的混合上进行训练，ReasonIR-8B在BRIGHT基准上达到了29.9的nDCG@10（不使用重排序器）和36.9的nDCG@10（使用重排序器），并在RAG任务中显著提高了MMLU和GPQA的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.20157', 'title': 'Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models', 'url': 'https://huggingface.co/papers/2504.20157', 'abstract': "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.", 'score': 24, 'issue_id': 3504, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': '18f16590c380c078', 'authors': ['Zae Myung Kim', 'Chanwoo Park', 'Vipul Raheja', 'Dongyeop Kang'], 'affiliations': ['Grammarly', 'MIT', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2504.20157.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rl', '#open_source', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение с подкреплением для больших языковых моделей', 'desc': 'Статья представляет новый метод под названием Meta Policy Optimization (MPO) для обучения с подкреплением больших языковых моделей. MPO использует мета-модель вознаграждения, которая динамически корректирует промпт модели вознаграждения в процессе обучения. Это позволяет решить проблемы эксплуатации сигнала вознаграждения и зависимости от ручной разработки промптов. Метод показывает результаты на уровне или лучше моделей с тщательно разработанными вручную промптами вознаграждения.'}, 'en': {'title': 'Dynamic Reward Adjustment for Robust LLM Alignment', 'desc': 'This paper presents Meta Policy Optimization (MPO), a new framework designed to improve reward-based alignment methods for large language models (LLMs). MPO tackles two main issues: the risk of reward hacking and the need for complex prompt engineering. By using a meta-reward model, MPO dynamically adjusts the reward prompts during training, ensuring that the reward signal remains effective and less exploitable. The results show that MPO achieves comparable or superior performance to traditional methods while simplifying the alignment process across various tasks.'}, 'zh': {'title': '元策略优化：提升大型语言模型的对齐能力', 'desc': '本文介绍了一种名为元策略优化（MPO）的新框架，旨在解决大型语言模型（LLMs）在基于奖励的对齐方法中面临的两个主要问题：奖励黑客和脆弱的提示工程。MPO通过引入一个动态调整奖励模型提示的元奖励模型，来提高对齐的稳定性和适应性。该方法能够在训练过程中监控环境变化，持续优化奖励信号，从而减少手动设计奖励提示的需求。实验结果表明，MPO在多种任务上表现优异，且不需要专门的奖励设计，具有更强的适应性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2504.20879', 'title': 'The Leaderboard Illusion', 'url': 'https://huggingface.co/papers/2504.20879', 'abstract': "Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field", 'score': 23, 'issue_id': 3505, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'c1d3b5cc6840e6e6', 'authors': ['Shivalika Singh', 'Yiyang Nan', 'Alex Wang', "Daniel D'Souza", 'Sayash Kapoor', 'Ahmet Üstün', 'Sanmi Koyejo', 'Yuntian Deng', 'Shayne Longpre', 'Noah Smith', 'Beyza Ermis', 'Marzieh Fadaee', 'Sara Hooker'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Cohere', 'Cohere Labs', 'Massachusetts Institute of Technology', 'Princeton University', 'Stanford University', 'University of Washington', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2504.20879.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#ethics'], 'emoji': '⚖️', 'ru': {'title': 'Неравенство в оценке ИИ: проблемы и решения для Chatbot Arena', 'desc': 'Статья анализирует проблемы в методологии оценки чат-ботов на платформе Chatbot Arena. Авторы выявили, что некоторые компании имеют преимущество из-за возможности приватного тестирования и выборочного раскрытия результатов. Исследование показало значительную асимметрию в доступе к данным между закрытыми и открытыми моделями. Авторы предлагают рекомендации по реформированию системы оценки для обеспечения более справедливого и прозрачного бенчмаркинга в области машинного обучения.'}, 'en': {'title': 'Towards Fairer AI Benchmarking: Reforming the Chatbot Arena', 'desc': 'This paper discusses the challenges in measuring progress in AI through benchmarks, specifically focusing on the Chatbot Arena leaderboard. It highlights how undisclosed private testing practices create an uneven playing field, favoring certain providers who can selectively disclose their best scores. The authors reveal that proprietary models receive more testing opportunities and data access compared to open-weight models, leading to biased performance evaluations. They propose reforms to enhance transparency and fairness in the benchmarking process, ensuring that all AI systems are evaluated on a level playing field.'}, 'zh': {'title': '推动公平透明的AI基准测试', 'desc': '本文探讨了在人工智能领域中，基准测试的重要性及其潜在的扭曲问题。我们发现，某些提供者通过私下测试和选择性披露成绩，导致了Chatbot Arena的评分偏差。特别是，Meta在Llama-4发布前测试了27个私有LLM变体，这使得其评分更具优势。我们建议对Chatbot Arena的评估框架进行改革，以促进更公平和透明的基准测试。'}}}, {'id': 'https://huggingface.co/papers/2504.20995', 'title': 'TesserAct: Learning 4D Embodied World Models', 'url': 'https://huggingface.co/papers/2504.20995', 'abstract': "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.", 'score': 11, 'issue_id': 3503, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '6339aba982c02561', 'authors': ['Haoyu Zhen', 'Qiao Sun', 'Hongxin Zhang', 'Junyan Li', 'Siyuan Zhou', 'Yilun Du', 'Chuang Gan'], 'affiliations': ['HKUST', 'Harvard University', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2504.20995.jpg', 'data': {'categories': ['#robotics', '#video', '#3d'], 'emoji': '🤖', 'ru': {'title': '4D-модели мира: новый уровень предсказания динамики воплощенных агентов', 'desc': 'В статье представлен эффективный подход к обучению новых 4D-моделей воплощенного мира, которые предсказывают динамическую эволюцию 3D-сцен во времени в ответ на действия воплощенного агента. Авторы предлагают обучать 4D-модель мира на видео RGB-DN (RGB, глубина и нормали), что превосходит традиционные 2D-модели. Метод включает дополнение существующих наборов данных видео роботизированных манипуляций информацией о глубине и нормалях, а также тонкую настройку модели генерации видео. Предложенный алгоритм позволяет напрямую преобразовывать сгенерированные RGB-DN видео в высококачественную 4D-сцену мира.'}, 'en': {'title': 'Revolutionizing 4D Scene Prediction for Embodied Agents', 'desc': 'This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments.'}, 'zh': {'title': '学习四维世界模型，提升具身智能的预测能力', 'desc': '本文提出了一种有效的方法，用于学习新颖的四维具身世界模型，该模型能够预测三维场景在具身智能体动作下的动态演变，确保时空一致性。我们通过训练RGB-DN（RGB、深度和法线）视频来学习四维世界模型，这种方法超越了传统的二维模型，能够将详细的形状、配置和时间变化纳入预测中。具体而言，我们首先利用现成模型扩展现有的机器人操作视频数据集，加入深度和法线信息。接着，我们在这个标注数据集上微调视频生成模型，联合预测每一帧的RGB-DN，最终将生成的RGB、深度和法线视频直接转换为高质量的四维场景。'}}}, {'id': 'https://huggingface.co/papers/2504.20998', 'title': 'YoChameleon: Personalized Vision and Language Generation', 'url': 'https://huggingface.co/papers/2504.20998', 'abstract': 'Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo\'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo\'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo\'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.', 'score': 9, 'issue_id': 3502, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '21fed074912b3e2f', 'authors': ['Thao Nguyen', 'Krishna Kumar Singh', 'Jing Shi', 'Trung Bui', 'Yong Jae Lee', 'Yuheng Li'], 'affiliations': ['Adobe Research', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.20998.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#multimodal'], 'emoji': '🦎', 'ru': {'title': 'Персонализация мультимодальных ИИ-моделей на основе нескольких примеров', 'desc': "Статья представляет Yo'Chameleon - первую попытку персонализации больших мультимодальных моделей. Система использует метод soft-prompt tuning для встраивания информации о конкретном объекте на основе 3-5 изображений. Yo'Chameleon обучается отвечать на вопросы об объекте и воссоздавать его детали на новых изображениях. В процессе обучения применяются механизм самопромптинга и подход 'soft-positive' для улучшения качества генерации изображений."}, 'en': {'title': "Personalizing Multimodal Models with Yo'Chameleon", 'desc': "This paper presents Yo'Chameleon, a novel approach to personalize large multimodal models like GPT-4 for specific user concepts. By using 3-5 images of a subject, Yo'Chameleon applies soft-prompt tuning to incorporate personalized information, enabling the model to answer questions and generate contextually relevant images. The training involves a self-prompting optimization mechanism that ensures balanced performance across different modalities, as well as a 'soft-positive' image generation technique to improve image quality in few-shot scenarios. This work addresses the gap in adapting personalization methods for image generation within multimodal frameworks."}, 'zh': {'title': '个性化多模态模型的创新探索', 'desc': "大型多模态模型（如GPT-4、Gemini、Chameleon）已经发展成为强大的工具，拥有数百万用户。然而，这些模型仍然是通用的，缺乏对特定用户概念的个性化知识。本文介绍了Yo'Chameleon，这是首次研究大型多模态模型个性化的方法。Yo'Chameleon通过软提示调优，将特定主题的信息嵌入模型，以回答关于该主题的问题并在新环境中重建图像的像素级细节。"}}}, {'id': 'https://huggingface.co/papers/2504.20630', 'title': 'ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting', 'url': 'https://huggingface.co/papers/2504.20630', 'abstract': 'Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.', 'score': 9, 'issue_id': 3510, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': 'e4f8da880ab13ca8', 'authors': ['Yu Zhang', 'Wenxiang Guo', 'Changhao Pan', 'Zhiyuan Zhu', 'Tao Jin', 'Zhou Zhao'], 'affiliations': ['Zhejiang Univeristy'], 'pdf_title_img': 'assets/pdf/title_img/2504.20630.jpg', 'data': {'categories': ['#cv', '#multimodal', '#story_generation', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'Погружение в виртуальную драму: мультимодальная генерация пространственного аудио', 'desc': 'Статья представляет новый подход к генерации иммерсивной пространственной драмы на основе мультимодальных подсказок. Авторы создали первый датасет MRSDrama, содержащий бинауральные аудио драмы, сценарии, видео и другие модальности. Они разработали модель ISDrama, включающую мультимодальный кодировщик поз и иммерсивный драма-трансформер на основе Mamba. Экспериментальные результаты показывают превосходство ISDrama над базовыми моделями по объективным и субъективным метрикам.'}, 'en': {'title': 'Revolutionizing Drama Generation with Multimodal AI', 'desc': 'This paper introduces a novel approach to generating immersive spatial drama using machine learning techniques. It presents MRSDrama, a unique dataset that includes various multimodal inputs such as binaural audio, scripts, and videos, which are essential for training the model. The proposed ISDrama model utilizes a Multimodal Pose Encoder and an Immersive Drama Transformer to effectively capture spatial dynamics and dramatic prosody. Experimental results demonstrate that ISDrama significantly improves the quality of generated drama compared to existing models, showcasing its potential for applications in augmented and virtual reality.'}, 'zh': {'title': '多模态沉浸式戏剧生成的创新之路', 'desc': '这篇论文关注于多模态沉浸式空间戏剧生成，旨在基于多模态提示创建连续的多语者双耳语音，具有戏剧性的语调。该任务需要同时建模空间信息和戏剧性语调，但数据收集成本较高。我们构建了MRSDrama，这是第一个多模态录制的空间戏剧数据集，包含双耳戏剧音频、剧本、视频、几何姿态和文本提示。我们提出的ISDrama模型通过多模态提示生成沉浸式空间戏剧，采用了多模态姿态编码器和沉浸式戏剧变换器，实验结果表明ISDrama在客观和主观指标上均优于基线模型。'}}}, {'id': 'https://huggingface.co/papers/2504.16046', 'title': 'Certified Mitigation of Worst-Case LLM Copyright Infringement', 'url': 'https://huggingface.co/papers/2504.16046', 'abstract': 'The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.', 'score': 8, 'issue_id': 3504, 'pub_date': '2025-04-22', 'pub_date_card': {'ru': '22 апреля', 'en': 'April 22', 'zh': '4月22日'}, 'hash': '3a2630d279485a85', 'authors': ['Jingyu Zhang', 'Jiacan Yu', 'Marc Marone', 'Benjamin Van Durme', 'Daniel Khashabi'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2504.16046.jpg', 'data': {'categories': ['#data', '#inference', '#ethics', '#leakage'], 'emoji': '🛡️', 'ru': {'title': 'BloomScrub: защита языковых моделей от нарушения авторских прав', 'desc': 'Статья представляет метод BloomScrub для снижения риска нарушения авторских прав крупными языковыми моделями. Этот подход использует обнаружение цитат и техники переписывания текста во время вывода модели, чтобы трансформировать потенциально нарушающие авторские права сегменты. BloomScrub применяет эффективные структуры данных (фильтры Блума) для масштабируемой проверки авторских прав. Метод показывает высокую эффективность в снижении риска нарушений при сохранении полезности модели.'}, 'en': {'title': 'BloomScrub: Smart Copyright Protection for Language Models', 'desc': "This paper addresses the issue of copyright infringement by large language models (LLMs) that may unintentionally generate content similar to copyrighted material. It introduces BloomScrub, a novel method that detects and rewrites long quotes from copyrighted sources during the model's inference phase. By using Bloom filters for efficient quote detection, BloomScrub can effectively screen large datasets while maintaining the model's utility. The approach not only reduces the risk of copyright infringement but also allows for flexible enforcement levels through adaptive abstention when necessary."}, 'zh': {'title': 'BloomScrub：有效的版权保护方法', 'desc': '本文讨论了大型语言模型在预训练过程中接触到版权材料所带来的版权侵权风险。为了解决这一问题，研究者们开发了"版权撤销"方法，旨在防止模型生成与版权内容相似的文本。我们提出了一种名为BloomScrub的方法，通过在推理时检测引用并进行重写，来有效地减少版权侵权风险。实验结果表明，BloomScrub在降低侵权风险的同时，保持了模型的实用性，并能够适应不同的执行严格性。'}}}, {'id': 'https://huggingface.co/papers/2504.20996', 'title': 'X-Fusion: Introducing New Modality to Frozen Large Language Models', 'url': 'https://huggingface.co/papers/2504.20996', 'abstract': "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.", 'score': 5, 'issue_id': 3507, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '2427da9ba7d7d3f4', 'authors': ['Sicheng Mo', 'Thao Nguyen', 'Xun Huang', 'Siddharth Srinivasan Iyer', 'Yijun Li', 'Yuchen Liu', 'Abhishek Tandon', 'Eli Shechtman', 'Krishna Kumar Singh', 'Yong Jae Lee', 'Bolei Zhou', 'Yuheng Li'], 'affiliations': ['Adobe Research', 'University of California, Los Angeles', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2504.20996.jpg', 'data': {'categories': ['#optimization', '#architecture', '#small_models', '#agi', '#multimodal', '#cv'], 'emoji': '🔀', 'ru': {'title': 'X-Fusion: Мультимодальное расширение языковых моделей без ущерба для языковых способностей', 'desc': 'X-Fusion - это фреймворк, расширяющий возможности предобученных больших языковых моделей (LLM) для мультимодальных задач. Он использует архитектуру с двумя башнями и модальностно-специфичными весами, сохраняя параметры LLM неизменными. X-Fusion превосходит альтернативные архитектуры в задачах преобразования изображения в текст и текста в изображение. Исследования показали, что включение данных для понимания улучшает качество генерации, а уменьшение шума в данных изображений повышает общую производительность.'}, 'en': {'title': 'X-Fusion: Uniting Language and Vision for Superior Multimodal Performance', 'desc': 'The paper introduces X-Fusion, a new framework that enhances pretrained Large Language Models (LLMs) for tasks involving both text and images. It uses a dual-tower architecture that allows the model to maintain its language processing abilities while integrating visual information. The results show that X-Fusion outperforms other models in tasks that convert images to text and vice versa. Additionally, the study highlights the importance of using understanding-focused data and feature alignment to improve model performance and training efficiency.'}, 'zh': {'title': 'X-Fusion：高效的多模态模型框架', 'desc': '我们提出了X-Fusion框架，旨在扩展预训练的大型语言模型（LLMs）以处理多模态任务，同时保持其语言能力。X-Fusion采用双塔设计，使用特定于模态的权重，保持LLM的参数不变，同时整合视觉特定信息以进行理解和生成。实验结果表明，X-Fusion在图像到文本和文本到图像任务上始终优于其他架构。我们的研究发现，结合以理解为重点的数据可以提高生成质量，减少图像数据噪声可以增强整体性能，而特征对齐加速了小模型的收敛，但对大模型的影响较小。'}}}, {'id': 'https://huggingface.co/papers/2504.18087', 'title': 'Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation', 'url': 'https://huggingface.co/papers/2504.18087', 'abstract': "Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.", 'score': 4, 'issue_id': 3509, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': 'c6ed690774bd93ba', 'authors': ['Weipeng Tan', 'Chuming Lin', 'Chengming Xu', 'FeiFan Xu', 'Xiaobin Hu', 'Xiaozhong Ji', 'Junwei Zhu', 'Chengjie Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'Youtu Lab, Tencent, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.18087.jpg', 'data': {'categories': ['#multimodal', '#emotion', '#video', '#diffusion', '#games'], 'emoji': '🎭', 'ru': {'title': 'DICE-Talk: реалистичные эмоции в генерации говорящих голов', 'desc': 'Статья представляет DICE-Talk - новый подход к генерации эмоциональных говорящих голов. Авторы решают проблемы недостаточного использования эмоциональных сигналов из аудио, утечки идентичности в представлениях эмоций и изолированного обучения корреляций эмоций. DICE-Talk использует разделение идентичности и эмоций, а также объединение схожих эмоций для более точной генерации выражений лица. Эксперименты показывают превосходство метода над существующими подходами в точности передачи эмоций при сохранении качества синхронизации губ.'}, 'en': {'title': 'DICE-Talk: Emotionally Expressive Talking Heads with Identity Preservation', 'desc': 'This paper presents DICE-Talk, a new framework for Talking Head Generation (THG) that enhances emotional expressiveness while maintaining speaker identity. It addresses key issues in current methods, such as the underutilization of audio emotional cues and identity leakage in emotion representations. The framework employs a disentangled emotion embedder to model emotional cues and a correlation-enhanced emotion conditioning module to capture relationships between emotions. Experimental results show that DICE-Talk outperforms existing methods in emotion accuracy and preserves identity in generated portraits.'}, 'zh': {'title': '解耦身份与情感，生成丰富的说话头像', 'desc': '本文提出了一种新的框架DICE-Talk，用于生成情感丰富的说话头像，同时保持说话者的身份。我们识别了当前情感生成方法的三个主要限制，包括对音频情感线索的利用不足、情感表示中的身份泄漏以及情感关联的孤立学习。DICE-Talk通过解耦身份与情感，并结合相似特征的情感来解决这些问题。实验结果表明，我们的方法在情感准确性上优于现有的最先进方法，同时保持了竞争力的口型同步性能。'}}}, {'id': 'https://huggingface.co/papers/2504.20114', 'title': 'TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering', 'url': 'https://huggingface.co/papers/2504.20114', 'abstract': 'Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional "Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined "Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\\%-0.4\\% of the model parameter size and reducing the query latency by approximately 99\\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop.', 'score': 2, 'issue_id': 3511, 'pub_date': '2025-04-28', 'pub_date_card': {'ru': '28 апреля', 'en': 'April 28', 'zh': '4月28日'}, 'hash': '56755beaa151585a', 'authors': ['Zhonghao Li', 'Kunpeng Zhang', 'Jinghuai Ou', 'Shuliang Liu', 'Xuming Hu'], 'affiliations': ['Hong Kong University of Science and Technology', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.20114.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag'], 'emoji': '🌳', 'ru': {'title': 'TreeHop: Эффективный многоэтапный поиск без языковых моделей', 'desc': "TreeHop - это новый подход к многоэтапному вопросно-ответному поиску в системах генерации с дополнением извлечением (RAG). Он использует обновление векторных представлений запросов на уровне эмбеддингов, избегая необходимости в языковых моделях для переформулировки запросов. TreeHop значительно снижает вычислительные затраты, заменяя традиционный цикл 'Извлечение-Переписывание-Векторизация-Извлечение' на оптимизированный цикл 'Извлечение-Эмбеддинг-Извлечение'. Экспериментальные результаты показывают, что TreeHop сопоставим по эффективности с передовыми методами RAG, при этом используя всего 5%-0.4% от размера параметров модели и сокращая задержку запросов примерно на 99%."}, 'en': {'title': 'Streamlining Multi-Hop Question Answering with TreeHop', 'desc': "The paper introduces TreeHop, a new framework designed to improve multi-hop question answering (MHQA) in retrieval-augmented generation (RAG) systems. Unlike traditional methods that rely on large language models (LLMs) for query rewriting, TreeHop operates at the embedding level, dynamically updating query embeddings by integrating information from previous queries and retrieved documents. This approach simplifies the retrieval process by replacing the complex cycle of retrieving, rewriting, and vectorizing with a more efficient 'Retrieve-Embed-Retrieve' loop. Experimental results demonstrate that TreeHop achieves competitive performance with significantly lower computational costs and faster query processing times, making it a practical solution for knowledge-intensive applications."}, 'zh': {'title': 'TreeHop：高效的多跳问答解决方案', 'desc': '本论文提出了一种名为TreeHop的框架，旨在解决多跳问答中的高计算成本问题。与传统的基于大语言模型（LLM）的查询重写方法不同，TreeHop通过融合先前查询和检索文档的语义信息，动态更新查询嵌入。该方法简化了检索过程，采用了“检索-嵌入-检索”的循环，显著降低了计算开销。实验结果表明，TreeHop在多个开放领域的多跳问答数据集上表现出色，且模型参数量仅为传统方法的5%-0.4%，查询延迟减少约99%。'}}}, {'id': 'https://huggingface.co/papers/2504.20690', 'title': 'In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer', 'url': 'https://huggingface.co/papers/2504.20690', 'abstract': "Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.", 'score': 1, 'issue_id': 3515, 'pub_date': '2025-04-29', 'pub_date_card': {'ru': '29 апреля', 'en': 'April 29', 'zh': '4月29日'}, 'hash': '857739e05043be6d', 'authors': ['Zechuan Zhang', 'Ji Xie', 'Yu Lu', 'Zongxin Yang', 'Yi Yang'], 'affiliations': ['DBMI, HMS, Harvard University', 'ReLER, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.20690.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Эффективное редактирование изображений с помощью ИИ: точность без компромиссов', 'desc': 'Эта статья представляет новый подход к редактированию изображений на основе текстовых инструкций. Авторы предлагают использовать Диффузионный Трансформер (DiT) для улучшения качества генерации и понимания контекста. Метод включает в себя фреймворк редактирования в контексте, гибридную стратегию обучения LoRA-MoE и масштабирование начального шума с помощью VLM. Результаты показывают превосходство предложенного метода над существующими подходами при значительно меньших требованиях к данным и вычислительным ресурсам.'}, 'en': {'title': 'Efficient and Precise Instruction-Based Image Editing', 'desc': 'This paper presents a novel approach to instruction-based image editing that balances precision and efficiency. It introduces a framework that allows for zero-shot editing compliance using in-context prompting, which avoids the need for structural changes in images. The authors also propose a hybrid tuning strategy that combines LoRA and MoE techniques to enhance flexibility and reduce the need for extensive retraining. Finally, they implement an inference-time scaling method that improves edit quality by selecting better initial noise using vision-language models, demonstrating superior performance with minimal training data and parameters.'}, 'zh': {'title': '高效精准的指令引导图像编辑新范式', 'desc': '本论文提出了一种新的图像编辑方法，利用大型扩散变换器（DiT）来提高图像编辑的精度和效率。我们引入了一个上下文编辑框架，能够在零-shot情况下遵循指令，同时避免结构性变化。通过结合LoRA-MoE混合调优策略，我们实现了灵活的适应性和动态专家路由，而无需大量重新训练。此外，我们还提出了一种早期过滤推理时间缩放方法，利用视觉-语言模型（VLMs）选择更好的初始噪声，从而提高编辑质量。'}}}, {'id': 'https://huggingface.co/papers/2504.18738', 'title': 'A Review of 3D Object Detection with Vision-Language Models', 'url': 'https://huggingface.co/papers/2504.18738', 'abstract': 'This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI', 'score': 1, 'issue_id': 3515, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '991de2200cc55a55', 'authors': ['Ranjan Sapkota', 'Konstantinos I Roumeliotis', 'Rahul Harsha Cheppally', 'Marco Flores Calero', 'Manoj Karkee'], 'affiliations': ['Cornell University, USA', 'Kansas State University, USA', 'Universidad de las Fuerzas Armadas, Ecuador', 'University of Peloponnese, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2504.18738.jpg', 'data': {'categories': ['#multimodal', '#3d', '#survey', '#benchmark', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Новые горизонты 3D-зрения: VLM на страже объектов', 'desc': 'Эта статья представляет собой систематический обзор 3D-обнаружения объектов с использованием моделей зрения-языка (VLM). Авторы анализируют более 100 исследовательских работ, рассматривая уникальные проблемы этой области, такие как пространственное мышление и сложность данных. Сравниваются традиционные подходы с использованием облаков точек и воксельных сеток с современными фреймворками, такими как CLIP и 3D LLM, которые позволяют выполнять обнаружение с открытым словарем и обобщение без примеров. Обсуждаются ключевые архитектуры, стратегии предварительного обучения и методы инженерии промптов для эффективного 3D-обнаружения объектов с помощью VLM.'}, 'en': {'title': 'Advancing 3D Object Detection with Vision-Language Models', 'desc': 'This paper reviews the field of 3D object detection using vision-language models (VLMs), which combine visual and textual information. It analyzes over 100 research papers to identify the unique challenges faced in 3D detection compared to 2D detection, particularly in spatial reasoning and data complexity. The authors compare traditional methods, like point clouds, with modern VLM approaches that allow for open-vocabulary detection and zero-shot generalization. They also discuss key architectures, pretraining strategies, and the importance of prompt engineering in aligning textual and 3D features for improved detection performance.'}, 'zh': {'title': '推动3D物体检测的视觉-语言模型研究', 'desc': '这篇综述文章系统分析了使用视觉-语言模型（VLMs）进行3D物体检测的研究进展。通过审查超过100篇研究论文，文章首次提供了专门针对3D物体检测的系统分析，强调了与2D检测在空间推理和数据复杂性方面的不同挑战。文章比较了传统的点云和体素网格方法与现代的视觉-语言框架，如CLIP和3D LLMs，这些框架支持开放词汇检测和零样本泛化。最后，文章讨论了当前的挑战和未来的研究方向，以推动3D物体检测技术的发展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents', '#agi (1)', '#alignment (1)', '#architecture (2)', '#audio', '#benchmark (4)', '#cv (4)', '#data (2)', '#dataset (2)', '#diffusion (2)', '#ethics (2)', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage (1)', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (6)', '#plp', '#rag (3)', '#reasoning (4)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation (1)', '#survey (1)', '#synthetic (1)', '#training (5)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-30 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-30 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-30 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    