{
    "date": {
        "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 25",
        "zh": "11æœˆ25æ—¥"
    },
    "time_utc": "2025-11-25 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-25",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.18423",
            "title": "General Agentic Memory Via Deep Research",
            "url": "https://huggingface.co/papers/2511.18423",
            "abstract": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
            "score": 157,
            "issue_id": 1,
            "pub_date": "2025-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "45deed7c794d5cb3",
            "authors": [
                "B. Y. Yan",
                "Chaofan Li",
                "Hongjin Qian",
                "Shuqi Lu",
                "Zheng Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Hong Kong Polytechnic University",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18423.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GAM â€” Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ just-in-time ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, GAM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Memorizer Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Researcher Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ (page-store), Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ğ°Ğ¿Ğµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Optimizing AI Memory with Just-in-Time Learning",
                    "desc": "The paper introduces a new framework called General Agentic Memory (GAM) that enhances memory efficiency for AI agents by using principles from just-in-time (JIT) compilation. Unlike traditional static memory systems that can lose important information, GAM creates optimized memory contexts at runtime while retaining essential historical data in a universal page-store. It features a dual-component design: a Memorizer that captures key information and a Researcher that retrieves relevant data for real-time tasks. The framework shows significant improvements in task completion performance compared to existing memory systems, leveraging reinforcement learning for optimization."
                },
                "zh": {
                    "title": "GAMï¼šä¼˜åŒ–è®°å¿†ç®¡ç†çš„æ–°æ¡†æ¶",
                    "desc": "GAMæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé‡‡ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰åŸåˆ™ï¼Œæ—¨åœ¨æé«˜å†…å­˜æ•ˆç‡å’Œä»»åŠ¡å®Œæˆåº¦ã€‚å®ƒé€šè¿‡è½»é‡çº§çš„è®°å¿†å™¨å’Œç ”ç©¶è€…çš„ç»“åˆï¼Œä¼˜åŒ–äº†AIä»£ç†çš„è®°å¿†ç®¡ç†ã€‚GAMåœ¨ç¦»çº¿é˜¶æ®µä¿ç•™ç®€å•ä½†æœ‰ç”¨çš„è®°å¿†ï¼Œè€Œåœ¨è¿è¡Œæ—¶åˆ›å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAMåœ¨å¤šç§åŸºäºè®°å¿†çš„ä»»åŠ¡å®Œæˆåœºæ™¯ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰çš„å†…å­˜ç³»ç»Ÿï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19304",
            "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
            "url": "https://huggingface.co/papers/2511.19304",
            "abstract": "AutoEnv and AutoEnv-36 provide a standardized framework and dataset for evaluating cross-environment learning in agents, highlighting the challenges and limitations of existing learning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
            "score": 89,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "0e253e5c3dd456fa",
            "authors": [
                "Jiayi Zhang",
                "Yiran Peng",
                "Fanqi Kong",
                "Cheng Yang",
                "Yifan Wu",
                "Zhaoyang Yu",
                "Jinyu Xiang",
                "Jianhao Ruan",
                "Jinlin Wang",
                "Maojia Song",
                "HongZhang Liu",
                "Xiangru Tang",
                "Bang Liu",
                "Chenglin Wu",
                "Yuyu Luo"
            ],
            "affiliations": [
                "DeepWisdom",
                "Mila",
                "Peking University",
                "Singapore University of Technology and Design",
                "Sydney University",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Universite de Montreal",
                "Yale University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19304.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AutoEnv â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ², Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AutoEnv-36 Ğ¸Ğ· 36 Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑĞµĞ¼ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 12-49% Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ’Ñ‹Ğ±Ğ¾Ñ€, ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞÑ†ĞµĞ½ĞºĞ°, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑÑĞ½Ğ²Ğ°Ğ¹Ñ€Ğ¾Ğ½Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹."
                },
                "en": {
                    "title": "Empowering Agents to Learn Across Diverse Environments",
                    "desc": "The paper introduces AutoEnv and AutoEnv-36, a framework and dataset designed to evaluate how well agents can learn across different environments. It highlights that while humans adapt to various settings, current AI agents often struggle because they are trained in fixed environments. AutoEnv allows for the creation of diverse environments by treating them as distributions of transitions, observations, and rewards, leading to the development of 36 unique environments. The study shows that traditional learning methods do not perform well as the number of environments increases, emphasizing the need for adaptive learning strategies in cross-environment scenarios."
                },
                "zh": {
                    "title": "è·¨ç¯å¢ƒå­¦ä¹ çš„æ–°æ ‡å‡†ä¸æŒ‘æˆ˜",
                    "desc": "AutoEnvå’ŒAutoEnv-36æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚ç°æœ‰çš„å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒæ˜¯å›ºå®šçš„ï¼Œè€ŒAutoEnvé€šè¿‡å°†ç¯å¢ƒè§†ä¸ºå¯åˆ†è§£çš„åˆ†å¸ƒï¼Œå…è®¸ç”Ÿæˆå¤šæ ·åŒ–çš„ç¯å¢ƒã€‚æˆ‘ä»¬æ„å»ºäº†AutoEnv-36æ•°æ®é›†ï¼ŒåŒ…å«36ä¸ªç¯å¢ƒå’Œ358ä¸ªéªŒè¯çº§åˆ«ï¼Œå±•ç¤ºäº†åœ¨è¿™äº›ç¯å¢ƒä¸­å­¦ä¹ çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå›ºå®šçš„å­¦ä¹ æ–¹æ³•åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æ•ˆæœè¿…é€Ÿä¸‹é™ï¼Œè€Œç¯å¢ƒè‡ªé€‚åº”çš„å­¦ä¹ æ–¹æ³•è™½ç„¶èƒ½æé«˜æ€§èƒ½ï¼Œä½†åœ¨æ–¹æ³•ç©ºé—´æ‰©å±•æ—¶æ”¶ç›Šé€’å‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19365",
            "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
            "url": "https://huggingface.co/papers/2511.19365",
            "abstract": "The frequency-DeCoupled pixel diffusion framework improves image generation efficiency and quality by separating high-frequency details and low-frequency semantics, achieving superior performance compared to existing pixel diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
            "score": 63,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "1bfd45ed7112fc2f",
            "authors": [
                "Zehong Ma",
                "Longhui Wei",
                "Shuai Wang",
                "Shiliang Zhang",
                "Qi Tian"
            ],
            "affiliations": [
                "Huawei Inc.",
                "Nanjing University",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19365.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VAE. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº FID Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Decoupling Frequencies for Superior Image Generation",
                    "desc": "The frequency-DeCoupled pixel diffusion framework enhances image generation by separating high-frequency details from low-frequency semantics. This method allows for more efficient training and inference by using a lightweight pixel decoder for high-frequency generation, while a diffusion transformer focuses on low-frequency aspects. By implementing a frequency-aware flow-matching loss, the model prioritizes important visual frequencies and reduces noise from less significant ones. Experimental results demonstrate that this approach significantly outperforms existing pixel diffusion models, achieving competitive scores on benchmark datasets."
                },
                "zh": {
                    "title": "é¢‘ç‡è§£è€¦ï¼Œæå‡å›¾åƒç”Ÿæˆæ•ˆç‡ä¸è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢‘ç‡è§£è€¦åƒç´ æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜é¢‘ç»†èŠ‚å’Œä½é¢‘è¯­ä¹‰åˆ†å¼€å¤„ç†ï¼Œå…‹æœäº†ç°æœ‰åƒç´ æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„æ…¢é€Ÿé—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨è½»é‡çº§çš„åƒç´ è§£ç å™¨ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ä¸“æ³¨äºä½é¢‘è¯­ä¹‰å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥çš„é¢‘ç‡æ„ŸçŸ¥æµåŒ¹é…æŸå¤±å¼ºè°ƒäº†è§†è§‰ä¸Šæ˜¾è‘—çš„é¢‘ç‡ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡äº†ç”Ÿæˆæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19399",
            "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
            "url": "https://huggingface.co/papers/2511.19399",
            "abstract": "Reinforcement Learning with Evolving Rubrics (RLER) enables training of deep research models for long-form tasks, outperforming existing models and proprietary systems while being more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
            "score": 55,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "3bd93d1f23649d74",
            "authors": [
                "Rulin Shao",
                "Akari Asai",
                "Shannon Zejiang Shen",
                "Hamish Ivison",
                "Varsha Kishore",
                "Jingming Zhuo",
                "Xinran Zhao",
                "Molly Park",
                "Samuel G. Finlayson",
                "David Sontag",
                "Tyler Murray",
                "Sewon Min",
                "Pradeep Dasigi",
                "Luca Soldaini",
                "Faeze Brahman",
                "Wen-tau Yih",
                "Tongshuang Wu",
                "Luke Zettlemoyer",
                "Yoon Kim",
                "Hannaneh Hajishirzi",
                "Pang Wei Koh"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Carnegie Mellon University",
                "Massachusetts Institute of Technology",
                "Seattle Childrens Hospital",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19399.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#reasoning",
                    "#science",
                    "#benchmark",
                    "#healthcare",
                    "#rlhf",
                    "#open_source",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforcement Learning with Evolving Rubrics (RLER) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ½Ğ¾ RLER Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Deep Research Tulu-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Evolving Rubrics for Superior Long-Form Research Models",
                    "desc": "Reinforcement Learning with Evolving Rubrics (RLER) introduces a novel approach to training deep research models for long-form tasks, which are typically more complex than short-form question answering. Unlike traditional methods that rely on fixed rewards, RLER adapts the evaluation criteria, or rubrics, in tandem with the model's learning process, allowing for more relevant and nuanced feedback. This method led to the development of Deep Research Tulu (DR Tulu-8B), a model specifically designed for open-ended research tasks, which significantly outperforms existing models in various domains. The authors also provide all resources, including data and code, to support further advancements in deep research systems."
                },
                "zh": {
                    "title": "è¿›åŒ–è¯„åˆ†æ ‡å‡†åŠ©åŠ›æ·±åº¦ç ”ç©¶æ¨¡å‹",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–è¯„åˆ†æ ‡å‡†ï¼ˆRLERï¼‰ä½¿å¾—æ·±åº¦ç ”ç©¶æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿ç¯‡ä»»åŠ¡ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹å’Œä¸“æœ‰ç³»ç»Ÿï¼ŒåŒæ—¶æˆæœ¬æ›´ä½ã€‚å¤§å¤šæ•°å¼€æ”¾çš„æ·±åº¦ç ”ç©¶æ¨¡å‹ä»…åœ¨çŸ­ç¯‡é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— æ³•é€‚åº”å¤æ‚çš„é•¿ç¯‡ä»»åŠ¡ã€‚RLERé€šè¿‡æ„å»ºå’Œç»´æŠ¤ä¸ç­–ç•¥æ¨¡å‹å…±åŒè¿›åŒ–çš„è¯„åˆ†æ ‡å‡†ï¼Œæä¾›äº†æ›´å…·åŒºåˆ†æ€§çš„åé¦ˆï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ”¹è¿›ã€‚æˆ‘ä»¬å¼€å‘çš„æ·±åº¦ç ”ç©¶Tuluï¼ˆDR Tulu-8Bï¼‰æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¼€æ”¾å¼é•¿ç¯‡æ·±åº¦ç ”ç©¶è®­ç»ƒçš„å¼€æ”¾æ¨¡å‹ï¼Œè¡¨ç°ä¼˜å¼‚ä¸”èµ„æºæ¶ˆè€—ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15567",
            "title": "Computer-Use Agents as Judges for Generative User Interface",
            "url": "https://huggingface.co/papers/2511.15567",
            "abstract": "A framework leveraging Computer-Use Agents as judges to assist coding-oriented language models in designing efficient and functional GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
            "score": 51,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "10a52993cad05a95",
            "authors": [
                "Kevin Qinghong Lin",
                "Siyuan Hu",
                "Linjie Li",
                "Zhengyuan Yang",
                "Lijuan Wang",
                "Philip Torr",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Microsoft",
                "Show Lab, National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15567.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº ÑÑƒĞ´ÑŒĞ¸: Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AUI-Gym â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ â€” ÑÑƒĞ´ÑŒÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 1560 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 52 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Coder-CUA Ğ² ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ, Ğ³Ğ´Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· CUA Dashboard. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Empowering Agents in GUI Design: Efficiency Over Aesthetics",
                    "desc": "This paper presents a new framework that uses Computer-Use Agents (CUA) as judges to help coding-oriented language models (Coder) create better Graphical User Interfaces (GUI). The authors introduce AUI-Gym, a benchmark that includes 52 applications and 1560 tasks to simulate real-world scenarios for automatic GUI design. The Coder generates and revises the GUI, while the CUA evaluates its functionality and provides feedback for improvements. The focus is on making GUIs more efficient for agents rather than just visually appealing for humans, enhancing the overall usability in digital environments."
                },
                "zh": {
                    "title": "ä»£ç†ä¸ç¼–ç æ¨¡å‹åä½œï¼Œæå‡GUIè®¾è®¡æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰ä½œä¸ºè¯„å®¡ï¼Œå¸®åŠ©ç¼–ç å¯¼å‘çš„è¯­è¨€æ¨¡å‹ï¼ˆCoderï¼‰è®¾è®¡é«˜æ•ˆä¸”åŠŸèƒ½é½å…¨çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†AUI-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–52ä¸ªåº”ç”¨ç¨‹åºçš„è‡ªåŠ¨GUIå¼€å‘åŸºå‡†ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹åˆæˆ1560ä¸ªæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„ä»»åŠ¡ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªéªŒè¯å™¨ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡åœ¨å…¶ç¯å¢ƒä¸­å¯æ‰§è¡Œï¼Œä»è€Œæé«˜ä»»åŠ¡çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†CUAä½œä¸ºè¯„å®¡ï¼Œè¯„ä¼°åŠŸèƒ½æ€§å¹¶ä¼˜åŒ–è®¾è®¡ï¼Œæ¨åŠ¨ä»£ç†åœ¨æ•°å­—ç¯å¢ƒä¸­çš„ä¸»åŠ¨å‚ä¸ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18050",
            "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
            "url": "https://huggingface.co/papers/2511.18050",
            "abstract": "UltraFlux, a Flux-based DiT trained on a 4K dataset, addresses failures in diffusion transformers at 4K resolution through enhanced positional encoding, improved VAE compression, gradient rebalancing, and aesthetic curriculum learning, achieving superior performance compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2025-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "7ab1aa44b87d2af4",
            "authors": [
                "Tian Ye",
                "Song Fei",
                "Lei Zhu"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18050.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#open_source",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞÑ‚ ĞºĞ¸Ğ»Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ¼ĞµĞ³Ğ°Ğ¿Ğ¸ĞºÑĞµĞ»ÑĞ¼: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² 4K",
                    "desc": "UltraFlux â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Flux, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ 4K Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RoPE Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ Ğ´Ğ»Ğ¸Ğ½ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ VAE Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ SNR-Aware Huber Wavelet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑƒÑ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Stage-wise Aesthetic Curriculum Learning, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² 4K Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½."
                },
                "en": {
                    "title": "UltraFlux: Elevating 4K Image Generation with Advanced Techniques",
                    "desc": "UltraFlux is a new model designed to improve the performance of diffusion transformers for generating images at 4K resolution. It addresses key issues like positional encoding and VAE compression that affect image quality when scaling up to higher resolutions. By using advanced techniques such as gradient rebalancing and aesthetic curriculum learning, UltraFlux enhances the model's ability to produce detailed and aesthetically pleasing images. The model has been trained on a large dataset and shows superior results compared to existing models in various metrics related to image fidelity and aesthetics."
                },
                "zh": {
                    "title": "UltraFluxï¼šæå‡4Kå›¾åƒç”Ÿæˆçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "UltraFluxæ˜¯ä¸€ç§åŸºäºFluxçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œä¸“é—¨é’ˆå¯¹4Kåˆ†è¾¨ç‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡å¢å¼ºçš„ä½ç½®ç¼–ç ã€æ”¹è¿›çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å‹ç¼©ã€æ¢¯åº¦é‡å¹³è¡¡å’Œç¾å­¦è¯¾ç¨‹å­¦ä¹ ï¼Œè§£å†³äº†åœ¨4Kåˆ†è¾¨ç‡ä¸‹æ‰©æ•£å˜æ¢å™¨çš„å¤±è´¥é—®é¢˜ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒUltraFluxåœ¨å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€ç¾å­¦å’Œå¯¹é½åº¦ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„é•¿å®½æ¯”ä¸‹ç¨³å®šç”Ÿæˆé«˜è´¨é‡çš„4Kå›¾åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19401",
            "title": "In-Video Instructions: Visual Signals as Generative Control",
            "url": "https://huggingface.co/papers/2511.19401",
            "abstract": "Video generative models can interpret and execute visual instructions embedded within frames, enhancing controllability in image-to-video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "f2e6e877078bd945",
            "authors": [
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19401.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² ĞºĞ°Ğ´Ñ€Ñ‹, Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸, Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (Veo 3.1, Kling 2.5, Wan 2.2) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Controllable Video Generation with Visual Instructions",
                    "desc": "This paper explores a new method called In-Video Instruction for generating videos from images. It allows users to embed visual instructions directly into video frames, such as arrows or text, which helps the model understand what actions to take. Unlike traditional prompt-based methods that use text descriptions, this approach provides clearer and more precise guidance for each object in the scene. The authors demonstrate that their method works effectively with advanced video generative models, even in complicated situations with multiple objects."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆä¸­çš„å¯æ§æ€§ï¼šé€šè¿‡è§†è§‰æŒ‡ä»¤å®ç°",
                    "desc": "è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒåµŒå…¥åœ¨å¸§ä¸­çš„è§†è§‰æŒ‡ä»¤ï¼Œä»è€Œå¢å¼ºå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„å¯æ§æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºâ€œè§†é¢‘å†…æŒ‡ä»¤â€ï¼Œé€šè¿‡åœ¨è§†è§‰åŸŸä¸­ç›´æ¥ç¼–ç ç”¨æˆ·æŒ‡å¯¼ï¼Œæ¥å®ç°å¯æ§çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚ä¸åŸºäºæ–‡æœ¬çš„æç¤ºæ§åˆ¶ä¸åŒï¼Œè§†é¢‘å†…æŒ‡ä»¤ä½¿ç”¨å åŠ æ–‡æœ¬ã€ç®­å¤´æˆ–è½¨è¿¹ç­‰å…ƒç´ ï¼Œæä¾›æ˜ç¡®çš„ç©ºé—´æ„ŸçŸ¥å’Œæ— æ­§ä¹‰çš„è§†è§‰æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è§£é‡Šå’Œæ‰§è¡Œè¿™äº›åµŒå…¥çš„è§†è§‰æŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19418",
            "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
            "url": "https://huggingface.co/papers/2511.19418",
            "abstract": "Chain-of-Visual-Thought (COVT) enables Vision-Language Models to reason through visual tokens, improving their performance on perceptual tasks by capturing dense visual information.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "efa9eec6c65df1f9",
            "authors": [
                "Yiming Qin",
                "Bomin Wei",
                "Jiaxin Ge",
                "Konstantinos Kallidromitis",
                "Stephanie Fu",
                "Trevor Darrell",
                "XuDong Wang"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "UC Berkeley",
                "UCLA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19418.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Chain-of-Visual-Thought (COVT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. COVT Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¸Ğ· 20 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°: 2D-Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ DINO. ĞŸÑ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Vision-Language Models with Visual Reasoning",
                    "desc": "Chain-of-Visual-Thought (COVT) enhances Vision-Language Models (VLMs) by allowing them to reason with visual tokens, which are compact representations of visual information. This approach addresses the challenge VLMs face in understanding complex visual tasks that require detailed spatial and geometric reasoning. By using a limited number of visual tokens, COVT captures essential visual features like depth and edge structure, improving the model's ability to interpret and predict visual data. The integration of COVT into existing VLMs has shown significant performance improvements across various perception benchmarks, demonstrating its effectiveness in enhancing multimodal intelligence."
                },
                "zh": {
                    "title": "é“¾å¼è§†è§‰æ€ç»´æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "é“¾å¼è§†è§‰æ€ç»´ï¼ˆCOVTï¼‰ä½¿å¾—è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§†è§‰æ ‡è®°è¿›è¡Œæ¨ç†ï¼Œä»è€Œæå‡å…¶åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦å¯†é›†è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ç©ºé—´æ¨ç†å’Œå‡ ä½•æ„è¯†ï¼Œè¡¨ç°è¾ƒå·®ã€‚COVTæ¡†æ¶é€šè¿‡ä½¿ç”¨è¿ç»­çš„è§†è§‰æ ‡è®°ï¼Œæ•æ‰ä¸°å¯Œçš„æ„ŸçŸ¥çº¿ç´¢ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒä¸­é‡å»ºå¯†é›†çš„ç›‘ç£ä¿¡å·ã€‚ç»è¿‡è¯„ä¼°ï¼ŒCOVTçš„é›†æˆæ˜¾è‘—æé«˜äº†å¤šç§æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­çš„æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº†ç´§å‡‘çš„è§†è§‰æ€ç»´èƒ½å¤Ÿå¢å¼ºå¤šæ¨¡æ€æ™ºèƒ½çš„ç²¾ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20256",
            "title": "The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation",
            "url": "https://huggingface.co/papers/2511.20256",
            "abstract": "An adversarial reward learning framework in reinforcement learning for image generation improves image quality and aesthetics by using dense visual signals from vision foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "6ac56cd7871128d7",
            "authors": [
                "Weijia Mao",
                "Hao Chen",
                "Zhenheng Yang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20256.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Adv-GRPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ñ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº DINO) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ñ…Ğ°ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ ÑƒÑĞ¿ĞµÑ…Ğ° 70-72% Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ."
                },
                "en": {
                    "title": "Enhancing Image Generation with Adversarial Rewards",
                    "desc": "This paper presents Adv-GRPO, a novel reinforcement learning framework that enhances image generation by utilizing an adversarial reward system. Unlike traditional methods that rely on scalar rewards from pre-trained models, Adv-GRPO employs dense visual signals from vision foundation models to provide richer feedback. This approach mitigates issues like reward hacking and biases found in existing reward functions, leading to improved image quality and aesthetics. The framework demonstrates superior performance in human evaluations, achieving higher win rates compared to previous methods."
                },
                "zh": {
                    "title": "å¯¹æŠ—å¥–åŠ±å­¦ä¹ æå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹æŠ—å¥–åŠ±å­¦ä¹ æ¡†æ¶ï¼ˆAdv-GRPOï¼‰ï¼Œç”¨äºå›¾åƒç”Ÿæˆä¸­çš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶é€šè¿‡ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æä¾›çš„å¯†é›†è§†è§‰ä¿¡å·ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œç¾å­¦ã€‚ä¸ä¼ ç»Ÿçš„æ ‡é‡å¥–åŠ±æ¨¡å‹ä¸åŒï¼ŒAdv-GRPOé€šè¿‡å‚è€ƒå›¾åƒä½œä¸ºæ­£æ ·æœ¬æ¥ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œå‡å°‘äº†å¥–åŠ±è¢«æ“æ§çš„é£é™©ã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œç¾å­¦æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17006",
            "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
            "url": "https://huggingface.co/papers/2511.17006",
            "abstract": "Budget-aware methods improve the scaling of tool-augmented agents by providing continuous budget awareness and adaptive planning, leading to better cost-performance trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "ae8b49b11d8db602",
            "authors": [
                "Tengxiao Liu",
                "Zifeng Wang",
                "Jin Miao",
                "I-Hung Hsu",
                "Jun Yan",
                "Jiefeng Chen",
                "Rujun Han",
                "Fangyuan Xu",
                "Yanfei Chen",
                "Ke Jiang",
                "Samira Daruki",
                "Yi Liang",
                "William Yang Wang",
                "Tomas Pfister",
                "Chen-Yu Lee"
            ],
            "affiliations": [
                "Google Cloud AI Research",
                "Google DeepMind",
                "New York University",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17006.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹, Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ. Ğ ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Budget Tracker, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…, Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BATS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Agents with Budget Awareness for Optimal Performance",
                    "desc": "This paper introduces budget-aware methods to enhance the performance of tool-augmented agents by ensuring they are continuously aware of their resource limits. The authors present the Budget Tracker, a tool that helps agents manage their tool-call budgets effectively, leading to improved decision-making during task execution. They also propose BATS, a framework that allows agents to adapt their strategies based on remaining resources, optimizing their actions between exploring deeper or shifting focus. The study provides a unified cost metric for evaluating performance, demonstrating that budget-aware approaches yield better cost-performance trade-offs and improve scaling in these agents."
                },
                "zh": {
                    "title": "é¢„ç®—æ„è¯†æå‡å·¥å…·å¢å¼ºä»£ç†çš„æ€§èƒ½",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†é¢„ç®—æ„è¯†æ–¹æ³•å¦‚ä½•æ”¹å–„å·¥å…·å¢å¼ºä»£ç†çš„æ‰©å±•æ€§ã€‚é€šè¿‡æä¾›æŒç»­çš„é¢„ç®—æ„è¯†å’Œè‡ªé€‚åº”è§„åˆ’ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´å¥½çš„æˆæœ¬ä¸æ€§èƒ½çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†é¢„ç®—è·Ÿè¸ªå™¨å’ŒBATSæ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®å‰©ä½™èµ„æºåŠ¨æ€è°ƒæ•´å…¶è§„åˆ’ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé¢„ç®—æ„è¯†æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ¨åŠ¨æˆæœ¬æ€§èƒ½çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæå‡ä»£ç†åœ¨é¢„ç®—é™åˆ¶ä¸‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18870",
            "title": "HunyuanVideo 1.5 Technical Report",
            "url": "https://huggingface.co/papers/2511.18870",
            "abstract": "HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network.  \t\t\t\t\tAI-generated summary \t\t\t\t We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "439a866927eb13d5",
            "authors": [
                "Bing Wu",
                "Chang Zou",
                "Changlin Li",
                "Duojun Huang",
                "Fang Yang",
                "Hao Tan",
                "Jack Peng",
                "Jianbing Wu",
                "Jiangfeng Xiong",
                "Jie Jiang",
                "Linus",
                "Patrol",
                "Peizhen Zhang",
                "Peng Chen",
                "Penghao Zhao",
                "Qi Tian",
                "Songtao Liu",
                "Weijie Kong",
                "Weiyan Wang",
                "Xiao He",
                "Xin Li",
                "Xinchi Deng",
                "Xuefei Zhe",
                "Yang Li",
                "Yanxin Long",
                "Yuanbo Peng",
                "Yue Wu",
                "Yuhong Liu",
                "Zhenyu Wang",
                "Zuozhuo Dai",
                "Bo Peng",
                "Coopers Li",
                "Gu Gong",
                "Guojian Xiao",
                "Jiahe Tian",
                "Jiaxin Lin",
                "Jie Liu",
                "Jihong Zhang",
                "Jiesong Lian",
                "Kaihang Pan",
                "Lei Wang",
                "Lin Niu",
                "Mingtao Chen",
                "Mingyang Chen",
                "Mingzhe Zheng",
                "Miles Yang",
                "Qiangqiang Hu",
                "Qi Yang",
                "Qiuyong Xiao",
                "Runzhou Wu",
                "Ryan Xu",
                "Rui Yuan",
                "Shanshan Sang",
                "Shisheng Huang",
                "Siruis Gong",
                "Shuo Huang",
                "Weiting Guo",
                "Xiang Yuan",
                "Xiaojia Chen",
                "Xiawei Hu",
                "Wenzhi Sun",
                "Xiele Wu",
                "Xianshun Ren",
                "Xiaoyan Yuan",
                "Xiaoyue Mi",
                "Yepeng Zhang",
                "Yifu Sun",
                "Yiting Lu",
                "Yitong Li",
                "You Huang",
                "Yu Tang",
                "Yixuan Li",
                "Yuhang Deng",
                "Yuan Zhou",
                "Zhichao Hu",
                "Zhiguang Liu",
                "Zhihe Yang",
                "Zilin Yang",
                "Zhenzhi Lu",
                "Zixiang Zhou",
                "Zhao Zhong"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18870.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#multilingual",
                    "#open_source",
                    "#inference",
                    "#data"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ›Ñ‘Ğ³ĞºĞ°Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²ÑĞµÑ…",
                    "desc": "HunyuanVideo 1.5 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ 8.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ DiT Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SSTA) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ĞºÑ€ÑƒĞ³Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with HunyuanVideo 1.5",
                    "desc": "HunyuanVideo 1.5 is a cutting-edge video generation model that combines high visual quality with smooth motion coherence while being lightweight, containing only 8.3 billion parameters. It utilizes a DiT architecture enhanced with selective and sliding tile attention (SSTA) and incorporates an efficient video super-resolution network. The model supports both text-to-video and image-to-video generation, allowing for various durations and resolutions. By making this model open-source, it aims to democratize access to advanced video generation technology for researchers and creators alike."
                },
                "zh": {
                    "title": "è½»é‡çº§è§†é¢‘ç”Ÿæˆï¼Œè§†è§‰è´¨é‡æ–°æ ‡æ†",
                    "desc": "HunyuanVideo 1.5 æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å…ˆè¿›çš„è§†è§‰è´¨é‡å’Œè¿åŠ¨ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº† DiT æ¶æ„å’Œé€‰æ‹©æ€§æ»‘åŠ¨å—æ³¨æ„åŠ›ï¼ˆSSTAï¼‰ï¼Œå¹¶ç»“åˆäº†é«˜æ•ˆçš„è§†é¢‘è¶…åˆ†è¾¨ç‡ç½‘ç»œã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ•´ç†å’ŒåŒè¯­ç†è§£çš„å¢å¼ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªæ—¶é•¿å’Œåˆ†è¾¨ç‡ä¸‹å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬å°†ä»£ç å’Œæ¨¡å‹æƒé‡å…¬å¼€ï¼Œé™ä½äº†è§†é¢‘åˆ›ä½œå’Œç ”ç©¶çš„é—¨æ§›ï¼Œä½¿æ›´å¹¿æ³›çš„ç”¨æˆ·èƒ½å¤Ÿæ¥è§¦åˆ°å…ˆè¿›çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17803",
            "title": "Pillar-0: A New Frontier for Radiology Foundation Models",
            "url": "https://huggingface.co/papers/2511.17803",
            "abstract": "Pillar-0, a radiology foundation model pretrained on diverse imaging datasets, outperforms existing models across various tasks and extends to new applications using RATE for label extraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "9690d6043af51893",
            "authors": [
                "Kumar Krishna Agrawal",
                "Longchao Liu",
                "Long Lian",
                "Michael Nercessian",
                "Natalia Harguindeguy",
                "Yufu Wu",
                "Peter Mikhael",
                "Gigin Lin",
                "Lecia V. Sequist",
                "Florian Fintelmann",
                "Trevor Darrell",
                "Yutong Bai",
                "Maggie Chung",
                "Adam Yala"
            ],
            "affiliations": [
                "Clinical Metabolomics Core and Imaging Core Laboratory, Institute for Radiological Research, Chang Gung Memorial Hospital at Linkou and Chang Gung University, Taiwan",
                "Computational Precision Health, UC Berkeley and UC San Francisco, USA",
                "Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA",
                "Department of Electrical Engineering and Computer Science, UC Berkeley, USA",
                "Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou, Taiwan",
                "Department of Medical Imaging and Radiological Sciences, Chang Gung University, Taiwan",
                "Department of Radiology and Biomedical Imaging, UC San Francisco, USA",
                "Harvard Medical School, USA",
                "Mass General Brigham Cancer Institute, USA",
                "Massachusetts General Hospital, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17803.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#benchmark",
                    "#transfer_learning",
                    "#healthcare",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ«€",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸",
                    "desc": "Pillar-0 â€” ÑÑ‚Ğ¾ foundation model Ğ´Ğ»Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (ĞšĞ¢ Ğ¸ ĞœĞ Ğ˜) Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğµ Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ RATE â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ 366 Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Pillar-0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ°ĞºĞ° Ğ»Ñ‘Ğ³ĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ñ‹Ñ… ĞºÑ€Ğ¾Ğ²Ğ¾Ğ¸Ğ·Ğ»Ğ¸ÑĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Pillar-0: Revolutionizing Radiology with Advanced Imaging Insights",
                    "desc": "Pillar-0 is a radiology foundation model that has been pretrained on a vast array of imaging datasets, significantly enhancing its performance across various radiology tasks. Unlike existing models that treat volumetric CT and MRI scans as low-quality 2D images, Pillar-0 retains critical grayscale contrast information, leading to improved accuracy in detecting radiologic findings. The model utilizes RATE, a framework that efficiently extracts structured labels for numerous findings, achieving near-perfect accuracy. With superior performance metrics, Pillar-0 not only surpasses other leading models but also extends its capabilities to new applications, such as lung cancer risk prediction and brain hemorrhage detection, making it a robust tool for modern radiology."
                },
                "zh": {
                    "title": "Pillar-0ï¼šæ”¾å°„å­¦çš„æœªæ¥åŸºç¡€æ¨¡å‹",
                    "desc": "Pillar-0æ˜¯ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨äº†å¤šç§å½±åƒæ•°æ®é›†ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¤„ç†ä½“ç§¯CTå’ŒMRIæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™é‡è¦çš„ç°åº¦å¯¹æ¯”ä¿¡æ¯ï¼Œå¹¶ä¸”å…·å¤‡åæ˜ çœŸå®ä¸´åºŠå®è·µçš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡RATEæ¡†æ¶ï¼ŒPillar-0èƒ½å¤Ÿé«˜æ•ˆæå–366ç§æ”¾å°„å­¦å‘ç°çš„ç»“æ„åŒ–æ ‡ç­¾ï¼Œå‡†ç¡®ç‡æ¥è¿‘å®Œç¾ã€‚Pillar-0åœ¨å¤šä¸ªå†…éƒ¨æµ‹è¯•é›†å’Œå¤–éƒ¨éªŒè¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†æ”¾å°„å­¦ä»»åŠ¡çš„æ€§èƒ½è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13288",
            "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
            "url": "https://huggingface.co/papers/2511.13288",
            "abstract": "M-GRPO, an extension of Group Relative Policy Optimization for hierarchical multi-agent systems, improves stability and efficiency in tool-augmented reasoning tasks by aligning heterogeneous trajectories and decoupling agent training.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "73b58df5a8f5c31b",
            "authors": [
                "Haoyang Hong",
                "Jiajun Yin",
                "Yuan Wang",
                "Jingnan Liu",
                "Zhe Chen",
                "Ailing Yu",
                "Ji Li",
                "Zhiling Ye",
                "Hansong Xiao",
                "Yefei Chen",
                "Hualei Zhou",
                "Yun Yue",
                "Minghui Yang",
                "Chunxiao Guo",
                "Junwei Liu",
                "Peng Wei",
                "Jinjie Gu"
            ],
            "affiliations": [
                "Ant Group",
                "Imperial College London"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13288.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ M-GRPO â€” Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ³Ğ´Ğµ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ĞµĞµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞµÑ€Ğ²ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ M-GRPO Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Training with M-GRPO",
                    "desc": "M-GRPO is a new method designed to improve the training of hierarchical multi-agent systems, particularly in complex reasoning tasks. It addresses the challenges of training agents that operate at different frequencies and on separate servers by using a decoupled training approach. By aligning the trajectories of heterogeneous agents and maintaining a hierarchical credit assignment, M-GRPO enhances the stability and efficiency of the training process. Experiments show that this method outperforms existing approaches, leading to better performance in specialized reasoning tasks."
                },
                "zh": {
                    "title": "M-GRPOï¼šæå‡å¤šæ™ºèƒ½ä½“æ¨ç†ä»»åŠ¡çš„ç¨³å®šæ€§ä¸æ•ˆç‡",
                    "desc": "M-GRPOæ˜¯å¯¹ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ‰©å±•ï¼Œä¸“ä¸ºå±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ã€‚å®ƒé€šè¿‡å¯¹å¼‚æ„è½¨è¿¹çš„å¯¹é½å’Œæ™ºèƒ½ä½“è®­ç»ƒçš„è§£è€¦ï¼Œæé«˜äº†å·¥å…·å¢å¼ºæ¨ç†ä»»åŠ¡çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼Œä¿æŒå±‚æ¬¡åŒ–çš„ä¿¡ç”¨åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17986",
            "title": "Plan-X: Instruct Video Generation via Semantic Planning",
            "url": "https://huggingface.co/papers/2511.17986",
            "abstract": "Plan-X integrates a Semantic Planner and diffusion models to reduce visual hallucinations and improve instruction-aligned video generation by using multimodal semantic tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "bbd0dd1dc8b70c17",
            "authors": [
                "Lun Huang",
                "You Xie",
                "Hongyi Xu",
                "Tianpei Gu",
                "Chenxu Zhang",
                "Guoxian Song",
                "Zenan Li",
                "Xiaochen Zhao",
                "Linjie Luo",
                "Guillermo Sapiro"
            ],
            "affiliations": [
                "Apple",
                "ByteDance Intelligent Creation",
                "Duke University",
                "Princeton University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17986.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#alignment",
                    "#architecture",
                    "#diffusion",
                    "#hallucinations",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Plan-X â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑĞºĞ¸Ğ·Ğ°Ğ¼Ğ¸Â» Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Plan-X: Enhancing Video Generation with Semantic Planning",
                    "desc": "Plan-X is a novel framework that combines a Semantic Planner with diffusion models to enhance video generation by minimizing visual hallucinations. It utilizes multimodal semantic tokens to better align generated videos with user instructions, particularly in complex scenarios. The Semantic Planner interprets user intent from both text and visual inputs, creating structured semantic tokens that guide the video generation process. By integrating high-level reasoning with advanced visual synthesis, Plan-X significantly improves the quality and relevance of AI-generated videos."
                },
                "zh": {
                    "title": "Plan-Xï¼šå‡å°‘è§†è§‰å¹»è§‰ï¼Œå®ç°æŒ‡ä»¤å¯¹é½çš„è§†é¢‘ç”Ÿæˆ",
                    "desc": "Plan-X æ˜¯ä¸€ä¸ªç»“åˆäº†è¯­ä¹‰è§„åˆ’å™¨å’Œæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è§†è§‰å¹»è§‰å¹¶æ”¹å–„ä¸æŒ‡ä»¤å¯¹é½çš„è§†é¢‘ç”Ÿæˆã€‚å®ƒé€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€è¯­ä¹‰æ ‡è®°ï¼Œæ˜ç¡®æ‰§è¡Œé«˜å±‚æ¬¡çš„è¯­ä¹‰è§„åˆ’ï¼Œä»¥æŒ‡å¯¼è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡ä¸­æ¨ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶è‡ªå›å½’ç”Ÿæˆä¸€ç³»åˆ—åŸºäºæ–‡æœ¬çš„æ—¶ç©ºè¯­ä¹‰æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼ŒPlan-X æ˜¾è‘—å‡å°‘äº†è§†è§‰å¹»è§‰ï¼Œå¹¶å®ç°äº†ä¸å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´çš„ç»†ç²’åº¦æŒ‡ä»¤å¯¹é½è§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17729",
            "title": "M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
            "url": "https://huggingface.co/papers/2511.17729",
            "abstract": "M^3-Bench evaluates multimodal tool use with a focus on visual grounding, textual reasoning, and tool dependencies using a novel similarity-driven alignment method and interpretable metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "2ae5abf91a13d0fc",
            "authors": [
                "Yang Zhou",
                "Mingyu Zhao",
                "Zhenting Wang",
                "Difei Gu",
                "Bangwei Guo",
                "Ruosong Ye",
                "Ligong Han",
                "Can Jin",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Rutgers University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17729.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ¼ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²",
                    "desc": "M^3-Bench â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° Model Context Protocol. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°-driven Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµĞ½Ğ³ĞµÑ€ÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Evaluating Multimodal Tool Use with M^3-Bench",
                    "desc": "M^3-Bench is a new benchmark designed to assess how well multimodal models can use various tools in complex workflows. It focuses on tasks that require understanding both visual and textual information, as well as managing dependencies between different tools. The benchmark uses a unique similarity-driven alignment method to ensure accurate matching of tool calls and provides clear metrics to evaluate performance. Results show that current multimodal models struggle with maintaining argument fidelity and structural consistency, highlighting the need for improved reasoning across different data types."
                },
                "zh": {
                    "title": "M^3-Benchï¼šå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„è¯„ä¼°åŸºå‡†",
                    "desc": "M^3-Benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨è§†è§‰å®šä½ã€æ–‡æœ¬æ¨ç†å’Œå·¥å…·ä¾èµ–æ€§ã€‚è¯¥åŸºå‡†é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½æ–¹æ³•ï¼Œèƒ½å¤Ÿåºåˆ—åŒ–æ¯ä¸ªå·¥å…·è°ƒç”¨ï¼Œå¹¶é€šè¿‡å¥å­ç¼–ç å™¨åµŒå…¥ç­¾åï¼Œè¿›è¡Œç›¸ä¼¼æ€§åˆ†æ¡¶çš„åŒˆç‰™åˆ©åŒ¹é…ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„å¯è§£é‡Šæ€§æŒ‡æ ‡å°†è¯­ä¹‰ä¿çœŸåº¦ä¸å·¥ä½œæµä¸€è‡´æ€§è§£è€¦ï¼Œå¸®åŠ©æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è¡¨ç°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨è®ºè¯ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18922",
            "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
            "url": "https://huggingface.co/papers/2511.18922",
            "abstract": "One4D is a unified framework for 4D generation and reconstruction that uses a novel decoupled approach to produce high-quality RGB frames and pointmaps from varying sparsities of input frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "60b27b8165a2d909",
            "authors": [
                "Zhenxing Mi",
                "Yuxin Wang",
                "Dan Xu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology (HKUST)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18922.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#synthetic",
                    "#multimodal",
                    "#training",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "One4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ RGB-ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Unified Masked Conditioning (UMC), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ 4D Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB Ğ¸ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Decoupled LoRA Control (DLC) â€” Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 4D Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "One4D: Revolutionizing 4D Generation and Reconstruction",
                    "desc": "One4D is a new framework designed for generating and reconstructing 4D content, which includes synchronized RGB images and pointmaps. It uses a Unified Masked Conditioning (UMC) mechanism to effectively manage different levels of input frame sparsity, allowing for flexible transitions between generating from a single image and reconstructing from full videos. The framework incorporates Decoupled LoRA Control (DLC) to maintain high-quality outputs by separating the processing of RGB frames and pointmaps while ensuring they remain consistent. Trained on diverse datasets, One4D aims to enhance the quality of 4D world modeling using advanced video diffusion techniques."
                },
                "zh": {
                    "title": "One4Dï¼šé«˜è´¨é‡4Dç”Ÿæˆä¸é‡å»ºçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "One4Dæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äº4Dç”Ÿæˆå’Œé‡å»ºï¼Œèƒ½å¤Ÿä»ä¸åŒç¨€ç–åº¦çš„è¾“å…¥å¸§ä¸­ç”Ÿæˆé«˜è´¨é‡çš„RGBå¸§å’Œç‚¹å›¾ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„æ©ç æ¡ä»¶ï¼ˆUMCï¼‰æœºåˆ¶ï¼Œçµæ´»å¤„ç†æ¡ä»¶å¸§çš„ç¨€ç–æ€§ï¼Œå®ç°ä»å•å¼ å›¾åƒç”Ÿæˆ4Då†…å®¹ã€ä»å®Œæ•´è§†é¢‘é‡å»º4Då†…å®¹ä»¥åŠä»ç¨€ç–å¸§è¿›è¡Œæ··åˆç”Ÿæˆå’Œé‡å»ºã€‚One4Dé‡‡ç”¨å¼ºå¤§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œæ¶æ„ï¼Œè¿›è¡ŒRGBå¸§å’Œç‚¹å›¾çš„è”åˆç”Ÿæˆã€‚ä¸ºäº†è§£å†³æ·±åº¦å›¾æˆ–ç‚¹å›¾é‡å»ºä¸­å¸¸ç”¨çš„æ‰©æ•£å¾®è°ƒç­–ç•¥çš„ä¸è¶³ï¼ŒOne4Då¼•å…¥äº†è§£è€¦çš„LoRAæ§åˆ¶ï¼ˆDLCï¼‰ï¼Œé€šè¿‡ç‰¹å®šæ¨¡æ€çš„LoRAé€‚é…å™¨å½¢æˆè§£è€¦è®¡ç®—åˆ†æ”¯ï¼Œé€æ­¥å­¦ä¹ åƒç´ çº§ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17405",
            "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
            "url": "https://huggingface.co/papers/2511.17405",
            "abstract": "ReVeL, a framework that converts multiple-choice questions to open-form questions, improves data efficiency and robustness in fine-tuning multimodal language models and reveals score inflation in MCQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "f6e4151deb74bb5e",
            "authors": [
                "Yesheng Liu",
                "Hao Li",
                "Haiyu Xu",
                "Baoqi Pei",
                "Jiahao Wang",
                "Mingxuan Zhao",
                "Jingshu Zheng",
                "Zheqi He",
                "JG Yao",
                "Bowen Qin",
                "Xi Yang",
                "Jiajun Zhang"
            ],
            "affiliations": [
                "BAAI FlagEval Team",
                "BUAA",
                "Institute of Automation, CAS",
                "PKU",
                "School of Artificial Intelligence, UCAS",
                "ZJU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17405.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#synthetic",
                    "#benchmark",
                    "#rlhf",
                    "#open_source",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ: ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ReVeL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² MCQA ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ GRPO Ğ´Ğ»Ñ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2.5-VL. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° 20 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MCQA."
                },
                "en": {
                    "title": "Transforming MCQA for Better Model Training and Evaluation",
                    "desc": "ReVeL is a novel framework designed to transform multiple-choice questions into open-form questions, enhancing the efficiency and robustness of fine-tuning multimodal language models. By addressing the limitations of multiple-choice question answering (MCQA), such as score inflation and answer guessing, ReVeL provides a more reliable evaluation method. The framework categorizes questions based on answer types and applies tailored rewriting and verification techniques. Results show that models fine-tuned with ReVeL not only match MCQA accuracy but also significantly improve OpenQA performance, revealing hidden biases in traditional benchmarks."
                },
                "zh": {
                    "title": "ReVeLï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é—®ç­”èƒ½åŠ›",
                    "desc": "ReVeLæ˜¯ä¸€ä¸ªå°†å¤šé¡¹é€‰æ‹©é¢˜è½¬æ¢ä¸ºå¼€æ”¾å¼é—®é¢˜çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•°æ®æ•ˆç‡å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é—®é¢˜åˆ†ç±»å¹¶åº”ç”¨ä¸åŒçš„é‡å†™å’ŒéªŒè¯æ–¹æ¡ˆï¼Œç¡®ä¿ç­”æ¡ˆåœ¨å¯èƒ½çš„æƒ…å†µä¸‹å¯éªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šé¡¹é€‰æ‹©é¢˜çš„é€‰é¡¹å¯èƒ½æ³„éœ²å¯åˆ©ç”¨çš„ä¿¡å·ï¼Œå¯¼è‡´å‡†ç¡®æ€§æŒ‡æ ‡ä¸å¯é ï¼Œå¹¶é¼“åŠ±åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡ŒçŒœæµ‹è¡Œä¸ºã€‚ä½¿ç”¨ReVeLè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©åŸºå‡†æµ‹è¯•ä¸­ä¸MCQAçš„å‡†ç¡®æ€§ç›¸åŒ¹é…ï¼Œå¹¶åœ¨å¼€æ”¾å¼é—®ç­”ä¸­æé«˜äº†çº¦å…­ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®æ•ˆç‡å’Œæ›´å¼ºçš„å¥–åŠ±ä¿¡å·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18945",
            "title": "MIST: Mutual Information Via Supervised Training",
            "url": "https://huggingface.co/papers/2511.18945",
            "abstract": "A data-driven neural network approach estimates mutual information using a meta-dataset of synthetic distributions, offering flexibility, efficiency, and uncertainty quantification.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "f83a8119f734d9ea",
            "authors": [
                "German Gritsai",
                "Megan Richards",
                "Maxime MÃ©loux",
                "Kyunghyun Cho",
                "Maxime Peyrard"
            ],
            "affiliations": [
                "New York University",
                "Prescient Design, Genentech",
                "Universite Grenoble Alpes, CNRS, Grenoble INP, LIG"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18945.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (MI), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°-Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¸Ğ· 625,000 ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ MI. ĞœĞ¾Ğ´ĞµĞ»ÑŒ MIST Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°Ğ¼. Ğ”Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ñ‹ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ MI, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Mutual Information Estimation with Neural Networks",
                    "desc": "This paper presents a novel approach to estimating mutual information (MI) using a neural network called MIST, which is trained on a large dataset of synthetic distributions. The method allows for flexibility and efficiency by employing a two-dimensional attention mechanism to handle varying sample sizes and dimensions. Additionally, it incorporates quantile regression to provide uncertainty quantification, resulting in well-calibrated confidence intervals. The proposed estimators outperform traditional methods and can be integrated into larger machine learning frameworks, making them versatile for different data types."
                },
                "zh": {
                    "title": "çµæ´»é«˜æ•ˆçš„äº’ä¿¡æ¯ä¼°è®¡æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥è®¾è®¡äº’ä¿¡æ¯ï¼ˆMIï¼‰ä¼°è®¡å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆMISTï¼‰å¯¹MIä¼°è®¡å‡½æ•°è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶é€šè¿‡625,000ä¸ªå·²çŸ¥çœŸå®MIçš„åˆæˆè”åˆåˆ†å¸ƒçš„å¤§å‹å…ƒæ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚ä¸ºäº†å¤„ç†å¯å˜çš„æ ·æœ¬å¤§å°å’Œç»´åº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†äºŒç»´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿è¾“å…¥æ ·æœ¬çš„ç½®æ¢ä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–åˆ†ä½æ•°å›å½’æŸå¤±æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œä½¿å¾—ä¼°è®¡å™¨èƒ½å¤Ÿè¿‘ä¼¼MIçš„é‡‡æ ·åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…è¿”å›å•ä¸€çš„ç‚¹ä¼°è®¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16249",
            "title": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation",
            "url": "https://huggingface.co/papers/2511.16249",
            "abstract": "Controllable Layer Decomposition (CLD) enables fine-grained and controllable separation of raster images into RGBA layers, surpassing existing methods in quality and practical use.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "be07904cd9d90e02",
            "authors": [
                "Zihao Liu",
                "Zunnan Xu",
                "Shi Shu",
                "Jun Zhou",
                "Ruicheng Zhang",
                "Zhenchao Tang",
                "Xiu Li"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16249.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Controllable Layer Decomposition (CLD) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ RGBA ÑĞ»Ğ¾Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: LayerDecompose-DiT Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Multi-Layer Conditional Adapter Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ CLD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ matting Ğ¸ inpainting Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Controllable Layer Decomposition",
                    "desc": "Controllable Layer Decomposition (CLD) is a novel method that allows for the precise separation of raster images into RGBA layers, enhancing the quality and usability of image editing. Unlike traditional methods that struggle with controllability and segmentation accuracy, CLD introduces two innovative modules: LayerDecompose-DiT (LD-DiT) for distinct layer separation and Multi-Layer Conditional Adapter (MLCA) for targeted image generation. This approach not only improves the decomposition process but also allows designers to manipulate layers directly in popular design software. The method has been rigorously evaluated against existing techniques, demonstrating superior performance in both quality and practical application."
                },
                "zh": {
                    "title": "å¯æ§å±‚åˆ†è§£ï¼šå›¾åƒå¤„ç†çš„æ–°çªç ´",
                    "desc": "å¯æ§å±‚åˆ†è§£ï¼ˆCLDï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥å°†å…‰æ …å›¾åƒç²¾ç»†ä¸”å¯æ§åœ°åˆ†è§£ä¸ºRGBAå±‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚è®¾è®¡å¸ˆé€šå¸¸ç‹¬ç«‹ç”Ÿæˆå’Œç¼–è¾‘æ¯ä¸ªRGBAå±‚ï¼Œä½†ä¸€æ—¦åˆæˆï¼Œå±‚çº§ç¼–è¾‘å°±å˜å¾—ä¸å¯èƒ½ã€‚CLDé€šè¿‡ä¸¤ä¸ªå…³é”®æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šLayerDecompose-DiTï¼ˆLD-DiTï¼‰ç”¨äºå°†å›¾åƒå…ƒç´ è§£è€¦ä¸ºä¸åŒå±‚ï¼Œå¹¶å®ç°ç²¾ç»†æ§åˆ¶ï¼›Multi-Layer Conditional Adapterï¼ˆMLCAï¼‰åˆ™å°†ç›®æ ‡å›¾åƒä¿¡æ¯æ³¨å…¥å¤šå±‚ä»¤ç‰Œï¼Œä»¥å®ç°ç²¾ç¡®çš„æ¡ä»¶ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLDåœ¨åˆ†è§£è´¨é‡å’Œå¯æ§æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åˆ†ç¦»çš„å±‚å¯ä»¥ç›´æ¥åœ¨å¸¸ç”¨è®¾è®¡å·¥å…·ä¸­æ“ä½œï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åˆ›ä½œå·¥ä½œæµç¨‹ä¸­çš„ä»·å€¼å’Œé€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16397",
            "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
            "url": "https://huggingface.co/papers/2511.16397",
            "abstract": "A novel extraction pipeline using a language model improves web data quality, significantly enhancing the performance of large language models trained on extracted corpora.  \t\t\t\t\tAI-generated summary \t\t\t\t While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "708884c5c7b5a0be",
            "authors": [
                "Ren Ma",
                "Jiantao Qiu",
                "Chao Xu",
                "Pei Chu",
                "Kaiwen Liu",
                "Pengli Ren",
                "Yuan Qu",
                "Jiahui Peng",
                "Linfeng Hou",
                "Mengjie Liu",
                "Lindong Lu",
                "Wenchang Ning",
                "Jia Yu",
                "Rui Min",
                "Jin Shi",
                "Haojiong Chen",
                "Peng Zhang",
                "Wenjian Zhang",
                "Qian Jiang",
                "Zengjie Hu",
                "Guoqiang Yang",
                "Zhenxiang Li",
                "Fukai Shang",
                "Runyuan Ma",
                "Chenlin Su",
                "Zhongying Tu",
                "Wentao Zhang",
                "Dahua Lin",
                "Conghui He"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16397.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#synthetic",
                    "#benchmark",
                    "#multilingual",
                    "#open_source",
                    "#small_models",
                    "#data"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM - ĞºĞ»ÑÑ‡ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MinerU-HTML Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ (sequence labeling), Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 0,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹, ĞºĞ¾Ğ´ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MinerU-HTML Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ AICC Ğ¸Ğ· 7,3 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° AICC, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ¿Ğ° HTML-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Enhancing Web Data Quality with Intelligent Extraction",
                    "desc": "This paper presents MinerU-HTML, a new extraction pipeline that enhances the quality of web data for training large language models. Unlike traditional methods that rely on heuristics, MinerU-HTML treats content extraction as a sequence labeling task, utilizing a language model to better understand and preserve the structure of documents. The results show that this approach significantly improves the preservation of structured elements and overall data quality, leading to better performance in downstream tasks. The authors also introduce AICC, a large multilingual corpus created using this improved extraction method, which outperforms existing datasets in various benchmarks."
                },
                "zh": {
                    "title": "æå‡ç½‘ç»œæ•°æ®è´¨é‡ï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æå–ç®¡é“MinerU-HTMLï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹æ¥æé«˜ç½‘ç»œæ•°æ®çš„è´¨é‡ï¼Œä»è€Œæ˜¾è‘—æå‡åŸºäºæå–è¯­æ–™åº“è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¯å‘å¼çš„æ–¹æ³•ä¸åŒï¼ŒMinerU-HTMLå°†å†…å®¹æå–è§†ä¸ºä¸€ä¸ªåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ–‡æ¡£ç»“æ„å’Œè¯­ä¹‰å…ƒç´ ã€‚é€šè¿‡åœ¨æå–è¿‡ç¨‹ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µæ ¼å¼åŒ–ç®¡é“ï¼ŒMinerU-HTMLèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†ç±»è¯­ä¹‰å…ƒç´ å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MinerU-HTMLæå–çš„æ•°æ®åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†æå–è´¨é‡å¯¹æ¨¡å‹èƒ½åŠ›çš„é‡è¦å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19314",
            "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
            "url": "https://huggingface.co/papers/2511.19314",
            "abstract": "PRInTS, a generative process reward model, enhances information-seeking abilities in AI agents by providing dense scoring and trajectory summarization, outperforming existing models with smaller backbones.  \t\t\t\t\tAI-generated summary \t\t\t\t Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "df12a4a99a8a162d",
            "authors": [
                "Jaewoo Lee",
                "Archiki Prasad",
                "Justin Chih-Yao Chen",
                "Zaid Khan",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "University of North Carolina at Chapel Hill",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19314.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ PRInTS, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…: Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ²) Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. PRInTS ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ PRInTS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing AI Information-Seeking with PRInTS",
                    "desc": "PRInTS is a novel generative process reward model designed to improve the information-seeking capabilities of AI agents. It achieves this by providing dense scoring that evaluates multiple dimensions of reasoning and summarizing long trajectories of information-gathering tasks. Unlike traditional models that struggle with complex, multi-step tasks, PRInTS effectively captures interactions with tools and interprets their outputs. Evaluations show that PRInTS significantly enhances the performance of both open-source and specialized AI models, even with smaller architectures, outperforming existing reward modeling approaches."
                },
                "zh": {
                    "title": "PRInTSï¼šæå‡AIä»£ç†çš„ä¿¡æ¯è·å–èƒ½åŠ›",
                    "desc": "PRInTSæ˜¯ä¸€ç§ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡äººå·¥æ™ºèƒ½ä»£ç†çš„ä¿¡æ¯è·å–èƒ½åŠ›ã€‚å®ƒé€šè¿‡æä¾›å¯†é›†è¯„åˆ†å’Œè½¨è¿¹æ‘˜è¦ï¼Œå¸®åŠ©ä»£ç†åœ¨é•¿æ—¶é—´çš„ä»»åŠ¡ä¸­æ›´å¥½åœ°æ”¶é›†å’Œæ¨ç†ä¿¡æ¯ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒPRInTSèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä¿¡æ¯è·å–æ­¥éª¤ï¼Œå¹¶æœ‰æ•ˆç®¡ç†å¿«é€Ÿå¢é•¿çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRInTSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„å¥–åŠ±å»ºæ¨¡åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16301",
            "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
            "url": "https://huggingface.co/papers/2511.16301",
            "abstract": "Upsample Anything is a lightweight test-time optimization framework that enhances low-resolution features to high-resolution outputs without training, using an anisotropic Gaussian kernel for precise reconstruction in tasks like semantic segmentation and depth estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "c94b9804541077a2",
            "authors": [
                "Minseok Seo",
                "Mark Hamilton",
                "Changick Kim"
            ],
            "affiliations": [
                "KAIST",
                "MIT",
                "Microsoft"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16301.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Upsample Anything â€” ÑÑ‚Ğ¾ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Vision Foundation Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ğ“Ğ°ÑƒÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ğ´ĞµĞ¸ Ğ¸Ğ· Gaussian Splatting Ğ¸ Joint Bilateral Upsampling. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğº Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Low-Resolution Features with Upsample Anything",
                    "desc": "Upsample Anything is a novel test-time optimization framework that enhances low-resolution features to high-resolution outputs without requiring any training. It utilizes an anisotropic Gaussian kernel to achieve precise reconstruction, making it suitable for tasks like semantic segmentation and depth estimation. This approach overcomes limitations of existing methods that often need retraining or complex optimization, allowing for better scalability and generalization across different models. The framework operates quickly, processing images in approximately 0.419 seconds, while achieving state-of-the-art results in various pixel-level applications."
                },
                "zh": {
                    "title": "è½»æ¾æå‡ä½åˆ†è¾¨ç‡ç‰¹å¾è‡³é«˜åˆ†è¾¨ç‡",
                    "desc": "Upsample Anything æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†ä½åˆ†è¾¨ç‡ç‰¹å¾æå‡ä¸ºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚è¯¥æ–¹æ³•ä½¿ç”¨å„å‘å¼‚æ€§é«˜æ–¯æ ¸è¿›è¡Œç²¾ç¡®é‡å»ºï¼Œé€‚ç”¨äºè¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ã€‚ç°æœ‰çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•é€šå¸¸éœ€è¦ç‰¹å®šæ•°æ®é›†çš„é‡è®­ç»ƒæˆ–å¤æ‚çš„éšå¼ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚Upsample Anything é€šè¿‡ç®€å•çš„é€å›¾åƒä¼˜åŒ–ï¼Œå­¦ä¹ ä¸€ä¸ªç»“åˆç©ºé—´å’ŒèŒƒå›´çº¿ç´¢çš„é«˜æ–¯æ ¸ï¼Œå®ç°äº†åœ¨ä¸åŒæ¶æ„å’Œæ¨¡æ€é—´çš„æ— ç¼è½¬ç§»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18373",
            "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.18373",
            "abstract": "A method that enhances vision language models with spatial-temporal signals and motion tracking improves their performance on physics-driven video reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "3acc9219091a8d50",
            "authors": [
                "Xiyang Wu",
                "Zongxia Li",
                "Jihui Jin",
                "Guangyao Shi",
                "Gouthaman KV",
                "Vishnu Raj",
                "Nilotpal Sinha",
                "Jingxi Chen",
                "Fan Du",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "Dolby Laboratories",
                "University of Maryland",
                "University of Southern California"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18373.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#interpretability",
                    "#training",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision Language Models (VLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MASS-Bench â€” Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 4350 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 8361 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ 3D Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ MASS Ğ¸Ğ½Ğ¶ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· 3D ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ reinforcement fine-tuning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ½Ğ° 8.7% Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ SoTA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Gemini-2.5-Flash."
                },
                "en": {
                    "title": "Enhancing VLMs with Motion Tracking for Better Physics Reasoning",
                    "desc": "This paper presents a method to improve Vision Language Models (VLMs) by incorporating spatial-temporal signals and motion tracking, specifically for physics-driven video reasoning tasks. The authors introduce MASS-Bench, a new benchmark with thousands of videos and question-answer pairs that focus on understanding physics in videos. They propose a model-agnostic method called MASS, which enhances VLMs by integrating depth-based 3D encoding and visual grounding, along with a motion tracker to capture object dynamics. Experimental results demonstrate that the enhanced VLMs significantly outperform existing models, achieving state-of-the-art performance in physics reasoning tasks."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ—¶ç©ºä¿¡å·å’Œè¿åŠ¨è·Ÿè¸ªï¼Œæå‡å…¶åœ¨ç‰©ç†é©±åŠ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„VLMsåœ¨å¤„ç†æ¶‰åŠè¿åŠ¨åŠ¨æ€å’Œç©ºé—´äº¤äº’çš„ç‰©ç†æ¨ç†æ—¶å­˜åœ¨å±€é™ï¼Œå½±å“äº†å…¶å¯¹çœŸå®æˆ–AIç”Ÿæˆå†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MASS-Benchï¼Œä¸€ä¸ªåŒ…å«4350ä¸ªçœŸå®å’ŒAIç”Ÿæˆè§†é¢‘çš„åŸºå‡†æ•°æ®é›†ï¼Œä¸“æ³¨äºç‰©ç†ç›¸å…³çš„ç†è§£ä»»åŠ¡ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ³¨é‡Šã€‚é€šè¿‡å°†æ—¶ç©ºä¿¡å·æ³¨å…¥VLMè¯­è¨€ç©ºé—´ï¼Œå¹¶ç»“åˆè¿åŠ¨è·Ÿè¸ªï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†å’Œç†è§£æ–¹é¢çš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19428",
            "title": "Flow Map Distillation Without Data",
            "url": "https://huggingface.co/papers/2511.19428",
            "abstract": "A data-free framework that samples from the prior distribution surpasses data-based alternatives in flow map distillation, achieving state-of-the-art fidelity with minimal sampling steps.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "49c37f902422b5a3",
            "authors": [
                "Shangyuan Tong",
                "Nanye Ma",
                "Saining Xie",
                "Tommi Jaakkola"
            ],
            "affiliations": [
                "MIT",
                "NYU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19428.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ· Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ² FID 1.45 Ğ½Ğ° ImageNet 256x256 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Data-Free Flow Map Distillation: A New Era in Generative Modeling",
                    "desc": "This paper presents a novel data-free framework for flow map distillation that samples solely from the prior distribution, avoiding the pitfalls of relying on external datasets. Traditional methods face risks of Teacher-Data Mismatch, where the static dataset may not accurately represent the teacher's generative capabilities. The proposed approach learns to predict the teacher's sampling path while correcting its own errors, achieving high fidelity with minimal sampling steps. As a result, this method outperforms existing data-based techniques, setting a new state-of-the-art in generative modeling."
                },
                "zh": {
                    "title": "æ— æ•°æ®æµå›¾è’¸é¦ï¼šè¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„åˆ›æ–°",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ— æ•°æ®æ¡†æ¶ï¼Œé€šè¿‡ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œè¶…è¶Šäº†åŸºäºæ•°æ®çš„æµå›¾è’¸é¦æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¿çœŸåº¦ï¼Œå¹¶ä¸”åªéœ€æœ€å°‘çš„é‡‡æ ·æ­¥éª¤ã€‚ä¼ ç»Ÿçš„æµå›¾è’¸é¦ä¾èµ–äºå¤–éƒ¨æ•°æ®é›†ï¼Œè¿™å¯èƒ½å¯¼è‡´æ•™å¸ˆä¸æ•°æ®ä¹‹é—´çš„ä¸åŒ¹é…é£é™©ã€‚æˆ‘ä»¬æå‡ºçš„æ— æ•°æ®æ–¹æ³•é¿å…äº†è¿™ç§é£é™©ï¼Œç¡®ä¿äº†æ•™å¸ˆçš„é‡‡æ ·è·¯å¾„ä¸å…ˆéªŒåˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ImageNetä¸Šå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œå±•ç¤ºäº†æ— æ•°æ®æµå›¾è’¸é¦çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.16166",
            "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
            "url": "https://huggingface.co/papers/2511.16166",
            "abstract": "EvoVLA, a self-supervised VLA framework, enhances long-horizon robotic manipulation by addressing stage hallucination through triplet contrastive learning, pose-based exploration, and long-horizon memory, achieving improved success rates and sample efficiency on both simulated and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-20",
            "pub_date_card": {
                "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 20",
                "zh": "11æœˆ20æ—¥"
            },
            "hash": "37b6a8ac6938b252",
            "authors": [
                "Zeting Liu",
                "Zida Yang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "Peking University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.16166.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#benchmark",
                    "#hallucinations",
                    "#open_source",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ· ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ²",
                    "desc": "EvoVLA â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Vision-Language-Action (VLA) Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ñ‹Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑĞ±Ğ¾Ğ¿Ñ‹Ñ‚ÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10.2 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ 38.5% Ğ´Ğ¾ 14.8% Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 54.6% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "EvoVLA: Enhancing Robotic Manipulation with Self-Supervised Learning",
                    "desc": "EvoVLA is a self-supervised framework designed to improve long-horizon robotic manipulation by addressing the problem of stage hallucination in Vision-Language-Action (VLA) models. It employs triplet contrastive learning to ensure agents do not take shortcuts in multi-step tasks, thus enhancing the accuracy of task completion. Additionally, it incorporates pose-based exploration to focus on the relationship between objects and the gripper, rather than just visual input. The framework also utilizes long-horizon memory to maintain relevant context during extended tasks, resulting in significant improvements in success rates and sample efficiency in both simulated and real-world environments."
                },
                "zh": {
                    "title": "EvoVLAï¼šæå‡æœºå™¨äººæ“ä½œçš„è‡ªç›‘ç£æ¡†æ¶",
                    "desc": "EvoVLAæ˜¯ä¸€ä¸ªè‡ªç›‘ç£çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡é•¿æ—¶é—´èŒƒå›´çš„æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸‰ç§äº’è¡¥çš„ç»„ä»¶æ¥è§£å†³é˜¶æ®µå¹»è§‰é—®é¢˜ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸‰å…ƒå¯¹æ¯”å­¦ä¹ çš„é˜¶æ®µå¯¹é½å¥–åŠ±(SAR)ã€åŸºäºå§¿æ€çš„ç‰©ä½“æ¢ç´¢(POE)ä»¥åŠé•¿æ—¶é—´è®°å¿†ã€‚EvoVLAåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä»»åŠ¡ä¸­éƒ½æ˜¾ç¤ºå‡ºæ›´é«˜çš„æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ï¼ŒæˆåŠŸç‡æé«˜äº†10.2ä¸ªç™¾åˆ†ç‚¹ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†ä¸€å€åŠã€‚è¯¥æ¡†æ¶åœ¨ç‰©ç†æœºå™¨äººä¸Šçš„å®é™…éƒ¨ç½²ä¹Ÿå–å¾—äº†54.6%çš„å¹³å‡æˆåŠŸç‡ï¼Œå±•ç¤ºäº†æœ‰æ•ˆçš„æ¨¡æ‹Ÿåˆ°ç°å®è½¬ç§»å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17792",
            "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?",
            "url": "https://huggingface.co/papers/2511.17792",
            "abstract": "Target-Bench evaluates state-of-the-shelf world models on mapless path planning tasks, showing significant improvement through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "a9c7fe06feb1ec0d",
            "authors": [
                "Dingrui Wang",
                "Hongyuan Ye",
                "Zhihao Liang",
                "Zhexiao Sun",
                "Zhaowei Lu",
                "Yuchen Zhang",
                "Yuyu Zhao",
                "Yuan Gao",
                "Marvin Seegert",
                "Finn SchÃ¤fer",
                "Haotong Qin",
                "Wei Li",
                "Luigi Palmieri",
                "Felix Jahncke",
                "Mattia Piccinini",
                "Johannes Betz"
            ],
            "affiliations": [
                "Bosch AI Center",
                "ETH",
                "NJU",
                "TUM"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17792.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸: Ğ¾Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Target-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ±ĞµĞ· ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 450 Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 45 ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ†ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sora 2, Veo 3.1, ÑĞµÑ€Ğ¸Ñ Wan) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ 5-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 325 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 15% Ğ²Ñ‹ÑˆĞµ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ²Ñ‹ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸."
                },
                "en": {
                    "title": "Target-Bench: Advancing Robot Path Planning with World Models",
                    "desc": "This paper introduces Target-Bench, a benchmark designed to assess the performance of world models in mapless path planning tasks. It highlights the limitations of current state-of-the-art models in achieving effective robot navigation towards semantic targets. The authors demonstrate that fine-tuning a large open-source model significantly enhances its planning capabilities, achieving a notable improvement over existing models. The study provides a comprehensive evaluation framework that includes metrics for target-reaching, trajectory accuracy, and directional consistency, paving the way for future advancements in robotic path planning."
                },
                "zh": {
                    "title": "Target-Benchï¼šè¯„ä¼°ä¸–ç•Œæ¨¡å‹çš„æ— åœ°å›¾è·¯å¾„è§„åˆ’èƒ½åŠ›",
                    "desc": "Target-Benchæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ä¸–ç•Œæ¨¡å‹åœ¨æ— åœ°å›¾è·¯å¾„è§„åˆ’ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æä¾›äº†450ä¸ªæœºå™¨äººæ”¶é›†çš„è§†é¢‘åºåˆ—ï¼Œæ¶µç›–45ä¸ªè¯­ä¹‰ç±»åˆ«ï¼Œå¹¶åŸºäºSLAMæŠ€æœ¯æä¾›çœŸå®è½¨è¿¹ã€‚é€šè¿‡å¯¹ç”Ÿæˆè§†é¢‘çš„ç›¸æœºè¿åŠ¨è¿›è¡Œæ¢å¤ï¼Œè¯„ä¼°ç®¡é“ä½¿ç”¨äº”ä¸ªäº’è¡¥æŒ‡æ ‡æ¥é‡åŒ–ç›®æ ‡åˆ°è¾¾èƒ½åŠ›ã€è½¨è¿¹å‡†ç¡®æ€§å’Œæ–¹å‘ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜è·¯å¾„è§„åˆ’æ€§èƒ½ï¼Œè¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19319",
            "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
            "url": "https://huggingface.co/papers/2511.19319",
            "abstract": "SyncMV4D generates realistic and consistent multi-view 3D Hand-Object Interaction videos and 4D motions by integrating visual priors, motion dynamics, and multi-view geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "e186177136bab7a0",
            "authors": [
                "Lingwei Dang",
                "Zonghan Li",
                "Juntong Li",
                "Hongwen Zhang",
                "Liang An",
                "Yebin Liu",
                "Qingyao Wu"
            ],
            "affiliations": [
                "Beijing Normal University",
                "South China University of Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19319.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#multimodal",
                    "#robotics",
                    "#3d"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "SyncMV4D â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ñ… Ğ¸ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Multi-view Joint Diffusion Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Diffusion Points Aligner Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 4D Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ³Ğ´Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğµ, Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Hand-Object Interaction with SyncMV4D",
                    "desc": "SyncMV4D is a novel framework that generates realistic multi-view videos of hand-object interactions and their corresponding 4D motions. It addresses the limitations of existing methods by integrating visual priors, motion dynamics, and multi-view geometry, allowing for a more comprehensive understanding of 3D interactions. The model employs a Multi-view Joint Diffusion (MJD) approach to co-generate videos and motions, while a Diffusion Points Aligner (DPA) refines these motions into aligned 4D point tracks. This innovative closed-loop system enhances both the visual quality and motion accuracy, outperforming current state-of-the-art techniques in realism and consistency."
                },
                "zh": {
                    "title": "ç”ŸæˆçœŸå®ä¸€è‡´çš„å¤šè§†è§’3Dæ‰‹ç‰©ä½“äº¤äº’è§†é¢‘",
                    "desc": "SyncMV4Dæ˜¯ä¸€ç§ç”ŸæˆçœŸå®ä¸”ä¸€è‡´çš„å¤šè§†è§’3Dæ‰‹ç‰©ä½“äº¤äº’è§†é¢‘å’Œ4Dè¿åŠ¨çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡æ•´åˆè§†è§‰å…ˆéªŒã€è¿åŠ¨åŠ¨æ€å’Œå¤šè§†è§’å‡ ä½•æ¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬å¤šè§†è§’è”åˆæ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ç‚¹å¯¹é½å™¨ï¼Œèƒ½å¤Ÿå…±åŒç”ŸæˆHOIè§†é¢‘å’Œä¸­é—´è¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncMV4Dåœ¨è§†è§‰çœŸå®æ„Ÿã€è¿åŠ¨åˆç†æ€§å’Œå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.19166",
            "title": "Representational Stability of Truth in Large Language Models",
            "url": "https://huggingface.co/papers/2511.19166",
            "abstract": "LLMs exhibit varying levels of stability in encoding truth representations, influenced more by epistemic familiarity than linguistic form, as assessed through perturbation analysis of their activations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-24",
            "pub_date_card": {
                "ru": "24 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 24",
                "zh": "11æœˆ24æ—¥"
            },
            "hash": "adc4803db77e6c81",
            "authors": [
                "Samantha Dies",
                "Courtney Maynard",
                "Germans Savcisens",
                "Tina Eliassi-Rad"
            ],
            "affiliations": [
                "Khoury College of Computer Sciences, Northeastern University",
                "Network Science Institute, Northeastern University",
                "Santa Fe Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.19166.jpg",
            "data": {
                "categories": [
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Â«Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸Â» Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ĞµÑ‘ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… (Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸), Ñ‡ĞµĞ¼ Ğ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ¾Ñ‚Ğ°Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ ÑĞºĞ¾Ñ€ĞµĞµ Ğ¾Ñ‚ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ²Ğ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ñ‡ĞµĞ¼ Ğ¾Ñ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Stability of Truth in Language Models: Familiarity Matters!",
                    "desc": "This paper investigates how large language models (LLMs) represent truth and falsehood in their internal structures. It introduces the concept of representational stability, which measures how consistent an LLM's truth representations are when faced with changes in the definitions of truth. The study finds that LLMs are more stable in their truth representations when they are familiar with the content, while unfamiliar statements lead to significant shifts in their truth judgments. This research highlights the importance of epistemic familiarity over mere linguistic form in understanding how LLMs encode factual information."
                },
                "zh": {
                    "title": "çŸ¥è¯†ç†Ÿæ‚‰åº¦å½±å“çœŸç›¸è¡¨ç¤ºçš„ç¨³å®šæ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç çœŸç›¸è¡¨ç¤ºæ—¶è¡¨ç°å‡ºä¸åŒçš„ç¨³å®šæ€§ï¼Œè¿™ç§ç¨³å®šæ€§æ›´å¤šåœ°å—åˆ°çŸ¥è¯†ç†Ÿæ‚‰åº¦çš„å½±å“ï¼Œè€Œä¸æ˜¯è¯­è¨€å½¢å¼ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡¨ç¤ºç¨³å®šæ€§è¿™ä¸€æ¦‚å¿µï¼Œæ¥è¡¡é‡LLMåœ¨çœŸç›¸å®šä¹‰å˜åŒ–ä¸‹çš„çœŸå®æ€§è¡¨ç¤ºçš„ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹16ä¸ªå¼€æºæ¨¡å‹çš„æ¿€æ´»è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¸ç†Ÿæ‚‰çš„é™ˆè¿°ä¼šå¯¼è‡´æ›´å¤§çš„å†³ç­–è¾¹ç•Œå˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨è„†å¼±é¢†åŸŸä¸­ï¼ŒçœŸä¼ªåˆ¤æ–­çš„ç¿»è½¬ç‡å¯è¾¾40%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¡¨ç¤ºç¨³å®šæ€§ä¸»è¦æºäºçŸ¥è¯†çš„ç†Ÿæ‚‰ç¨‹åº¦ï¼Œè€Œéè¯­è¨€çš„å½¢å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18047",
            "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
            "url": "https://huggingface.co/papers/2511.18047",
            "abstract": "SPINRec, a model-agnostic approach, enhances explanation fidelity in recommender systems by using stochastic baseline sampling and path-integration techniques to capture both observed and unobserved interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "9afe2b0fd0c831cc",
            "authors": [
                "Oren Barkan",
                "Yahlly Schein",
                "Yehonatan Elisha",
                "Veronika Bogina",
                "Mikhail Baklanov",
                "Noam Koenigstein"
            ],
            "affiliations": [
                "Tel Aviv University, Israel",
                "The Open University, Israel"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18047.jpg",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ§ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹",
                    "desc": "SPINRec â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ·Ğ¸ÑĞ°, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ñ‘Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº."
                },
                "en": {
                    "title": "Enhancing Explainability in Recommendations with SPINRec",
                    "desc": "SPINRec is a new method designed to improve how well explanations in recommender systems reflect the actual reasoning of the models. It uses stochastic baseline sampling and path-integration techniques to better understand both the interactions that are seen and those that are not. By sampling various user profiles from real data, SPINRec finds the most accurate way to explain recommendations. This approach leads to more reliable and personalized explanations, setting a new standard for explainability in recommendation systems."
                },
                "zh": {
                    "title": "æå‡æ¨èç³»ç»Ÿè§£é‡Šå¯ä¿¡åº¦çš„SPINRec",
                    "desc": "SPINRecæ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨èç³»ç»Ÿä¸­è§£é‡Šçš„å¯ä¿¡åº¦ã€‚å®ƒé€šè¿‡éšæœºåŸºçº¿é‡‡æ ·å’Œè·¯å¾„ç§¯åˆ†æŠ€æœ¯ï¼Œæ•æ‰è§‚å¯Ÿåˆ°å’Œæœªè§‚å¯Ÿåˆ°çš„äº¤äº’ã€‚SPINRecå…‹æœäº†ä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿæä¾›æ›´ç¨³å®šå’Œä¸ªæ€§åŒ–çš„è§£é‡Šã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†çš„å…¨é¢è¯„ä¼°ï¼ŒSPINRecåœ¨è§£é‡Šçš„å¯ä¿¡åº¦ä¸Šè®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.18024",
            "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
            "url": "https://huggingface.co/papers/2511.18024",
            "abstract": "A Sparse Autoencoder method extracts interpretable latent dimensions from user and item embeddings in recommender systems, aligning with model predictions and supporting controllable personalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "264e06cc15e5277c",
            "authors": [
                "Dor Arviv",
                "Yehonatan Elisha",
                "Oren Barkan",
                "Noam Koenigstein"
            ],
            "affiliations": [
                "Tel Aviv University",
                "The Open University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18024.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Sparse Autoencoder. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ objective Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¶Ğ°Ğ½Ñ€, Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Interpretability in Recommendations with Sparse Autoencoders",
                    "desc": "This paper introduces a Sparse Autoencoder (SAE) method that extracts interpretable latent dimensions from user and item embeddings in recommender systems. The method focuses on creating monosemantic neurons, which represent clear and coherent concepts while maintaining the relationships between user and item embeddings. By using a prediction-aware training objective, the approach aligns the learned latent structure with the recommender's predictions, allowing for better personalization. This technique enables targeted filtering and content promotion, making it a valuable tool for enhancing interpretability and control in recommendation systems."
                },
                "zh": {
                    "title": "å¯è§£é‡Šçš„ä¸ªæ€§åŒ–æ¨èæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoder, SAEï¼‰æ–¹æ³•ï¼Œç”¨äºä»æ¨èç³»ç»Ÿä¸­çš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥ä¸­æå–å¯è§£é‡Šçš„æ½œåœ¨ç»´åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡æ­ç¤ºé¢„è®­ç»ƒè¡¨ç¤ºä¸­çš„è¯­ä¹‰ç»“æ„ï¼Œå¸®åŠ©å®ç°å¯æ§çš„ä¸ªæ€§åŒ–æ¨èã€‚ä¸è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œæ¨èç³»ç»Ÿä¸­çš„å•ä¹‰æ€§éœ€è¦ä¿æŒç”¨æˆ·å’Œç‰©å“åµŒå…¥ä¹‹é—´çš„äº¤äº’ã€‚æœ€ç»ˆï¼Œæå–çš„ç¥ç»å…ƒèƒ½å¤Ÿæ•æ‰æµæ´¾ã€å—æ¬¢è¿ç¨‹åº¦å’Œæ—¶é—´è¶‹åŠ¿ç­‰ç‰¹æ€§ï¼Œæ”¯æŒåç»­çš„è¿‡æ»¤å’Œå†…å®¹æ¨å¹¿æ“ä½œï¼Œè€Œæ— éœ€ä¿®æ”¹åŸºç¡€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.12810",
            "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
            "url": "https://huggingface.co/papers/2511.12810",
            "abstract": "A Multi-Scale Recursive Network using a Pyramid Vision Transformer and specialized units improves camouflaged object detection by enhancing feature extraction and recursive feature refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "7968ac29ec6a5fb1",
            "authors": [
                "Leena Alghamdi",
                "Muhammad Usman",
                "Hafeez Anwar",
                "Abdul Bais",
                "Saeed Anwar"
            ],
            "affiliations": [
                "Department of Computer Science and Software Engineering, University of Western Australia, Perth 6009, Australia",
                "Department of Computer Science, National University of Computer and Emerging Sciences, Peshawar 24720, Pakistan",
                "Electronic Systems Engineering, University of Regina, Regina S4S 0A2, Canada",
                "Faculty of Science, Ontario Tech University, Oshawa L1G 0C5, Canada",
                "Information and Computer Science, King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.12810.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ Ğ² Ñ†Ğ²ĞµÑ‚Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Multi-Scale Recursive Network, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Pyramid Vision Transformer Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Attention-Based Scale Integration Units Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Multi-Granularity Fusion Units Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ°Ğ¼ÑƒÑ„Ğ»ÑĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Camouflaged Object Detection with Multi-Scale Recursive Networks",
                    "desc": "This paper presents a Multi-Scale Recursive Network designed to improve the detection of camouflaged objects in complex environments. It utilizes a Pyramid Vision Transformer to extract features at multiple scales and employs specialized Attention-Based Scale Integration Units for effective feature merging. The model incorporates a recursive feedback mechanism that enhances global context understanding, allowing for better detection of small and multiple camouflaged objects. The proposed method outperforms existing techniques on benchmark datasets, demonstrating significant advancements in camouflaged object detection."
                },
                "zh": {
                    "title": "å¤šå°ºåº¦é€’å½’ç½‘ç»œï¼šæå‡ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦é€’å½’ç½‘ç»œï¼Œåˆ©ç”¨é‡‘å­—å¡”è§†è§‰å˜æ¢å™¨å’Œä¸“é—¨çš„å•å…ƒæ¥æé«˜ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¤šå°ºåº¦ç‰¹å¾å¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èåˆï¼Œä»è€Œå¢å¼ºäº†ç‰¹å¾æå–å’Œé€’å½’ç‰¹å¾ä¼˜åŒ–ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æ£€æµ‹å°å‹å’Œå¤šä¸ªä¼ªè£…ç‰©ä½“ï¼Œè§£ç å™¨é€šè¿‡å¤šç²’åº¦èåˆå•å…ƒé€’å½’åœ°ç»†åŒ–ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-24.html",
    "link_next": "2025-11-26.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "24.11",
        "en": "11/24",
        "zh": "11æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 2,
        "#benchmark": 16,
        "#agents": 7,
        "#cv": 6,
        "#rl": 4,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 7,
        "#multimodal": 11,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 12,
        "#robotics": 3,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 4,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 11,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}