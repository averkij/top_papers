
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. October 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 октября</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-08.html">⬅️ <span id="prev-date">08.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-10.html">➡️ <span id="next-date">10.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'};
        let feedDateNext = {'ru': '10.10', 'en': '10/10', 'zh': '10月10日'};
        let feedDatePrev = {'ru': '08.10', 'en': '10/08', 'zh': '10月8日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.04199', 'title': 'LongGenBench: Long-context Generation Benchmark', 'url': 'https://huggingface.co/papers/2410.04199', 'abstract': 'Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.', 'score': 17, 'issue_id': 25, 'pub_date': '2024-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '12ea9c1effd189a8', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#architecture', '#synthetic'], 'emoji': '📏', 'ru': {'title': 'LongGenBench: новый рубеж в оценке генерации длинных текстов', 'desc': 'LongGenBench - это новый синтетический бенчмарк для оценки способности языковых моделей к генерации длинных контекстов. В отличие от существующих тестов, фокусирующихся на поиске информации, LongGenBench требует от моделей создания целостных длинных ответов. Исследование показало, что все модели, как API-доступные, так и с открытым исходным кодом, демонстрируют снижение производительности при работе с длинными контекстами. Модели Gemini-1.5-Flash и Qwen2 показали наименьшую деградацию среди API-моделей и моделей с открытым кодом соответственно.'}, 'en': {'title': '"LongGenBench: Elevating LLMs to New Contextual Heights"', 'desc': "The paper introduces LongGenBench, a new benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that focus on retrieval tasks, LongGenBench assesses how well LLMs can generate coherent and contextually accurate text over long passages. The study finds that many models, both API-based and open source, show significant performance drops in long-context generation, with some models like Gemini-1.5-Flash and Qwen2 series performing better than others. This benchmark aims to fill the gap in evaluating LLMs' ability to handle extended text generation, providing a more comprehensive understanding of their capabilities."}, 'zh': {'title': '突破长文本生成的评估瓶颈', 'desc': '这篇论文介绍了一种新的基准测试，名为LongGenBench，用于评估大型语言模型在长文本生成中的表现。传统的基准测试主要关注信息检索能力，而LongGenBench则专注于生成连贯且上下文准确的长文本。研究发现，不同的模型在长文本生成中表现不同，其中Gemini-1.5-Flash和Qwen2系列模型的性能下降最小。通过这种新的测试方法，可以更全面地评估模型的长文本生成能力。'}}}, {'id': 'https://huggingface.co/papers/2410.04717', 'title': '$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization', 'url': 'https://huggingface.co/papers/2410.04717', 'abstract': "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $textbf{specialist} and textbf{generalist}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.", 'score': 17, 'issue_id': 25, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '30a0e92854b70969', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Разнообразие данных - ключ к обобщению навыков языковых моделей', 'desc': 'Статья исследует факторы, позволяющие языковым моделям обобщать навыки на новые инструкции. Авторы проводят эксперименты, демонстрирующие, что такое обобщение возникает только при достаточном разнообразии обучающих данных по семантическим доменам. Исследование показывает, что диверсификация данных между доменами значительно повышает адаптивность модели. Результаты применимы как для специализированных, так и для универсальных моделей, подчеркивая важность стратегической диверсификации данных при обучении.'}, 'en': {'title': '"Diversity Drives Understanding: Enhancing LLMs with Cross-Domain Data"', 'desc': 'This paper explores how large language models (LLMs) can better understand and follow instructions by examining the factors that help them generalize to new tasks. The study shows that models perform better when trained on data that is diverse across different semantic domains, rather than just within a single domain. By diversifying the types of instructions in the training data, even with limited data, models become more adaptable and effective. The research provides guidelines for improving model performance by strategically diversifying training data, benefiting both specialist and generalist models.'}, 'zh': {'title': '数据多样化：提升模型泛化能力的关键', 'desc': '这篇论文探讨了如何让大型语言模型更好地理解和执行未见过的指令。研究表明，只有在训练数据在语义领域上足够多样化时，模型才能实现良好的泛化。通过实验，作者发现跨领域的数据多样化，即使在数据量有限的情况下，也能显著提高模型的适应性。论文还指出，增加数据的语义多样性比单纯增加相似数据的数量更有效。'}}}, {'id': 'https://huggingface.co/papers/2410.01912', 'title': 'A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation', 'url': 'https://huggingface.co/papers/2410.01912', 'abstract': "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.", 'score': 13, 'issue_id': 28, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '63855d0547d5d08f', 'data': {'categories': ['#science', '#dataset', '#cv', '#optimization', '#open_source', '#architecture', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'DnD-Transformer: Новый взгляд на авторегрессивную генерацию изображений', 'desc': 'Статья представляет новую архитектуру модели под названием 2-Dimensional Autoregression (DnD) Transformer для генерации изображений. DnD-Transformer решает проблему потери информации при векторном квантовании, вводя новое направление авторегрессии - глубину модели. По сравнению с традиционными методами, DnD-Transformer генерирует изображения более высокого качества при той же длине последовательности. Модель также демонстрирует способность генерировать изображения с текстом и графическими элементами, показывая понимание комбинированных модальностей.'}, 'en': {'title': '"DnD-Transformer: Elevating Image Generation with Depth and Direction"', 'desc': 'The paper introduces the DnD-Transformer, a new model architecture for autoregressive image generation that addresses the information loss bottleneck of vector-quantization. By adding a new autoregression direction, the model predicts more codes for an image, improving image quality without increasing model size. Unlike previous models, the DnD-Transformer can generate images with complex text and graphical elements, showcasing a unique understanding of combined modalities. This advancement suggests a new level of vision-language intelligence in image generation models.'}, 'zh': {'title': 'DnD-Transformer：突破图像生成的自回归新视角', 'desc': '这篇论文提出了一种新的模型架构，称为二维自回归（DnD）Transformer，以解决矢量量化自回归图像生成中的信息损失瓶颈。DnD-Transformer通过引入新的自回归方向和模型深度，预测图像的更多编码。与传统的一维自回归和类似的二维图像分解方法相比，DnD-Transformer能够在相同的模型大小和序列长度下生成更高质量的图像。实验表明，DnD-Transformer不仅能生成自然图像，还能在自监督的情况下生成包含丰富文本和图形元素的图像，展示了对这些组合模态的理解。'}}}, {'id': 'https://huggingface.co/papers/2410.05193', 'title': 'RevisEval: Improving LLM-as-a-Judge via Response-Adapted References', 'url': 'https://huggingface.co/papers/2410.05193', 'abstract': "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.", 'score': 12, 'issue_id': 25, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'dcb3bbd44f2e2a94', 'data': {'categories': ['#inference', '#ethics', '#optimization', '#interpretability', '#benchmark', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'RevisEval: Революция в оценке качества генерации текста', 'desc': 'RevisEval - это новая парадигма оценки генерации текста, использующая адаптированные под ответ эталоны. Метод задействует возможности больших языковых моделей для пересмотра ответа и создания релевантного эталона. Эксперименты показывают, что RevisEval превосходит традиционные методы оценки без эталона и с эталоном, использующие LLM в качестве судьи. Более того, адаптированные эталоны улучшают классические метрики текста, такие как BLEU и BERTScore.'}, 'en': {'title': 'RevisEval: Revolutionizing Text Evaluation with Adaptive References', 'desc': "The paper introduces RevisEval, a new method for evaluating text generation by using response-adapted references. This approach leverages large language models to revise responses, creating more relevant references for evaluation. RevisEval has been shown to outperform traditional evaluation methods, improving metrics like BLEU and BERTScore. The study also highlights RevisEval's ability to reduce bias and enhance evaluation reliability."}, 'zh': {'title': 'RevisEval：通过响应适应的参考提升文本评估', 'desc': '这篇论文介绍了一种新的文本生成评估方法，称为RevisEval，它通过响应适应的参考来提高评估的准确性。RevisEval利用大型语言模型的文本修订能力，将生成的文本进行适应性修订，然后将修订后的文本作为参考进行评估。实验表明，RevisEval在自然语言生成任务中优于传统的无参考和基于参考的评估方法。更重要的是，RevisEval的参考可以提升经典文本指标的表现，如BLEU和BERTScore，并且在某些情况下甚至可以媲美LLM-as-a-Judge。'}}}, {'id': 'https://huggingface.co/papers/2410.03864', 'title': 'DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search', 'url': 'https://huggingface.co/papers/2410.03864', 'abstract': 'Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called "reasoning actions"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.', 'score': 10, 'issue_id': 34, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'c9a263348ca9157d', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'DOTS: Динамическое рассуждение для языковых моделей через оптимальный поиск траекторий', 'desc': 'Статья представляет DOTS - подход, позволяющий языковым моделям динамически рассуждать путем поиска оптимальной траектории рассуждений. Метод включает определение атомарных модулей действий рассуждения, поиск оптимальной траектории действий для каждого вопроса и обучение языковой модели планированию траекторий рассуждений для новых вопросов. Эксперименты показывают, что DOTS превосходит статические методы рассуждения и стандартный подход с инструкциями. Анализ выявляет, что метод позволяет языковым моделям адаптировать вычисления в зависимости от сложности задачи.'}, 'en': {'title': "Dynamic Reasoning: Tailoring AI's Thought Process for Better Problem Solving", 'desc': "This paper introduces DOTS, a method to enhance large language models' reasoning by dynamically tailoring reasoning strategies to each question. Unlike previous static approaches, DOTS searches for the best reasoning path for each question, using atomic reasoning modules. The method involves training a language model to plan reasoning paths for new questions, either by using an external planner or by enhancing the model's internal planning capabilities. Experiments show that DOTS improves performance across various reasoning tasks by allowing models to allocate more resources to complex problems."}, 'zh': {'title': 'DOTS：动态推理路径搜索，提升语言模型推理能力', 'desc': '这篇论文提出了一种名为DOTS的方法，旨在提升大型语言模型（LLM）的推理能力。与以往使用静态推理策略不同，DOTS通过动态搜索最佳推理路径，根据每个问题的特性和LLM的能力进行调整。该方法包括定义基本推理动作模块、为每个问题寻找最佳推理路径，以及利用这些路径训练LLM以应对新问题。实验结果表明，DOTS在多个推理任务中表现优于传统方法，能够根据问题难度调整计算深度。'}}}, {'id': 'https://huggingface.co/papers/2410.04343', 'title': 'Inference Scaling for Long-Context Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2410.04343', 'abstract': "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.", 'score': 9, 'issue_id': 48, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '1f6403a22be1233e', 'data': {'categories': ['#reasoning', '#long_context', '#rag', '#inference', '#optimization', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Оптимизация вычислений для RAG: больше - не всегда лучше, важно как', 'desc': 'Статья исследует масштабирование вычислений для вывода в контексте генерации с использованием извлечения информации (RAG) для больших языковых моделей. Авторы изучают две стратегии: обучение в контексте и итеративный промптинг. Они обнаруживают, что оптимальное распределение вычислительных ресурсов приводит к почти линейному росту производительности RAG. На основе этих наблюдений разрабатывается модель для предсказания оптимальных параметров вывода при различных вычислительных ограничениях.'}, 'en': {'title': 'Unlocking Performance: Optimal Inference Scaling for RAG in LLMs', 'desc': 'This paper explores how to improve the performance of retrieval augmented generation (RAG) in large language models (LLMs) by scaling inference computation. It highlights that simply increasing the amount of external knowledge does not always lead to better results unless it is effectively utilized. The authors investigate two strategies: in-context learning and iterative prompting, which allow for more flexible use of computational resources during inference. Their findings show that with optimal allocation of inference resources, RAG performance can significantly improve, demonstrating a nearly linear relationship between increased computation and performance gains.'}, 'zh': {'title': '推理计算扩展，提升RAG性能！', 'desc': '本文探讨了推理计算的扩展如何提升检索增强生成（RAG）模型的性能。我们提出了两种推理扩展策略：上下文学习和迭代提示，这些策略可以灵活地增加测试时的计算量。研究表明，合理配置推理计算时，RAG的性能几乎呈线性提升。我们还开发了计算分配模型，以预测在不同计算约束下的最佳推理参数，从而优化RAG的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.04081', 'title': '$ε$-VAE: Denoising as Visual Decoding', 'url': 'https://huggingface.co/papers/2410.04081', 'abstract': 'In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.', 'score': 7, 'issue_id': 48, 'pub_date': '2024-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '2006fe3703285810', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Диффузия как декодер: новый взгляд на визуальную токенизацию', 'desc': 'Статья предлагает новый подход к визуальной токенизации в генеративном моделировании. Вместо традиционной структуры автоэнкодера, авторы используют процесс диффузии в качестве декодера. Этот метод позволяет итеративно улучшать изображение, начиная с шума и руководствуясь латентными представлениями от энкодера. Исследователи оценивают качество реконструкции и генерации, сравнивая свой подход с современными методами автокодирования.'}, 'en': {'title': 'Iterative Denoising: A New Era in Generative Modeling', 'desc': 'This paper presents a novel approach to generative modeling by introducing a denoising process as a method of decoding. Instead of the traditional single-step reconstruction used in autoencoders, the authors propose an iterative refinement technique that utilizes a diffusion process to enhance image recovery. By leveraging latent representations from the encoder, the model progressively reduces noise to generate high-quality images. The effectiveness of this method is evaluated through metrics that assess both reconstruction fidelity and generation quality, showing potential improvements over existing autoencoding techniques.'}, 'zh': {'title': '迭代去噪，提升生成质量', 'desc': '在生成建模中，标记化将复杂数据简化为紧凑的结构化表示，从而创建更高效的可学习空间。对于高维视觉数据，它减少了冗余并强调关键特征，以实现高质量的生成。当前的视觉标记化方法依赖于传统的自编码器框架，其中编码器将数据压缩为潜在表示，解码器重建原始输入。我们提出了一种新的视角，通过将去噪作为解码，转变为迭代精炼的过程，以提高压缩和生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2410.02705', 'title': 'ControlAR: Controllable Image Generation with Autoregressive Models', 'url': 'https://huggingface.co/papers/2410.02705', 'abstract': "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model's efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at https://github.com/hustvl/ControlAR.", 'score': 7, 'issue_id': 28, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '455b112b5c885449', 'data': {'categories': ['#cv', '#benchmark', '#games', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'ControlAR: Революция в управляемой генерации изображений', 'desc': 'ControlAR - это новый эффективный метод для интеграции пространственного контроля в авторегрессионные модели генерации изображений. Он использует легковесный энкодер для преобразования пространственных входных данных в контрольные токены. ControlAR применяет условное декодирование для генерации следующего токена изображения на основе послойного слияния контрольных и изображенческих токенов. Этот подход позволяет генерировать изображения произвольного разрешения и превосходит предыдущие модели управляемой диффузии по качеству и эффективности.'}, 'en': {'title': 'ControlAR: Elevating Image Generation with Precision and Efficiency', 'desc': 'The paper introduces ControlAR, a new framework for autoregressive (AR) models that enhances control-to-image generation by integrating spatial controls. ControlAR uses a lightweight control encoder to convert spatial inputs like edges or depth maps into control tokens, which are then used in conditional decoding to improve image generation quality. This method allows AR models to generate images with better control and efficiency compared to previous methods like ControlNet. Experiments show that ControlAR not only improves controllability but also supports arbitrary-resolution image generation, outperforming existing diffusion models.'}, 'zh': {'title': 'ControlAR：自回归图像生成的新控制力', 'desc': '这篇论文介绍了一种名为ControlAR的新框架，用于在自回归图像生成模型中集成空间控制。通过引入轻量级控制编码器，将空间输入转换为控制标记，并使用条件解码方法生成图像标记。与预填充标记的方法相比，条件解码显著增强了自回归模型的控制能力，同时保持了模型的效率。实验表明，ControlAR在多种输入条件下的可控性优于现有的可控扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2410.04422', 'title': 'Hyper-multi-step: The Truth Behind Difficult Long-context Tasks', 'url': 'https://huggingface.co/papers/2410.04422', 'abstract': 'Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: "multi-matching retrieval," which requires the simultaneous retrieval of multiple items, and "logic-based retrieval," which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them.', 'score': 7, 'issue_id': 28, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '6077c1d0003654bc', 'data': {'categories': ['#reasoning', '#long_context', '#rag', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие корней сложности: почему языковые модели спотыкаются на длинном контексте', 'desc': 'Исследование посвящено языковым моделям с длинным контекстом (LCLM) и их трудностям в решении сложных задач. Авторы выявили две основные проблемы: многократное сопоставление при извлечении информации и логическое извлечение данных. Эти задачи оказываются гипермногоступенчатыми, что превышает возможности современных LCLM. Результаты исследования объясняют, почему языковые модели испытывают трудности с продвинутыми задачами на длинном контексте.'}, 'en': {'title': 'Unraveling the Complexity of Long-Context Language Models', 'desc': "The paper explores why long-context language models (LCLMs) struggle with certain complex tasks. It identifies two main challenges: 'multi-matching retrieval,' which involves retrieving multiple items at once, and 'logic-based retrieval,' which requires logical reasoning. These tasks are hyper-multi-step, meaning they need many steps to solve, which exceeds the current capabilities of LCLMs. Understanding these challenges helps in rethinking how to improve LCLMs for better performance on advanced tasks."}, 'zh': {'title': '揭示长上下文任务的隐藏挑战', 'desc': '这篇论文研究了长上下文语言模型（LCLM）在处理复杂任务时遇到的困难。研究发现，这些困难主要来自于两个基本问题：多匹配检索和基于逻辑的检索。尽管这些问题看似简单，但实际上需要超多步骤才能解决，超出了LCLM的能力范围。这一发现为重新思考解决方案提供了更准确的视角。'}}}, {'id': 'https://huggingface.co/papers/2410.05076', 'title': 'TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention', 'url': 'https://huggingface.co/papers/2410.05076', 'abstract': 'Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.', 'score': 6, 'issue_id': 33, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '299609c59ee293c9', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'TidalDecode: Эффективное декодирование LLM с сохранением позиции разреженного внимания', 'desc': 'Статья представляет TidalDecode - алгоритм для быстрого и точного декодирования больших языковых моделей (LLM) с помощью разреженного внимания с сохранением позиции. TidalDecode использует пространственную когерентность токенов, выбранных существующими методами разреженного внимания, и вводит несколько слоев выбора токенов с полным вниманием. Это позволяет значительно снизить накладные расходы на выбор токенов для разреженного внимания без ущерба для качества генерируемых результатов. Оценка на различных LLM и задачах показывает, что TidalDecode соответствует производительности методов с полным вниманием, уменьшая задержку декодирования LLM до 2,1 раза.'}, 'en': {'title': '"TidalDecode: Streamlining LLM Decoding with Smart Sparse Attention"', 'desc': 'The paper addresses the memory constraints in large language models (LLMs) caused by the expanding key-value cache size during the decoding phase. It critiques existing sparse attention mechanisms for not effectively identifying relevant tokens and ignoring spatial coherence across Transformer layers. The authors propose TidalDecode, an algorithm that uses position persistent sparse attention to improve token selection efficiency without compromising performance. TidalDecode significantly reduces decoding latency while maintaining high-quality results, as demonstrated in various LLM tasks.'}, 'zh': {'title': 'TidalDecode：提升大语言模型解码效率的新方法', 'desc': '这篇论文介绍了一种名为TidalDecode的新算法，用于提高大语言模型的解码效率。TidalDecode通过位置持久稀疏注意力机制，解决了现有稀疏注意力方法在选择相关词元时的不足。它在少数层中使用全注意力来识别高注意力分数的词元，而其他层则使用预选词元进行稀疏注意力。实验表明，TidalDecode在保持生成质量的同时，将解码延迟减少了最多2.1倍。'}}}, {'id': 'https://huggingface.co/papers/2410.03290', 'title': 'Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models', 'url': 'https://huggingface.co/papers/2410.03290', 'abstract': "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.", 'score': 6, 'issue_id': 29, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '6bcaebbfbd863fb6', 'data': {'categories': ['#reasoning', '#video', '#dataset', '#training', '#graphs', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Грануляция времени: новый уровень понимания видео для ИИ', 'desc': 'Grounded-VideoLLM - это новая модель Video-LLM, способная к детальному пониманию видео и временной локализации. Модель использует дополнительный временной поток для кодирования связей между кадрами и дискретные временные токены для представления временных меток. Обучение проводится поэтапно, начиная с простых задач описания видео и постепенно усложняясь до задач временной локализации. Эксперименты показывают эффективность Grounded-VideoLLM в задачах временной локализации предложений, плотного описания видео и ответов на вопросы с привязкой ко времени.'}, 'en': {'title': "Mastering Time: Grounded-VideoLLM's Leap in Video Understanding", 'desc': "The paper introduces Grounded-VideoLLM, a new model designed to improve fine-grained temporal understanding in videos. It addresses the limitations of existing Video-LLMs by adding a temporal stream to better capture frame relationships and using discrete temporal tokens for precise timestamp representation. The model is trained using a multi-stage approach, starting with simple tasks and gradually tackling more complex temporal grounding challenges. Additionally, a specialized dataset is created to enhance the model's temporal reasoning, resulting in superior performance in tasks like temporal sentence grounding and dense video captioning."}, 'zh': {'title': '细粒度视频理解的新突破', 'desc': '现有的视频大语言模型在理解视频的细节时存在困难，因为它们缺乏有效的时间建模和时间戳表示。为了解决这个问题，我们提出了一种新模型，Grounded-VideoLLM，通过增加时间流和离散时间标记来增强时间感知能力。我们采用多阶段训练策略，从简单的视频字幕任务开始，逐步引入复杂的视频时间定位任务。实验表明，Grounded-VideoLLM在细粒度任务中表现出色，并有潜力成为通用的视频助手。'}}}, {'id': 'https://huggingface.co/papers/2410.02743', 'title': 'MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions', 'url': 'https://huggingface.co/papers/2410.02743', 'abstract': 'Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .', 'score': 5, 'issue_id': 26, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '2db1c25c0ecd97fd', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#rl', '#plp', '#alignment', '#open_source', '#rlhf'], 'emoji': '🚀', 'ru': {'title': 'Ускоренное обучение языковых моделей с помощью макродействий', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Предложенный метод MA-RLHF использует макродействия - последовательности токенов или высокоуровневые языковые конструкции - для улучшения процесса обучения. Это позволяет решить проблему назначения кредита для длинных последовательностей и повысить эффективность обучения. Эксперименты показывают значительное улучшение производительности по сравнению со стандартным RLHF на различных задачах обработки естественного языка.'}, 'en': {'title': '"Boosting Learning Efficiency with Macro Actions in RLHF"', 'desc': 'The paper introduces MA-RLHF, a new framework for reinforcement learning from human feedback that uses macro actions to improve learning efficiency. By focusing on sequences of tokens or higher-level constructs, the approach addresses the credit assignment problem, making it easier to link actions to rewards. This method enhances the stability of policy gradient estimates and speeds up training without adding computational complexity. Experiments show significant performance improvements in tasks like text summarization and dialogue generation compared to traditional RLHF methods.'}, 'zh': {'title': '通过宏观动作提升强化学习效率', 'desc': '这篇论文介绍了一种新的强化学习方法，称为MA-RLHF，通过引入宏观动作来提高学习效率。宏观动作是指一系列的词或更高层次的语言结构，这样可以缩短动作与奖励之间的时间距离。通过这种方法，模型能够更快、更准确地进行信用分配，从而提高学习效率。实验结果表明，这种方法在文本摘要、对话生成、问答和程序合成等任务中表现优异，训练时间也大大缩短。'}}}, {'id': 'https://huggingface.co/papers/2410.03399', 'title': 'EBES: Easy Benchmarking for Event Sequences', 'url': 'https://huggingface.co/papers/2410.03399', 'abstract': 'Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.', 'score': 4, 'issue_id': 30, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'c88e217d2ab5a78c', 'data': {'categories': ['#survey', '#dataset', '#healthcare', '#graphs', '#data', '#optimization', '#benchmark', '#open_source', '#synthetic'], 'emoji': '📊', 'ru': {'title': 'EBES: Стандартизация оценки моделей для последовательностей событий', 'desc': 'Статья представляет EBES - инструмент для стандартизированного бенчмаркинга моделей, работающих с последовательностями событий. EBES фокусируется на задачах регрессии и классификации с целевыми переменными на уровне последовательности. Инструмент включает синтетический датасет и предобработанные реальные данные, в том числе крупнейший публично доступный банковский датасет. Авторы анализируют важность моделирования временных и последовательных компонентов, а также исследуют робастность и масштабируемость моделей.'}, 'en': {'title': 'Standardizing Event Sequence Evaluation with EBES', 'desc': 'The paper introduces EBES, a benchmarking tool designed to standardize the evaluation of machine learning models on event sequences, which are common in fields like healthcare and finance. EBES provides a unified interface for benchmarking, adding datasets, and integrating methods, making it easier to compare results across different studies. It includes both synthetic and real-world datasets, offering insights into which datasets are suitable for model comparison. The tool aims to promote reproducible research and accelerate progress in understanding the temporal and sequential aspects of event sequence data.'}, 'zh': {'title': 'EBES：事件序列数据评估的标准化工具', 'desc': '这篇论文介绍了一种名为EBES的工具，用于标准化事件序列数据的评估。事件序列数据在医疗、金融等领域很常见，但缺乏统一的评估标准。EBES提供了一个统一的接口，简化了基准测试和方法集成。研究结果强调了时间和序列建模的重要性，并为未来研究指明了方向。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (1)', '#architecture (9)', '#audio', '#benchmark (6)', '#cv (3)', '#data (2)', '#dataset (4)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs (2)', '#hallucinations', '#healthcare (1)', '#inference (3)', '#interpretability (1)', '#leakage', '#long_context (4)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (1)', '#open_source (5)', '#optimization (9)', '#plp (1)', '#rag (2)', '#reasoning (6)', '#rl (2)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (5)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-09 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-09 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-09 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    