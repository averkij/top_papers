
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (48 ÑÑ‚Ğ°Ñ‚ĞµĞ¹)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">48 ÑÑ‚Ğ°Ñ‚ĞµĞ¹</span></p>
        </div>
        <div class="theme-switch">f
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-08.html">â¬…ï¸ <span id="prev-date">08.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-10.html">â¡ï¸ <span id="next-date">10.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ</span>
            <span class="nav-item" id="nav-weekly">Ğ¢Ğ¾Ğ¿ Ğ·Ğ° Ğ¼ĞµÑÑÑ†</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ¾Ğ±Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        let feedDate = {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'};
        let feedDateNext = {'ru': '10.10', 'en': '10/10', 'zh': '10æœˆ10æ—¥'};
        let feedDatePrev = {'ru': '08.10', 'en': '10/08', 'zh': '10æœˆ8æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'Published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05254', 'title': 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments', 'url': 'https://huggingface.co/papers/2410.05254', 'abstract': "Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.", 'score': 60, 'issue_id': 41, 'pub_date': '2024-10-07', 'pub_date_ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#agents', '#multimodal'], 'emoji': 'ğŸ²', 'ru': {'title': 'LLM Ğ² ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ³Ñ€ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑ€ĞµĞ´Ñ‹ Ğ½Ğ° Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Standardizing LLMs in Economic Games: A New Benchmark for Fair Play', 'desc': 'This paper explores the behavior of Large Language Models (LLMs) in economic and strategic interactions, focusing on their rationality, mimicry of human behavior, and ability to achieve fair outcomes. The authors introduce a benchmark for standardizing research on two-player, sequential, language-based games, inspired by economic literature. They develop an open-source framework to simulate interactions and collect datasets of LLM vs. LLM and human vs. LLM interactions. The study aims to compare LLM behavior to human players, evaluate performance, and understand the impact of economic environments on agent behavior.'}, 'zh': {'title': 'æ ‡å‡†åŒ–LLMåœ¨ç»æµäº’åŠ¨ä¸­çš„è¡¨ç°è¯„ä¼°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»æµå’Œæˆ˜ç•¥äº’åŠ¨ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€äº¤æµä¸­ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ä¸ªåŸºå‡†ï¼Œç”¨äºæ ‡å‡†åŒ–ä¸¤äººé¡ºåºè¯­è¨€æ¸¸æˆçš„ç ”ç©¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°LLMçš„è¡¨ç°ã€‚é€šè¿‡å¼€å‘å¼€æºæ¡†æ¶å’Œæ”¶é›†æ•°æ®é›†ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ¯”è¾ƒLLMä¸äººç±»ç©å®¶åœ¨ä¸åŒç»æµç¯å¢ƒä¸­çš„è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ¡†æ¶å¯ä»¥æœ‰æ•ˆè¯„ä¼°LLMåœ¨ä¸ªä½“å’Œé›†ä½“è¡¨ç°ä¸Šçš„è¡¨ç°ï¼Œå¹¶é‡åŒ–ç»æµç¯å¢ƒç‰¹å¾å¯¹ä»£ç†è¡Œä¸ºçš„å½±å“ã€‚'}}, 'hash': 'ec706b594e8a3b49', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07113', 'title': 'Personalized Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.07113', 'abstract': 'Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.', 'score': 55, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#multimodal'], 'emoji': 'ğŸ‘¤', 'ru': {'title': "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ 'ÑĞ»ĞµĞ¿Ğ¾Ñ‚Ñ‹' Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸", 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PVIT (Personalized Visual Instruction Tuning), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²ĞµÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº P-Bench Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ.'}, 'en': {'title': "Breaking the 'Face Blindness' Barrier in Multimodal Models", 'desc': "The paper addresses the limitation of multimodal large language models (MLLMs) in recognizing and engaging with specific individuals, a problem termed as 'face blindness'. To overcome this, the authors propose Personalized Visual Instruction Tuning (PVIT), a framework that curates and trains MLLMs to identify individuals in images and conduct personalized dialogues. The framework uses a pipeline that autonomously generates training data by integrating visual experts, image generation models, and large language models. The effectiveness of this approach is validated through a benchmark called P-Bench, showing improved personalized performance after fine-tuning."}, 'zh': {'title': 'æ‰“ç ´â€œé¢ç›²ç—‡â€ï¼šä¸ªæ€§åŒ–è§†è§‰å¯¹è¯æ–°çªç ´', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨ä¸€ä¸ªæ˜¾è‘—çš„å±€é™ï¼Œå³â€œé¢ç›²ç—‡â€ï¼Œæ— æ³•è¿›è¡Œé’ˆå¯¹ç‰¹å®šä¸ªä½“çš„ä¸ªæ€§åŒ–å¯¹è¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä¸ªæ€§åŒ–è§†è§‰æŒ‡ä»¤è°ƒä¼˜ï¼ˆPVITï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿MLLMsèƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­çš„ç›®æ ‡ä¸ªä½“å¹¶è¿›è¡Œä¸ªæ€§åŒ–å¯¹è¯ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€æ¡å¤æ‚çš„æµæ°´çº¿ï¼Œè‡ªåŠ¨ç”ŸæˆåŒ…å«ä¸ªæ€§åŒ–å¯¹è¯çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åˆ©ç”¨è§†è§‰ä¸“å®¶ã€å›¾åƒç”Ÿæˆæ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡æˆ‘ä»¬ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†å¾®è°ƒåï¼ŒMLLMsçš„ä¸ªæ€§åŒ–æ€§èƒ½æ˜¾è‘—æå‡ã€‚'}}, 'hash': 'c923f3711f603f2c', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07171', 'title': 'IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.07171', 'abstract': 'Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rag', '#diffusion', '#cv'], 'emoji': 'ğŸ§©', 'ru': {'title': 'IterComp: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ ÑĞ¸Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'IterComp - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. IterComp Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'IterComp: Uniting Model Strengths for Superior Image Composition', 'desc': 'The paper introduces IterComp, a framework designed to improve text-to-image generation by combining the strengths of multiple diffusion models. It evaluates models based on their ability to handle attribute binding, spatial, and non-spatial relationships, creating a dataset to train reward models. IterComp uses an iterative feedback learning approach to refine both the base diffusion model and reward models, enhancing compositionality over time. The results show that IterComp outperforms existing methods in generating complex images with multiple objects and semantic alignment.'}, 'zh': {'title': 'IterCompï¼šæå‡ç»„åˆç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºIterCompçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç»„åˆèƒ½åŠ›ã€‚IterCompé€šè¿‡èšåˆå¤šä¸ªæ¨¡å‹çš„ç»„åˆåå¥½ï¼Œå¹¶é‡‡ç”¨è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•æ¥å¢å¼ºç»„åˆç”Ÿæˆã€‚æˆ‘ä»¬è¯„ä¼°äº†å…­ä¸ªå¼€æºæ‰©æ•£æ¨¡å‹çš„ä¸‰ä¸ªå…³é”®ç»„åˆæŒ‡æ ‡ï¼šå±æ€§ç»‘å®šã€ç©ºé—´å…³ç³»å’Œéç©ºé—´å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIterCompåœ¨å¤šç±»åˆ«å¯¹è±¡ç»„åˆå’Œå¤æ‚è¯­ä¹‰å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}, 'hash': 'aca07d3bd7236f38', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05993', 'title': 'Aria: An Open Multimodal Native Mixture-of-Experts Model', 'url': 'https://huggingface.co/papers/2410.05993', 'abstract': 'Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#architecture', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Aria: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ˜Ğ˜ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Aria - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ 3,9 Ğ¼Ğ»Ñ€Ğ´ Ğ¸ 3,5 Ğ¼Ğ»Ñ€Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Aria Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Pixtral-12B Ğ¸ Llama3.2-11B, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ 4-ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñƒ, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Aria: Open-Source Multimodal Mastery', 'desc': 'The paper introduces Aria, an open-source multimodal native AI model designed to integrate diverse types of information for a comprehensive understanding of real-world data. Aria is a mixture-of-expert model with billions of parameters, enabling it to excel in tasks involving language, multimodal data, and coding. It surpasses existing models like Pixtral-12B and Llama3.2-11B in performance and is competitive with top proprietary models. The model is pre-trained using a four-stage pipeline to enhance its capabilities, and its open-source nature allows for easy adoption and adaptation in various applications.'}, 'zh': {'title': 'Ariaï¼šå¼€æºå¤šæ¨¡æ€æ¨¡å‹çš„å“è¶Šè¡¨ç°', 'desc': 'Ariaæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€åŸç”Ÿæ¨¡å‹ï¼Œæ—¨åœ¨æ•´åˆå¤šç§ä¿¡æ¯æ¥æºä»¥æä¾›å…¨é¢çš„ç†è§£ã€‚å®ƒåœ¨è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†è®¸å¤šä¸“æœ‰æ¨¡å‹ã€‚Ariaé€šè¿‡å››é˜¶æ®µçš„é¢„è®­ç»ƒæµç¨‹ï¼Œä»é›¶å¼€å§‹é€æ­¥å¢å¼ºå…¶è¯­è¨€ç†è§£å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€æ”¾äº†Ariaçš„æ¨¡å‹æƒé‡å’Œä»£ç åº“ï¼Œæ–¹ä¾¿åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œé‡‡ç”¨å’Œé€‚åº”ã€‚'}}, 'hash': 'a04f813494758df0', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07073', 'title': 'Pixtral 12B', 'url': 'https://huggingface.co/papers/2410.07073', 'abstract': 'We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.', 'score': 34, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#cv', '#benchmark', '#long_context'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Pixtral-12B: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ', 'desc': 'Pixtral-12B - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Pixtral Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ½ÑƒĞ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ Ğ¸Ğ· 128 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': '"Pixtral-12B: Small but Mighty in Multimodal Mastery!"', 'desc': 'Pixtral-12B is a powerful multimodal language model with 12 billion parameters, designed to understand both images and text effectively. It achieves top performance on various benchmarks, outperforming larger models while maintaining strong natural language capabilities. The model features a new vision encoder that processes images at their natural resolution, offering flexibility in token usage and handling up to 128K tokens in its context window. Additionally, Pixtral-12B introduces an open-source benchmark for evaluating vision-language models, providing tools for standardized assessments.'}, 'zh': {'title': 'Pixtral-12Bï¼šå°ä½“ç§¯ï¼Œå¤§èƒ½é‡çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹', 'desc': 'Pixtral-12B æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 120 äº¿å‚æ•°çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶å›¾åƒå’Œæ–‡æ¡£ï¼Œå¹¶åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸è®¸å¤šå¼€æºæ¨¡å‹ä¸åŒï¼ŒPixtral åœ¨ä¸ç‰ºç‰²è‡ªç„¶è¯­è¨€æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚å®ƒä½¿ç”¨å…¨æ–°è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼Œå¯ä»¥ä»¥è‡ªç„¶åˆ†è¾¨ç‡å’Œçºµæ¨ªæ¯”å¤„ç†å›¾åƒï¼Œçµæ´»ä½¿ç”¨ 128K ä»¤ç‰Œçš„é•¿ä¸Šä¸‹æ–‡çª—å£å¤„ç†ä»»æ„æ•°é‡çš„å›¾åƒã€‚Pixtral-12B çš„æ€§èƒ½è¶…è¿‡äº†è®¸å¤šåŒç±»å¤§å°çš„å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä½“ç§¯ä¸Šæ¯”ä¸€äº›æ›´å¤§çš„æ¨¡å‹å° 7 å€ã€‚'}}, 'hash': '336b26133f29e630', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05363', 'title': 'Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation', 'url': 'https://huggingface.co/papers/2410.05363', 'abstract': "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench", 'score': 32, 'issue_id': 37, 'pub_date': '2024-10-07', 'pub_date_ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#video', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PhyGenBench: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PhyGenBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ (T2V). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 160 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 27 Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ PhyGenEval - Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ T2V Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñƒ ÑĞ¼Ñ‹ÑĞ»Ñƒ.'}, 'en': {'title': 'Building Smarter Simulations: Teaching AI Intuitive Physics', 'desc': "The paper introduces PhyGenBench, a benchmark designed to evaluate the ability of text-to-video models to understand and represent intuitive physics. It includes 160 prompts based on 27 physical laws to test models' grasp of physical commonsense. Alongside, PhyGenEval is proposed as a framework for assessing these models using advanced vision-language and large language models. The study finds that current models struggle with physical commonsense, suggesting that improvements are needed beyond just scaling or prompt engineering."}, 'zh': {'title': 'è¶…è¶Šå¨±ä¹ï¼šæå‡æ¨¡å‹çš„ç‰©ç†å¸¸è¯†ç†è§£', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhyGenBenchçš„ç‰©ç†ç”ŸæˆåŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨ç‰©ç†å¸¸è¯†æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆç¬¦åˆç‰©ç†å¸¸è¯†çš„è§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç®€å•åœ°æ‰©å¤§æ¨¡å‹è§„æ¨¡æˆ–ä½¿ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯å¹¶ä¸èƒ½å®Œå…¨è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„ç‰©ç†å¸¸è¯†ç†è§£èƒ½åŠ›ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶PhyGenEvalã€‚é€šè¿‡è¿™äº›å·¥å…·ï¼Œç ”ç©¶äººå‘˜å¸Œæœ›æ¨åŠ¨ç¤¾åŒºåœ¨æ¨¡å‹ä¸­ä¼˜å…ˆå­¦ä¹ ç‰©ç†å¸¸è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯å¨±ä¹åº”ç”¨ã€‚'}}, 'hash': '17a92a30d25bd139', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07167', 'title': 'Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate', 'url': 'https://huggingface.co/papers/2410.07167', 'abstract': 'We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.', 'score': 30, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'MIR: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Modality Integration Rate (MIR) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (Large Vision Language Models, LVLMs). MIR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MIR Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': '"MIR: A New Lens for Multi-Modal Model Mastery"', 'desc': 'The paper introduces the Modality Integration Rate (MIR), a new metric designed to evaluate the quality of multi-modal pre-training in Large Vision Language Models (LVLMs). Traditional metrics like loss and perplexity are not effective for assessing how well a language model aligns with new modalities during pre-training. MIR measures the inter-modal distribution distance, providing a more accurate indication of pre-training quality and its potential performance after fine-tuning. The authors demonstrate that MIR is effective, robust, and generalizable across different training setups, aiding in better data selection and model design.'}, 'zh': {'title': 'æ¨¡æ€æ•´åˆç‡ï¼šæå‡å¤šæ¨¡æ€é¢„è®­ç»ƒè´¨é‡çš„æ–°æŒ‡æ ‡', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œç§°ä¸ºæ¨¡æ€æ•´åˆç‡ï¼ˆMIRï¼‰ï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€é¢„è®­ç»ƒè´¨é‡ã€‚ä¼ ç»Ÿçš„é¢„è®­ç»ƒæŒ‡æ ‡å¦‚æŸå¤±å’Œå›°æƒ‘åº¦åœ¨ä¸æ–°æ¨¡æ€å¯¹é½æ—¶æ•ˆæœä¸ä½³ï¼Œè€ŒMIRå¯ä»¥æœ‰æ•ˆè¡¨ç¤ºé¢„è®­ç»ƒè´¨é‡ï¼Œå¹¶ä¸ç›‘ç£å¾®è°ƒåçš„åŸºå‡†æ€§èƒ½å‘ˆæ­£ç›¸å…³ã€‚MIRåœ¨ä¸åŒçš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä¸Šè¡¨ç°å‡ºç¨³å¥æ€§ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„è®­ç»ƒé…ç½®å’Œæ¶æ„é€‰æ‹©ä¸­æ¨å¹¿ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œä½œè€…å‘ç°MIRåœ¨è®­ç»ƒæ•°æ®é€‰æ‹©ã€è®­ç»ƒç­–ç•¥å®‰æ’å’Œæ¨¡å‹æ¶æ„è®¾è®¡ä¸­å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚'}}, 'hash': '8e1f683abf35b291', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06373', 'title': 'Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning', 'url': 'https://huggingface.co/papers/2410.06373', 'abstract': 'This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.', 'score': 23, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#cv', '#architecture', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼' (BOCB). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ CNN Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ¼ SGD-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ViT Ğ¸ ConvNeXt, Ñ‚ĞµÑĞ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ BOCB Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ°ÑÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Rethink Your Backbone: Optimizer Matters!', 'desc': "This paper explores how different vision model architectures, known as backbones, interact with various optimizers, revealing a phenomenon called backbone-optimizer coupling bias (BOCB). It finds that traditional convolutional neural networks (CNNs) like VGG and ResNet work best with stochastic gradient descent (SGD) optimizers, while newer models like Vision Transformers (ViTs) and ConvNeXt are more compatible with adaptive learning rate optimizers. The study shows that both the choice of optimizer and the design of the backbone can introduce BOCB, affecting the model's performance during pre-training and fine-tuning. The authors provide recommendations for choosing optimizers and insights into designing robust vision backbones, encouraging the community to rethink established practices."}, 'zh': {'title': 'é‡æ–°æ€è€ƒè§†è§‰éª¨å¹²ä¸ä¼˜åŒ–å™¨çš„è€¦åˆå…³ç³»', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è§†è§‰éª¨å¹²ç½‘ç»œå’Œä¼˜åŒ–å™¨ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªç§°ä¸ºéª¨å¹²-ä¼˜åŒ–å™¨è€¦åˆåå·®ï¼ˆBOCBï¼‰çš„ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¦‚VGGå’ŒResNetï¼‰ä¸SGDä¼˜åŒ–å™¨å®¶æ—æœ‰æ˜æ˜¾çš„ä¾èµ–å…³ç³»ï¼Œè€Œæ–°å‹æ¶æ„å¦‚ViTså’ŒConvNeXtåˆ™ä¸è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨ç´§å¯†è€¦åˆã€‚BOCBå¯ä»¥ç”±ä¼˜åŒ–å™¨å’ŒæŸäº›éª¨å¹²è®¾è®¡å¼•å…¥ï¼Œå¹¶å¯èƒ½æ˜¾è‘—å½±å“è§†è§‰æ¨¡å‹çš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸å¾®è°ƒã€‚é€šè¿‡æ·±å…¥çš„å®è¯åˆ†æï¼Œè®ºæ–‡æ€»ç»“äº†æ¨èçš„ä¼˜åŒ–å™¨å’Œç¨³å¥çš„è§†è§‰éª¨å¹²æ¶æ„çš„è§è§£ã€‚'}}, 'hash': 'bf22e906e7d30eea', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05954', 'title': 'Pyramidal Flow Matching for Efficient Video Generative Modeling', 'url': 'https://huggingface.co/papers/2410.05954', 'abstract': 'Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.', 'score': 22, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#video', '#diffusion'], 'emoji': 'ğŸï¸', 'ru': {'title': 'ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº ÑĞµÑ€Ğ¸Ñ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ³Ğ´Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ end-to-end Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Diffusion Transformer (DiT). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 5-10 ÑĞµĞºÑƒĞ½Ğ´ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 768p Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ 24 ĞºĞ°Ğ´Ñ€Ğ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ.'}, 'en': {'title': '"Efficient Video Generation with Pyramidal Flow Matching"', 'desc': 'This paper presents a novel approach to video generation by introducing a unified pyramidal flow matching algorithm. The method reinterprets the denoising process as a series of pyramid stages, optimizing only the final stage at full resolution to enhance efficiency. By linking flows across pyramid stages, the approach maintains continuity and supports autoregressive video generation with a temporal pyramid. The framework is optimized end-to-end using a single Diffusion Transformer, achieving high-quality video generation with reduced computational demands.'}, 'zh': {'title': 'é‡‘å­—å¡”æµåŒ¹é…ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‘å­—å¡”æµåŒ¹é…ç®—æ³•ï¼Œç”¨äºæ›´é«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆå»ºæ¨¡ã€‚é€šè¿‡å°†å»å™ªè½¨è¿¹é‡æ–°è§£é‡Šä¸ºä¸€ç³»åˆ—é‡‘å­—å¡”é˜¶æ®µï¼Œåªæœ‰æœ€åä¸€ä¸ªé˜¶æ®µåœ¨å…¨åˆ†è¾¨ç‡ä¸‹æ“ä½œï¼Œä»è€Œå‡å°‘è®¡ç®—å¤æ‚æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—¶é—´é‡‘å­—å¡”å‹ç¼©å…¨åˆ†è¾¨ç‡å†å²ï¼Œå®ç°è‡ªå›å½’è§†é¢‘ç”Ÿæˆã€‚æ•´ä¸ªæ¡†æ¶å¯ä»¥é€šè¿‡å•ä¸€çš„ç»Ÿä¸€æ‰©æ•£å˜å‹å™¨è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚'}}, 'hash': '365906882dc00532', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07177', 'title': 'MM-Ego: Towards Building Egocentric Multimodal LLMs', 'url': 'https://huggingface.co/papers/2410.07177', 'abstract': 'This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models\' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel "Memory Pointer Prompting" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.', 'score': 16, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#architecture'], 'emoji': 'ğŸ‘€', 'ru': {'title': 'MM-Ego: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Memory Pointer Prompting', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."}, 'en': {'title': 'Unlocking the Secrets of Egocentric Videos with MM-Ego', 'desc': 'This paper presents the development of a multimodal foundation model called MM-Ego for understanding egocentric videos. The researchers created a large dataset of 7 million QA samples to address the lack of data in this area, and introduced a challenging benchmark to test the model\'s ability to recognize and remember visual details. They also developed a new evaluation method to reduce language bias and proposed a novel architecture with a "Memory Pointer Prompting" mechanism to improve video comprehension. The result is a powerful model that enhances the understanding of long and complex egocentric video content.'}, 'zh': {'title': 'MM-Egoï¼šè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹', 'desc': 'è¿™é¡¹ç ”ç©¶æ—¨åœ¨æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œç”¨äºç†è§£ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ•°æ®å¼•æ“ï¼Œç”Ÿæˆäº†700ä¸‡é«˜è´¨é‡çš„é—®ç­”æ ·æœ¬ï¼Œè¿™æ˜¯ç›®å‰æœ€å¤§çš„è‡ªæˆ‘ä¸­å¿ƒé—®ç­”æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«629ä¸ªè§†é¢‘å’Œ7026ä¸ªé—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å»åè¯„ä¼°æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨çš„å¤šæ¨¡æ€æ¶æ„ï¼ŒåŒ…å«â€œè®°å¿†æŒ‡é’ˆæç¤ºâ€æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°ç†è§£é•¿è§†é¢‘å†…å®¹ã€‚'}}, 'hash': '46a93631041dba07', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05355', 'title': 'Falcon Mamba: The First Competitive Attention-free 7B Language Model', 'url': 'https://huggingface.co/papers/2410.05355', 'abstract': 'In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.', 'score': 15, 'issue_id': 40, 'pub_date': '2024-10-07', 'pub_date_ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#architecture', '#inference'], 'emoji': 'ğŸ¦…', 'ru': {'title': 'Falcon Mamba 7B: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Falcon Mamba 7B - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mamba. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 5,8 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Mistral 7B Ğ¸ Llama3.1 8B. Falcon Mamba 7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mamba ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Mamba Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba-Transformer.'}, 'en': {'title': 'Falcon Mamba 7B: Redefining Efficiency and Performance in Language Models', 'desc': 'Falcon Mamba 7B is a new large language model that uses the innovative Mamba architecture, trained on a vast dataset of 5.8 trillion tokens. It outperforms other leading models like Mistral 7B and Llama3.1 8B, and is comparable to Gemma 7B, showcasing the strength of the pure Mamba design. The model is efficient, requiring less memory and offering faster inference for long sequences, challenging the notion that hybrid models are superior. The weights for Falcon Mamba 7B are available for public use, promoting further research and development in the field.'}, 'zh': {'title': 'çº¯ Mamba æ¶æ„çš„åŠ›é‡ï¼šFalcon Mamba 7B çš„å“è¶Šè¡¨ç°', 'desc': 'Falcon Mamba 7B æ˜¯ä¸€ç§åŸºäºå…¨æ–° Mamba æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡ 5.8 ä¸‡äº¿ä¸ªæ ‡è®°çš„è®­ç»ƒã€‚ä¸å…¶ä»–åŸºäº Transformer çš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚å°½ç®¡æ··åˆæ¶æ„é€šå¸¸è¢«è®¤ä¸ºæ›´ä¼˜ï¼Œä½† Falcon Mamba 7B è¯æ˜äº†çº¯ Mamba æ¶æ„ä¹Ÿèƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Šæ··åˆæ¶æ„çš„æ•ˆæœã€‚è¯¥æ¨¡å‹çš„æƒé‡å·²åœ¨ Hugging Face ä¸Šå…¬å¼€ï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚'}}, 'hash': 'c700bbc81473edd9', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06244', 'title': 'Story-Adapter: A Training-free Iterative Framework for Long Story Visualization', 'url': 'https://huggingface.co/papers/2410.06244', 'abstract': 'Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#story_generation', '#diffusion', '#cv'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Story-Adapter: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Story-Adapter Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Story-Adapter Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Storytelling with Smarter Image Generation', 'desc': 'The paper introduces Story-Adapter, a framework designed to improve the generation of coherent images from long narratives using text-to-image models. It addresses challenges like maintaining semantic consistency and generating detailed interactions without extensive computational demands. The framework uses an iterative process with a global reference cross-attention module to refine images by incorporating text prompts and previously generated images. Experiments show that Story-Adapter enhances both the consistency and quality of images in long story visualizations.'}, 'zh': {'title': 'Story-Adapterï¼šæå‡é•¿ç¯‡æ•…äº‹å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStory-Adapterçš„æ¡†æ¶ï¼Œç”¨äºæå‡é•¿ç¯‡æ•…äº‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜ï¼Œé€šè¿‡è¿­ä»£çš„æ–¹å¼ä¼˜åŒ–æ¯ä¸€å¸§å›¾åƒã€‚æ ¸å¿ƒæŠ€æœ¯æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å…¨å±€å‚è€ƒäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStory-Adapteråœ¨é•¿ç¯‡æ•…äº‹çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œç»†ç²’åº¦äº¤äº’ç”Ÿæˆèƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚'}}, 'hash': '8289ac05d8c302b2', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06961', 'title': 'Self-Boosting Large Language Models with Synthetic Preference Data', 'url': 'https://huggingface.co/papers/2410.06961', 'abstract': 'Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rlhf', '#alignment'], 'emoji': 'ğŸ”„', 'ru': {'title': 'SynPO: ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SynPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. SynPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ SynPO, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama3-8B Ğ¸ Mistral-7B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ¸Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering LLMs with Self-Improving Synthetic Data', 'desc': "The paper introduces SynPO, a method that uses synthetic data to align Large Language Models (LLMs) with human preferences, making them more honest, harmless, and helpful. SynPO works by generating prompts and refining responses iteratively, allowing models to learn and improve without extensive human input. This self-boosting approach significantly enhances the performance of LLMs like Llama3-8B and Mistral-7B, as shown by their improved scores in various evaluations. Overall, SynPO reduces the need for large-scale human annotation while boosting the models' ability to follow instructions and perform diverse tasks."}, 'zh': {'title': 'SynPOï¼šç”¨åˆæˆæ•°æ®è‡ªæˆ‘æå‡è¯­è¨€æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSynPOçš„è‡ªæˆ‘æå‡æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚SynPOé€šè¿‡ç”Ÿæˆåˆæˆåå¥½æ•°æ®ï¼Œå‡å°‘äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è‡ªæˆ‘æç¤ºç”Ÿæˆå™¨å’Œå“åº”æ”¹è¿›å™¨ï¼Œé€æ­¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºã€‚ç»è¿‡å¤šæ¬¡è¿­ä»£ï¼Œæ¨¡å‹åœ¨ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚'}}, 'hash': 'cbb5c59488cb0db9', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07170', 'title': 'One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation', 'url': 'https://huggingface.co/papers/2410.07170', 'abstract': 'Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.', 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rl', '#cv', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'EVA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ EVA (Explained Variance Adaptation). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ LoRA, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. EVA Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€Ğ°Ğ½Ğ³Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'EVA: Boosting Model Fine-Tuning with Data-Driven Weight Initialization', 'desc': 'The paper introduces a new method called Explained Variance Adaptation (EVA) to improve the fine-tuning of foundation models. EVA enhances the low-rank adaptation (LoRA) technique by using data-driven initialization of weights through singular value decomposition. This approach allows for a more efficient distribution of ranks across weight matrices, leading to faster convergence and better performance. EVA is tested on various tasks, showing superior results compared to existing methods.'}, 'zh': {'title': 'è§£é‡Šæ–¹å·®é€‚åº”ï¼šæå‡åŸºç¡€æ¨¡å‹å¾®è°ƒæ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„ä½ç§©é€‚åº”æ–¹æ³•ï¼Œç§°ä¸ºè§£é‡Šæ–¹å·®é€‚åº”ï¼ˆEVAï¼‰ï¼Œç”¨äºå¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚EVAé€šè¿‡å¯¹æ¿€æ´»å‘é‡çš„å°æ‰¹é‡è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼Œä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼åˆå§‹åŒ–æ–°çš„æƒé‡çŸ©é˜µã€‚ç„¶åï¼Œä½¿ç”¨è·å¾—çš„å³å¥‡å¼‚å‘é‡åˆå§‹åŒ–LoRAçŸ©é˜µï¼Œå¹¶é‡æ–°åˆ†é…æ‰€æœ‰æƒé‡çŸ©é˜µçš„ç§©ï¼Œä»¥è§£é‡Šæœ€å¤§é‡çš„æ–¹å·®ã€‚å®éªŒè¡¨æ˜ï¼ŒEVAåœ¨å¤šç§ä»»åŠ¡ä¸­æ¯”å…¶ä»–æ–¹æ³•æ”¶æ•›æ›´å¿«ï¼Œå¹¶å–å¾—äº†æ›´é«˜çš„å¹³å‡åˆ†æ•°ã€‚'}}, 'hash': 'd987cfa2ee91a2b5', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05591', 'title': 'TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation', 'url': 'https://huggingface.co/papers/2410.05591', 'abstract': "Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts. Results and source code are in our anonymous project page.", 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#diffusion', '#video', '#cv', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'TweedieMix: Ğ¡Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'TweedieMix - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ½ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° Ğ¢Ğ²Ğ¸Ğ´Ğ¸ Ğ´Ğ»Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. TweedieMix Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'TweedieMix: Mastering Multi-Concept Image and Video Generation', 'desc': "The paper introduces TweedieMix, a new method for generating images and videos that combine multiple personalized concepts using diffusion models. It divides the reverse diffusion sampling process into two stages: the first ensures the inclusion of target objects, and the second blends custom concepts using Tweedie's formula. This approach results in higher fidelity images and videos compared to existing methods. Additionally, TweedieMix can be easily adapted for image-to-video diffusion models, enhancing its versatility."}, 'zh': {'title': 'TweedieMixï¼šå¤šæ¦‚å¿µå›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTweedieMixçš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨ç†é˜¶æ®µç»„åˆå®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡åˆ†æåå‘æ‰©æ•£é‡‡æ ·çš„ç‰¹æ€§ï¼Œè¯¥æ–¹æ³•å°†é‡‡æ ·è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨åˆå§‹é˜¶æ®µï¼Œåº”ç”¨å¤šå¯¹è±¡æ„ŸçŸ¥é‡‡æ ·æŠ€æœ¯ä»¥ç¡®ä¿ç›®æ ‡å¯¹è±¡çš„åŒ…å«ã€‚åœ¨åæœŸé˜¶æ®µï¼Œä½¿ç”¨Tweedieå…¬å¼åœ¨å»å™ªå›¾åƒç©ºé—´ä¸­èåˆå®šåˆ¶æ¦‚å¿µçš„å¤–è§‚ã€‚'}}, 'hash': '1d4bfe307ff6d748', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06166', 'title': 'Temporal Reasoning Transfer from Text to Video', 'url': 'https://huggingface.co/papers/2410.06166', 'abstract': "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.", 'score': 11, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rl', '#transfer_learning', '#video', '#reasoning'], 'emoji': 'â³', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (Video LLMs) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğµ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Textual Temporal reasoning Transfer (T3), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ T3 Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LongVA-7B Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Temporal Understanding in Video LLMs with Textual Insights', 'desc': "The paper explores the limitations of Video Large Language Models (Video LLMs) in understanding temporal changes and relationships in videos. It identifies that the main issue lies not in the video data itself but in the LLM's inherent difficulty with temporal concepts, as shown by poor performance on text-based temporal tasks. To address this, the authors introduce Textual Temporal reasoning Transfer (T3), which uses text-based tasks to improve temporal reasoning without video data. This approach significantly enhances the model's performance on video benchmarks, demonstrating the effectiveness of transferring temporal reasoning skills from text to video."}, 'zh': {'title': 'ä»æ–‡æœ¬åˆ°è§†é¢‘ï¼šæ—¶é—´æ¨ç†èƒ½åŠ›çš„è·¨åŸŸè½¬ç§»', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰åœ¨ç†è§£æ—¶é—´å˜åŒ–å’Œæ¨ç†æ—¶é—´å…³ç³»æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œé—®é¢˜çš„å…³é”®åœ¨äºåº•å±‚è¯­è¨€æ¨¡å‹å¯¹æ—¶é—´æ¦‚å¿µçš„ç†è§£å›°éš¾ï¼Œè€Œä¸æ˜¯è§†é¢‘è¡¨ç¤ºæœ¬èº«çš„ä¿¡æ¯ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºæ–‡æœ¬æ—¶é—´æ¨ç†è½¬ç§»ï¼ˆT3ï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä»ç°æœ‰çš„å›¾æ–‡æ•°æ®é›†ä¸­åˆæˆå¤šæ ·çš„çº¯æ–‡æœ¬æ—¶é—´æ¨ç†ä»»åŠ¡æ¥æå‡æ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒT3æ–¹æ³•åœ¨ä¸ä½¿ç”¨è§†é¢‘æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤æ‚æ—¶é—´åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚'}}, 'hash': '5cd9bf212c19151b', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07002', 'title': 'CursorCore: Assist Programming through Aligning Anything', 'url': 'https://huggingface.co/papers/2410.07002', 'abstract': 'Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.', 'score': 10, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#plp', '#benchmark', '#dataset'], 'emoji': 'ğŸ’»', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº APEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ pipeline Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CursorCore Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Revolutionizing Coding Assistance with Integrated Conversational Frameworks', 'desc': 'The paper introduces a new conversational framework for programming assistance that integrates coding history, current code, and user instructions to improve automation in tasks like code completion and editing. It presents a benchmark called APEval to evaluate how well models align with different information types and the quality of their outputs. A data generation pipeline, Programming-Instruct, is developed to synthesize training data from sources like GitHub, resulting in 219K samples used to fine-tune models. The resulting CursorCore models outperform others of similar size, enhancing coding assistants with features like inline chat and automated editing.'}, 'zh': {'title': 'CursorCoreï¼šç»Ÿä¸€ç¼–ç¨‹åŠ©æ‰‹çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯æ¡†æ¶ï¼Œå¯ä»¥å…¨é¢æ•´åˆç¼–ç¨‹è¿‡ç¨‹ä¸­çš„å„ç§ä¿¡æ¯æ¥æºï¼Œå¦‚ç¼–ç å†å²ã€å½“å‰ä»£ç å’Œç”¨æˆ·æŒ‡ä»¤ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•APEvalã€‚é€šè¿‡ä¸€ä¸ªåä¸ºProgramming-Instructçš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç ”ç©¶è€…ä»GitHubç­‰å¤šç§æ¥æºåˆæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶ç”Ÿæˆäº†219Kæ ·æœ¬ã€‚æœ€ç»ˆï¼Œå¼€å‘çš„CursorCoreç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–åŒç±»æ¨¡å‹ã€‚'}}, 'hash': '70a480b72654e749', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05295', 'title': 'AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs', 'url': 'https://huggingface.co/papers/2410.05295', 'abstract': 'In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.', 'score': 10, 'issue_id': 39, 'pub_date': '2024-10-03', 'pub_date_ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#agents', '#security'], 'emoji': 'ğŸ”“', 'ru': {'title': 'AutoDAN-Turbo: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ·Ğ»Ğ¾Ğ¼ Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'AutoDAN-Turbo - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 74.3% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4-1106-turbo AutoDAN-Turbo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 88.5% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, AutoDAN-Turbo Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ 93.4% Ğ½Ğ° GPT-4-1106-turbo.'}, 'en': {'title': 'AutoDAN-Turbo: Unleashing Automated AI Vulnerability Discovery', 'desc': 'AutoDAN-Turbo is a machine learning framework designed to automatically discover jailbreak strategies for AI models without human input. It significantly improves attack success rates, outperforming existing methods by 74.3% on average. The framework is versatile, allowing the integration of human-designed strategies to further enhance its effectiveness, achieving a 93.4% success rate on GPT-4-1106-turbo. This approach demonstrates the potential of automated systems in identifying vulnerabilities in AI models.'}, 'zh': {'title': 'AutoDAN-Turboï¼šè‡ªåŠ¨åŒ–ç ´è§£ç­–ç•¥çš„é©æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAutoDAN-Turboçš„é»‘ç®±ç ´è§£æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•äººä¸ºå¹²é¢„æˆ–é¢„å®šä¹‰èŒƒå›´çš„æƒ…å†µä¸‹ï¼Œä»å¤´å¼€å§‹è‡ªåŠ¨å‘ç°å°½å¯èƒ½å¤šçš„ç ´è§£ç­–ç•¥ã€‚AutoDAN-Turboåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡æ¯”åŸºçº¿æ–¹æ³•é«˜å‡º74.3%ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒåœ¨GPT-4-1106-turboä¸Šè¾¾åˆ°äº†88.5%çš„æ”»å‡»æˆåŠŸç‡ã€‚é€šè¿‡æ•´åˆç°æœ‰çš„äººä¸ºè®¾è®¡çš„ç ´è§£ç­–ç•¥ï¼ŒAutoDAN-Turboçš„æ”»å‡»æˆåŠŸç‡ç”šè‡³å¯ä»¥æé«˜åˆ°93.4%ã€‚'}}, 'hash': 'bc2003c7b895855f', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05651', 'title': 'ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler', 'url': 'https://huggingface.co/papers/2410.05651', 'abstract': 'Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.', 'score': 10, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#video', '#diffusion'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚ĞµĞ¹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº CFG++ Ğ¸ DDS, Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Smooth Transitions: Revolutionizing Video Interpolation with Bidirectional Sampling', 'desc': 'This paper introduces a new approach to improve video generation by focusing on keyframe interpolation using diffusion models. The authors propose a bidirectional sampling strategy that effectively addresses off-manifold issues without needing extensive re-noising. By conditioning on both start and end frames, the method ensures smoother and more coherent generation of intermediate frames. The integration of advanced guidance techniques, CFG++ and DDS, further enhances the quality and efficiency of the interpolation process, achieving state-of-the-art results.'}, 'zh': {'title': 'åŒå‘é‡‡æ ·ï¼šæå‡å…³é”®å¸§æ’å€¼çš„æ–°ç­–ç•¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŒå‘é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºæ”¹è¿›å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®å¸§æ’å€¼æ–¹é¢ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†ä¸¤ä¸ªæ¡ä»¶å¸§ï¼ˆèµ·å§‹å’Œç»“æŸå¸§ï¼‰æ—¶ï¼Œå¸¸å‡ºç°ç¦»å¼€æµå½¢çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä¸­é—´å¸§å‡ºç°ç‘•ç–µã€‚æ–°æ–¹æ³•é€šè¿‡åœ¨å‰å‘å’Œåå‘è·¯å¾„ä¸Šè¿›è¡Œé¡ºåºé‡‡æ ·ï¼Œç¡®ä¿ç”Ÿæˆçš„ä¸­é—´å¸§æ›´åŠ è¿è´¯å’Œç¬¦åˆæµå½¢ã€‚ç»“åˆå…ˆè¿›çš„æŒ‡å¯¼æŠ€æœ¯CFG++å’ŒDDSï¼Œè¯¥æ–¹æ³•åœ¨å…³é”®å¸§æ’å€¼ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}, 'hash': 'd350fdc5aa2b2edc', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06084', 'title': 'Diversity-Rewarded CFG Distillation', 'url': 'https://huggingface.co/papers/2410.06084', 'abstract': 'Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.', 'score': 8, 'issue_id': 42, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rl', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ diversity-rewarded CFG distillation. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Classifier-Free Guidance (CFG) Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MusicLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ CFG.'}, 'en': {'title': 'Boosting Creativity: Diverse Outputs Without Extra Cost', 'desc': 'This paper introduces a new method called diversity-rewarded CFG distillation to improve generative models used in creative fields like music generation. The approach combines a distillation objective, which helps the model mimic CFG-augmented predictions, with a reinforcement learning objective that rewards diversity in outputs. By finetuning the model, it achieves high-quality and diverse outputs without increasing inference costs. The method also allows for weight-based model merging, enabling control over the quality-diversity trade-off and enhancing performance.'}, 'zh': {'title': 'å¤šæ ·æ€§å¥–åŠ±è’¸é¦ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ä¸å¤šæ ·æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºå¤šæ ·æ€§å¥–åŠ±çš„CFGè’¸é¦ï¼Œæ—¨åœ¨è§£å†³CFGåœ¨ç”Ÿæˆå†…å®¹æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡ä¼˜åŒ–è’¸é¦ç›®æ ‡å’Œå¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œè¯¥æ–¹æ³•åœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæå‡äº†ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨MusicLMæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºè¿™ç§æ–¹æ³•åœ¨è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„CFGã€‚é€šè¿‡æƒé‡æ’å€¼ç­–ç•¥ï¼Œå¯ä»¥åœ¨éƒ¨ç½²æ—¶çµæ´»è°ƒæ•´ç”Ÿæˆå†…å®¹çš„è´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚'}}, 'hash': '99610ace8463ca57', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06885', 'title': 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', 'url': 'https://huggingface.co/papers/2410.06885', 'abstract': "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development.", 'score': 8, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#audio', '#multilingual', '#inference'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'F5-TTS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸', 'desc': 'F5-TTS - ÑÑ‚Ğ¾ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diffusion Transformer. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, F5-TTS Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´ Ğ´Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ConvNeXt Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Sway Sampling Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. F5-TTS Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑ‡Ğ¸, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'F5-TTS: Simplifying Speech Synthesis with Flow Matching and Diffusion Transformers', 'desc': 'The paper presents F5-TTS, a novel text-to-speech system that simplifies the process by using flow matching with a Diffusion Transformer, eliminating the need for complex components like duration models and phoneme alignment. By padding text inputs to match the length of speech inputs, the system performs denoising to generate speech, improving upon the slow convergence and low robustness of previous models. The introduction of ConvNeXt refines text representation, and a new Sway Sampling strategy enhances performance and efficiency, achieving faster training and inference times. F5-TTS demonstrates natural and expressive speech capabilities, including zero-shot ability and seamless code-switching, trained on a large multilingual dataset.'}, 'zh': {'title': 'F5-TTSï¼šæµç•…è‡ªç„¶çš„æ–‡æœ¬åˆ°è¯­éŸ³æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºF5-TTSçš„å…¨éè‡ªå›å½’æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿï¼ŒåŸºäºæµåŒ¹é…å’Œæ‰©æ•£å˜å‹å™¨ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œå®ƒä¸éœ€è¦å¤æ‚çš„è®¾è®¡ï¼Œå¦‚æ—¶é•¿æ¨¡å‹ã€æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³ç´ å¯¹é½ï¼Œè€Œæ˜¯é€šè¿‡å¡«å……æ ‡è®°æ¥ç®€åŒ–è¾“å…¥ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ¨ç†æ—¶çš„Swayé‡‡æ ·ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨ConvNeXtæ¥ä¼˜åŒ–æ–‡æœ¬è¡¨ç¤ºã€‚è¯¥ç³»ç»Ÿåœ¨å¤šè¯­è¨€æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç¤ºäº†è‡ªç„¶æµç•…çš„è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¸”ä»£ç å·²å…¬å¼€ä»¥ä¿ƒè¿›ç¤¾åŒºå‘å±•ã€‚'}}, 'hash': '753a8935b426d722', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05677', 'title': 'T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design', 'url': 'https://huggingface.co/papers/2410.05677', 'abstract': 'In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.', 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#video', '#diffusion', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: T2V-Turbo-v2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ T2V-Turbo-v2 Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ T2V Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ T2V-Turbo-v2 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ ÑÑ‡ĞµÑ‚Ğ¾Ğ¼ 85.13.'}, 'en': {'title': 'Turbocharge Your Text-to-Video Models with T2V-Turbo-v2!', 'desc': "The paper presents T2V-Turbo-v2, a method to improve text-to-video models by distilling a consistency model from a pretrained version. It uses high-quality data, feedback from reward models, and conditional guidance to enhance the model's performance. The study emphasizes the importance of using tailored datasets and diverse reward models to improve visual quality and text-video alignment. The approach also explores conditional guidance strategies, particularly focusing on motion guidance, to improve the motion quality of generated videos, achieving state-of-the-art results on VBench."}, 'zh': {'title': 'T2V-Turbo-v2ï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºT2V-Turbo-v2çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ä»é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­æå–ä¸€è‡´æ€§æ¨¡å‹æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€å¥–åŠ±æ¨¡å‹åé¦ˆå’Œæ¡ä»¶å¼•å¯¼ç­‰å¤šç§ç›‘ç£ä¿¡å·ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å’Œæ–‡æœ¬è§†é¢‘å¯¹é½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šå­¦ä¹ ç›®æ ‡å®šåˆ¶æ•°æ®é›†å’Œä»å¤šæ ·åŒ–å¥–åŠ±æ¨¡å‹ä¸­å­¦ä¹ æ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ã€‚é€šè¿‡è®¾è®¡æœ‰æ•ˆçš„èƒ½é‡å‡½æ•°ï¼Œæ”¹è¿›äº†è¿åŠ¨å¼•å¯¼ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨è´¨é‡ã€‚'}}, 'hash': 'e30f39e021f2ee4b', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05643', 'title': 'TRACE: Temporal Grounding Video LLM via Causal Event Modeling', 'url': 'https://huggingface.co/papers/2410.05643', 'abstract': "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE.", 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#video', '#multimodal', '#agents'], 'emoji': 'ğŸ¬', 'ru': {'title': 'TRACE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (Video Temporal Grounding). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TRACE - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ TRACE Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… VTG.'}, 'en': {'title': 'TRACE: Structuring Videos for Superior Understanding', 'desc': 'The paper introduces a new approach to Video Temporal Grounding (VTG) by using a causal event modeling framework, which structures videos as sequences of events. This framework allows for more effective video understanding by predicting current events based on previous ones, using timestamps, salient scores, and textual captions. The proposed model, TRACE, processes these components as distinct tasks with specialized encoders and decoders, improving the handling of VTG tasks. Experiments show that TRACE outperforms existing video language models, enhancing video browsing and editing capabilities.'}, 'zh': {'title': 'TRACEï¼šé©æ–°è§†é¢‘æ—¶é—´å®šä½çš„å› æœäº‹ä»¶å»ºæ¨¡', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å› æœäº‹ä»¶å»ºæ¨¡æ¡†æ¶ï¼Œç”¨äºè§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å°†è§†é¢‘è¡¨ç¤ºä¸ºäº‹ä»¶åºåˆ—ï¼Œé€šè¿‡å…ˆå‰äº‹ä»¶ã€è§†é¢‘è¾“å…¥å’Œæ–‡æœ¬æŒ‡ä»¤æ¥é¢„æµ‹å½“å‰äº‹ä»¶ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTRACEçš„æ–°å‹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå®ç°å› æœäº‹ä»¶å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRACEåœ¨å¤šç§è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}, 'hash': '7241bf4a739cd624', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07064', 'title': 'Data Selection via Optimal Control for Language Models', 'url': 'https://huggingface.co/papers/2410.07064', 'abstract': "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.", 'score': 8, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#data', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ñ Ñ†ĞµĞ»ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼Ğ° ĞŸĞ¾Ğ½Ñ‚Ñ€ÑĞ³Ğ¸Ğ½Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PDS Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PDS ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': '"Smart Data, Smarter Models: Optimizing Pre-training for Better AI"', 'desc': "This paper explores how to choose the best pre-training data from large datasets to improve language models (LMs) for future tasks. The authors use a mathematical approach called Pontryagin's Maximum Principle to find the best data selection strategy, which they call PMP-based Data Selection (PDS). Experiments show that PDS helps LMs learn faster and perform better on various tasks, even with very large models. Additionally, PDS makes better use of limited data, reducing the need for large datasets by almost half."}, 'zh': {'title': 'é€šè¿‡PMPä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»å¤§é‡è¯­æ–™åº“ä¸­é€‰æ‹©é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ•°æ®é€‰æ‹©é—®é¢˜è¡¨è¿°ä¸ºä¸€ä¸ªå¹¿ä¹‰çš„æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œå¹¶é€šè¿‡åºç‰¹é‡Œäºšé‡‘æœ€å¤§å€¼åŸç†ï¼ˆPMPï¼‰æ¥è§£å†³ï¼Œå¾—å‡ºæè¿°æœ€ä½³æ•°æ®é€‰æ‹©ä¸è¯­è¨€æ¨¡å‹è®­ç»ƒåŠ¨æ€å…³ç³»çš„å¿…è¦æ¡ä»¶ã€‚åŸºäºè¿™äº›ç†è®ºç»“æœï¼Œæˆ‘ä»¬å¼•å…¥äº†PMPæ•°æ®é€‰æ‹©ï¼ˆPDSï¼‰æ¡†æ¶ï¼Œé€šè¿‡è§£å†³PMPæ¡ä»¶æ¥è¿‘ä¼¼æœ€ä½³æ•°æ®é€‰æ‹©ã€‚åœ¨å®éªŒä¸­ï¼ŒPDSé€‰æ‹©çš„æ•°æ®åŠ é€Ÿäº†è¯­è¨€æ¨¡å‹çš„å­¦ä¹ ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æŒç»­æå‡å…¶æ€§èƒ½ã€‚'}}, 'hash': 'f8459aee9413c793', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06458', 'title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'url': 'https://huggingface.co/papers/2410.06458', 'abstract': 'Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs\' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs\' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM\'s response needs refinement. Our results show that DeCRIM improves Mistral\'s performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.', 'score': 7, 'issue_id': 43, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#dataset', '#rag'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RealInstruct - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ GPT-4 Ğ½Ğµ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 21% Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ pipeline DeCRIM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ DeCRIM Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Mistral Ğ½Ğ° 7.3% Ğ² RealInstruct Ğ¸ 8.0% Ğ² IFEval.'}, 'en': {'title': 'Mastering Multi-Constraint Instructions with DeCRIM', 'desc': 'The paper introduces RealInstruct, a benchmark designed to evaluate large language models (LLMs) on their ability to follow real-world instructions with multiple constraints. It highlights that even advanced models like GPT-4 struggle with such tasks, failing to meet at least one constraint in over 21% of cases. To improve performance, the authors propose the Decompose, Critique, and Refine (DeCRIM) pipeline, which breaks down instructions into constraints and uses a Critic model to refine responses. The study shows that DeCRIM significantly enhances the performance of open-source models, even surpassing GPT-4 with strong feedback.'}, 'zh': {'title': 'çªç ´å¤šé‡çº¦æŸï¼šæå‡LLMçš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºRealInstructçš„æ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤šé‡çº¦æŸæŒ‡ä»¤æ—¶çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„GPT-4æ¨¡å‹ï¼Œåœ¨21%ä»¥ä¸Šçš„æŒ‡ä»¤ä¸­ä¹Ÿæœªèƒ½æ»¡è¶³è‡³å°‘ä¸€ä¸ªçº¦æŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†DeCRIMè‡ªæˆ‘ä¿®æ­£æµç¨‹ï¼Œé€šè¿‡åˆ†è§£æŒ‡ä»¤å’Œä½¿ç”¨æ‰¹è¯„æ¨¡å‹æ¥æé«˜LLMçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDeCRIMæ˜¾è‘—æå‡äº†å¼€æºæ¨¡å‹çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†GPT-4ã€‚'}}, 'hash': 'f8cc2a4349e584a6', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.02465', 'title': 'Response Tuning: Aligning Large Language Models without Instruction', 'url': 'https://huggingface.co/papers/2410.02465', 'abstract': 'Instruction tuning-supervised fine-tuning using instruction-response pairs-is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs.', 'score': 7, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Response Tuning: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Response Tuning (RT) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. RT Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RT Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ¶Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Response Tuning: Unlocking LLM Potential with Focused Supervision', 'desc': 'The paper explores a method called Response Tuning (RT) to improve the performance of Large Language Models (LLMs) as chat assistants. Instead of using instruction-response pairs, RT focuses solely on supervising the response space, which helps the models respond effectively to various instructions. The study shows that RT-trained models can be as helpful as those trained with traditional instruction tuning. Additionally, by controlling the response distribution during training, these models can be guided to exhibit desired behaviors, such as refusing unsafe requests.'}, 'zh': {'title': 'å“åº”è°ƒä¼˜ï¼šé‡Šæ”¾é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è½¬å˜ä¸ºæœ‰ç”¨ä¸”å®‰å…¨çš„èŠå¤©åŠ©æ‰‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºå“åº”è°ƒä¼˜çš„æ–¹æ³•ï¼Œä¸“æ³¨äºå¯¹æ¨¡å‹è¾“å‡ºçš„ç›‘ç£ï¼Œè€Œä¸æ˜¯æŒ‡ä»¤è°ƒä¼˜ã€‚å®éªŒè¡¨æ˜ï¼Œä»…é€šè¿‡å“åº”è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å„ç§æŒ‡ä»¤ï¼Œå¹¶ä¸”è¡¨ç°å‡ºä¸æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ç›¸å½“çš„å¸®åŠ©æ€§ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé€šè¿‡æ§åˆ¶è®­ç»ƒå“åº”çš„åˆ†å¸ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”¨æˆ·åå¥½ï¼Œå¹¶åœ¨å¤„ç†ä¸å®‰å…¨æŸ¥è¯¢æ—¶è¡¨ç°å‡ºæ‹’ç»ååŠ©çš„è¡Œä¸ºã€‚'}}, 'hash': '0682a2364e4a8840', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.04223', 'title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'url': 'https://huggingface.co/papers/2410.04223', 'abstract': 'While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-05', 'pub_date_ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#multimodal', '#cv', '#graph', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Llamole: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Llamole - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Graph Diffusion Transformer Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹. Llamole Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ A* Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ 14 Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ 12 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Llamole: Bridging Text and Graphs for Smarter Molecular Design', 'desc': 'The paper introduces Llamole, a groundbreaking multimodal large language model that can generate both text and graphs, specifically designed for applications in materials and drug design. Llamole combines a base language model with advanced graph processing techniques like the Graph Diffusion Transformer and Graph Neural Networks to handle complex molecular generation and reaction inference. It also uses A* search with language model-based cost functions to improve retrosynthetic planning, making it more efficient. Extensive experiments show that Llamole outperforms other adapted language models in controllable molecular design and retrosynthetic planning, demonstrating its potential in these fields.'}, 'zh': {'title': 'Llamoleï¼šçªç ´å›¾å½¢ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹', 'desc': 'å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å›¾å½¢æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨ææ–™å’Œè¯ç‰©è®¾è®¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Llamoleï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬å’Œå›¾å½¢çš„å¤šæ¨¡æ€LLMã€‚Llamoleç»“åˆäº†å›¾æ‰©æ•£å˜å‹å™¨å’Œå›¾ç¥ç»ç½‘ç»œï¼Œå®ç°äº†åˆ†å­é€†å‘è®¾è®¡å’Œååº”æ¨æ–­ã€‚å®éªŒè¡¨æ˜ï¼ŒLlamoleåœ¨å¯æ§åˆ†å­è®¾è®¡å’Œé€†åˆæˆè§„åˆ’æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚'}}, 'hash': '9c153ad16c36d327', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.02503', 'title': 'Mixed-Session Conversation with Egocentric Memory', 'url': 'https://huggingface.co/papers/2410.02503', 'abstract': "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.", 'score': 6, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#agents', '#multimodal'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mixed-Session Conversation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²ĞµÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MiSC, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 6 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑÑĞ¸Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EMMA Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² MiSC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ², Ğ° EMMA ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Dialogue Systems with Multi-Session Dynamics', 'desc': 'The paper introduces a new dialogue system called Mixed-Session Conversation (MiSC) designed to handle dynamic, long-term interactions with multiple partners. It addresses the limitations of current systems by incorporating deeply layered interactions and expanded conversation networks. A novel dataset, MiSC, is used to train the system, featuring dialogue episodes with multiple sessions and speakers. The proposed model, EMMA, uses an innovative memory management mechanism to ensure continuity and coherence in conversations, validated through extensive human evaluations.'}, 'zh': {'title': 'å¤šæ–¹é•¿æ—¶é—´äº’åŠ¨çš„å¯¹è¯æ–°çºªå…ƒ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¯¹è¯ç³»ç»Ÿï¼Œç§°ä¸ºæ··åˆä¼šè¯å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„å¤šæ–¹é•¿æ—¶é—´äº’åŠ¨ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†MiSCï¼Œç”¨äºæ”¯æŒè¯¥ç³»ç»Ÿçš„å®ç°ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è¯åŒ…å«6ä¸ªè¿ç»­ä¼šè¯å’Œå››ä¸ªå‚ä¸è€…ã€‚ä¸ºäº†å¢å¼ºå¯¹è¯çš„è¿ç»­æ€§ï¼Œè®ºæ–‡ä¸­è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„å¯¹è¯æ¨¡å‹EMMAï¼Œå®ƒé€šè¿‡ä»ä¸»è¯´è¯è€…çš„è§†è§’æ”¶é›†å’Œç®¡ç†è®°å¿†æ¥å®ç°ã€‚é€šè¿‡å¹¿æ³›çš„äººç±»è¯„ä¼°ï¼Œè¯æ˜MiSCä¸­çš„å¯¹è¯å³ä½¿åœ¨ä¼šè¯ä¼™ä¼´å˜åŒ–æ—¶ä¹Ÿèƒ½ä¿æŒæµç•…ï¼ŒEMMAåœ¨æ•´ä¸ªå¯¹è¯ä¸­ä¿æŒé«˜è®°å¿†æ€§ä¸”æ— çŸ›ç›¾ã€‚'}}, 'hash': '87a67ed9acbc74a2', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06555', 'title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'url': 'https://huggingface.co/papers/2410.06555', 'abstract': "As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.", 'score': 6, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ING-VP: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ING-VP Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 6 Ğ¸Ğ³Ñ€ Ñ 300 ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ¸ 6 ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 60 000 Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ING-VP Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Claude-3.5 Sonnet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ 3.37%, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Pushing Boundaries: Evaluating Spatial Reasoning in MLLMs with ING-VP', 'desc': 'The paper introduces ING-VP, a new benchmark designed to test the spatial reasoning and multi-step planning abilities of multimodal large language models (MLLMs). It features interactive games that challenge models to understand and plan based on spatial relationships in images. The benchmark allows for various comparison settings, such as image-text versus text-only inputs, to evaluate different aspects of model performance. Despite testing several advanced models, the highest accuracy achieved was only 3.37%, highlighting the difficulty of the tasks and the need for further advancements in this area.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„ç©ºé—´æ¨ç†æ–°é«˜åº¦', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºING-VPï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç©ºé—´æƒ³è±¡å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ING-VPé€šè¿‡6ä¸ªä¸åŒçš„æ¸¸æˆå’Œ300ä¸ªå…³å¡ï¼Œæµ‹è¯•æ¨¡å‹åœ¨å›¾åƒä¸­çš„ç©ºé—´å…³ç³»ä¸Šçš„å¤šæ­¥è§„åˆ’èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¹Ÿè¿œä½äºé¢„æœŸï¼Œè¡¨æ˜å½“å‰æ¨¡å‹åœ¨å¤æ‚ç©ºé—´æ¨ç†å’Œè§„åˆ’æ–¹é¢è¿˜æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚è¿™ä¸ªå·¥ä½œæ—¨åœ¨æ¨åŠ¨MLLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„èƒ½åŠ›å‘å±•ã€‚'}}, 'hash': 'dabc7be39f163aa7', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05791', 'title': 'FÃ¼rElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance', 'url': 'https://huggingface.co/papers/2410.05791', 'abstract': 'Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.', 'score': 6, 'issue_id': 40, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#3d', '#rl', '#diffusion'], 'emoji': 'ğŸ¹', 'ru': {'title': 'Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ¾Ğ·Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ½Ğ° Ğ¿Ğ¸Ğ°Ğ½Ğ¸Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 10 Ñ‡Ğ°ÑĞ¾Ğ² 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€ÑƒĞº Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾Ñ‚ 15 Ğ¿Ğ¸Ğ°Ğ½Ğ¸ÑÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¸Ğ³Ñ€Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑÑŠĞµĞ¼ĞºĞ¸ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€ÑƒĞº Ğ¿Ñ€Ğ¸ Ğ¸Ğ³Ñ€Ğµ Ğ½Ğ° Ğ¿Ğ¸Ğ°Ğ½Ğ¸Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°.'}, 'en': {'title': 'Mastering Piano with AI: Crafting Realistic Hand Motions', 'desc': 'The paper introduces a novel dataset capturing 3D hand motions and audio from elite pianists, using advanced pose estimation and inverse kinematics to ensure accuracy. This dataset is used to develop a model that synthesizes realistic hand movements for piano playing, employing imitation and reinforcement learning for bimanual control. A diffusion model enhances sampling efficiency by generating reference motions, which are further refined using musical similarity to improve precision. The resulting model can produce natural and dexterous hand motions for musical pieces not included in the training data.'}, 'zh': {'title': 'é’¢ç´æ¼”å¥çš„æ‰‹éƒ¨åŠ¨ä½œå»ºæ¨¡æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«15ä½é¡¶çº§é’¢ç´å®¶æ¼”å¥153é¦–å¤å…¸éŸ³ä¹çš„3Dæ‰‹éƒ¨åŠ¨ä½œå’ŒéŸ³é¢‘æ•°æ®ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨æ— æ ‡è®°çš„å¤šè§†è§’è§†é¢‘å’Œå…ˆè¿›çš„å§¿æ€ä¼°è®¡æ¨¡å‹æ¥é‡å»ºè‡ªç„¶çš„æ¼”å¥åŠ¨ä½œï¼Œå¹¶é€šè¿‡é€†è¿åŠ¨å­¦å’Œé«˜åˆ†è¾¨ç‡MIDIæ•°æ®è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åˆæˆç‰©ç†åˆç†çš„æ‰‹éƒ¨åŠ¨ä½œçš„æµç¨‹ã€‚ä¸ºäº†æé«˜é‡‡æ ·æ•ˆç‡ï¼Œä»–ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè‡ªç„¶çš„å‚è€ƒåŠ¨ä½œï¼Œå¹¶é€šè¿‡éŸ³ä¹ç›¸ä¼¼æ€§å¢å¼ºæ•°æ®ï¼Œæå‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„ç²¾åº¦ã€‚'}}, 'hash': 'cba54e31692e353c', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06172', 'title': 'Multimodal Situational Safety', 'url': 'https://huggingface.co/papers/2410.06172', 'abstract': 'Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.', 'score': 6, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#agents'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MSSBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ MLLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ MLLM.'}, 'en': {'title': 'Ensuring Safety in Multimodal AI: A New Benchmark for Contextual Understanding', 'desc': 'This paper introduces the concept of Multimodal Situational Safety, focusing on how safety considerations change based on the context in which a Multimodal Large Language Model (MLLM) operates. The authors present a new benchmark, MSSBench, to evaluate how well MLLMs can assess safety by analyzing 1,820 language query-image pairs. The study finds that current MLLMs struggle with understanding and responding to situational safety challenges, indicating a need for further research in this area. To address these challenges, the authors propose multi-agent pipelines that improve safety performance over standard MLLM responses.'}, 'zh': {'title': 'å¤šæ¨¡æ€æƒ…å¢ƒå®‰å…¨ï¼šMLLMsçš„æ–°æŒ‘æˆ˜', 'desc': 'å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œå±•ç°å‡ºä½œä¸ºå¤šæ¨¡æ€åŠ©æ‰‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å®‰å…¨æ€§é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºå¹¶åˆ†æäº†ä¸€ç§æ–°çš„å®‰å…¨æŒ‘æˆ˜ï¼Œç§°ä¸ºå¤šæ¨¡æ€æƒ…å¢ƒå®‰å…¨ï¼Œç ”ç©¶äº†ç”¨æˆ·æˆ–ä»£ç†åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹çš„å®‰å…¨è€ƒé‡ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šæ¨¡æ€æƒ…å¢ƒå®‰å…¨åŸºå‡†ï¼ˆMSSBenchï¼‰ï¼Œç”¨äºè¯„ä¼°å½“å‰MLLMsçš„æƒ…å¢ƒå®‰å…¨æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„MLLMsåœ¨å¤„ç†è¿™ç§å¤æ‚çš„å®‰å…¨é—®é¢˜æ—¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤è·Ÿéšçš„æƒ…å†µä¸‹ã€‚'}}, 'hash': '9fee460887b3cb38', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05664', 'title': 'Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning', 'url': 'https://huggingface.co/papers/2410.05664', 'abstract': 'As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.', 'score': 6, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#diffusion', '#ethics'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': "Unlearning: Cleaning Up AI's Act", 'desc': "This paper explores the concept of model unlearning in text-to-image diffusion models, which aims to remove harmful or unwanted information from pre-trained models. The authors highlight that while unlearning is often evaluated based on the model's ability to generate specific concepts without losing image quality, it is usually tested in limited scenarios. They conduct a thorough analysis of unlearning across various situations, identifying that all current methods have side effects or limitations, particularly in complex environments. By providing a comprehensive evaluation framework, the authors aim to encourage further research to develop more reliable unlearning techniques."}, 'zh': {'title': 'æ¢ç´¢æ¨¡å‹é—å¿˜ï¼šæ¶ˆé™¤ä¸è‰¯ä¿¡æ¯çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'éšç€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œå®ƒä»¬åœ¨å•†ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›è¶Šæ¥è¶Šå¤§ï¼Œä½†ä¹Ÿå¼•å‘äº†å¯¹å…¶å¯èƒ½è¢«æ¶æ„ä½¿ç”¨çš„æ‹…å¿§ã€‚æ¨¡å‹é—å¿˜æŠ€æœ¯è¢«æå‡ºä»¥æ¶ˆé™¤é¢„è®­ç»ƒæ¨¡å‹ä¸­ä¸éœ€è¦çš„å’Œæ½œåœ¨æœ‰å®³çš„ä¿¡æ¯ã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é—å¿˜æ•ˆæœï¼Œå‘ç°æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶å‰¯ä½œç”¨æˆ–å±€é™æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå¸Œæœ›èƒ½æ¿€å‘æ›´å¤šå…³äºå¯é å’Œæœ‰æ•ˆçš„é—å¿˜æ–¹æ³•çš„ç ”ç©¶ã€‚'}}, 'hash': '9630c1ce042de601', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.02428', 'title': 'Collective Critics for Creative Story Generation', 'url': 'https://huggingface.co/papers/2410.02428', 'abstract': "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.", 'score': 5, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#story_generation', '#multimodal'], 'emoji': 'ğŸ“š', 'ru': {'title': 'CritiCS: ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ CritiCS. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹. CritiCS ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ° (CrPlan) Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° (CrText), Ğ³Ğ´Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° Ğ˜Ğ˜-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ»Ğ¸Ğ´ĞµÑ€ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ CritiCS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'CritiCS: Elevating Storytelling with Creative Collaboration', 'desc': 'The paper introduces a new framework called CritiCS for generating long stories using Large Language Models (LLMs). CritiCS consists of two stages: CrPlan for refining story plans and CrText for generating the story, both involving a group of LLM critics and a leader to enhance creativity and expressiveness. This approach not only maintains narrative coherence but also significantly boosts creativity and reader engagement, as shown by extensive human evaluations. Additionally, the framework supports interactive collaboration between human writers and the machine, allowing humans to participate actively in the critique process.'}, 'zh': {'title': 'CritiCSï¼šæå‡æ•…äº‹åˆ›é€ æ€§ä¸è¿è´¯æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶CritiCSï¼Œç”¨äºç”Ÿæˆå…·æœ‰åˆ›é€ æ€§å’Œå™è¿°è¿è´¯æ€§çš„é•¿ç¯‡æ•…äº‹ã€‚CritiCSæ¡†æ¶åŒ…æ‹¬è®¡åˆ’ä¼˜åŒ–é˜¶æ®µï¼ˆCrPlanï¼‰å’Œæ•…äº‹ç”Ÿæˆé˜¶æ®µï¼ˆCrTextï¼‰ï¼Œé€šè¿‡é›†ä½“ä¿®è®¢æœºåˆ¶æé«˜æ•…äº‹çš„åˆ›é€ æ€§å’Œå¸å¼•åŠ›ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„è®ºè€…å’Œä¸€ä¸ªé¢†å¯¼è€…åˆä½œï¼Œé€æ­¥å®Œå–„è®¡åˆ’å’Œæ•…äº‹è‰ç¨¿ã€‚äººç±»è¯„ä¼°è¡¨æ˜ï¼ŒCritiCSæ¡†æ¶æ˜¾è‘—æå‡äº†æ•…äº‹çš„åˆ›é€ æ€§å’Œè¯»è€…çš„å‚ä¸åº¦ï¼ŒåŒæ—¶ä¿æŒäº†å™è¿°çš„è¿è´¯æ€§ã€‚'}}, 'hash': 'fd2a584ee28c9b94', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07071', 'title': 'Retrieval-Augmented Decision Transformer: External Memory for In-context RL', 'url': 'https://huggingface.co/papers/2410.07071', 'abstract': "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.", 'score': 5, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rl', '#rag', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RA-DT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. RA-DT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing In-Context Learning with RA-DT: Efficient, Domain-Agnostic, and Powerful', 'desc': 'The paper introduces the Retrieval-Augmented Decision Transformer (RA-DT), a novel approach in Reinforcement Learning that enhances in-context learning by using an external memory to store and retrieve relevant sub-trajectories. Unlike previous methods that require entire episodes, RA-DT efficiently handles complex environments with long episodes and sparse rewards by focusing only on pertinent past experiences. The retrieval mechanism in RA-DT is domain-agnostic and does not need training, making it versatile across different tasks. Evaluations show that RA-DT outperforms existing methods in grid-worlds and other environments, highlighting its potential to overcome current limitations in in-context RL.'}, 'zh': {'title': 'æ£€ç´¢å¢å¼ºï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ£€ç´¢å¢å¼ºå†³ç­–å˜æ¢å™¨ï¼ˆRA-DTï¼‰ï¼Œç”¨äºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å®ç°ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚RA-DTé€šè¿‡å¤–éƒ¨è®°å¿†æœºåˆ¶å­˜å‚¨è¿‡å»çš„ç»éªŒï¼Œå¹¶æ£€ç´¢ä¸å½“å‰æƒ…å†µç›¸å…³çš„å­è½¨è¿¹ï¼Œä»è€Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­é•¿æ—¶é—´æ®µå’Œç¨€ç–å¥–åŠ±çš„é—®é¢˜ã€‚RA-DTåœ¨ç½‘æ ¼ä¸–ç•Œã€æœºå™¨äººæ¨¡æ‹Ÿå’Œç¨‹åºç”Ÿæˆçš„è§†é¢‘æ¸¸æˆä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†å½“å‰ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}, 'hash': '0815ef9a074b159d', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06241', 'title': 'BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way', 'url': 'https://huggingface.co/papers/2410.06241', 'abstract': 'The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'BroadWay: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BroadWay Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ°Ñ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. BroadWay Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Temporal Self-Guidance Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ Fourier-based Motion Enhancement Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'BroadWay: Elevating Text-to-Video Quality Without Extra Costs', 'desc': 'The paper introduces BroadWay, a method to enhance text-to-video generation by addressing common issues like structural implausibility and temporal inconsistency. It identifies that disparities in temporal attention maps lead to these inconsistencies and that the energy in these maps correlates with motion amplitude. BroadWay uses Temporal Self-Guidance to align attention maps for better video consistency and Fourier-based Motion Enhancement to boost motion richness. This approach improves video quality without adding extra parameters or increasing computational costs.'}, 'zh': {'title': 'BroadWayï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è§çš„é—®é¢˜ï¼Œå¦‚ç»“æ„ä¸åˆç†å’Œæ—¶é—´ä¸ä¸€è‡´ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒå—çš„æ—¶é—´æ³¨æ„åŠ›å›¾ä¹‹é—´çš„å·®å¼‚ä¸æ—¶é—´ä¸ä¸€è‡´æœ‰å…³ï¼Œè€Œæ³¨æ„åŠ›å›¾ä¸­çš„èƒ½é‡ä¸è§†é¢‘ä¸­çš„è¿åŠ¨å¹…åº¦æœ‰å…³ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸ºBroadWayçš„æ–¹æ³•ï¼Œé€šè¿‡å‡å°‘æ³¨æ„åŠ›å›¾çš„å·®å¼‚å’Œå¢å¼ºèƒ½é‡æ¥æé«˜è§†é¢‘è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒBroadWayåœ¨ä¸å¢åŠ é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚'}}, 'hash': 'b38bb081870f60d1', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06462', 'title': 'Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders', 'url': 'https://huggingface.co/papers/2410.06462', 'abstract': 'The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert\'s models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM\'s directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners\' safety policies when posed directly without the accompanying coding support request.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#security', '#hallucinations'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': "Ğ”Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ·ĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ API-ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ñ‚Ğ°Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞ¼ĞµĞ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ĞºĞ°Ğº Ğ½Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ñ‚Ğ°Ğº Ñ‚Ğ¸Ğ¿Ğ° 'living off the land' Ğ² Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğµ Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¼ ĞŸĞ."}, 'en': {'title': 'Exploiting Context: The Hidden Vulnerability in AI Code Recommendations', 'desc': "This paper explores how adversaries can exploit large language models (LLMs) to introduce malicious code into popular code repositories by manipulating context. It highlights that while LLMs from companies like OpenAI and Google have safeguards against harmful outputs, these can be bypassed when the context of a request changes, such as during programming challenges. The study provides examples of how repositories like GitHub and content delivery networks can be used to spread malicious code through seemingly innocent API recommendations. The research compares this method to 'living off the land' attacks, where LLMs inadvertently suggest harmful actions by misinterpreting user prompts."}, 'zh': {'title': 'åŸºç¡€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ¼æ´ï¼šç¼–ç¨‹æŒ‘æˆ˜ä¸­çš„éšæ‚£', 'desc': 'è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨æµè¡Œä»£ç åº“ä¸­å¼•å…¥æ¶æ„ä»£ç çš„å¯¹æŠ—æ€§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¤åˆ¶ä»£ç æˆ–è™šæ„çš„AIå»ºè®®ã€‚å°½ç®¡OpenAIã€Googleå’ŒAnthropicçš„åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æœ‰å®³è¡Œä¸ºå’Œæœ‰æ¯’å­—ç¬¦ä¸²æœ‰é˜²æŠ¤ï¼Œä½†åœ¨æ•°å­¦è§£å†³æ–¹æ¡ˆä¸­åµŒå…¥æœ‰å®³æç¤ºçš„å·¥ä½œè¡¨æ˜ï¼Œè¿™äº›é˜²æŠ¤æªæ–½åœ¨ä¸“å®¶ç¯å¢ƒä¸­å¯èƒ½æœ‰æ‰€ä¸åŒã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“é—®é¢˜çš„ä¸Šä¸‹æ–‡å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒåŸºç¡€æ¨¡å‹å¯èƒ½ä¼šæ”¾æ¾è­¦æƒ•ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³ç¼–ç¨‹æŒ‘æˆ˜æ—¶ã€‚é€šè¿‡å®è¯ä¾‹å­å±•ç¤ºäº†åœ¨GitHubã€NPMç­‰å¹³å°ä¸Šï¼Œæ¶æ„ä»£ç å¦‚ä½•åˆ©ç”¨è¿™äº›æ¼æ´è¿›è¡Œæ”»å‡»ã€‚'}}, 'hash': 'a2ceecf54aefe4d6', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06949', 'title': 'Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach', 'url': 'https://huggingface.co/papers/2410.06949', 'abstract': 'In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.', 'score': 5, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#agents', '#plp'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Seeker: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµÑ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Seeker - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹. Seeker Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ LLM Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': '"Seeker: Revolutionizing Exception Handling with AI"', 'desc': 'The paper addresses the challenge of poor exception handling in software development, which affects code robustness and reliability. It highlights the use of large language models (LLMs) to improve exception handling by identifying common issues like insensitive detection, inaccurate capture, and distorted handling of exceptions. The authors propose a multi-agent framework called Seeker, which includes agents like Scanner and Handler to assist LLMs in better managing exceptions. This study is the first to systematically explore using LLMs for enhancing exception handling, offering insights for improving software quality.'}, 'zh': {'title': 'ç”¨LLMsæå‡ä»£ç å¼‚å¸¸å¤„ç†çš„å¯é æ€§', 'desc': 'åœ¨è½¯ä»¶å¼€å‘ä¸­ï¼Œä¸å½“æˆ–ç¼ºå¤±çš„å¼‚å¸¸å¤„ç†ä¼šå½±å“ä»£ç çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ”¹å–„ä»£ç ä¸­çš„å¼‚å¸¸å¤„ç†ã€‚é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šè„†å¼±ä»£ç çš„æ£€æµ‹ä¸æ•æ„Ÿã€å¼‚å¸¸ç±»å‹æ•è·ä¸å‡†ç¡®ä»¥åŠå¤„ç†æ–¹æ¡ˆå¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSeekerçš„å¤šä»£ç†æ¡†æ¶ï¼Œå¸®åŠ©LLMsæ›´æœ‰æ•ˆåœ°æ£€æµ‹ã€æ•è·å’Œè§£å†³å¼‚å¸¸ã€‚'}}, 'hash': '7b96c448f8692316', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06985', 'title': 'Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control', 'url': 'https://huggingface.co/papers/2410.06985', 'abstract': 'Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.', 'score': 4, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ PBR Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Collaborative Control Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Text-to-Texture Ñ PBR (Physically Based Rendering). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Collaborative Control Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ PBR-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Achieving Multi-View Harmony in PBR Text-to-Texture Models', 'desc': 'The paper addresses the challenge of achieving multi-view consistency in image diffusion models, particularly in the Text-to-Texture problem where geometric correspondences are known. It introduces a Collaborative Control workflow that directly models PBR image probability distributions, including normal bump maps, making it unique in its ability to output full PBR stacks. The authors discuss the design choices that enhance multi-view consistency and validate their approach through ablation studies and practical applications. This work aims to improve the alignment of predictions across views, reducing the need for complex fusion methods.'}, 'zh': {'title': 'å®ç°å¤šè§†å›¾ä¸€è‡´æ€§çš„PBRå›¾åƒæ‰©æ•£æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°çº¹ç†é—®é¢˜ä¸­ã€‚å³ä½¿åœ¨å‡ ä½•å¯¹åº”å…³ç³»å·²çŸ¥çš„æƒ…å†µä¸‹ï¼Œè®¸å¤šæ–¹æ³•ä»æ— æ³•åœ¨ä¸åŒè§†å›¾ä¸­ç”Ÿæˆå¯¹é½çš„é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨PBRæ–‡æœ¬åˆ°çº¹ç†çš„åä½œæ§åˆ¶å·¥ä½œæµç¨‹ä¸­è§£å†³è¿™ä¸€é—®é¢˜çš„æ–¹æ³•ã€‚é€šè¿‡ç›´æ¥å»ºæ¨¡PBRå›¾åƒæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç›´æ¥è¾“å‡ºå®Œæ•´çš„PBRå †æ ˆï¼Œå¹¶åœ¨æ¶ˆèç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚'}}, 'hash': '0007650863977d09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06845', 'title': 'MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders', 'url': 'https://huggingface.co/papers/2410.06845', 'abstract': 'Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main', 'score': 4, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#rl', '#medicine', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MentalArena: Ğ˜Ğ˜-Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MentalArena - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Symptom Encoder Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Symptom Decoder Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MentalArena Ğ½Ğ° GPT-3.5 Ğ¸ Llama-3-8b, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ Ğ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ².'}, 'en': {'title': 'Revolutionizing Mental Health Care with AI-Powered Personalization', 'desc': 'The paper introduces MentalArena, a self-play framework designed to train language models for diagnosing and treating mental health disorders by generating personalized data. It uses a Symptom Encoder to simulate realistic patient behavior and cognition, and a Symptom Decoder to manage dialogue by comparing diagnosed and encoded symptoms. The framework was tested against six benchmarks and outperformed advanced models like GPT-4o, showing its effectiveness in personalized mental health care. This work aims to inspire further research in personalized treatment models while addressing privacy concerns in mental health data.'}, 'zh': {'title': 'MentalArenaï¼šä¸ªæ€§åŒ–å¿ƒç†å¥åº·è¯Šç–—çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMentalArenaçš„è‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥ç”Ÿæˆç‰¹å®šé¢†åŸŸçš„ä¸ªæ€§åŒ–æ•°æ®ï¼Œä»è€Œæ”¹å–„å¿ƒç†å¥åº·è¯Šæ–­å’Œæ²»ç–—ã€‚ä¸ºäº†æ¨¡æ‹ŸçœŸå®çš„å¿ƒç†å¥åº·æ‚£è€…ï¼Œç ”ç©¶è€…è®¾è®¡äº†ç—‡çŠ¶ç¼–ç å™¨ï¼Œä»è®¤çŸ¥å’Œè¡Œä¸ºè§’åº¦è¿›è¡Œæ¨¡æ‹Ÿã€‚ä¸ºäº†è§£å†³æ‚£è€…ä¸æ²»ç–—å¸ˆäº’åŠ¨ä¸­çš„æ„å›¾åå·®ï¼Œæå‡ºäº†ç—‡çŠ¶è§£ç å™¨ï¼Œä»¥åŠ¨æ€ç®¡ç†å¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡GPT-3.5å’ŒLlama-3-8bå¾®è°ƒçš„æ¨¡å‹åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚'}}, 'hash': 'd89e94812dda796d', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06524', 'title': 'Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA', 'url': 'https://huggingface.co/papers/2410.06524', 'abstract': 'Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.', 'score': 4, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#alignment', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'CAIMIRA - Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñƒ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ˜Ğ˜ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 300 000 Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ ~70 ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜ Ğ¸ 155 Ğ»ÑĞ´ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ›ÑĞ´Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ˜Ğ˜ Ğ² Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': "Bridging the Gap: Enhancing AI's Cognitive Abilities", 'desc': 'The paper introduces CAIMIRA, a framework using item response theory to evaluate the problem-solving skills of AI and human question-answering agents. By analyzing responses from both AI systems and humans, the study reveals that humans excel in knowledge-grounded abductive and conceptual reasoning. In contrast, AI systems like GPT-4 and LLaMA perform better in tasks involving targeted information retrieval and fact-based reasoning. The research suggests that future AI development should focus on enhancing higher-order reasoning and nuanced linguistic interpretation to better mimic human cognitive abilities.'}, 'zh': {'title': 'AIä¸äººç±»ï¼šé—®ç­”èƒ½åŠ›çš„è¾ƒé‡', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯ä¸äººç±»çš„æ¯”è¾ƒã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºCAIMIRAçš„æ–°æ¡†æ¶ï¼ŒåŸºäºé¡¹ç›®ååº”ç†è®ºï¼Œç”¨äºå®šé‡è¯„ä¼°å’Œæ¯”è¾ƒäººç±»å’ŒAIç³»ç»Ÿåœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è§£é¢˜èƒ½åŠ›ã€‚é€šè¿‡åˆ†æå¤§é‡AIç³»ç»Ÿå’Œäººç±»çš„å›ç­”ï¼Œå‘ç°äººç±»åœ¨çŸ¥è¯†åŸºç¡€çš„æ¨ç†å’Œæ¦‚å¿µæ¨ç†ä¸Šè¡¨ç°æ›´å¥½ï¼Œè€Œå…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢å’Œäº‹å®æ¨ç†ä¸Šæ›´èƒœä¸€ç­¹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœªæ¥çš„é—®ç­”ä»»åŠ¡éœ€è¦æ›´å…³æ³¨æŒ‘æˆ˜é«˜é˜¶æ¨ç†å’Œç§‘å­¦æ€ç»´çš„é—®é¢˜ï¼Œä»¥æ¨åŠ¨AIæ›´å¥½åœ°æ¨¡æ‹Ÿæˆ–è¡¥å……äººç±»çš„è®¤çŸ¥èƒ½åŠ›ã€‚'}}, 'hash': '5eec66236f57298a', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07062', 'title': 'TinyEmo: Scaling down Emotional Reasoning via Metric Projection', 'url': 'https://huggingface.co/papers/2410.07062', 'abstract': 'This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems.   We release code, models, and dataset at https://github.com/ggcr/TinyEmo', 'score': 3, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#dataset', '#data', '#multimodal', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TinyEmo: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'TinyEmo - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Metric Projector Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. TinyEmo ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡ĞµĞ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'TinyEmo: Small Models, Big Emotional Insights', 'desc': 'TinyEmo is a new set of small language models designed to understand and classify emotions efficiently. It uses a special dataset to train the models and a unique tool called the Metric Projector to make training faster and easier. These models are smaller but still perform better than larger models, thanks to their efficient design. They also help detect biases in AI systems, making them more reliable and fair.'}, 'zh': {'title': 'TinyEmoï¼šå°æ¨¡å‹ï¼Œå¤§æƒ…æ„Ÿ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†TinyEmoï¼Œä¸€ç§ç”¨äºæƒ…æ„Ÿæ¨ç†å’Œåˆ†ç±»çš„å°å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚TinyEmoä½¿ç”¨åˆæˆæƒ…æ„ŸæŒ‡ä»¤æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œå¹¶é€šè¿‡åº¦é‡æŠ•å½±å™¨æé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨ä½¿ç”¨æ›´å°‘å‚æ•°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¿‡äº†ä¸€äº›æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåº¦é‡æŠ•å½±å™¨è¿˜æä¾›äº†æ¨¡å‹å¯è§£é‡Šæ€§å’Œåå·®æ£€æµ‹çš„èƒ½åŠ›ã€‚'}}, 'hash': '354409ec230f329b', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07160', 'title': 'TextToon: Real-Time Text Toonify Head Avatar from Single Video', 'url': 'https://huggingface.co/papers/2410.07160', 'abstract': 'We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.', 'score': 3, 'issue_id': 40, 'pub_date': '2024-09-23', 'pub_date_ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#3d', '#cv', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'TextToon: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚ÑÑˆĞ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'TextToon - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚ÑÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Tri-plane Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ğ² Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Gaussian Splatting Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹. TextToon Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': '"Animate Your Avatar: Real-Time Toonification from Text and Video"', 'desc': 'TextToon is a novel method for creating animated avatars from a single video and a text description of the desired style. Unlike previous methods that require multiple camera angles and result in static models, TextToon uses a conditional embedding Tri-plane to create dynamic and stylized facial representations. The approach enhances 3D Gaussian Splatting with an adaptive pixel-translation neural network and patch-aware contrastive learning, resulting in high-quality, real-time animations. The system is efficient, running at 48 FPS on GPUs and 15-18 FPS on mobile devices, making it suitable for consumer applications.'}, 'zh': {'title': 'TextToonï¼šå®æ—¶ç”Ÿæˆå¯é©±åŠ¨å¡é€šåŒ–å¤´åƒçš„åˆ›æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTextToonçš„æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆå¯é©±åŠ¨çš„å¡é€šåŒ–å¤´åƒã€‚é€šè¿‡ä¸€ä¸ªçŸ­çš„å•ç›®è§†é¢‘åºåˆ—å’Œå…³äºå¤´åƒé£æ ¼çš„ä¹¦é¢æŒ‡ä»¤ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¡é€šåŒ–å¤´åƒï¼Œå¹¶èƒ½é€šè¿‡å¦ä¸€ä¸ªè§†é¢‘å®æ—¶é©±åŠ¨ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒTextToonä¸ä¾èµ–å¤šè§†è§’å»ºæ¨¡ï¼Œè€Œæ˜¯é‡‡ç”¨æ¡ä»¶åµŒå…¥ä¸‰å¹³é¢æ¥å­¦ä¹ çœŸå®å’Œé£æ ¼åŒ–çš„é¢éƒ¨è¡¨ç¤ºã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”åƒç´ ç¿»è¯‘ç¥ç»ç½‘ç»œå’Œåˆ©ç”¨è¡¥ä¸æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼ŒTextToonå®ç°äº†é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚'}}, 'hash': '425ed10e0d4ec2cd', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05873', 'title': 'MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment', 'url': 'https://huggingface.co/papers/2410.05873', 'abstract': 'English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.', 'score': 2, 'issue_id': 45, 'pub_date': '2024-10-08', 'pub_date_ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'MEXA: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'MEXA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ĞºĞ°Ğº ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ñ€Ğ¾Ğ´Ğ° ÑĞ·Ñ‹Ğº-Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸Ğº Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…. MEXA Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ (0,90) Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Multilingual Potential in English-Centric Models', 'desc': 'The paper introduces MEXA, a method to evaluate the multilingual capabilities of English-centric large language models (LLMs) using parallel sentences. MEXA leverages the use of English as a pivot language in LLMs to assess language understanding transfer to non-English languages. The method shows a high correlation with established multilingual benchmarks, indicating its reliability in estimating model performance across languages. This approach provides insights into the multilingual potential and internal mechanisms of LLMs.'}, 'zh': {'title': 'MEXAï¼šæ­ç¤ºè‹±è¯­ä¸­å¿ƒè¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMEXAçš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚MEXAåˆ©ç”¨å¹³è¡Œå¥å­æ¥è®¡ç®—è‹±è¯­ä¸éè‹±è¯­è¯­è¨€ä¹‹é—´çš„å¯¹é½ï¼Œä»è€Œè¯„ä¼°è¯­è¨€ç†è§£çš„è½¬ç§»æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMEXAåœ¨é»˜è®¤è®¾ç½®ä¸‹ï¼Œä¸ä¸‰ä¸ªå·²å»ºç«‹çš„ä¸‹æ¸¸ä»»åŠ¡çš„å¹³å‡çš®å°”é€Šç›¸å…³ç³»æ•°è¾¾åˆ°0.90ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°ä¸­çš„å¯é æ€§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥æ›´æ¸…æ™°åœ°äº†è§£è¿™äº›æ¨¡å‹çš„å¤šè¯­è¨€æ½œåŠ›å’Œå†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚'}}, 'hash': '98dd8fcc39110307', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07112', 'title': 'VHELM: A Holistic Evaluation of Vision Language Models', 'url': 'https://huggingface.co/papers/2410.07112', 'abstract': 'Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.', 'score': 1, 'issue_id': 47, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'VHELM: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ 9 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼', 'desc': 'VHELM - ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° HELM Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 9 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. VHELM ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': '"VHELM: A New Standard for Fair and Comprehensive VLM Evaluation"', 'desc': 'The paper introduces VHELM, a comprehensive framework for evaluating vision-language models (VLMs) across nine critical aspects, including fairness, multilinguality, and toxicity. By standardizing evaluation procedures and metrics, VHELM allows for fair comparisons between different models. The initial evaluation of 22 VLMs reveals that efficiency-focused models perform worse on bias benchmarks compared to their full counterparts. VHELM aims to be an evolving benchmark, continuously incorporating new datasets and models to provide a holistic view of VLM capabilities.'}, 'zh': {'title': 'VHELMï¼šå…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°æ ‡å‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶VHELMï¼Œç”¨äºå…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¤šæ–¹é¢èƒ½åŠ›ã€‚VHELMé€šè¿‡æ•´åˆå¤šä¸ªæ•°æ®é›†ï¼Œæ¶µç›–è§†è§‰æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ã€å…¬å¹³æ€§ã€å¤šè¯­è¨€æ€§ã€é²æ£’æ€§ã€æ¯’æ€§å’Œå®‰å…¨æ€§ç­‰ä¹ä¸ªæ–¹é¢ã€‚è¯¥æ¡†æ¶æ ‡å‡†åŒ–äº†æ¨ç†å‚æ•°ã€æç¤ºæ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä½¿å¾—æ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒæ›´åŠ å…¬å¹³ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œæ³¨é‡æ•ˆç‡çš„æ¨¡å‹åœ¨åè§æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨å…¶ä»–æ–¹é¢æ²¡æœ‰æ˜æ˜¾å·®è·ã€‚'}}, 'hash': 'f9db5fd76663d260', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.06468', 'title': 'Does Spatial Cognition Emerge in Frontier Models?', 'url': 'https://huggingface.co/papers/2410.06468', 'abstract': 'Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.', 'score': 1, 'issue_id': 42, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SPACE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜', 'desc': 'SPACE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ»ĞµÑ‚Ğ½Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞµ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼ĞµĞ»ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‚Ğ°ĞºÑƒÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ.'}, 'en': {'title': "Bridging the Gap: Evaluating AI's Spatial Smarts", 'desc': "The paper introduces SPACE, a benchmark designed to assess spatial cognition in advanced machine learning models. It draws from cognitive science to evaluate models' abilities in large-scale mapping, object shape reasoning, and spatial attention and memory. The benchmark uses both text and images to test large language and multimodal models. Findings indicate that current models struggle with spatial tasks, performing poorly compared to animals."}, 'zh': {'title': 'SPACEï¼šè¯„ä¼°æ¨¡å‹ç©ºé—´è®¤çŸ¥çš„æ–°åŸºå‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSPACEçš„åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿåœ°è¯„ä¼°å‰æ²¿æ¨¡å‹çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŸºäºè®¤çŸ¥ç§‘å­¦çš„å¤šå¹´ç ”ç©¶ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤§è§„æ¨¡åœ°å›¾æ„å»ºã€å°è§„æ¨¡ç‰©ä½“å½¢çŠ¶å’Œå¸ƒå±€æ¨ç†ä»¥åŠç©ºé—´æ³¨æ„åŠ›å’Œè®°å¿†ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ–‡æœ¬å’Œå›¾åƒçš„å¹³è¡Œå±•ç¤ºï¼ŒSPACEå¯ä»¥åŒæ—¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å‰æ²¿æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢ä»ä¸å¦‚åŠ¨ç‰©ï¼Œåœ¨è®¸å¤šç»å…¸çš„åŠ¨ç‰©è®¤çŸ¥æµ‹è¯•ä¸­è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ã€‚'}}, 'hash': 'b568df3a7211e7fa', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07095', 'title': 'MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering', 'url': 'https://huggingface.co/papers/2410.07095', 'abstract': "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.", 'score': 1, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'MLE-bench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸', 'desc': 'MLE-bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 75 ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Kaggle, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ ML-Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OpenAI o1-preview Ñ AIDE, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ±Ñ€Ğ¾Ğ½Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Kaggle Ğ² 16.9% ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': "Testing AI's Engineering Prowess with MLE-bench", 'desc': "MLE-bench is a new benchmark designed to evaluate how well AI agents can perform machine learning engineering tasks. It includes 75 competitions from Kaggle that test skills like model training, dataset preparation, and experiment execution. The benchmark uses human performance baselines from Kaggle leaderboards and evaluates AI models, finding that OpenAI's o1-preview with AIDE scaffolding performs at a bronze medal level in 16.9% of tasks. The study also explores how resource scaling affects AI performance and the influence of pre-training data contamination, with the benchmark code available for public use."}, 'zh': {'title': 'è¯„ä¼°AIåœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹ä¸­çš„è¡¨ç°', 'desc': 'æˆ‘ä»¬æ¨å‡ºäº†MLE-benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°AIä»£ç†åœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹ä¸­è¡¨ç°çš„åŸºå‡†ã€‚æˆ‘ä»¬ä»Kaggleä¸Šç²¾é€‰äº†75ä¸ªä¸MLå·¥ç¨‹ç›¸å…³çš„ç«èµ›ï¼Œæ¶µç›–äº†è®­ç»ƒæ¨¡å‹ã€å‡†å¤‡æ•°æ®é›†å’Œè¿è¡Œå®éªŒç­‰å®é™…æŠ€èƒ½ã€‚é€šè¿‡ä½¿ç”¨Kaggleçš„å…¬å¼€æ’è¡Œæ¦œï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªç«èµ›å»ºç«‹äº†äººç±»åŸºçº¿ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨OpenAIçš„o1-previewä¸AIDEæ¡†æ¶çš„ç»„åˆåœ¨16.9%çš„ç«èµ›ä¸­è¾¾åˆ°äº†Kaggleé“œç‰Œæ°´å¹³ã€‚'}}, 'hash': '103d4f2b02c940a9', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.05160', 'title': 'VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks', 'url': 'https://huggingface.co/papers/2410.05160', 'abstract': "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. Our results show that \\model achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-07', 'pub_date_ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ MMEB - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 36 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ 4 Ğ¼ĞµÑ‚Ğ°-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VLM2Vec - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° MMEB. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VLM2Vec Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 10-20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'VLM2Vec: Bridging Vision and Language for Universal Embeddings', 'desc': 'This paper introduces a new approach to creating universal multimodal embedding models that can handle a variety of tasks. The authors present MMEB, a comprehensive benchmark covering multiple tasks and datasets, and VLM2Vec, a framework that transforms vision-language models into versatile embedding models. VLM2Vec stands out by generating fixed-dimensional vectors from any combination of images and text, outperforming existing models like CLIP and BLIP. The results demonstrate significant improvements in performance across different datasets, highlighting the potential of VLM2Vec in advancing multimodal embeddings.'}, 'zh': {'title': 'é€šç”¨åµŒå…¥æ¨¡å‹ï¼šè·¨ä»»åŠ¡çš„å¤šæ¨¡æ€å¤„ç†', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ„å»ºé€šç”¨åµŒå…¥æ¨¡å‹çš„å¯èƒ½æ€§ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥å¤„ç†å¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºMMEBçš„å¤§è§„æ¨¡å¤šæ¨¡æ€åµŒå…¥åŸºå‡†ï¼Œæ¶µç›–äº†å››ä¸ªå…ƒä»»åŠ¡å’Œ36ä¸ªæ•°æ®é›†ã€‚è®ºæ–‡è¿˜ä»‹ç»äº†VLM2Vecï¼Œä¸€ç§å¯¹æ¯”è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥å°†ä»»ä½•å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºåµŒå…¥æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM2Vecåœ¨MMEBçš„è¯„ä¼°ä¸­æ¯”ç°æœ‰çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹æé«˜äº†10%åˆ°20%ã€‚'}}, 'hash': '36a094d66cdd6a3e', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}}, {'id': 'https://huggingface.co/papers/2410.07145', 'title': 'Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling', 'url': 'https://huggingface.co/papers/2410.07145', 'abstract': "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-09', 'pub_date_ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'data': {'categories': ['#inference', '#architecture', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RNN Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (RNN) Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ 'ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ' (state collapse), Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ñ€ĞµĞ·ĞºĞ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba-2 Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ¾Ğ»ĞµĞ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ."}, 'en': {'title': 'Unlocking RNNs for Long Sequences: Overcoming State Collapse', 'desc': "Recurrent neural networks (RNNs) are faster than transformer models for long sequences due to their linear computational complexity, but they struggle with sequences longer than those they were trained on. This paper identifies two main issues: the inability to handle longer sequences than trained on, and limited memory capacity. The authors explore 'state collapse,' where performance drops for longer sequences, and attribute it to overfitting. They propose three methods to improve RNNs' ability to handle longer sequences, showing that RNNs can be effective for long-context tasks with proper adjustments."}, 'zh': {'title': 'RNNé•¿åºåˆ—å¤„ç†çš„æœªæ¥æ½œåŠ›', 'desc': 'é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ—¶æ¯”åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹æ›´å¿«ï¼Œå› ä¸ºå®ƒä»¬çš„è®¡ç®—å¤æ‚åº¦ä¸åºåˆ—é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…¬å¼€çš„RNNåœ¨å¤„ç†è¶…è¿‡1ä¸‡æ ‡è®°çš„é•¿åºåˆ—æ—¶æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡ç ”ç©¶äº†RNNåœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†å…³é”®çš„æ”¹è¿›æªæ–½ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¿‡æ‹Ÿåˆå’ŒçŠ¶æ€å´©æºƒæ˜¯ä¸»è¦é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸‰ç§æ–¹æ³•æ¥æé«˜Mamba-2æ¨¡å‹çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚'}}, 'hash': '955fb73ec92505eb', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (8)', '#agi', '#alignment (3)', '#architecture (5)', '#audio (2)', '#benchmark (19)', '#cv (12)', '#data (3)', '#dataset (10)', '#diffusion (9)', '#edge_computing', '#ethics (1)', '#games', '#graph', '#graphs', '#hallucinations (1)', '#inference (3)', '#interpretability (2)', '#math', '#medicine (1)', '#multilingual (2)', '#multimodal (20)', '#optimization (2)', '#plp (2)', '#quantum', '#rag (3)', '#reasoning (3)', '#rl (6)', '#rlhf (1)', '#robotics', '#security (2)', '#story_generation (2)', '#survey', '#training (13)', '#transfer_learning (1)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">ğŸ“ ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-10 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-10 20:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    