
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (48 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        header {
            padding: 3em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p>9 октября | 48 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-08.html">⬅️ 08.10</a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-10.html">➡️ 10.10</a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05254', 'title': 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments', 'url': 'https://huggingface.co/papers/2410.05254', 'abstract': "Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.", 'score': 60, 'issue_id': 41, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новый бенчмарк для исследования поведения больших языковых моделей (LLM) в экономических и стратегических взаимодействиях. Авторы разработали фреймворк для симуляции и анализа игр между двумя участниками, основанных на естественном языке. Исследование включает сравнение поведения LLM-агентов с человеческими игроками в различных экономических контекстах. Результаты позволяют оценить индивидуальную и коллективную эффективность агентов, а также влияние экономических характеристик среды на их поведение.', 'tags': ['#LLM-агенты', '#экономические_игры', '#стратегические_взаимодействия'], 'emoji': '🎲', 'title': 'LLM в экономических играх: новый бенчмарк для оценки рациональности и эффективности', 'categories': ['#benchmark', '#agents', '#multimodal']}, 'hash': 'ec706b594e8a3b49'}, {'id': 'https://huggingface.co/papers/2410.07113', 'title': 'Personalized Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.07113', 'abstract': 'Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.', 'score': 55, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) для персонализированного общения. Авторы предлагают метод PVIT (Personalized Visual Instruction Tuning), который позволяет моделям идентифицировать конкретных людей на изображениях и вести персонализированный диалог. Для оценки эффективности метода разработан бенчмарк P-Bench с различными типами вопросов. Эксперименты показывают значительное улучшение персонализированной производительности после файнтюнинга на специально подготовленном датасете.', 'tags': ['#персонализированные_MLLM', '#визуальный_диалог', '#PVIT'], 'emoji': '👤', 'title': "Преодоление 'слепоты' мультимодальных моделей: персонализированное общение с изображениями", 'categories': ['#dataset', '#data', '#benchmark', '#multimodal']}, 'hash': 'c923f3711f603f2c'}, {'id': 'https://huggingface.co/papers/2410.07171', 'title': 'IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.07171', 'abstract': 'Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'IterComp - это новый фреймворк для улучшения композиционной генерации изображений по тексту. Он агрегирует предпочтения нескольких моделей диффузии и использует итеративное обучение с обратной связью. IterComp оценивает модели по трем ключевым метрикам композиции и создает набор данных для обучения моделей вознаграждения. Метод показывает превосходство над современными подходами, особенно в композиции объектов нескольких категорий и сложном семантическом выравнивании.', 'tags': ['#композиционнаяГенерация', '#итеративноеОбучение', '#диффузионныеМодели'], 'emoji': '🧩', 'title': 'IterComp: Объединяя силы моделей для идеальной композиции изображений', 'categories': ['#rag', '#diffusion', '#cv', '#training']}, 'hash': 'aca07d3bd7236f38'}, {'id': 'https://huggingface.co/papers/2410.05993', 'title': 'Aria: An Open Multimodal Native Mixture-of-Experts Model', 'url': 'https://huggingface.co/papers/2410.05993', 'abstract': 'Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет Aria - открытую мультимодальную модель искусственного интеллекта. Модель использует архитектуру смеси экспертов и имеет 3,9 млрд и 3,5 млрд активируемых параметров для визуальных и текстовых токенов соответственно. Aria превосходит Pixtral-12B и Llama3.2-11B, конкурируя с лучшими проприетарными моделями в различных мультимодальных задачах. Модель обучается по 4-этапному конвейеру, постепенно приобретая сильные способности в понимании языка, мультимодальном понимании, работе с длинным контекстом и следовании инструкциям.', 'tags': ['#мультимодальный_ИИ', '#смесь_экспертов', '#открытая_модель'], 'emoji': '🧠', 'title': 'Aria: открытая мультимодальная модель ИИ с производительностью мирового класса', 'categories': ['#multimodal', '#architecture', '#training']}, 'hash': 'a04f813494758df0'}, {'id': 'https://huggingface.co/papers/2410.07073', 'title': 'Pixtral 12B', 'url': 'https://huggingface.co/papers/2410.07073', 'abstract': 'We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.', 'score': 34, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Pixtral-12B - это мультимодальная языковая модель с 12 миллиардами параметров, способная понимать как изображения, так и текст. Она превосходит более крупные модели по различным мультимодальным показателям, при этом не уступая в задачах обработки естественного языка. Pixtral использует новый энкодер изображений, обученный с нуля, что позволяет обрабатывать изображения в их естественном разрешении и соотношении сторон. Модель может обрабатывать любое количество изображений в контекстном окне из 128 тысяч токенов.', 'tags': ['#мультимодальное_обучение', '#компьютерное_зрение', '#обработка_естественного_языка'], 'emoji': '🖼️', 'title': 'Pixtral-12B: Революция в мультимодальном искусственном интеллекте', 'categories': ['#multimodal', '#cv', '#benchmark']}, 'hash': '336b26133f29e630'}, {'id': 'https://huggingface.co/papers/2410.05363', 'title': 'Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation', 'url': 'https://huggingface.co/papers/2410.05363', 'abstract': "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench", 'score': 32, 'issue_id': 37, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет PhyGenBench - комплексный бенчмарк для оценки корректности физического здравого смысла в моделях текст-в-видео (T2V). Бенчмарк включает 160 промптов, охватывающих 27 физических законов в четырех фундаментальных областях. Авторы также предлагают PhyGenEval - новую структуру оценки, использующую передовые модели компьютерного зрения и большие языковые модели. Результаты показывают, что текущие T2V модели испытывают трудности с генерацией видео, соответствующих физическому здравому смыслу.', 'tags': ['#PhyGenBench', '#T2V', '#ИнтуитивнаяФизика'], 'emoji': '🧠', 'title': 'PhyGenBench: оценка физического здравого смысла в моделях текст-в-видео', 'categories': ['#benchmark', '#video', '#cv']}, 'hash': '17a92a30d25bd139'}, {'id': 'https://huggingface.co/papers/2410.07167', 'title': 'Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate', 'url': 'https://huggingface.co/papers/2410.07167', 'abstract': 'We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.', 'score': 30, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'В статье представлен новый метрический показатель Modality Integration Rate (MIR) для оценки качества предварительного обучения мультимодальных моделей компьютерного зрения и обработки естественного языка (Large Vision Language Models, LVLMs). MIR позволяет эффективно оценивать качество обучения без необходимости дорогостоящей процедуры дообучения с учителем. Метрика основана на измерении межмодального расстояния распределений и демонстрирует положительную корреляцию с результатами тестирования после дообучения. MIR показывает устойчивость к различным конфигурациям обучения и архитектурам моделей.', 'tags': ['#мультимодальныемодели', '#предобучение', '#метрикикачества'], 'emoji': '🔬', 'title': 'MIR: новый способ оценки качества мультимодальных моделей', 'categories': ['#multimodal', '#benchmark']}, 'hash': '8e1f683abf35b291'}, {'id': 'https://huggingface.co/papers/2410.06373', 'title': 'Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning', 'url': 'https://huggingface.co/papers/2410.06373', 'abstract': 'This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.', 'score': 23, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': "Статья исследует взаимосвязь между архитектурами нейронных сетей для компьютерного зрения и оптимизаторами, выявляя феномен 'смещения связи между архитектурой и оптимизатором' (BOCB). Авторы обнаружили, что классические CNN лучше работают с семейством SGD-оптимизаторов, а современные архитектуры, такие как ViT и ConvNeXt, тесно связаны с адаптивными оптимизаторами. Исследование показывает, что BOCB может влиять на предварительное обучение и дообучение моделей компьютерного зрения. На основе эмпирического анализа авторы дают рекомендации по выбору оптимизаторов и созданию устойчивых архитектур нейронных сетей для задач компьютерного зрения.", 'tags': ['#backboneOptimizerCoupling', '#visionArchitectures', '#optimizerSelection'], 'emoji': '🔍', 'title': 'Раскрывая тайны взаимосвязи архитектур и оптимизаторов в компьютерном зрении', 'categories': ['#cv', '#architecture', '#optimization', '#training']}, 'hash': 'bf22e906e7d30eea'}, {'id': 'https://huggingface.co/papers/2410.05954', 'title': 'Pyramidal Flow Matching for Efficient Video Generative Modeling', 'url': 'https://huggingface.co/papers/2410.05954', 'abstract': 'Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.', 'score': 22, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет новый алгоритм пирамидального сопоставления потоков для генерации видео. Этот метод переосмысливает траекторию шумоподавления как серию пирамидальных этапов, где только финальный этап работает в полном разрешении. Подход позволяет оптимизировать весь процесс end-to-end с использованием единого Diffusion Transformer (DiT). Эксперименты показывают, что метод способен генерировать высококачественные видео длительностью 5-10 секунд с разрешением 768p и частотой 24 кадра в секунду.', 'tags': ['#video-generation', '#flow-matching', '#diffusion-transformer'], 'emoji': '🎞️', 'title': 'Пирамидальное сопоставление потоков: новый подход к эффективной генерации видео', 'categories': ['#video', '#diffusion']}, 'hash': '365906882dc00532'}, {'id': 'https://huggingface.co/papers/2410.07177', 'title': 'MM-Ego: Towards Building Egocentric Multimodal LLMs', 'url': 'https://huggingface.co/papers/2410.07177', 'abstract': 'This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models\' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel "Memory Pointer Prompting" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.', 'score': 16, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': "Исследование направлено на создание мультимодальной модели для понимания эгоцентрического видео. Авторы разработали генератор данных, создающий 7 миллионов высококачественных пар вопрос-ответ для эгоцентрических видео различной длительности. Они также представили новый бенчмарк для оценки способности моделей распознавать и запоминать визуальные детали в видео разной длины. Предложенная авторами архитектура включает механизм 'Memory Pointer Prompting', который позволяет модели эффективнее понимать длинные видео.", 'tags': ['#EgocentricVideoUnderstanding', '#MultimodalFoundationModel', '#MemoryPointerPrompting'], 'emoji': '👀', 'title': 'MM-Ego: Мультимодальная модель для глубокого понимания эгоцентрического видео', 'categories': ['#dataset', '#benchmark', '#multimodal', '#architecture']}, 'hash': '46a93631041dba07'}, {'id': 'https://huggingface.co/papers/2410.05355', 'title': 'Falcon Mamba: The First Competitive Attention-free 7B Language Model', 'url': 'https://huggingface.co/papers/2410.05355', 'abstract': 'In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.', 'score': 15, 'issue_id': 40, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Falcon Mamba 7B - это новая базовая языковая модель, основанная на архитектуре Mamba. Она обучена на 5,8 триллионах токенов и превосходит ведущие модели на основе трансформеров, такие как Mistral 7B и Llama3.1 8B. Falcon Mamba 7B демонстрирует лучшую производительность среди моделей Mamba своего масштаба и значительно быстрее работает при инференсе. Модель показывает, что чистая архитектура Mamba может достигать результатов, сравнимых или превосходящих гибридные модели Mamba-Transformer.', 'tags': ['#FalconMamba', '#LanguageModelArchitecture', '#InferenceEfficiency'], 'emoji': '🦅', 'title': 'Falcon Mamba 7B: Прорыв в эффективности языковых моделей', 'categories': ['#architecture', '#inference', '#training']}, 'hash': 'c700bbc81473edd9'}, {'id': 'https://huggingface.co/papers/2410.06244', 'title': 'Story-Adapter: A Training-free Iterative Framework for Long Story Visualization', 'url': 'https://huggingface.co/papers/2410.06244', 'abstract': 'Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'В статье представлен фреймворк Story-Adapter для улучшения генерации изображений на основе длинных историй. Он использует итеративный подход с глобальным модулем кросс-внимания для сохранения семантической согласованности. Story-Adapter не требует дополнительного обучения и эффективен с вычислительной точки зрения. Эксперименты подтверждают превосходство метода в улучшении согласованности и детализации генерируемых изображений, особенно для длинных историй.', 'tags': ['#StoryVisualization', '#DiffusionModels', '#CrossAttention'], 'emoji': '🎞️', 'title': 'Story-Adapter: Новый подход к визуализации длинных историй', 'categories': ['#story_generation', '#diffusion', '#cv']}, 'hash': '8289ac05d8c302b2'}, {'id': 'https://huggingface.co/papers/2410.06961', 'title': 'Self-Boosting Large Language Models with Synthetic Preference Data', 'url': 'https://huggingface.co/papers/2410.06961', 'abstract': 'Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'В статье представлен метод SynPO для улучшения языковых моделей без использования ручной разметки. SynPO использует итеративный механизм, где генератор создает разнообразные промпты, а улучшатель ответов постепенно совершенствует ответы модели. После четырех итераций SynPO, модели Llama3-8B и Mistral-7B показали значительное улучшение в способности следовать инструкциям и общей производительности. Этот подход позволяет языковым моделям автономно обучаться генеративным наградам для своих собственных выходных данных.', 'tags': ['#синтетические_данные', '#самообучение_ЯМ', '#итеративное_улучшение'], 'emoji': '🔄', 'title': 'SynPO: самоусиление языковых моделей без ручной разметки', 'categories': ['#rlhf', '#alignment', '#training']}, 'hash': 'cbb5c59488cb0db9'}, {'id': 'https://huggingface.co/papers/2410.07170', 'title': 'One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation', 'url': 'https://huggingface.co/papers/2410.07170', 'abstract': 'Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.', 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет новый метод тонкой настройки фундаментальных моделей под названием EVA (Explained Variance Adaptation). Этот метод улучшает традиционный подход LoRA, инициализируя новые веса на основе данных с помощью сингулярного разложения. EVA перераспределяет ранги между матрицами весов для объяснения максимального количества дисперсии. Метод показывает более быструю сходимость и лучшие результаты на различных задачах, включая генерацию текста, понимание языка, классификацию изображений и обучение с подкреплением.', 'tags': ['#LoRA', '#SVD', '#FineTuning'], 'emoji': '🔬', 'title': 'EVA: Эффективная тонкая настройка фундаментальных моделей', 'categories': ['#training', '#rl', '#cv', '#multimodal']}, 'hash': 'd987cfa2ee91a2b5'}, {'id': 'https://huggingface.co/papers/2410.05591', 'title': 'TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation', 'url': 'https://huggingface.co/papers/2410.05591', 'abstract': "Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts. Results and source code are in our anonymous project page.", 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'TweedieMix - это новый метод для объединения персонализированных диффузионных моделей на этапе вывода. Он разделяет процесс сэмплирования на два этапа: сначала применяется техника сэмплирования с учетом нескольких объектов, а затем используется формула Твиди для смешивания внешнего вида персонализированных концепций. Метод позволяет генерировать изображения и видео с несколькими персонализированными концепциями более качественно, чем существующие подходы. TweedieMix также легко расширяется на диффузионные модели для преобразования изображений в видео.', 'tags': ['#диффузионные_модели', '#персонализированная_генерация', '#многообъектный_синтез'], 'emoji': '🎨', 'title': 'TweedieMix: Смешивание персонализированных концепций в генерации изображений и видео', 'categories': ['#diffusion', '#video', '#cv', '#multimodal']}, 'hash': '1d4bfe307ff6d748'}, {'id': 'https://huggingface.co/papers/2410.06166', 'title': 'Temporal Reasoning Transfer from Text to Video', 'url': 'https://huggingface.co/papers/2410.06166', 'abstract': "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.", 'score': 11, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Исследователи обнаружили, что видео-модели большого языка (Video LLMs) испытывают трудности с пониманием временных отношений не из-за неэффективного кодирования визуальных входных данных, а из-за ограничений базовой языковой модели в понимании временных концепций. Для решения этой проблемы был разработан метод Textual Temporal reasoning Transfer (T3), который синтезирует задачи временного рассуждения в текстовом формате из существующих наборов данных изображений и текста. Применение T3 позволило значительно улучшить временное понимание модели LongVA-7B без использования видеоданных. Результаты показали сильную корреляцию между производительностью в текстовых и видео задачах временного рассуждения.', 'tags': ['#TemporalReasoning', '#VideoLLM', '#TextualTransfer'], 'emoji': '⏳', 'title': 'Улучшение временного понимания видео-LLM через текстовый перенос', 'categories': ['#rl', '#transfer_learning', '#video', '#reasoning']}, 'hash': '5cd9bf212c19151b'}, {'id': 'https://huggingface.co/papers/2410.07002', 'title': 'CursorCore: Assist Programming through Aligning Anything', 'url': 'https://huggingface.co/papers/2410.07002', 'abstract': 'Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.', 'score': 10, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет новую систему для помощи в программировании, использующую большие языковые модели. Авторы разработали фреймворк, интегрирующий историю кодирования, текущий код и инструкции пользователя. Они также создали новый бенчмарк APEval для оценки моделей и pipeline для генерации обучающих данных. Результаты показывают, что их модель CursorCore превосходит аналоги сопоставимого размера.', 'tags': ['#программная_помощь', '#генерация_кода', '#языковые_модели'], 'emoji': '💻', 'title': 'Интеллектуальный помощник программиста: новый подход к автоматизации кодирования', 'categories': ['#plp', '#benchmark', '#dataset', '#training']}, 'hash': '70a480b72654e749'}, {'id': 'https://huggingface.co/papers/2410.05295', 'title': 'AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs', 'url': 'https://huggingface.co/papers/2410.05295', 'abstract': 'In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.', 'score': 10, 'issue_id': 39, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'AutoDAN-Turbo - это метод автоматического обнаружения уязвимостей в системах искусственного интеллекта без вмешательства человека. Он значительно превосходит базовые методы, достигая на 74.3% более высокого среднего показателя успешности атак на публичных бенчмарках. На модели GPT-4-1106-turbo AutoDAN-Turbo достигает 88.5% успешности атак. Кроме того, AutoDAN-Turbo может интегрировать существующие стратегии взлома, разработанные людьми, повышая успешность атак до 93.4% на GPT-4-1106-turbo.', 'tags': ['#jailbreak', '#AutoDAN', '#red-teaming'], 'emoji': '🔓', 'title': 'AutoDAN-Turbo: Автоматический взлом ИИ без участия человека', 'categories': ['#agents', '#security']}, 'hash': 'bc2003c7b895855f'}, {'id': 'https://huggingface.co/papers/2410.05651', 'title': 'ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler', 'url': 'https://huggingface.co/papers/2410.05651', 'abstract': 'Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.', 'score': 10, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет новую стратегию двунаправленной выборки для улучшения интерполяции ключевых кадров в моделях диффузии изображение-видео. Авторы предлагают последовательную выборку вдоль прямого и обратного путей, обусловленную начальным и конечным кадрами, что обеспечивает более согласованную генерацию промежуточных кадров. Метод включает в себя передовые техники направленной генерации, такие как CFG++ и DDS, для дальнейшего улучшения процесса интерполяции. Результаты демонстрируют высокую эффективность и качество генерации видео между ключевыми кадрами.', 'tags': ['#keyframe-interpolation', '#bidirectional-sampling', '#image-to-video-diffusion'], 'emoji': '🎞️', 'title': 'Революционный подход к интерполяции видео с использованием двунаправленной выборки', 'categories': ['#video', '#diffusion']}, 'hash': 'd350fdc5aa2b2edc'}, {'id': 'https://huggingface.co/papers/2410.06084', 'title': 'Diversity-Rewarded CFG Distillation', 'url': 'https://huggingface.co/papers/2410.06084', 'abstract': 'Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.', 'score': 8, 'issue_id': 42, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет новый метод обучения генеративных моделей для музыки, называемый diversity-rewarded CFG distillation. Этот подход объединяет дистилляцию Classifier-Free Guidance (CFG) с обучением с подкреплением для поощрения разнообразия. Авторы применяют метод к модели MusicLM для генерации музыки по текстовому описанию. Результаты показывают улучшение качества и разнообразия генерируемой музыки по сравнению с базовой моделью с CFG.', 'tags': ['#музыкальнаягенерация', '#CFGдистилляция', '#обучениесподкреплением'], 'emoji': '🎵', 'title': 'Повышение качества и разнообразия генеративных моделей музыки', 'categories': ['#rl', '#training', '#audio']}, 'hash': '99610ace8463ca57'}, {'id': 'https://huggingface.co/papers/2410.06885', 'title': 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', 'url': 'https://huggingface.co/papers/2410.06885', 'abstract': "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development.", 'score': 8, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'F5-TTS - это неавторегрессивная система синтеза речи, основанная на сопоставлении потоков с использованием Diffusion Transformer. В отличие от сложных моделей, F5-TTS просто дополняет текстовый ввод до длины речи и выполняет шумоподавление для генерации. Система использует ConvNeXt для улучшения представления текста и стратегию Sway Sampling для повышения производительности. F5-TTS демонстрирует высокую естественность речи, возможность переключения кодов и контроль скорости.', 'tags': ['#text-to-speech', '#flow-matching', '#non-autoregressive'], 'emoji': '🗣️', 'title': 'F5-TTS: Революция в синтезе речи без авторегрессии', 'categories': ['#audio', '#multilingual', '#inference']}, 'hash': '753a8935b426d722'}, {'id': 'https://huggingface.co/papers/2410.05677', 'title': 'T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design', 'url': 'https://huggingface.co/papers/2410.05677', 'abstract': 'In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.', 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'В статье представлен метод T2V-Turbo-v2 для улучшения модели преобразования текста в видео на основе диффузии. Авторы предлагают дистиллировать модель согласованности из предобученной T2V модели, используя различные сигналы обучения. Метод включает интеграцию высококачественных данных, обратную связь от модели вознаграждения и условное руководство в процесс дистилляции согласованности. Эмпирически T2V-Turbo-v2 устанавливает новый state-of-the-art результат на бенчмарке VBench с общим счетом 85.13.', 'tags': ['#текст-в-видео', '#дистилляция_согласованности', '#условное_руководство'], 'emoji': '🎬', 'title': 'Революция в генерации видео: T2V-Turbo-v2 превосходит конкурентов', 'categories': ['#video', '#diffusion', '#benchmark']}, 'hash': 'e30f39e021f2ee4b'}, {'id': 'https://huggingface.co/papers/2410.05643', 'title': 'TRACE: Temporal Grounding Video LLM via Causal Event Modeling', 'url': 'https://huggingface.co/papers/2410.05643', 'abstract': "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE.", 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет новый подход к задаче временной локализации в видео (Video Temporal Grounding). Авторы вводят концепцию каузального моделирования событий, представляя видео как последовательность событий. Они предлагают модель TRACE - мультизадачную видео-LLM, которая обрабатывает визуальные кадры, временные метки, оценки значимости и текст как отдельные задачи. Эксперименты показывают превосходство TRACE над современными видео-LLM в различных задачах VTG.', 'tags': ['#VideoTemporalGrounding', '#CausalEventModeling', '#MultitaskVideoLLM'], 'emoji': '🎬', 'title': 'TRACE: Революция в понимании структуры видео с помощью каузального моделирования событий', 'categories': ['#video', '#multimodal', '#agents']}, 'hash': '7241bf4a739cd624'}, {'id': 'https://huggingface.co/papers/2410.07064', 'title': 'Data Selection via Optimal Control for Language Models', 'url': 'https://huggingface.co/papers/2410.07064', 'abstract': "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.", 'score': 8, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Эта работа исследует выбор высококачественных данных для предобучения из массивных корпусов с целью улучшения возможностей языковых моделей. Авторы формулируют выбор данных как обобщенную задачу оптимального управления, решаемую с помощью принципа максимума Понтрягина. На основе теоретических результатов они представляют фреймворк PDS для приближенного оптимального выбора данных. Эксперименты показывают, что PDS ускоряет обучение моделей и повышает их эффективность на различных задачах, а также улучшает использование данных при ограниченных ресурсах.', 'tags': ['#dataSelection', '#optimalControl', '#languageModelPretraining'], 'emoji': '🎯', 'title': 'Оптимизация выбора данных для эффективного предобучения языковых моделей', 'categories': ['#data', '#training', '#optimization']}, 'hash': 'f8459aee9413c793'}, {'id': 'https://huggingface.co/papers/2410.06458', 'title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'url': 'https://huggingface.co/papers/2410.06458', 'abstract': 'Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs\' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs\' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM\'s response needs refinement. Our results show that DeCRIM improves Mistral\'s performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.', 'score': 7, 'issue_id': 43, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет RealInstruct - первый бенчмарк для оценки способности языковых моделей следовать реальным многоограниченным инструкциям. Исследование показывает, что даже GPT-4 не соблюдает хотя бы одно ограничение в более чем 21% инструкций. Авторы предлагают pipeline DeCRIM для улучшения способности моделей следовать ограничениям. Результаты демонстрируют, что DeCRIM повышает производительность Mistral на 7.3% в RealInstruct и 8.0% в IFEval.', 'tags': ['#многоограниченные_инструкции', '#DeCRIM', '#RealInstruct'], 'categories': ['#benchmark', '#dataset', '#rag'], 'emoji': '🎯', 'title': 'Преодоление ограничений в следовании инструкциям для языковых моделей'}, 'hash': 'f8cc2a4349e584a6'}, {'id': 'https://huggingface.co/papers/2410.02465', 'title': 'Response Tuning: Aligning Large Language Models without Instruction', 'url': 'https://huggingface.co/papers/2410.02465', 'abstract': 'Instruction tuning-supervised fine-tuning using instruction-response pairs-is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs.', 'score': 7, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'Исследователи предлагают метод Response Tuning (RT) для обучения языковых моделей без использования инструкций. RT фокусируется только на пространстве ответов, позволяя моделям эффективно реагировать на различные запросы. Эксперименты показывают, что модели RT могут быть такими же полезными, как и модели, обученные с инструкциями. Контроль распределения ответов при обучении может улучшить предпочтения пользователей и желаемое поведение моделей.', 'tags': ['#ResponseTuning', '#LanguageModelAlignment', '#OutputSpaceSupervision'], 'emoji': '🎯', 'title': 'Response Tuning: эффективное обучение языковых моделей без инструкций', 'categories': ['#alignment', '#training']}, 'hash': '0682a2364e4a8840'}, {'id': 'https://huggingface.co/papers/2410.04223', 'title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'url': 'https://huggingface.co/papers/2410.04223', 'abstract': 'While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-05', 'pub_date_ru': '5 октября', 'data': {'desc': 'Llamole - это первая мультимодальная языковая модель, способная генерировать текст и графы молекул. Она объединяет базовую языковую модель с Graph Diffusion Transformer и графовыми нейронными сетями для многоусловной генерации молекул и вывода реакций. Llamole также интегрирует A* поиск с функциями стоимости на основе языковой модели для эффективного ретросинтетического планирования. Модель значительно превосходит 14 адаптированных языковых моделей по 12 метрикам для контролируемого дизайна молекул и ретросинтетического планирования.', 'tags': ['#молекулярный_дизайн', '#ретросинтез', '#мультимодальные_LLM'], 'emoji': '🧪', 'title': 'Llamole: прорыв в генерации молекул с помощью мультимодальных языковых моделей', 'categories': ['#multimodal', '#cv', '#graph', '#benchmark']}, 'hash': '9c153ad16c36d327'}, {'id': 'https://huggingface.co/papers/2410.02503', 'title': 'Mixed-Session Conversation with Egocentric Memory', 'url': 'https://huggingface.co/papers/2410.02503', 'abstract': "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.", 'score': 6, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'Статья представляет новую систему диалогов под названием Mixed-Session Conversation, которая способна вести разговоры с несколькими партнерами в многосессионном формате. Авторы предлагают набор данных MiSC, состоящий из 6 последовательных сессий с участием четырех собеседников. Также представлена модель EMMA с новым механизмом управления памятью, которая собирает и сохраняет воспоминания с точки зрения главного говорящего. Оценки показывают, что диалоги в MiSC демонстрируют непрерывный разговорный поток даже при смене собеседников, а EMMA сохраняет высокую запоминаемость без противоречий.', 'tags': ['#multi-session-dialogue', '#memory-enhanced-model', '#conversational-continuity'], 'emoji': '🗣️', 'title': 'Революция в диалоговых системах: непрерывное общение с множеством партнеров', 'categories': ['#dataset', '#agents', '#multimodal']}, 'hash': '87a67ed9acbc74a2'}, {'id': 'https://huggingface.co/papers/2410.06555', 'title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'url': 'https://huggingface.co/papers/2410.06555', 'abstract': "As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.", 'score': 6, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Представлен новый бенчмарк ING-VP для оценки пространственного воображения и многошагового рассуждения мультимодальных языковых моделей. Бенчмарк содержит 6 игр с 300 уровнями и 6 конфигурациями каждый, что позволяет провести более 60 000 раундов взаимодействия. ING-VP оценивает модели в различных условиях, включая сравнение входных данных изображение-текст и только текст. Лучшая модель Claude-3.5 Sonnet показала точность всего 3.37%, что подчеркивает сложность задачи.', 'tags': ['#пространственное_рассуждение', '#мультимодальные_модели', '#интерактивные_бенчмарки'], 'emoji': '🧠', 'title': 'ING-VP: Новый рубеж в оценке пространственного мышления ИИ', 'categories': ['#benchmark', '#multimodal', '#reasoning']}, 'hash': 'dabc7be39f163aa7'}, {'id': 'https://huggingface.co/papers/2410.05791', 'title': 'FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance', 'url': 'https://huggingface.co/papers/2410.05791', 'abstract': 'Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.', 'score': 6, 'issue_id': 40, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Эта статья представляет новый набор данных, содержащий 10 часов 3D-движений рук и аудио от 15 пианистов высокого уровня, играющих классическую музыку. Авторы разработали систему безмаркерного захвата движений с использованием мультиракурсной съемки и современных моделей оценки позы. На основе собранных данных создан конвейер для синтеза реалистичных движений рук при игре на пианино, используя имитационное обучение и обучение с подкреплением. Модель генерирует естественные и точные движения, которые обобщаются на музыку вне обучающего набора.', 'tags': ['#3DHandMotionCapture', '#PianoPerformanceModeling', '#DiffusionModelForMotion'], 'emoji': '🎹', 'title': 'Виртуозное воссоздание игры на пианино с помощью ИИ', 'categories': ['#dataset', '#3d', '#rl', '#diffusion']}, 'hash': 'cba54e31692e353c'}, {'id': 'https://huggingface.co/papers/2410.06172', 'title': 'Multimodal Situational Safety', 'url': 'https://huggingface.co/papers/2410.06172', 'abstract': 'Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.', 'score': 6, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет первый анализ проблемы многомодальной ситуационной безопасности в контексте мультимодальных больших языковых моделей (MLLM). Авторы разработали набор данных MSSBench для оценки способности MLLM учитывать безопасность в различных ситуациях. Исследование показало, что существующие MLLM испытывают трудности с комплексным решением задач ситуационной безопасности. Предложенные мультиагентные подходы демонстрируют улучшение безопасности по сравнению с исходными ответами MLLM.', 'tags': ['#ситуационнаяБезопасность', '#MSSBench', '#мультимодальныеМодели'], 'emoji': '🛡️', 'title': 'Новый рубеж в безопасности ИИ: оценка ситуационной осведомленности мультимодальных моделей', 'categories': ['#dataset', '#benchmark', '#multimodal', '#agents']}, 'hash': '9fee460887b3cb38'}, {'id': 'https://huggingface.co/papers/2410.05664', 'title': 'Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning', 'url': 'https://huggingface.co/papers/2410.05664', 'abstract': 'As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.', 'score': 6, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья посвящена исследованию методов разобучения моделей генерации изображений для удаления нежелательной информации. Авторы проанализировали существующие подходы к разобучению по пяти ключевым аспектам в различных сценариях. Исследование выявило, что все методы имеют побочные эффекты или ограничения, особенно в сложных реалистичных ситуациях. Авторы представили комплексную систему оценки с открытым исходным кодом для стимулирования дальнейших исследований в этой области.', 'tags': ['#разобучение_моделей', '#диффузионные_модели', '#этика_ИИ'], 'emoji': '🧠', 'title': 'Разобучение генеративных моделей: проблемы и перспективы', 'categories': ['#benchmark', '#diffusion', '#ethics']}, 'hash': '9630c1ce042de601'}, {'id': 'https://huggingface.co/papers/2410.02428', 'title': 'Collective Critics for Creative Story Generation', 'url': 'https://huggingface.co/papers/2410.02428', 'abstract': "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.", 'score': 5, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'desc': 'Статья представляет новый подход к генерации длинных историй с помощью больших языковых моделей, называемый CritiCS. Этот метод использует коллективный механизм критики для улучшения креативности и выразительности генерируемых историй. CritiCS состоит из двух этапов: уточнения плана (CrPlan) и генерации текста (CrText), где группа ИИ-критиков и лидер сотрудничают для постепенного улучшения черновиков. Оценка показала, что CritiCS значительно повышает креативность историй и вовлеченность читателей, сохраняя при этом связность повествования.', 'tags': ['#генерация_длинных_историй', '#коллективная_критика_ИИ', '#интерактивное_сотрудничество_человек_ИИ'], 'emoji': '📚', 'title': 'CritiCS: Коллективный подход к созданию креативных историй с помощью ИИ', 'categories': ['#story_generation', '#multimodal']}, 'hash': 'fd2a584ee28c9b94'}, {'id': 'https://huggingface.co/papers/2410.07071', 'title': 'Retrieval-Augmented Decision Transformer: External Memory for In-context RL', 'url': 'https://huggingface.co/papers/2410.07071', 'abstract': "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.", 'score': 5, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет новый метод обучения с подкреплением в контексте - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT использует внешнюю память для хранения прошлого опыта и извлекает только релевантные текущей ситуации под-траектории. Метод не требует обучения компонента извлечения и может быть независимым от домена. RA-DT превосходит базовые модели на сеточных мирах, используя лишь часть длины их контекста.', 'tags': ['#InContextRL', '#RetrievalAugmentation', '#DecisionTransformer'], 'emoji': '🧠', 'title': 'RA-DT: Эффективное обучение с подкреплением в контексте с помощью извлечения релевантного опыта', 'categories': ['#rl', '#rag', '#agents']}, 'hash': '0815ef9a074b159d'}, {'id': 'https://huggingface.co/papers/2410.06241', 'title': 'BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way', 'url': 'https://huggingface.co/papers/2410.06241', 'abstract': 'The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'Статья представляет метод BroadWay для улучшения качества генерации видео из текста без дополнительного обучения или параметров. Метод основан на наблюдении связи между различиями во временных картах внимания и временной несогласованностью в видео. BroadWay включает два компонента: Temporal Self-Guidance для улучшения правдоподобности и согласованности, и Fourier-based Motion Enhancement для усиления движения. Эксперименты показывают значительное улучшение качества генерации видео с минимальными дополнительными затратами.', 'tags': ['#text2video', '#attentionMaps', '#motionEnhancement'], 'emoji': '🎬', 'title': 'BroadWay: Повышение качества генерации видео без дополнительного обучения', 'categories': ['#video', '#multimodal']}, 'hash': 'b38bb081870f60d1'}, {'id': 'https://huggingface.co/papers/2410.06462', 'title': 'Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders', 'url': 'https://huggingface.co/papers/2410.06462', 'abstract': 'The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert\'s models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM\'s directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners\' safety policies when posed directly without the accompanying coding support request.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': "Данное исследование посвящено изучению потенциальных уязвимостей больших языковых моделей (LLM) в контексте рекомендаций кода. Авторы демонстрируют, что при резком изменении контекста, например, при решении задачи программирования, LLM могут снизить свою защиту и предложить потенциально вредоносный код. Эксперименты показывают, как модели могут рекомендовать API-эндпоинты, которые злоумышленники могут использовать для атак. Исследование сравнивает этот тип атаки с предыдущими работами по смене контекста и рассматривает его как новую версию атак типа 'living off the land' в литературе о вредоносном ПО.", 'tags': ['#LLMVulnerabilities', '#CodeInjection', '#ContextShifting'], 'emoji': '🕵️', 'title': 'Неожиданные уязвимости LLM при рекомендации кода', 'categories': ['#security', '#hallucination']}, 'hash': 'a2ceecf54aefe4d6'}, {'id': 'https://huggingface.co/papers/2410.06949', 'title': 'Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach', 'url': 'https://huggingface.co/papers/2410.06949', 'abstract': 'In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.', 'score': 5, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья посвящена проблеме обработки исключений в разработке программного обеспечения и предлагает решение с использованием больших языковых моделей (LLM). Авторы выявили три ключевые проблемы: нечувствительное обнаружение хрупкого кода, неточный захват типов исключений и искаженные решения по обработке. Предложена система Seeker - мультиагентный фреймворк, вдохновленный стратегиями экспертов-разработчиков для обработки исключений. Seeker использует агенты для помощи LLM в более эффективном обнаружении, захвате и разрешении исключений.', 'tags': ['#обработка_исключений', '#мультиагентные_системы', '#надежность_кода'], 'emoji': '🛡️', 'title': 'Seeker: Улучшение обработки исключений с помощью LLM и мультиагентного подхода', 'categories': ['#agents', '#plp']}, 'hash': '7b96c448f8692316'}, {'id': 'https://huggingface.co/papers/2410.06985', 'title': 'Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control', 'url': 'https://huggingface.co/papers/2410.06985', 'abstract': 'Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.', 'score': 4, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья посвящена проблеме согласованности представлений в моделях диффузии изображений. Авторы исследуют этот вопрос в контексте рабочего процесса Collaborative Control для задачи Text-to-Texture с PBR (Physically Based Rendering). Модель Collaborative Control напрямую моделирует распределения вероятностей PBR-изображений, включая карты нормалей. В работе обсуждаются решения по обеспечению согласованности между различными ракурсами и демонстрируется эффективность предложенного подхода.', 'tags': ['#текст-в-текстуру', '#PBR', '#согласованность-ракурсов'], 'emoji': '🎨', 'title': 'Согласованность многоракурсных представлений в диффузионных моделях для PBR текстурирования', 'categories': ['#cv', '#3d', '#diffusion']}, 'hash': '0007650863977d09'}, {'id': 'https://huggingface.co/papers/2410.06845', 'title': 'MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders', 'url': 'https://huggingface.co/papers/2410.06845', 'abstract': 'Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main', 'score': 4, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет MentalArena - фреймворк для обучения языковых моделей в области психического здоровья путем генерации персонализированных данных. Авторы разработали Symptom Encoder для моделирования поведения пациентов и Symptom Decoder для управления диалогом между пациентом и терапевтом. Модели, обученные с помощью MentalArena на GPT-3.5 и Llama-3-8b, превзошли другие современные модели на различных тестах по биомедицине и психическому здоровью. Исследование направлено на улучшение персонализированной диагностики и лечения психических расстройств.', 'tags': ['#MentalHealthAI', '#SelfPlayLM', '#PersonalizedDiagnostics'], 'emoji': '🧠', 'title': 'MentalArena: ИИ-терапевт нового поколения для персонализированной диагностики', 'categories': ['#rl', '#medicine', '#training', '#benchmark']}, 'hash': 'd89e94812dda796d'}, {'id': 'https://huggingface.co/papers/2410.06524', 'title': 'Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA', 'url': 'https://huggingface.co/papers/2410.06524', 'abstract': 'Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.', 'score': 4, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'CAIMIRA - новая система оценки способностей к решению задач у людей и ИИ в области обработки естественного языка. Исследование, основанное на анализе более 300 000 ответов от ~70 систем ИИ и 155 людей, выявило различия в профессиональных навыках в разных областях знаний. Люди превосходят ИИ в абдуктивном и концептуальном мышлении, в то время как современные языковые модели лучше справляются с извлечением информации и фактологическими рассуждениями. Результаты подчеркивают необходимость разработки задач, требующих более сложного мышления и применения знаний в различных контекстах.', 'tags': ['#CAIMIRA', '#ItemResponseTheory', '#AIvsHuman'], 'emoji': '🧠', 'title': 'Сравнение когнитивных способностей человека и ИИ: новый взгляд на обработку естественного языка', 'categories': ['#alignment', '#reasoning', '#interpretability']}, 'hash': '5eec66236f57298a'}, {'id': 'https://huggingface.co/papers/2410.07062', 'title': 'TinyEmo: Scaling down Emotional Reasoning via Metric Projection', 'url': 'https://huggingface.co/papers/2410.07062', 'abstract': 'This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems.   We release code, models, and dataset at https://github.com/ggcr/TinyEmo', 'score': 3, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'TinyEmo - это семейство небольших мультимодальных языковых моделей для эмоционального анализа и классификации. Модель использует синтетический набор данных для обучения и настройки, а также включает Metric Projector для эффективной классификации. TinyEmo способна выполнять классификацию эмоций и эмоциональные рассуждения, используя значительно меньше параметров, чем сопоставимые модели. Кроме того, модель предлагает подход к пониманию и улучшению систем ИИ через интерпретируемость и косвенное обнаружение предвзятости.', 'tags': ['#EmotionalAI', '#EfficientMLM', '#InterpretableML'], 'emoji': '🧠', 'title': 'TinyEmo: Эффективный анализ эмоций с меньшими ресурсами', 'categories': ['#dataset', '#data', '#multimodal', '#interpretability', '#training']}, 'hash': '354409ec230f329b'}, {'id': 'https://huggingface.co/papers/2410.07160', 'title': 'TextToon: Real-Time Text Toonify Head Avatar from Single Video', 'url': 'https://huggingface.co/papers/2410.07160', 'abstract': 'We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.', 'score': 3, 'issue_id': 40, 'pub_date': '2024-09-23', 'pub_date_ru': '23 сентября', 'data': {'desc': 'TextToon - это метод создания управляемого мультяшного аватара на основе короткого видео и текстового описания стиля. Он использует условное встраивание Tri-plane для реалистичного и стилизованного представления лица в гауссовом поле деформации. Модель улучшает возможности стилизации 3D Gaussian Splatting с помощью адаптивной нейронной сети пиксельного преобразования и контрастного обучения с учетом патчей. TextToon работает в режиме реального времени и превосходит существующие методы по качеству и анимации.', 'tags': ['#3DGaussianSplatting', '#AvatarGeneration', '#RealTimeRendering'], 'emoji': '🎭', 'title': 'TextToon: Создание управляемых мультяшных аватаров по видео и тексту', 'categories': ['#3d', '#cv', '#multimodal']}, 'hash': '425ed10e0d4ec2cd'}, {'id': 'https://huggingface.co/papers/2410.05873', 'title': 'MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment', 'url': 'https://huggingface.co/papers/2410.05873', 'abstract': 'English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.', 'score': 2, 'issue_id': 45, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'desc': 'MEXA - это новый метод оценки многоязычных возможностей предобученных англоцентричных языковых моделей (LLM), использующий параллельные предложения. Метод основан на том, что англоцентричные LLM используют английский как своего рода язык-посредник в промежуточных слоях. MEXA вычисляет выравнивание между английским и другими языками, чтобы оценить перенос понимания языка с английского на другие языки. Результаты показывают высокую корреляцию (0,90) с существующими задачами на нескольких моделях и наборах данных.', 'tags': ['#multilingual-evaluation', '#parallel-sentences', '#language-alignment'], 'categories': ['#benchmark', '#multilingual', '#dataset'], 'emoji': '🌐', 'title': 'MEXA: Новый способ оценки многоязычности LLM через параллельные предложения'}, 'hash': '98dd8fcc39110307'}, {'id': 'https://huggingface.co/papers/2410.07112', 'title': 'VHELM: A Holistic Evaluation of Vision Language Models', 'url': 'https://huggingface.co/papers/2410.07112', 'abstract': 'Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.', 'score': 1, 'issue_id': 47, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'VHELM - это расширение фреймворка HELM для комплексной оценки моделей компьютерного зрения и языка (VLM). Он охватывает 9 аспектов, включая визуальное восприятие, знания, рассуждения, предвзятость, справедливость, мультиязычность, надежность, токсичность и безопасность. VHELM стандартизирует параметры вывода, методы промптинга и метрики оценки для обеспечения справедливых сравнений между моделями. Исследование выявило интересные результаты, например, что модели, ориентированные на эффективность, значительно хуже справляются с тестами на предвзятость.', 'tags': ['#VisionLanguageModels', '#HolisticEvaluation', '#BenchmarkStandardization'], 'categories': ['#benchmark', '#cv', '#multimodal'], 'emoji': '🔍', 'title': 'VHELM: Комплексная оценка мультимодальных моделей по 9 ключевым аспектам'}, 'hash': 'f9db5fd76663d260'}, {'id': 'https://huggingface.co/papers/2410.06468', 'title': 'Does Spatial Cognition Emerge in Frontier Models?', 'url': 'https://huggingface.co/papers/2410.06468', 'abstract': 'Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.', 'score': 1, 'issue_id': 42, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'SPACE - это новый бенчмарк для оценки пространственного мышления у передовых моделей искусственного интеллекта. Он основан на многолетних исследованиях в когнитивной науке и оценивает способности к крупномасштабному картографированию, мелкомасштабному рассуждению о формах и расположении объектов, а также когнитивную инфраструктуру, такую как пространственное внимание и память. Бенчмарк позволяет тестировать как языковые, так и мультимодальные модели. Результаты показывают, что современные модели значительно уступают животным в пространственном интеллекте.', 'tags': ['#пространственноеМышление', '#когнитивныеТесты', '#бенчмаркингМоделей'], 'emoji': '🧠', 'title': 'SPACE: новый рубеж в оценке пространственного интеллекта ИИ', 'categories': ['#benchmark', '#multimodal']}, 'hash': 'b568df3a7211e7fa'}, {'id': 'https://huggingface.co/papers/2410.07095', 'title': 'MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering', 'url': 'https://huggingface.co/papers/2410.07095', 'abstract': "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.", 'score': 1, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'MLE-bench - это новый бенчмарк для оценки способностей ИИ-агентов в области инженерии машинного обучения. Он включает 75 соревнований с Kaggle, охватывающих различные аспекты ML-инженерии. Авторы установили базовые показатели человеческой производительности и протестировали несколько языковых моделей, используя открытые фреймворки для агентов. Лучший результат показала модель OpenAI o1-preview с AIDE, достигнув уровня бронзовой медали Kaggle в 16.9% соревнований.', 'tags': ['#MLE-bench', '#ML-инженерия', '#ИИ-агенты'], 'emoji': '🤖', 'title': 'MLE-bench: измеряем инженерные навыки ИИ в машинном обучении', 'categories': ['#benchmark', '#agents']}, 'hash': '103d4f2b02c940a9'}, {'id': 'https://huggingface.co/papers/2410.05160', 'title': 'VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks', 'url': 'https://huggingface.co/papers/2410.05160', 'abstract': "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. Our results show that \\model achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'Статья представляет новый подход к созданию универсальных мультимодальных эмбеддингов. Авторы предлагают MMEB - масштабный бенчмарк для оценки мультимодальных эмбеддингов, охватывающий 36 датасетов и 4 мета-задачи. Также представлен VLM2Vec - фреймворк для преобразования любой современной vision-language модели в модель эмбеддингов путем обучения на MMEB. Результаты показывают, что VLM2Vec достигает улучшения на 10-20% по сравнению с существующими мультимодальными моделями эмбеддингов.', 'tags': ['#мультимодальные_эмбеддинги', '#VLM2Vec', '#MMEB'], 'categories': ['#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🔀', 'title': 'Универсальные мультимодальные эмбеддинги: новый подход и бенчмарк'}, 'hash': '36a094d66cdd6a3e'}, {'id': 'https://huggingface.co/papers/2410.07145', 'title': 'Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling', 'url': 'https://huggingface.co/papers/2410.07145', 'abstract': "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': "В статье исследуются причины неспособности рекуррентных нейронных сетей (RNN) обрабатывать длинные последовательности и предлагаются способы решения этой проблемы. Авторы выявляют явление 'схлопывания состояния' (state collapse), вызывающее резкое падение производительности на последовательностях длиннее тренировочных. Предложены три метода для смягчения этого эффекта, позволяющие модели Mamba-2 обрабатывать более миллиона токенов без схлопывания. Также эмпирически показано, что емкость рекуррентного состояния для задачи извлечения паролей масштабируется экспоненциально с размером состояния.", 'tags': ['#state_collapse', '#long_context_rnn', '#mamba2'], 'categories': ['#training', '#inference', '#architecture'], 'emoji': '🧠', 'title': 'RNN покоряют длинные последовательности: прорыв в обработке контекста в миллион токенов'}, 'hash': '955fb73ec92505eb'}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (8)', '#agi', '#alignment (3)', '#architecture (5)', '#audio (2)', '#benchmark (19)', '#cv (12)', '#data (3)', '#dataset (10)', '#diffusion (9)', '#edge_computing', '#ethics (1)', '#games', '#graph', '#graphs', '#hallucination (1)', '#inference (3)', '#interpretability (2)', '#math', '#medicine (1)', '#multilingual (2)', '#multimodal (20)', '#optimization (2)', '#plp (2)', '#quantum', '#rag (3)', '#reasoning (3)', '#rl (6)', '#rlhf (1)', '#robotics', '#security (2)', '#story_generation (2)', '#survey', '#training (13)', '#transfer_learning (1)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-10 20:13');
        } 
        function hideNextLink() {
            if (isToday('2024-10-10 20:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
    </script>
</body>
</html>
    