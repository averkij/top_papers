
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (48 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xec706b594e8a3b49 { background: url("https://hfday.ru/img/20241007/ec706b594e8a3b49.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xec706b594e8a3b49:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xc923f3711f603f2c { background: url("https://hfday.ru/img/20241009/c923f3711f603f2c.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xc923f3711f603f2c:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xaca07d3bd7236f38 { background: url("https://hfday.ru/img/20241009/aca07d3bd7236f38.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xaca07d3bd7236f38:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xa04f813494758df0 { background: url("https://hfday.ru/img/20241008/a04f813494758df0.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xa04f813494758df0:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x336b26133f29e630 { background: url("https://hfday.ru/img/20241009/336b26133f29e630.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x336b26133f29e630:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x17a92a30d25bd139 { background: url("https://hfday.ru/img/20241007/17a92a30d25bd139.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x17a92a30d25bd139:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x8e1f683abf35b291 { background: url("https://hfday.ru/img/20241009/8e1f683abf35b291.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x8e1f683abf35b291:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xbf22e906e7d30eea { background: url("https://hfday.ru/img/20241008/bf22e906e7d30eea.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xbf22e906e7d30eea:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x365906882dc00532 { background: url("https://hfday.ru/img/20241008/365906882dc00532.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x365906882dc00532:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 октября</span> | <span id="title-articles-count">48 статей</span></p>
        </div>
        <div class="theme-switch">f
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-08.html">⬅️ <span id="prev-date">08.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-10.html">➡️ <span id="next-date">10.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        let feedDate = {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'};
        let feedDateNext = {'ru': '10.10', 'en': '10/10', 'zh': '10月10日'};
        let feedDatePrev = {'ru': '08.10', 'en': '10/08', 'zh': '10月8日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05254', 'title': 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments', 'url': 'https://huggingface.co/papers/2410.05254', 'abstract': "Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.", 'score': 60, 'issue_id': 41, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#agents', '#multimodal'], 'emoji': '🎲', 'ru': {'title': 'LLM в экономических играх: новый бенчмарк для оценки рациональности и эффективности', 'desc': 'Статья представляет новый бенчмарк для исследования поведения больших языковых моделей (LLM) в экономических и стратегических взаимодействиях. Авторы разработали фреймворк для симуляции и анализа игр между двумя участниками, основанных на естественном языке. Исследование включает сравнение поведения LLM-агентов с человеческими игроками в различных экономических контекстах. Результаты позволяют оценить индивидуальную и коллективную эффективность агентов, а также влияние экономических характеристик среды на их поведение.'}, 'en': {'title': 'Standardizing LLMs in Economic Games: A New Benchmark for Fair Play', 'desc': 'This paper explores the behavior of Large Language Models (LLMs) in economic and strategic interactions, focusing on their rationality, mimicry of human behavior, and ability to achieve fair outcomes. The authors introduce a benchmark for standardizing research on two-player, sequential, language-based games, inspired by economic literature. They develop an open-source framework to simulate interactions and collect datasets of LLM vs. LLM and human vs. LLM interactions. The study aims to compare LLM behavior to human players, evaluate performance, and understand the impact of economic environments on agent behavior.'}, 'zh': {'title': '标准化LLM在经济互动中的表现评估', 'desc': '这篇论文探讨了大型语言模型（LLM）在经济和战略互动中的表现，尤其是在自然语言交流中。研究者们提出了一个基准，用于标准化两人顺序语言游戏的研究，以便更好地评估LLM的表现。通过开发开源框架和收集数据集，研究者能够比较LLM与人类玩家在不同经济环境中的行为。实验结果显示，这种框架可以有效评估LLM在个体和集体表现上的表现，并量化经济环境特征对代理行为的影响。'}}, 'hash': 'ec706b594e8a3b49', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.07113', 'title': 'Personalized Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.07113', 'abstract': 'Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.', 'score': 55, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#multimodal'], 'emoji': '👤', 'ru': {'title': "Преодоление 'слепоты' мультимодальных моделей: персонализированное общение с изображениями", 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) для персонализированного общения. Авторы предлагают метод PVIT (Personalized Visual Instruction Tuning), который позволяет моделям идентифицировать конкретных людей на изображениях и вести персонализированный диалог. Для оценки эффективности метода разработан бенчмарк P-Bench с различными типами вопросов. Эксперименты показывают значительное улучшение персонализированной производительности после файнтюнинга на специально подготовленном датасете.'}, 'en': {'title': "Breaking the 'Face Blindness' Barrier in Multimodal Models", 'desc': "The paper addresses the limitation of multimodal large language models (MLLMs) in recognizing and engaging with specific individuals, a problem termed as 'face blindness'. To overcome this, the authors propose Personalized Visual Instruction Tuning (PVIT), a framework that curates and trains MLLMs to identify individuals in images and conduct personalized dialogues. The framework uses a pipeline that autonomously generates training data by integrating visual experts, image generation models, and large language models. The effectiveness of this approach is validated through a benchmark called P-Bench, showing improved personalized performance after fine-tuning."}, 'zh': {'title': '打破“面盲症”：个性化视觉对话新突破', 'desc': '近年来，多模态大语言模型（MLLMs）取得了显著进展，但存在一个显著的局限，即“面盲症”，无法进行针对特定个体的个性化对话。为了解决这个问题，本文提出了一种名为个性化视觉指令调优（PVIT）的新框架，旨在使MLLMs能够识别图像中的目标个体并进行个性化对话。我们开发了一条复杂的流水线，自动生成包含个性化对话的训练数据，并利用视觉专家、图像生成模型和多模态大语言模型的能力。实验结果表明，经过我们精心策划的数据集微调后，MLLMs的个性化性能显著提升。'}}, 'hash': 'c923f3711f603f2c', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.07171', 'title': 'IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.07171', 'abstract': 'Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#rag', '#diffusion', '#cv'], 'emoji': '🧩', 'ru': {'title': 'IterComp: Объединяя силы моделей для идеальной композиции изображений', 'desc': 'IterComp - это новый фреймворк для улучшения композиционной генерации изображений по тексту. Он агрегирует предпочтения нескольких моделей диффузии и использует итеративное обучение с обратной связью. IterComp оценивает модели по трем ключевым метрикам композиции и создает набор данных для обучения моделей вознаграждения. Метод показывает превосходство над современными подходами, особенно в композиции объектов нескольких категорий и сложном семантическом выравнивании.'}, 'en': {'title': 'IterComp: Uniting Model Strengths for Superior Image Composition', 'desc': 'The paper introduces IterComp, a framework designed to improve text-to-image generation by combining the strengths of multiple diffusion models. It evaluates models based on their ability to handle attribute binding, spatial, and non-spatial relationships, creating a dataset to train reward models. IterComp uses an iterative feedback learning approach to refine both the base diffusion model and reward models, enhancing compositionality over time. The results show that IterComp outperforms existing methods in generating complex images with multiple objects and semantic alignment.'}, 'zh': {'title': 'IterComp：提升组合生成的新框架', 'desc': '本文介绍了一种名为IterComp的新框架，用于改进文本到图像生成中的组合能力。IterComp通过聚合多个模型的组合偏好，并采用迭代反馈学习方法来增强组合生成。我们评估了六个开源扩散模型的三个关键组合指标：属性绑定、空间关系和非空间关系。实验结果表明，IterComp在多类别对象组合和复杂语义对齐方面优于现有方法。'}}, 'hash': 'aca07d3bd7236f38', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05993', 'title': 'Aria: An Open Multimodal Native Mixture-of-Experts Model', 'url': 'https://huggingface.co/papers/2410.05993', 'abstract': 'Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#multimodal', '#architecture', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Aria: открытая мультимодальная модель ИИ с производительностью мирового класса', 'desc': 'Статья представляет Aria - открытую мультимодальную модель искусственного интеллекта. Модель использует архитектуру смеси экспертов и имеет 3,9 млрд и 3,5 млрд активируемых параметров для визуальных и текстовых токенов соответственно. Aria превосходит Pixtral-12B и Llama3.2-11B, конкурируя с лучшими проприетарными моделями в различных мультимодальных задачах. Модель обучается по 4-этапному конвейеру, постепенно приобретая сильные способности в понимании языка, мультимодальном понимании, работе с длинным контекстом и следовании инструкциям.'}, 'en': {'title': 'Aria: Open-Source Multimodal Mastery', 'desc': 'The paper introduces Aria, an open-source multimodal native AI model designed to integrate diverse types of information for a comprehensive understanding of real-world data. Aria is a mixture-of-expert model with billions of parameters, enabling it to excel in tasks involving language, multimodal data, and coding. It surpasses existing models like Pixtral-12B and Llama3.2-11B in performance and is competitive with top proprietary models. The model is pre-trained using a four-stage pipeline to enhance its capabilities, and its open-source nature allows for easy adoption and adaptation in various applications.'}, 'zh': {'title': 'Aria：开源多模态模型的卓越表现', 'desc': 'Aria是一个开源的多模态原生模型，旨在整合多种信息来源以提供全面的理解。它在视觉和文本任务中表现出色，超过了许多专有模型。Aria通过四阶段的预训练流程，从零开始逐步增强其语言理解和多模态理解能力。我们开放了Aria的模型权重和代码库，方便在实际应用中进行采用和适应。'}}, 'hash': 'a04f813494758df0', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.07073', 'title': 'Pixtral 12B', 'url': 'https://huggingface.co/papers/2410.07073', 'abstract': 'We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.', 'score': 34, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#multimodal', '#cv', '#benchmark', '#long_context'], 'emoji': '🖼️', 'ru': {'title': 'Pixtral-12B: Революция в мультимодальном искусственном интеллекте', 'desc': 'Pixtral-12B - это мультимодальная языковая модель с 12 миллиардами параметров, способная понимать как изображения, так и текст. Она превосходит более крупные модели по различным мультимодальным показателям, при этом не уступая в задачах обработки естественного языка. Pixtral использует новый энкодер изображений, обученный с нуля, что позволяет обрабатывать изображения в их естественном разрешении и соотношении сторон. Модель может обрабатывать любое количество изображений в контекстном окне из 128 тысяч токенов.'}, 'en': {'title': '"Pixtral-12B: Small but Mighty in Multimodal Mastery!"', 'desc': 'Pixtral-12B is a powerful multimodal language model with 12 billion parameters, designed to understand both images and text effectively. It achieves top performance on various benchmarks, outperforming larger models while maintaining strong natural language capabilities. The model features a new vision encoder that processes images at their natural resolution, offering flexibility in token usage and handling up to 128K tokens in its context window. Additionally, Pixtral-12B introduces an open-source benchmark for evaluating vision-language models, providing tools for standardized assessments.'}, 'zh': {'title': 'Pixtral-12B：小体积，大能量的多模态语言模型', 'desc': 'Pixtral-12B 是一个拥有 120 亿参数的多模态语言模型，能够理解自然图像和文档，并在多种多模态基准测试中表现出色。与许多开源模型不同，Pixtral 在不牺牲自然语言性能的情况下，在多模态任务中也表现优异。它使用全新训练的视觉编码器，可以以自然分辨率和纵横比处理图像，灵活使用 128K 令牌的长上下文窗口处理任意数量的图像。Pixtral-12B 的性能超过了许多同类大小的开源模型，并且在体积上比一些更大的模型小 7 倍。'}}, 'hash': '336b26133f29e630', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05363', 'title': 'Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation', 'url': 'https://huggingface.co/papers/2410.05363', 'abstract': "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench", 'score': 32, 'issue_id': 37, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#video', '#cv'], 'emoji': '🧠', 'ru': {'title': 'PhyGenBench: оценка физического здравого смысла в моделях текст-в-видео', 'desc': 'Статья представляет PhyGenBench - комплексный бенчмарк для оценки корректности физического здравого смысла в моделях текст-в-видео (T2V). Бенчмарк включает 160 промптов, охватывающих 27 физических законов в четырех фундаментальных областях. Авторы также предлагают PhyGenEval - новую структуру оценки, использующую передовые модели компьютерного зрения и большие языковые модели. Результаты показывают, что текущие T2V модели испытывают трудности с генерацией видео, соответствующих физическому здравому смыслу.'}, 'en': {'title': 'Building Smarter Simulations: Teaching AI Intuitive Physics', 'desc': "The paper introduces PhyGenBench, a benchmark designed to evaluate the ability of text-to-video models to understand and represent intuitive physics. It includes 160 prompts based on 27 physical laws to test models' grasp of physical commonsense. Alongside, PhyGenEval is proposed as a framework for assessing these models using advanced vision-language and large language models. The study finds that current models struggle with physical commonsense, suggesting that improvements are needed beyond just scaling or prompt engineering."}, 'zh': {'title': '超越娱乐：提升模型的物理常识理解', 'desc': '这篇论文介绍了一种名为PhyGenBench的物理生成基准，用于评估文本到视频模型在物理常识方面的表现。研究发现，现有模型在生成符合物理常识的视频方面存在困难，简单地扩大模型规模或使用提示工程技术并不能完全解决这些问题。为了更好地评估模型的物理常识理解能力，作者还提出了一种新的评估框架PhyGenEval。通过这些工具，研究人员希望推动社区在模型中优先学习物理常识，而不仅仅是娱乐应用。'}}, 'hash': '17a92a30d25bd139', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.07167', 'title': 'Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate', 'url': 'https://huggingface.co/papers/2410.07167', 'abstract': 'We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.', 'score': 30, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'MIR: новый способ оценки качества мультимодальных моделей', 'desc': 'В статье представлен новый метрический показатель Modality Integration Rate (MIR) для оценки качества предварительного обучения мультимодальных моделей компьютерного зрения и обработки естественного языка (Large Vision Language Models, LVLMs). MIR позволяет эффективно оценивать качество обучения без необходимости дорогостоящей процедуры дообучения с учителем. Метрика основана на измерении межмодального расстояния распределений и демонстрирует положительную корреляцию с результатами тестирования после дообучения. MIR показывает устойчивость к различным конфигурациям обучения и архитектурам моделей.'}, 'en': {'title': '"MIR: A New Lens for Multi-Modal Model Mastery"', 'desc': 'The paper introduces the Modality Integration Rate (MIR), a new metric designed to evaluate the quality of multi-modal pre-training in Large Vision Language Models (LVLMs). Traditional metrics like loss and perplexity are not effective for assessing how well a language model aligns with new modalities during pre-training. MIR measures the inter-modal distribution distance, providing a more accurate indication of pre-training quality and its potential performance after fine-tuning. The authors demonstrate that MIR is effective, robust, and generalizable across different training setups, aiding in better data selection and model design.'}, 'zh': {'title': '模态整合率：提升多模态预训练质量的新指标', 'desc': '这篇论文介绍了一种新的指标，称为模态整合率（MIR），用于评估大型视觉语言模型（LVLMs）的多模态预训练质量。传统的预训练指标如损失和困惑度在与新模态对齐时效果不佳，而MIR可以有效表示预训练质量，并与监督微调后的基准性能呈正相关。MIR在不同的训练和评估数据上表现出稳健性，并且可以在不同的训练配置和架构选择中推广。通过一系列实验，作者发现MIR在训练数据选择、训练策略安排和模型架构设计中具有指导意义。'}}, 'hash': '8e1f683abf35b291', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06373', 'title': 'Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning', 'url': 'https://huggingface.co/papers/2410.06373', 'abstract': 'This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.', 'score': 23, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#cv', '#architecture', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая тайны взаимосвязи архитектур и оптимизаторов в компьютерном зрении', 'desc': "Статья исследует взаимосвязь между архитектурами нейронных сетей для компьютерного зрения и оптимизаторами, выявляя феномен 'смещения связи между архитектурой и оптимизатором' (BOCB). Авторы обнаружили, что классические CNN лучше работают с семейством SGD-оптимизаторов, а современные архитектуры, такие как ViT и ConvNeXt, тесно связаны с адаптивными оптимизаторами. Исследование показывает, что BOCB может влиять на предварительное обучение и дообучение моделей компьютерного зрения. На основе эмпирического анализа авторы дают рекомендации по выбору оптимизаторов и созданию устойчивых архитектур нейронных сетей для задач компьютерного зрения."}, 'en': {'title': 'Rethink Your Backbone: Optimizer Matters!', 'desc': "This paper explores how different vision model architectures, known as backbones, interact with various optimizers, revealing a phenomenon called backbone-optimizer coupling bias (BOCB). It finds that traditional convolutional neural networks (CNNs) like VGG and ResNet work best with stochastic gradient descent (SGD) optimizers, while newer models like Vision Transformers (ViTs) and ConvNeXt are more compatible with adaptive learning rate optimizers. The study shows that both the choice of optimizer and the design of the backbone can introduce BOCB, affecting the model's performance during pre-training and fine-tuning. The authors provide recommendations for choosing optimizers and insights into designing robust vision backbones, encouraging the community to rethink established practices."}, 'zh': {'title': '重新思考视觉骨干与优化器的耦合关系', 'desc': '这篇论文研究了视觉骨干网络和优化器之间的相互作用，提出了一个称为骨干-优化器耦合偏差（BOCB）的现象。研究发现，传统的卷积神经网络（如VGG和ResNet）与SGD优化器家族有明显的依赖关系，而新型架构如ViTs和ConvNeXt则与自适应学习率优化器紧密耦合。BOCB可以由优化器和某些骨干设计引入，并可能显著影响视觉模型的预训练和下游微调。通过深入的实证分析，论文总结了推荐的优化器和稳健的视觉骨干架构的见解。'}}, 'hash': 'bf22e906e7d30eea', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.05954', 'title': 'Pyramidal Flow Matching for Efficient Video Generative Modeling', 'url': 'https://huggingface.co/papers/2410.05954', 'abstract': 'Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.', 'score': 22, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Пирамидальное сопоставление потоков: новый подход к эффективной генерации видео', 'desc': 'Статья представляет новый алгоритм пирамидального сопоставления потоков для генерации видео. Этот метод переосмысливает траекторию шумоподавления как серию пирамидальных этапов, где только финальный этап работает в полном разрешении. Подход позволяет оптимизировать весь процесс end-to-end с использованием единого Diffusion Transformer (DiT). Эксперименты показывают, что метод способен генерировать высококачественные видео длительностью 5-10 секунд с разрешением 768p и частотой 24 кадра в секунду.'}, 'en': {'title': '"Efficient Video Generation with Pyramidal Flow Matching"', 'desc': 'This paper presents a novel approach to video generation by introducing a unified pyramidal flow matching algorithm. The method reinterprets the denoising process as a series of pyramid stages, optimizing only the final stage at full resolution to enhance efficiency. By linking flows across pyramid stages, the approach maintains continuity and supports autoregressive video generation with a temporal pyramid. The framework is optimized end-to-end using a single Diffusion Transformer, achieving high-quality video generation with reduced computational demands.'}, 'zh': {'title': '金字塔流匹配：高效视频生成的新方法', 'desc': '这篇论文提出了一种新的金字塔流匹配算法，用于更高效的视频生成建模。通过将去噪轨迹重新解释为一系列金字塔阶段，只有最后一个阶段在全分辨率下操作，从而减少计算复杂性。该方法通过时间金字塔压缩全分辨率历史，实现自回归视频生成。整个框架可以通过单一的统一扩散变压器进行端到端优化。'}}, 'hash': '365906882dc00532', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.07177', 'title': 'MM-Ego: Towards Building Egocentric Multimodal LLMs', 'url': 'https://huggingface.co/papers/2410.07177', 'abstract': 'This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models\' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel "Memory Pointer Prompting" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.', 'score': 16, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#architecture'], 'emoji': '👀', 'ru': {'title': 'MM-Ego: Мультимодальная модель для глубокого понимания эгоцентрического видео', 'desc': "Исследование направлено на создание мультимодальной модели для понимания эгоцентрического видео. Авторы разработали генератор данных, создающий 7 миллионов высококачественных пар вопрос-ответ для эгоцентрических видео различной длительности. Они также представили новый бенчмарк для оценки способности моделей распознавать и запоминать визуальные детали в видео разной длины. Предложенная авторами архитектура включает механизм 'Memory Pointer Prompting', который позволяет модели эффективнее понимать длинные видео."}, 'en': {'title': 'Unlocking the Secrets of Egocentric Videos with MM-Ego', 'desc': 'This paper presents the development of a multimodal foundation model called MM-Ego for understanding egocentric videos. The researchers created a large dataset of 7 million QA samples to address the lack of data in this area, and introduced a challenging benchmark to test the model\'s ability to recognize and remember visual details. They also developed a new evaluation method to reduce language bias and proposed a novel architecture with a "Memory Pointer Prompting" mechanism to improve video comprehension. The result is a powerful model that enhances the understanding of long and complex egocentric video content.'}, 'zh': {'title': 'MM-Ego：自我中心视频理解的多模态基础模型', 'desc': '这项研究旨在构建一个多模态基础模型，用于理解以自我为中心的视频。首先，我们开发了一个数据引擎，生成了700万高质量的问答样本，这是目前最大的自我中心问答数据集。其次，我们提供了一个具有挑战性的基准测试，包含629个视频和7026个问题，并引入了一种新的去偏评估方法。最后，我们提出了一种专门的多模态架构，包含“记忆指针提示”机制，以更好地理解长视频内容。'}}, 'hash': '46a93631041dba07', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05355', 'title': 'Falcon Mamba: The First Competitive Attention-free 7B Language Model', 'url': 'https://huggingface.co/papers/2410.05355', 'abstract': 'In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.', 'score': 15, 'issue_id': 40, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#architecture', '#inference'], 'emoji': '🦅', 'ru': {'title': 'Falcon Mamba 7B: Прорыв в эффективности языковых моделей', 'desc': 'Falcon Mamba 7B - это новая базовая языковая модель, основанная на архитектуре Mamba. Она обучена на 5,8 триллионах токенов и превосходит ведущие модели на основе трансформеров, такие как Mistral 7B и Llama3.1 8B. Falcon Mamba 7B демонстрирует лучшую производительность среди моделей Mamba своего масштаба и значительно быстрее работает при инференсе. Модель показывает, что чистая архитектура Mamba может достигать результатов, сравнимых или превосходящих гибридные модели Mamba-Transformer.'}, 'en': {'title': 'Falcon Mamba 7B: Redefining Efficiency and Performance in Language Models', 'desc': 'Falcon Mamba 7B is a new large language model that uses the innovative Mamba architecture, trained on a vast dataset of 5.8 trillion tokens. It outperforms other leading models like Mistral 7B and Llama3.1 8B, and is comparable to Gemma 7B, showcasing the strength of the pure Mamba design. The model is efficient, requiring less memory and offering faster inference for long sequences, challenging the notion that hybrid models are superior. The weights for Falcon Mamba 7B are available for public use, promoting further research and development in the field.'}, 'zh': {'title': '纯 Mamba 架构的力量：Falcon Mamba 7B 的卓越表现', 'desc': 'Falcon Mamba 7B 是一种基于全新 Mamba 架构的大型语言模型，经过 5.8 万亿个标记的训练。与其他基于 Transformer 的模型相比，它在性能上有显著提升，并且在推理速度和内存使用上更具优势。尽管混合架构通常被认为更优，但 Falcon Mamba 7B 证明了纯 Mamba 架构也能达到甚至超越混合架构的效果。该模型的权重已在 Hugging Face 上公开，供研究人员使用。'}}, 'hash': 'c700bbc81473edd9', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.06244', 'title': 'Story-Adapter: A Training-free Iterative Framework for Long Story Visualization', 'url': 'https://huggingface.co/papers/2410.06244', 'abstract': 'Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#story_generation', '#diffusion', '#cv'], 'emoji': '🎞️', 'ru': {'title': 'Story-Adapter: Новый подход к визуализации длинных историй', 'desc': 'В статье представлен фреймворк Story-Adapter для улучшения генерации изображений на основе длинных историй. Он использует итеративный подход с глобальным модулем кросс-внимания для сохранения семантической согласованности. Story-Adapter не требует дополнительного обучения и эффективен с вычислительной точки зрения. Эксперименты подтверждают превосходство метода в улучшении согласованности и детализации генерируемых изображений, особенно для длинных историй.'}, 'en': {'title': 'Enhancing Storytelling with Smarter Image Generation', 'desc': 'The paper introduces Story-Adapter, a framework designed to improve the generation of coherent images from long narratives using text-to-image models. It addresses challenges like maintaining semantic consistency and generating detailed interactions without extensive computational demands. The framework uses an iterative process with a global reference cross-attention module to refine images by incorporating text prompts and previously generated images. Experiments show that Story-Adapter enhances both the consistency and quality of images in long story visualizations.'}, 'zh': {'title': 'Story-Adapter：提升长篇故事图像生成的新方法', 'desc': '这篇论文介绍了一种名为Story-Adapter的框架，用于提升长篇故事的图像生成能力。该框架无需训练，且计算效率高，通过迭代的方式优化每一帧图像。核心技术是一个无需训练的全局参考交叉注意力模块，能够在保持语义一致性的同时降低计算成本。实验结果表明，Story-Adapter在长篇故事的语义一致性和细粒度交互生成能力上表现优异。'}}, 'hash': '8289ac05d8c302b2', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06961', 'title': 'Self-Boosting Large Language Models with Synthetic Preference Data', 'url': 'https://huggingface.co/papers/2410.06961', 'abstract': 'Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#rlhf', '#alignment'], 'emoji': '🔄', 'ru': {'title': 'SynPO: самоусиление языковых моделей без ручной разметки', 'desc': 'В статье представлен метод SynPO для улучшения языковых моделей без использования ручной разметки. SynPO использует итеративный механизм, где генератор создает разнообразные промпты, а улучшатель ответов постепенно совершенствует ответы модели. После четырех итераций SynPO, модели Llama3-8B и Mistral-7B показали значительное улучшение в способности следовать инструкциям и общей производительности. Этот подход позволяет языковым моделям автономно обучаться генеративным наградам для своих собственных выходных данных.'}, 'en': {'title': 'Empowering LLMs with Self-Improving Synthetic Data', 'desc': "The paper introduces SynPO, a method that uses synthetic data to align Large Language Models (LLMs) with human preferences, making them more honest, harmless, and helpful. SynPO works by generating prompts and refining responses iteratively, allowing models to learn and improve without extensive human input. This self-boosting approach significantly enhances the performance of LLMs like Llama3-8B and Mistral-7B, as shown by their improved scores in various evaluations. Overall, SynPO reduces the need for large-scale human annotation while boosting the models' ability to follow instructions and perform diverse tasks."}, 'zh': {'title': 'SynPO：用合成数据自我提升语言模型', 'desc': '这篇论文介绍了一种名为SynPO的自我提升方法，用于改进大型语言模型的对齐能力。SynPO通过生成合成偏好数据，减少了对大规模人工标注的依赖。该方法使用自我提示生成器和响应改进器，逐步优化模型的输出。经过多次迭代，模型在任务执行能力上有显著提升。'}}, 'hash': 'cbb5c59488cb0db9', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.07170', 'title': 'One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation', 'url': 'https://huggingface.co/papers/2410.07170', 'abstract': 'Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.', 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#rl', '#cv', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'EVA: Эффективная тонкая настройка фундаментальных моделей', 'desc': 'Статья представляет новый метод тонкой настройки фундаментальных моделей под названием EVA (Explained Variance Adaptation). Этот метод улучшает традиционный подход LoRA, инициализируя новые веса на основе данных с помощью сингулярного разложения. EVA перераспределяет ранги между матрицами весов для объяснения максимального количества дисперсии. Метод показывает более быструю сходимость и лучшие результаты на различных задачах, включая генерацию текста, понимание языка, классификацию изображений и обучение с подкреплением.'}, 'en': {'title': 'EVA: Boosting Model Fine-Tuning with Data-Driven Weight Initialization', 'desc': 'The paper introduces a new method called Explained Variance Adaptation (EVA) to improve the fine-tuning of foundation models. EVA enhances the low-rank adaptation (LoRA) technique by using data-driven initialization of weights through singular value decomposition. This approach allows for a more efficient distribution of ranks across weight matrices, leading to faster convergence and better performance. EVA is tested on various tasks, showing superior results compared to existing methods.'}, 'zh': {'title': '解释方差适应：提升基础模型微调效率的新方法', 'desc': '这篇论文介绍了一种改进的低秩适应方法，称为解释方差适应（EVA），用于微调基础模型。EVA通过对激活向量的小批量进行奇异值分解，以数据驱动的方式初始化新的权重矩阵。然后，使用获得的右奇异向量初始化LoRA矩阵，并重新分配所有权重矩阵的秩，以解释最大量的方差。实验表明，EVA在多种任务中比其他方法收敛更快，并取得了更高的平均分数。'}}, 'hash': 'd987cfa2ee91a2b5', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05591', 'title': 'TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation', 'url': 'https://huggingface.co/papers/2410.05591', 'abstract': "Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts. Results and source code are in our anonymous project page.", 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#diffusion', '#video', '#cv', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'TweedieMix: Смешивание персонализированных концепций в генерации изображений и видео', 'desc': 'TweedieMix - это новый метод для объединения персонализированных диффузионных моделей на этапе вывода. Он разделяет процесс сэмплирования на два этапа: сначала применяется техника сэмплирования с учетом нескольких объектов, а затем используется формула Твиди для смешивания внешнего вида персонализированных концепций. Метод позволяет генерировать изображения и видео с несколькими персонализированными концепциями более качественно, чем существующие подходы. TweedieMix также легко расширяется на диффузионные модели для преобразования изображений в видео.'}, 'en': {'title': 'TweedieMix: Mastering Multi-Concept Image and Video Generation', 'desc': "The paper introduces TweedieMix, a new method for generating images and videos that combine multiple personalized concepts using diffusion models. It divides the reverse diffusion sampling process into two stages: the first ensures the inclusion of target objects, and the second blends custom concepts using Tweedie's formula. This approach results in higher fidelity images and videos compared to existing methods. Additionally, TweedieMix can be easily adapted for image-to-video diffusion models, enhancing its versatility."}, 'zh': {'title': 'TweedieMix：多概念图像生成的新突破', 'desc': '这篇论文介绍了一种名为TweedieMix的新方法，用于在推理阶段组合定制的扩散模型。通过分析反向扩散采样的特性，该方法将采样过程分为两个阶段。在初始阶段，应用多对象感知采样技术以确保目标对象的包含。在后期阶段，使用Tweedie公式在去噪图像空间中融合定制概念的外观。'}}, 'hash': '1d4bfe307ff6d748', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06166', 'title': 'Temporal Reasoning Transfer from Text to Video', 'url': 'https://huggingface.co/papers/2410.06166', 'abstract': "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.", 'score': 11, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#rl', '#transfer_learning', '#video', '#reasoning'], 'emoji': '⏳', 'ru': {'title': 'Улучшение временного понимания видео-LLM через текстовый перенос', 'desc': 'Исследователи обнаружили, что видео-модели большого языка (Video LLMs) испытывают трудности с пониманием временных отношений не из-за неэффективного кодирования визуальных входных данных, а из-за ограничений базовой языковой модели в понимании временных концепций. Для решения этой проблемы был разработан метод Textual Temporal reasoning Transfer (T3), который синтезирует задачи временного рассуждения в текстовом формате из существующих наборов данных изображений и текста. Применение T3 позволило значительно улучшить временное понимание модели LongVA-7B без использования видеоданных. Результаты показали сильную корреляцию между производительностью в текстовых и видео задачах временного рассуждения.'}, 'en': {'title': 'Unlocking Temporal Understanding in Video LLMs with Textual Insights', 'desc': "The paper explores the limitations of Video Large Language Models (Video LLMs) in understanding temporal changes and relationships in videos. It identifies that the main issue lies not in the video data itself but in the LLM's inherent difficulty with temporal concepts, as shown by poor performance on text-based temporal tasks. To address this, the authors introduce Textual Temporal reasoning Transfer (T3), which uses text-based tasks to improve temporal reasoning without video data. This approach significantly enhances the model's performance on video benchmarks, demonstrating the effectiveness of transferring temporal reasoning skills from text to video."}, 'zh': {'title': '从文本到视频：时间推理能力的跨域转移', 'desc': '这篇论文研究了视频大语言模型（Video LLMs）在理解时间变化和推理时间关系方面的挑战。研究发现，问题的关键在于底层语言模型对时间概念的理解困难，而不是视频表示本身的信息不足。为了解决这个问题，作者提出了一种名为文本时间推理转移（T3）的方法，通过从现有的图文数据集中合成多样的纯文本时间推理任务来提升模型的时间理解能力。结果表明，T3方法在不使用视频数据的情况下显著提高了模型在复杂时间场景下的表现。'}}, 'hash': '5cd9bf212c19151b', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.07002', 'title': 'CursorCore: Assist Programming through Aligning Anything', 'url': 'https://huggingface.co/papers/2410.07002', 'abstract': 'Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.', 'score': 10, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#plp', '#benchmark', '#dataset'], 'emoji': '💻', 'ru': {'title': 'Интеллектуальный помощник программиста: новый подход к автоматизации кодирования', 'desc': 'Статья представляет новую систему для помощи в программировании, использующую большие языковые модели. Авторы разработали фреймворк, интегрирующий историю кодирования, текущий код и инструкции пользователя. Они также создали новый бенчмарк APEval для оценки моделей и pipeline для генерации обучающих данных. Результаты показывают, что их модель CursorCore превосходит аналоги сопоставимого размера.'}, 'en': {'title': 'Revolutionizing Coding Assistance with Integrated Conversational Frameworks', 'desc': 'The paper introduces a new conversational framework for programming assistance that integrates coding history, current code, and user instructions to improve automation in tasks like code completion and editing. It presents a benchmark called APEval to evaluate how well models align with different information types and the quality of their outputs. A data generation pipeline, Programming-Instruct, is developed to synthesize training data from sources like GitHub, resulting in 219K samples used to fine-tune models. The resulting CursorCore models outperform others of similar size, enhancing coding assistants with features like inline chat and automated editing.'}, 'zh': {'title': 'CursorCore：统一编程助手的未来', 'desc': '这篇论文提出了一种新的对话框架，可以全面整合编程过程中的各种信息来源，如编码历史、当前代码和用户指令。为了评估模型在编程辅助任务中的表现，研究者引入了一个新的基准测试APEval。通过一个名为Programming-Instruct的数据生成管道，研究者从GitHub等多种来源合成训练数据，并生成了219K样本。最终，开发的CursorCore系列模型在性能上优于其他同类模型。'}}, 'hash': '70a480b72654e749', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05295', 'title': 'AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs', 'url': 'https://huggingface.co/papers/2410.05295', 'abstract': 'In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.', 'score': 10, 'issue_id': 39, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#agents', '#security'], 'emoji': '🔓', 'ru': {'title': 'AutoDAN-Turbo: Автоматический взлом ИИ без участия человека', 'desc': 'AutoDAN-Turbo - это метод автоматического обнаружения уязвимостей в системах искусственного интеллекта без вмешательства человека. Он значительно превосходит базовые методы, достигая на 74.3% более высокого среднего показателя успешности атак на публичных бенчмарках. На модели GPT-4-1106-turbo AutoDAN-Turbo достигает 88.5% успешности атак. Кроме того, AutoDAN-Turbo может интегрировать существующие стратегии взлома, разработанные людьми, повышая успешность атак до 93.4% на GPT-4-1106-turbo.'}, 'en': {'title': 'AutoDAN-Turbo: Unleashing Automated AI Vulnerability Discovery', 'desc': 'AutoDAN-Turbo is a machine learning framework designed to automatically discover jailbreak strategies for AI models without human input. It significantly improves attack success rates, outperforming existing methods by 74.3% on average. The framework is versatile, allowing the integration of human-designed strategies to further enhance its effectiveness, achieving a 93.4% success rate on GPT-4-1106-turbo. This approach demonstrates the potential of automated systems in identifying vulnerabilities in AI models.'}, 'zh': {'title': 'AutoDAN-Turbo：自动化破解策略的革新', 'desc': '这篇论文介绍了一种名为AutoDAN-Turbo的黑箱破解方法，它可以在没有任何人为干预或预定义范围的情况下，从头开始自动发现尽可能多的破解策略。AutoDAN-Turbo在公共基准测试中表现优异，平均攻击成功率比基线方法高出74.3%。特别是，它在GPT-4-1106-turbo上达到了88.5%的攻击成功率。通过整合现有的人为设计的破解策略，AutoDAN-Turbo的攻击成功率甚至可以提高到93.4%。'}}, 'hash': 'bc2003c7b895855f', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.05651', 'title': 'ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler', 'url': 'https://huggingface.co/papers/2410.05651', 'abstract': 'Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.', 'score': 10, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Революционный подход к интерполяции видео с использованием двунаправленной выборки', 'desc': 'Статья представляет новую стратегию двунаправленной выборки для улучшения интерполяции ключевых кадров в моделях диффузии изображение-видео. Авторы предлагают последовательную выборку вдоль прямого и обратного путей, обусловленную начальным и конечным кадрами, что обеспечивает более согласованную генерацию промежуточных кадров. Метод включает в себя передовые техники направленной генерации, такие как CFG++ и DDS, для дальнейшего улучшения процесса интерполяции. Результаты демонстрируют высокую эффективность и качество генерации видео между ключевыми кадрами.'}, 'en': {'title': 'Smooth Transitions: Revolutionizing Video Interpolation with Bidirectional Sampling', 'desc': 'This paper introduces a new approach to improve video generation by focusing on keyframe interpolation using diffusion models. The authors propose a bidirectional sampling strategy that effectively addresses off-manifold issues without needing extensive re-noising. By conditioning on both start and end frames, the method ensures smoother and more coherent generation of intermediate frames. The integration of advanced guidance techniques, CFG++ and DDS, further enhances the quality and efficiency of the interpolation process, achieving state-of-the-art results.'}, 'zh': {'title': '双向采样：提升关键帧插值的新策略', 'desc': '这篇论文介绍了一种新的双向采样策略，用于改进图像到视频的生成模型，特别是在关键帧插值方面。传统方法在处理两个条件帧（起始和结束帧）时，常出现离开流形的问题，导致生成的中间帧出现瑕疵。新方法通过在前向和后向路径上进行顺序采样，确保生成的中间帧更加连贯和符合流形。结合先进的指导技术CFG++和DDS，该方法在关键帧插值上达到了最先进的性能。'}}, 'hash': 'd350fdc5aa2b2edc', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06084', 'title': 'Diversity-Rewarded CFG Distillation', 'url': 'https://huggingface.co/papers/2410.06084', 'abstract': 'Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.', 'score': 8, 'issue_id': 42, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#rl', '#audio'], 'emoji': '🎵', 'ru': {'title': 'Повышение качества и разнообразия генеративных моделей музыки', 'desc': 'Статья представляет новый метод обучения генеративных моделей для музыки, называемый diversity-rewarded CFG distillation. Этот подход объединяет дистилляцию Classifier-Free Guidance (CFG) с обучением с подкреплением для поощрения разнообразия. Авторы применяют метод к модели MusicLM для генерации музыки по текстовому описанию. Результаты показывают улучшение качества и разнообразия генерируемой музыки по сравнению с базовой моделью с CFG.'}, 'en': {'title': 'Boosting Creativity: Diverse Outputs Without Extra Cost', 'desc': 'This paper introduces a new method called diversity-rewarded CFG distillation to improve generative models used in creative fields like music generation. The approach combines a distillation objective, which helps the model mimic CFG-augmented predictions, with a reinforcement learning objective that rewards diversity in outputs. By finetuning the model, it achieves high-quality and diverse outputs without increasing inference costs. The method also allows for weight-based model merging, enabling control over the quality-diversity trade-off and enhancing performance.'}, 'zh': {'title': '多样性奖励蒸馏：提升生成模型的质量与多样性', 'desc': '这篇论文介绍了一种新的微调方法，称为多样性奖励的CFG蒸馏，旨在解决CFG在生成内容时的局限性。通过优化蒸馏目标和强化学习目标，该方法在不增加推理成本的情况下，提升了生成内容的质量和多样性。研究人员在MusicLM模型上进行了实验，结果显示这种方法在质量和多样性方面优于传统的CFG。通过权重插值策略，可以在部署时灵活调整生成内容的质量与多样性之间的平衡。'}}, 'hash': '99610ace8463ca57', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06885', 'title': 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', 'url': 'https://huggingface.co/papers/2410.06885', 'abstract': "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development.", 'score': 8, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#audio', '#multilingual', '#inference'], 'emoji': '🗣️', 'ru': {'title': 'F5-TTS: Революция в синтезе речи без авторегрессии', 'desc': 'F5-TTS - это неавторегрессивная система синтеза речи, основанная на сопоставлении потоков с использованием Diffusion Transformer. В отличие от сложных моделей, F5-TTS просто дополняет текстовый ввод до длины речи и выполняет шумоподавление для генерации. Система использует ConvNeXt для улучшения представления текста и стратегию Sway Sampling для повышения производительности. F5-TTS демонстрирует высокую естественность речи, возможность переключения кодов и контроль скорости.'}, 'en': {'title': 'F5-TTS: Simplifying Speech Synthesis with Flow Matching and Diffusion Transformers', 'desc': 'The paper presents F5-TTS, a novel text-to-speech system that simplifies the process by using flow matching with a Diffusion Transformer, eliminating the need for complex components like duration models and phoneme alignment. By padding text inputs to match the length of speech inputs, the system performs denoising to generate speech, improving upon the slow convergence and low robustness of previous models. The introduction of ConvNeXt refines text representation, and a new Sway Sampling strategy enhances performance and efficiency, achieving faster training and inference times. F5-TTS demonstrates natural and expressive speech capabilities, including zero-shot ability and seamless code-switching, trained on a large multilingual dataset.'}, 'zh': {'title': 'F5-TTS：流畅自然的文本到语音新突破', 'desc': '这篇论文介绍了一种名为F5-TTS的全非自回归文本到语音系统，基于流匹配和扩散变压器。与传统方法不同，它不需要复杂的设计，如时长模型、文本编码器和音素对齐，而是通过填充标记来简化输入。为了提高模型的性能和效率，作者提出了一种推理时的Sway采样策略，并使用ConvNeXt来优化文本表示。该系统在多语言数据集上训练，展示了自然流畅的语音生成能力，并且代码已公开以促进社区发展。'}}, 'hash': '753a8935b426d722', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05677', 'title': 'T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design', 'url': 'https://huggingface.co/papers/2410.05677', 'abstract': 'In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.', 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#video', '#diffusion', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: T2V-Turbo-v2 превосходит конкурентов', 'desc': 'В статье представлен метод T2V-Turbo-v2 для улучшения модели преобразования текста в видео на основе диффузии. Авторы предлагают дистиллировать модель согласованности из предобученной T2V модели, используя различные сигналы обучения. Метод включает интеграцию высококачественных данных, обратную связь от модели вознаграждения и условное руководство в процесс дистилляции согласованности. Эмпирически T2V-Turbo-v2 устанавливает новый state-of-the-art результат на бенчмарке VBench с общим счетом 85.13.'}, 'en': {'title': 'Turbocharge Your Text-to-Video Models with T2V-Turbo-v2!', 'desc': "The paper presents T2V-Turbo-v2, a method to improve text-to-video models by distilling a consistency model from a pretrained version. It uses high-quality data, feedback from reward models, and conditional guidance to enhance the model's performance. The study emphasizes the importance of using tailored datasets and diverse reward models to improve visual quality and text-video alignment. The approach also explores conditional guidance strategies, particularly focusing on motion guidance, to improve the motion quality of generated videos, achieving state-of-the-art results on VBench."}, 'zh': {'title': 'T2V-Turbo-v2：提升文本到视频生成的新突破', 'desc': '这篇论文介绍了一种名为T2V-Turbo-v2的新方法，通过从预训练的文本到视频模型中提取一致性模型来增强扩散模型。该方法结合了高质量训练数据、奖励模型反馈和条件引导等多种监督信号，显著提高了视觉质量和文本视频对齐。研究表明，针对特定学习目标定制数据集和从多样化奖励模型中学习是提升模型性能的关键。通过设计有效的能量函数，改进了运动引导策略，显著提升了生成视频的运动质量。'}}, 'hash': 'e30f39e021f2ee4b', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.05643', 'title': 'TRACE: Temporal Grounding Video LLM via Causal Event Modeling', 'url': 'https://huggingface.co/papers/2410.05643', 'abstract': "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE.", 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#video', '#multimodal', '#agents'], 'emoji': '🎬', 'ru': {'title': 'TRACE: Революция в понимании структуры видео с помощью каузального моделирования событий', 'desc': 'Статья представляет новый подход к задаче временной локализации в видео (Video Temporal Grounding). Авторы вводят концепцию каузального моделирования событий, представляя видео как последовательность событий. Они предлагают модель TRACE - мультизадачную видео-LLM, которая обрабатывает визуальные кадры, временные метки, оценки значимости и текст как отдельные задачи. Эксперименты показывают превосходство TRACE над современными видео-LLM в различных задачах VTG.'}, 'en': {'title': 'TRACE: Structuring Videos for Superior Understanding', 'desc': 'The paper introduces a new approach to Video Temporal Grounding (VTG) by using a causal event modeling framework, which structures videos as sequences of events. This framework allows for more effective video understanding by predicting current events based on previous ones, using timestamps, salient scores, and textual captions. The proposed model, TRACE, processes these components as distinct tasks with specialized encoders and decoders, improving the handling of VTG tasks. Experiments show that TRACE outperforms existing video language models, enhancing video browsing and editing capabilities.'}, 'zh': {'title': 'TRACE：革新视频时间定位的因果事件建模', 'desc': '这篇论文介绍了一种新的因果事件建模框架，用于视频时间定位任务。该框架将视频表示为事件序列，通过先前事件、视频输入和文本指令来预测当前事件。论文提出了一种名为TRACE的新型视频大语言模型，能够有效实现因果事件建模。实验结果表明，TRACE在多种视频时间定位任务中表现优于现有的最先进模型。'}}, 'hash': '7241bf4a739cd624', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.07064', 'title': 'Data Selection via Optimal Control for Language Models', 'url': 'https://huggingface.co/papers/2410.07064', 'abstract': "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.", 'score': 8, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#data', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Оптимизация выбора данных для эффективного предобучения языковых моделей', 'desc': 'Эта работа исследует выбор высококачественных данных для предобучения из массивных корпусов с целью улучшения возможностей языковых моделей. Авторы формулируют выбор данных как обобщенную задачу оптимального управления, решаемую с помощью принципа максимума Понтрягина. На основе теоретических результатов они представляют фреймворк PDS для приближенного оптимального выбора данных. Эксперименты показывают, что PDS ускоряет обучение моделей и повышает их эффективность на различных задачах, а также улучшает использование данных при ограниченных ресурсах.'}, 'en': {'title': '"Smart Data, Smarter Models: Optimizing Pre-training for Better AI"', 'desc': "This paper explores how to choose the best pre-training data from large datasets to improve language models (LMs) for future tasks. The authors use a mathematical approach called Pontryagin's Maximum Principle to find the best data selection strategy, which they call PMP-based Data Selection (PDS). Experiments show that PDS helps LMs learn faster and perform better on various tasks, even with very large models. Additionally, PDS makes better use of limited data, reducing the need for large datasets by almost half."}, 'zh': {'title': '通过PMP优化数据选择，提升语言模型性能', 'desc': '这项研究探讨了如何从大量语料库中选择高质量的预训练数据，以增强语言模型在下游任务中的能力。我们将数据选择问题表述为一个广义的最优控制问题，并通过庞特里亚金最大值原理（PMP）来解决，得出描述最佳数据选择与语言模型训练动态关系的必要条件。基于这些理论结果，我们引入了PMP数据选择（PDS）框架，通过解决PMP条件来近似最佳数据选择。在实验中，PDS选择的数据加速了语言模型的学习，并在各种下游任务中持续提升其性能。'}}, 'hash': 'f8459aee9413c793', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06458', 'title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'url': 'https://huggingface.co/papers/2410.06458', 'abstract': 'Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs\' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs\' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM\'s response needs refinement. Our results show that DeCRIM improves Mistral\'s performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.', 'score': 7, 'issue_id': 43, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#benchmark', '#dataset', '#rag'], 'emoji': '🎯', 'ru': {'title': 'Преодоление ограничений в следовании инструкциям для языковых моделей', 'desc': 'Статья представляет RealInstruct - первый бенчмарк для оценки способности языковых моделей следовать реальным многоограниченным инструкциям. Исследование показывает, что даже GPT-4 не соблюдает хотя бы одно ограничение в более чем 21% инструкций. Авторы предлагают pipeline DeCRIM для улучшения способности моделей следовать ограничениям. Результаты демонстрируют, что DeCRIM повышает производительность Mistral на 7.3% в RealInstruct и 8.0% в IFEval.'}, 'en': {'title': 'Mastering Multi-Constraint Instructions with DeCRIM', 'desc': 'The paper introduces RealInstruct, a benchmark designed to evaluate large language models (LLMs) on their ability to follow real-world instructions with multiple constraints. It highlights that even advanced models like GPT-4 struggle with such tasks, failing to meet at least one constraint in over 21% of cases. To improve performance, the authors propose the Decompose, Critique, and Refine (DeCRIM) pipeline, which breaks down instructions into constraints and uses a Critic model to refine responses. The study shows that DeCRIM significantly enhances the performance of open-source models, even surpassing GPT-4 with strong feedback.'}, 'zh': {'title': '突破多重约束：提升LLM的指令执行能力', 'desc': '这篇论文介绍了一个名为RealInstruct的新基准，用于评估大型语言模型（LLM）在处理真实世界多重约束指令时的能力。研究发现，即使是先进的GPT-4模型，在21%以上的指令中也未能满足至少一个约束。为了解决这一问题，作者提出了DeCRIM自我修正流程，通过分解指令和使用批评模型来提高LLM的表现。结果显示，DeCRIM显著提升了开源模型的性能，甚至在某些情况下超过了GPT-4。'}}, 'hash': 'f8cc2a4349e584a6', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.02465', 'title': 'Response Tuning: Aligning Large Language Models without Instruction', 'url': 'https://huggingface.co/papers/2410.02465', 'abstract': 'Instruction tuning-supervised fine-tuning using instruction-response pairs-is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs.', 'score': 7, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#alignment'], 'emoji': '🎯', 'ru': {'title': 'Response Tuning: эффективное обучение языковых моделей без инструкций', 'desc': 'Исследователи предлагают метод Response Tuning (RT) для обучения языковых моделей без использования инструкций. RT фокусируется только на пространстве ответов, позволяя моделям эффективно реагировать на различные запросы. Эксперименты показывают, что модели RT могут быть такими же полезными, как и модели, обученные с инструкциями. Контроль распределения ответов при обучении может улучшить предпочтения пользователей и желаемое поведение моделей.'}, 'en': {'title': 'Response Tuning: Unlocking LLM Potential with Focused Supervision', 'desc': 'The paper explores a method called Response Tuning (RT) to improve the performance of Large Language Models (LLMs) as chat assistants. Instead of using instruction-response pairs, RT focuses solely on supervising the response space, which helps the models respond effectively to various instructions. The study shows that RT-trained models can be as helpful as those trained with traditional instruction tuning. Additionally, by controlling the response distribution during training, these models can be guided to exhibit desired behaviors, such as refusing unsafe requests.'}, 'zh': {'title': '响应调优：释放预训练模型的潜力', 'desc': '这篇论文探讨了如何将预训练的大型语言模型转变为有用且安全的聊天助手。研究提出了一种名为响应调优的方法，专注于对模型输出的监督，而不是指令调优。实验表明，仅通过响应训练的模型可以有效地处理各种指令，并且表现出与指令调优模型相当的帮助性。研究还发现，通过控制训练响应的分布，可以显著提高用户偏好，并在处理不安全查询时表现出拒绝协助的行为。'}}, 'hash': '0682a2364e4a8840', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.04223', 'title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'url': 'https://huggingface.co/papers/2410.04223', 'abstract': 'While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-05', 'pub_date_ru': '5 октября', 'data': {'categories': ['#multimodal', '#cv', '#graph', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'Llamole: прорыв в генерации молекул с помощью мультимодальных языковых моделей', 'desc': 'Llamole - это первая мультимодальная языковая модель, способная генерировать текст и графы молекул. Она объединяет базовую языковую модель с Graph Diffusion Transformer и графовыми нейронными сетями для многоусловной генерации молекул и вывода реакций. Llamole также интегрирует A* поиск с функциями стоимости на основе языковой модели для эффективного ретросинтетического планирования. Модель значительно превосходит 14 адаптированных языковых моделей по 12 метрикам для контролируемого дизайна молекул и ретросинтетического планирования.'}, 'en': {'title': 'Llamole: Bridging Text and Graphs for Smarter Molecular Design', 'desc': 'The paper introduces Llamole, a groundbreaking multimodal large language model that can generate both text and graphs, specifically designed for applications in materials and drug design. Llamole combines a base language model with advanced graph processing techniques like the Graph Diffusion Transformer and Graph Neural Networks to handle complex molecular generation and reaction inference. It also uses A* search with language model-based cost functions to improve retrosynthetic planning, making it more efficient. Extensive experiments show that Llamole outperforms other adapted language models in controllable molecular design and retrosynthetic planning, demonstrating its potential in these fields.'}, 'zh': {'title': 'Llamole：突破图形生成的多模态大语言模型', 'desc': '大语言模型（LLM）在处理图形数据时面临挑战，限制了其在材料和药物设计中的应用。为了解决这个问题，我们引入了Llamole，这是第一个能够生成文本和图形的多模态LLM。Llamole结合了图扩散变压器和图神经网络，实现了分子逆向设计和反应推断。实验表明，Llamole在可控分子设计和逆合成规划方面显著优于其他模型。'}}, 'hash': '9c153ad16c36d327', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}}, {'id': 'https://huggingface.co/papers/2410.02503', 'title': 'Mixed-Session Conversation with Egocentric Memory', 'url': 'https://huggingface.co/papers/2410.02503', 'abstract': "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.", 'score': 6, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#dataset', '#agents', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Революция в диалоговых системах: непрерывное общение с множеством партнеров', 'desc': 'Статья представляет новую систему диалогов под названием Mixed-Session Conversation, которая способна вести разговоры с несколькими партнерами в многосессионном формате. Авторы предлагают набор данных MiSC, состоящий из 6 последовательных сессий с участием четырех собеседников. Также представлена модель EMMA с новым механизмом управления памятью, которая собирает и сохраняет воспоминания с точки зрения главного говорящего. Оценки показывают, что диалоги в MiSC демонстрируют непрерывный разговорный поток даже при смене собеседников, а EMMA сохраняет высокую запоминаемость без противоречий.'}, 'en': {'title': 'Revolutionizing Dialogue Systems with Multi-Session Dynamics', 'desc': 'The paper introduces a new dialogue system called Mixed-Session Conversation (MiSC) designed to handle dynamic, long-term interactions with multiple partners. It addresses the limitations of current systems by incorporating deeply layered interactions and expanded conversation networks. A novel dataset, MiSC, is used to train the system, featuring dialogue episodes with multiple sessions and speakers. The proposed model, EMMA, uses an innovative memory management mechanism to ensure continuity and coherence in conversations, validated through extensive human evaluations.'}, 'zh': {'title': '多方长时间互动的对话新纪元', 'desc': '这篇论文介绍了一种新的对话系统，称为混合会话对话系统，旨在模拟真实世界中的多方长时间互动。研究者提出了一个新的数据集MiSC，用于支持该系统的实现，其中每个对话包含6个连续会话和四个参与者。为了增强对话的连续性，论文中还介绍了一种新的对话模型EMMA，它通过从主说话者的视角收集和管理记忆来实现。通过广泛的人类评估，证明MiSC中的对话即使在会话伙伴变化时也能保持流畅，EMMA在整个对话中保持高记忆性且无矛盾。'}}, 'hash': '87a67ed9acbc74a2', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.06555', 'title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'url': 'https://huggingface.co/papers/2410.06555', 'abstract': "As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.", 'score': 6, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ING-VP: Новый рубеж в оценке пространственного мышления ИИ', 'desc': 'Представлен новый бенчмарк ING-VP для оценки пространственного воображения и многошагового рассуждения мультимодальных языковых моделей. Бенчмарк содержит 6 игр с 300 уровнями и 6 конфигурациями каждый, что позволяет провести более 60 000 раундов взаимодействия. ING-VP оценивает модели в различных условиях, включая сравнение входных данных изображение-текст и только текст. Лучшая модель Claude-3.5 Sonnet показала точность всего 3.37%, что подчеркивает сложность задачи.'}, 'en': {'title': 'Pushing Boundaries: Evaluating Spatial Reasoning in MLLMs with ING-VP', 'desc': 'The paper introduces ING-VP, a new benchmark designed to test the spatial reasoning and multi-step planning abilities of multimodal large language models (MLLMs). It features interactive games that challenge models to understand and plan based on spatial relationships in images. The benchmark allows for various comparison settings, such as image-text versus text-only inputs, to evaluate different aspects of model performance. Despite testing several advanced models, the highest accuracy achieved was only 3.37%, highlighting the difficulty of the tasks and the need for further advancements in this area.'}, 'zh': {'title': '推动多模态模型的空间推理新高度', 'desc': '这篇论文介绍了一种新的基准测试，名为ING-VP，用于评估多模态大语言模型（MLLMs）的空间想象和多步推理能力。ING-VP通过6个不同的游戏和300个关卡，测试模型在图像中的空间关系上的多步规划能力。研究发现，即使是最先进的模型在这个基准测试中的表现也远低于预期，表明当前模型在复杂空间推理和规划方面还有很大提升空间。这个工作旨在推动MLLMs在复杂任务中的能力发展。'}}, 'hash': 'dabc7be39f163aa7', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05791', 'title': 'FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance', 'url': 'https://huggingface.co/papers/2410.05791', 'abstract': 'Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.', 'score': 6, 'issue_id': 40, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#dataset', '#3d', '#rl', '#diffusion'], 'emoji': '🎹', 'ru': {'title': 'Виртуозное воссоздание игры на пианино с помощью ИИ', 'desc': 'Эта статья представляет новый набор данных, содержащий 10 часов 3D-движений рук и аудио от 15 пианистов высокого уровня, играющих классическую музыку. Авторы разработали систему безмаркерного захвата движений с использованием мультиракурсной съемки и современных моделей оценки позы. На основе собранных данных создан конвейер для синтеза реалистичных движений рук при игре на пианино, используя имитационное обучение и обучение с подкреплением. Модель генерирует естественные и точные движения, которые обобщаются на музыку вне обучающего набора.'}, 'en': {'title': 'Mastering Piano with AI: Crafting Realistic Hand Motions', 'desc': 'The paper introduces a novel dataset capturing 3D hand motions and audio from elite pianists, using advanced pose estimation and inverse kinematics to ensure accuracy. This dataset is used to develop a model that synthesizes realistic hand movements for piano playing, employing imitation and reinforcement learning for bimanual control. A diffusion model enhances sampling efficiency by generating reference motions, which are further refined using musical similarity to improve precision. The resulting model can produce natural and dexterous hand motions for musical pieces not included in the training data.'}, 'zh': {'title': '钢琴演奏的手部动作建模新突破', 'desc': '这篇论文介绍了一种新的大规模数据集，包含15位顶级钢琴家演奏153首古典音乐的3D手部动作和音频数据。研究人员使用无标记的多视角视频和先进的姿态估计模型来重建自然的演奏动作，并通过逆运动学和高分辨率MIDI数据进行精细化处理。通过模仿学习和强化学习相结合的方法，研究人员开发了一种合成物理合理的手部动作的流程。为了提高采样效率，他们使用扩散模型生成自然的参考动作，并通过音乐相似性增强数据，提升强化学习策略的精度。'}}, 'hash': 'cba54e31692e353c', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06172', 'title': 'Multimodal Situational Safety', 'url': 'https://huggingface.co/papers/2410.06172', 'abstract': 'Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.', 'score': 6, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'Новый рубеж в безопасности ИИ: оценка ситуационной осведомленности мультимодальных моделей', 'desc': 'Статья представляет первый анализ проблемы многомодальной ситуационной безопасности в контексте мультимодальных больших языковых моделей (MLLM). Авторы разработали набор данных MSSBench для оценки способности MLLM учитывать безопасность в различных ситуациях. Исследование показало, что существующие MLLM испытывают трудности с комплексным решением задач ситуационной безопасности. Предложенные мультиагентные подходы демонстрируют улучшение безопасности по сравнению с исходными ответами MLLM.'}, 'en': {'title': 'Ensuring Safety in Multimodal AI: A New Benchmark for Contextual Understanding', 'desc': 'This paper introduces the concept of Multimodal Situational Safety, focusing on how safety considerations change based on the context in which a Multimodal Large Language Model (MLLM) operates. The authors present a new benchmark, MSSBench, to evaluate how well MLLMs can assess safety by analyzing 1,820 language query-image pairs. The study finds that current MLLMs struggle with understanding and responding to situational safety challenges, indicating a need for further research in this area. To address these challenges, the authors propose multi-agent pipelines that improve safety performance over standard MLLM responses.'}, 'zh': {'title': '多模态情境安全：MLLMs的新挑战', 'desc': '多模态大语言模型（MLLMs）正在迅速发展，展现出作为多模态助手的强大能力，但也带来了安全性问题。本文首次提出并分析了一种新的安全挑战，称为多模态情境安全，研究了用户或代理在特定情境下的安全考量。为了评估这种能力，我们开发了多模态情境安全基准（MSSBench），用于评估当前MLLMs的情境安全性能。研究发现，现有的MLLMs在处理这种复杂的安全问题时存在困难，尤其是在指令跟随的情况下。'}}, 'hash': '9fee460887b3cb38', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.05664', 'title': 'Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning', 'url': 'https://huggingface.co/papers/2410.05664', 'abstract': 'As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.', 'score': 6, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#benchmark', '#diffusion', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Разобучение генеративных моделей: проблемы и перспективы', 'desc': 'Статья посвящена исследованию методов разобучения моделей генерации изображений для удаления нежелательной информации. Авторы проанализировали существующие подходы к разобучению по пяти ключевым аспектам в различных сценариях. Исследование выявило, что все методы имеют побочные эффекты или ограничения, особенно в сложных реалистичных ситуациях. Авторы представили комплексную систему оценки с открытым исходным кодом для стимулирования дальнейших исследований в этой области.'}, 'en': {'title': "Unlearning: Cleaning Up AI's Act", 'desc': "This paper explores the concept of model unlearning in text-to-image diffusion models, which aims to remove harmful or unwanted information from pre-trained models. The authors highlight that while unlearning is often evaluated based on the model's ability to generate specific concepts without losing image quality, it is usually tested in limited scenarios. They conduct a thorough analysis of unlearning across various situations, identifying that all current methods have side effects or limitations, particularly in complex environments. By providing a comprehensive evaluation framework, the authors aim to encourage further research to develop more reliable unlearning techniques."}, 'zh': {'title': '探索模型遗忘：消除不良信息的挑战与机遇', 'desc': '随着文本到图像扩散模型的进步，它们在商业应用中的潜力越来越大，但也引发了对其可能被恶意使用的担忧。模型遗忘技术被提出以消除预训练模型中不需要的和潜在有害的信息。本文深入分析了在不同场景下的遗忘效果，发现每种方法都有其副作用或局限性。我们发布了一个全面的评估框架，希望能激发更多关于可靠和有效的遗忘方法的研究。'}}, 'hash': '9630c1ce042de601', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.02428', 'title': 'Collective Critics for Creative Story Generation', 'url': 'https://huggingface.co/papers/2410.02428', 'abstract': "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.", 'score': 5, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_ru': '3 октября', 'data': {'categories': ['#story_generation', '#multimodal'], 'emoji': '📚', 'ru': {'title': 'CritiCS: Коллективный подход к созданию креативных историй с помощью ИИ', 'desc': 'Статья представляет новый подход к генерации длинных историй с помощью больших языковых моделей, называемый CritiCS. Этот метод использует коллективный механизм критики для улучшения креативности и выразительности генерируемых историй. CritiCS состоит из двух этапов: уточнения плана (CrPlan) и генерации текста (CrText), где группа ИИ-критиков и лидер сотрудничают для постепенного улучшения черновиков. Оценка показала, что CritiCS значительно повышает креативность историй и вовлеченность читателей, сохраняя при этом связность повествования.'}, 'en': {'title': 'CritiCS: Elevating Storytelling with Creative Collaboration', 'desc': 'The paper introduces a new framework called CritiCS for generating long stories using Large Language Models (LLMs). CritiCS consists of two stages: CrPlan for refining story plans and CrText for generating the story, both involving a group of LLM critics and a leader to enhance creativity and expressiveness. This approach not only maintains narrative coherence but also significantly boosts creativity and reader engagement, as shown by extensive human evaluations. Additionally, the framework supports interactive collaboration between human writers and the machine, allowing humans to participate actively in the critique process.'}, 'zh': {'title': 'CritiCS：提升故事创造性与连贯性的创新框架', 'desc': '这篇论文提出了一种新的框架CritiCS，用于生成具有创造性和叙述连贯性的长篇故事。CritiCS框架包括计划优化阶段（CrPlan）和故事生成阶段（CrText），通过集体修订机制提高故事的创造性和吸引力。在每个阶段，多个大型语言模型的评论者和一个领导者合作，逐步完善计划和故事草稿。人类评估表明，CritiCS框架显著提升了故事的创造性和读者的参与度，同时保持了叙述的连贯性。'}}, 'hash': 'fd2a584ee28c9b94', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}}, {'id': 'https://huggingface.co/papers/2410.07071', 'title': 'Retrieval-Augmented Decision Transformer: External Memory for In-context RL', 'url': 'https://huggingface.co/papers/2410.07071', 'abstract': "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.", 'score': 5, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#rl', '#rag', '#agents'], 'emoji': '🧠', 'ru': {'title': 'RA-DT: Эффективное обучение с подкреплением в контексте с помощью извлечения релевантного опыта', 'desc': 'Статья представляет новый метод обучения с подкреплением в контексте - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT использует внешнюю память для хранения прошлого опыта и извлекает только релевантные текущей ситуации под-траектории. Метод не требует обучения компонента извлечения и может быть независимым от домена. RA-DT превосходит базовые модели на сеточных мирах, используя лишь часть длины их контекста.'}, 'en': {'title': 'Revolutionizing In-Context Learning with RA-DT: Efficient, Domain-Agnostic, and Powerful', 'desc': 'The paper introduces the Retrieval-Augmented Decision Transformer (RA-DT), a novel approach in Reinforcement Learning that enhances in-context learning by using an external memory to store and retrieve relevant sub-trajectories. Unlike previous methods that require entire episodes, RA-DT efficiently handles complex environments with long episodes and sparse rewards by focusing only on pertinent past experiences. The retrieval mechanism in RA-DT is domain-agnostic and does not need training, making it versatile across different tasks. Evaluations show that RA-DT outperforms existing methods in grid-worlds and other environments, highlighting its potential to overcome current limitations in in-context RL.'}, 'zh': {'title': '检索增强：强化学习中的新突破', 'desc': '这篇论文介绍了一种新的方法，称为检索增强决策变换器（RA-DT），用于在强化学习中实现上下文学习。RA-DT通过外部记忆机制存储过去的经验，并检索与当前情况相关的子轨迹，从而解决了传统方法在复杂环境中长时间段和稀疏奖励的问题。RA-DT在网格世界、机器人模拟和程序生成的视频游戏中表现优异，超越了基线方法。研究还揭示了当前上下文强化学习方法在复杂环境中的局限性，并讨论了未来的研究方向。'}}, 'hash': '0815ef9a074b159d', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06241', 'title': 'BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way', 'url': 'https://huggingface.co/papers/2410.06241', 'abstract': 'The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'BroadWay: Повышение качества генерации видео без дополнительного обучения', 'desc': 'Статья представляет метод BroadWay для улучшения качества генерации видео из текста без дополнительного обучения или параметров. Метод основан на наблюдении связи между различиями во временных картах внимания и временной несогласованностью в видео. BroadWay включает два компонента: Temporal Self-Guidance для улучшения правдоподобности и согласованности, и Fourier-based Motion Enhancement для усиления движения. Эксперименты показывают значительное улучшение качества генерации видео с минимальными дополнительными затратами.'}, 'en': {'title': 'BroadWay: Elevating Text-to-Video Quality Without Extra Costs', 'desc': 'The paper introduces BroadWay, a method to enhance text-to-video generation by addressing common issues like structural implausibility and temporal inconsistency. It identifies that disparities in temporal attention maps lead to these inconsistencies and that the energy in these maps correlates with motion amplitude. BroadWay uses Temporal Self-Guidance to align attention maps for better video consistency and Fourier-based Motion Enhancement to boost motion richness. This approach improves video quality without adding extra parameters or increasing computational costs.'}, 'zh': {'title': 'BroadWay：提升文本到视频生成质量的新方法', 'desc': '这篇论文研究了文本到视频生成模型中常见的问题，如结构不合理和时间不一致。研究发现，不同块的时间注意力图之间的差异与时间不一致有关，而注意力图中的能量与视频中的运动幅度有关。基于这些观察，提出了一种名为BroadWay的方法，通过减少注意力图的差异和增强能量来提高视频质量。实验表明，BroadWay在不增加额外成本的情况下显著提升了视频生成的效果。'}}, 'hash': 'b38bb081870f60d1', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.06462', 'title': 'Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders', 'url': 'https://huggingface.co/papers/2410.06462', 'abstract': 'The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert\'s models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM\'s directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners\' safety policies when posed directly without the accompanying coding support request.', 'score': 5, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#security', '#hallucinations'], 'emoji': '🕵️', 'ru': {'title': 'Неожиданные уязвимости LLM при рекомендации кода', 'desc': "Данное исследование посвящено изучению потенциальных уязвимостей больших языковых моделей (LLM) в контексте рекомендаций кода. Авторы демонстрируют, что при резком изменении контекста, например, при решении задачи программирования, LLM могут снизить свою защиту и предложить потенциально вредоносный код. Эксперименты показывают, как модели могут рекомендовать API-эндпоинты, которые злоумышленники могут использовать для атак. Исследование сравнивает этот тип атаки с предыдущими работами по смене контекста и рассматривает его как новую версию атак типа 'living off the land' в литературе о вредоносном ПО."}, 'en': {'title': 'Exploiting Context: The Hidden Vulnerability in AI Code Recommendations', 'desc': "This paper explores how adversaries can exploit large language models (LLMs) to introduce malicious code into popular code repositories by manipulating context. It highlights that while LLMs from companies like OpenAI and Google have safeguards against harmful outputs, these can be bypassed when the context of a request changes, such as during programming challenges. The study provides examples of how repositories like GitHub and content delivery networks can be used to spread malicious code through seemingly innocent API recommendations. The research compares this method to 'living off the land' attacks, where LLMs inadvertently suggest harmful actions by misinterpreting user prompts."}, 'zh': {'title': '基础模型的上下文漏洞：编程挑战中的隐患', 'desc': '这项研究探讨了在流行代码库中引入恶意代码的对抗性潜力，特别是通过复制代码或虚构的AI建议。尽管OpenAI、Google和Anthropic的基础大语言模型（LLM）对有害行为和有毒字符串有防护，但在数学解决方案中嵌入有害提示的工作表明，这些防护措施在专家环境中可能有所不同。研究表明，当问题的上下文发生变化时，基础模型可能会放松警惕，尤其是在解决编程挑战时。通过实证例子展示了在GitHub、NPM等平台上，恶意代码如何利用这些漏洞进行攻击。'}}, 'hash': 'a2ceecf54aefe4d6', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06949', 'title': 'Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach', 'url': 'https://huggingface.co/papers/2410.06949', 'abstract': 'In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.', 'score': 5, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#agents', '#plp'], 'emoji': '🛡️', 'ru': {'title': 'Seeker: Улучшение обработки исключений с помощью LLM и мультиагентного подхода', 'desc': 'Статья посвящена проблеме обработки исключений в разработке программного обеспечения и предлагает решение с использованием больших языковых моделей (LLM). Авторы выявили три ключевые проблемы: нечувствительное обнаружение хрупкого кода, неточный захват типов исключений и искаженные решения по обработке. Предложена система Seeker - мультиагентный фреймворк, вдохновленный стратегиями экспертов-разработчиков для обработки исключений. Seeker использует агенты для помощи LLM в более эффективном обнаружении, захвате и разрешении исключений.'}, 'en': {'title': '"Seeker: Revolutionizing Exception Handling with AI"', 'desc': 'The paper addresses the challenge of poor exception handling in software development, which affects code robustness and reliability. It highlights the use of large language models (LLMs) to improve exception handling by identifying common issues like insensitive detection, inaccurate capture, and distorted handling of exceptions. The authors propose a multi-agent framework called Seeker, which includes agents like Scanner and Handler to assist LLMs in better managing exceptions. This study is the first to systematically explore using LLMs for enhancing exception handling, offering insights for improving software quality.'}, 'zh': {'title': '用LLMs提升代码异常处理的可靠性', 'desc': '在软件开发中，不当或缺失的异常处理会影响代码的稳健性和可靠性。我们研究了使用大型语言模型（LLMs）来改善代码中的异常处理。通过分析，我们发现了三个主要问题：脆弱代码的检测不敏感、异常类型捕获不准确以及处理方案失真。为了解决这些问题，我们提出了一个名为Seeker的多代理框架，帮助LLMs更有效地检测、捕获和解决异常。'}}, 'hash': '7b96c448f8692316', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06985', 'title': 'Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control', 'url': 'https://huggingface.co/papers/2410.06985', 'abstract': 'Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.', 'score': 4, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Согласованность многоракурсных представлений в диффузионных моделях для PBR текстурирования', 'desc': 'Статья посвящена проблеме согласованности представлений в моделях диффузии изображений. Авторы исследуют этот вопрос в контексте рабочего процесса Collaborative Control для задачи Text-to-Texture с PBR (Physically Based Rendering). Модель Collaborative Control напрямую моделирует распределения вероятностей PBR-изображений, включая карты нормалей. В работе обсуждаются решения по обеспечению согласованности между различными ракурсами и демонстрируется эффективность предложенного подхода.'}, 'en': {'title': 'Achieving Multi-View Harmony in PBR Text-to-Texture Models', 'desc': 'The paper addresses the challenge of achieving multi-view consistency in image diffusion models, particularly in the Text-to-Texture problem where geometric correspondences are known. It introduces a Collaborative Control workflow that directly models PBR image probability distributions, including normal bump maps, making it unique in its ability to output full PBR stacks. The authors discuss the design choices that enhance multi-view consistency and validate their approach through ablation studies and practical applications. This work aims to improve the alignment of predictions across views, reducing the need for complex fusion methods.'}, 'zh': {'title': '实现多视图一致性的PBR图像扩散模型', 'desc': '这篇论文探讨了图像扩散模型在多视图一致性方面的挑战，特别是在文本到纹理问题中。即使在几何对应关系已知的情况下，许多方法仍无法在不同视图中生成对齐的预测结果。我们研究了在PBR文本到纹理的协作控制工作流程中解决这一问题的方法。通过直接建模PBR图像概率分布，我们的模型能够直接输出完整的PBR堆栈，并在消融研究和实际应用中展示了其有效性。'}}, 'hash': '0007650863977d09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06845', 'title': 'MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders', 'url': 'https://huggingface.co/papers/2410.06845', 'abstract': 'Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main', 'score': 4, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#rl', '#medicine', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'MentalArena: ИИ-терапевт нового поколения для персонализированной диагностики', 'desc': 'Статья представляет MentalArena - фреймворк для обучения языковых моделей в области психического здоровья путем генерации персонализированных данных. Авторы разработали Symptom Encoder для моделирования поведения пациентов и Symptom Decoder для управления диалогом между пациентом и терапевтом. Модели, обученные с помощью MentalArena на GPT-3.5 и Llama-3-8b, превзошли другие современные модели на различных тестах по биомедицине и психическому здоровью. Исследование направлено на улучшение персонализированной диагностики и лечения психических расстройств.'}, 'en': {'title': 'Revolutionizing Mental Health Care with AI-Powered Personalization', 'desc': 'The paper introduces MentalArena, a self-play framework designed to train language models for diagnosing and treating mental health disorders by generating personalized data. It uses a Symptom Encoder to simulate realistic patient behavior and cognition, and a Symptom Decoder to manage dialogue by comparing diagnosed and encoded symptoms. The framework was tested against six benchmarks and outperformed advanced models like GPT-4o, showing its effectiveness in personalized mental health care. This work aims to inspire further research in personalized treatment models while addressing privacy concerns in mental health data.'}, 'zh': {'title': 'MentalArena：个性化心理健康诊疗的未来', 'desc': '这篇论文介绍了一个名为MentalArena的自我对弈框架，用于训练语言模型以生成特定领域的个性化数据，从而改善心理健康诊断和治疗。为了模拟真实的心理健康患者，研究者设计了症状编码器，从认知和行为角度进行模拟。为了解决患者与治疗师互动中的意图偏差，提出了症状解码器，以动态管理对话。实验结果表明，经过GPT-3.5和Llama-3-8b微调的模型在多项基准测试中表现优于其他先进模型。'}}, 'hash': 'd89e94812dda796d', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06524', 'title': 'Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA', 'url': 'https://huggingface.co/papers/2410.06524', 'abstract': 'Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.', 'score': 4, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#alignment', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Сравнение когнитивных способностей человека и ИИ: новый взгляд на обработку естественного языка', 'desc': 'CAIMIRA - новая система оценки способностей к решению задач у людей и ИИ в области обработки естественного языка. Исследование, основанное на анализе более 300 000 ответов от ~70 систем ИИ и 155 людей, выявило различия в профессиональных навыках в разных областях знаний. Люди превосходят ИИ в абдуктивном и концептуальном мышлении, в то время как современные языковые модели лучше справляются с извлечением информации и фактологическими рассуждениями. Результаты подчеркивают необходимость разработки задач, требующих более сложного мышления и применения знаний в различных контекстах.'}, 'en': {'title': "Bridging the Gap: Enhancing AI's Cognitive Abilities", 'desc': 'The paper introduces CAIMIRA, a framework using item response theory to evaluate the problem-solving skills of AI and human question-answering agents. By analyzing responses from both AI systems and humans, the study reveals that humans excel in knowledge-grounded abductive and conceptual reasoning. In contrast, AI systems like GPT-4 and LLaMA perform better in tasks involving targeted information retrieval and fact-based reasoning. The research suggests that future AI development should focus on enhancing higher-order reasoning and nuanced linguistic interpretation to better mimic human cognitive abilities.'}, 'zh': {'title': 'AI与人类：问答能力的较量', 'desc': '这篇论文研究了大型语言模型在自然语言处理任务中的表现，特别是与人类的比较。研究引入了一个名为CAIMIRA的新框架，基于项目反应理论，用于定量评估和比较人类和AI系统在问答任务中的解题能力。通过分析大量AI系统和人类的回答，发现人类在知识基础的推理和概念推理上表现更好，而先进的语言模型在信息检索和事实推理上更胜一筹。研究结果表明，未来的问答任务需要更关注挑战高阶推理和科学思维的问题，以推动AI更好地模拟或补充人类的认知能力。'}}, 'hash': '5eec66236f57298a', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.07062', 'title': 'TinyEmo: Scaling down Emotional Reasoning via Metric Projection', 'url': 'https://huggingface.co/papers/2410.07062', 'abstract': 'This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems.   We release code, models, and dataset at https://github.com/ggcr/TinyEmo', 'score': 3, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#dataset', '#data', '#multimodal', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'TinyEmo: Эффективный анализ эмоций с меньшими ресурсами', 'desc': 'TinyEmo - это семейство небольших мультимодальных языковых моделей для эмоционального анализа и классификации. Модель использует синтетический набор данных для обучения и настройки, а также включает Metric Projector для эффективной классификации. TinyEmo способна выполнять классификацию эмоций и эмоциональные рассуждения, используя значительно меньше параметров, чем сопоставимые модели. Кроме того, модель предлагает подход к пониманию и улучшению систем ИИ через интерпретируемость и косвенное обнаружение предвзятости.'}, 'en': {'title': 'TinyEmo: Small Models, Big Emotional Insights', 'desc': 'TinyEmo is a new set of small language models designed to understand and classify emotions efficiently. It uses a special dataset to train the models and a unique tool called the Metric Projector to make training faster and easier. These models are smaller but still perform better than larger models, thanks to their efficient design. They also help detect biases in AI systems, making them more reliable and fair.'}, 'zh': {'title': 'TinyEmo：小模型，大情感', 'desc': '这篇论文介绍了TinyEmo，一种用于情感推理和分类的小型多模态语言模型。TinyEmo使用合成情感指令数据集进行预训练和微调，并通过度量投影器提高训练和推理效率。该模型在使用更少参数的情况下，能够在情感分类任务中表现优异，甚至超过了一些更大规模的模型。此外，度量投影器还提供了模型可解释性和偏差检测的能力。'}}, 'hash': '354409ec230f329b', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.07160', 'title': 'TextToon: Real-Time Text Toonify Head Avatar from Single Video', 'url': 'https://huggingface.co/papers/2410.07160', 'abstract': 'We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.', 'score': 3, 'issue_id': 40, 'pub_date': '2024-09-23', 'pub_date_ru': '23 сентября', 'data': {'categories': ['#3d', '#cv', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'TextToon: Создание управляемых мультяшных аватаров по видео и тексту', 'desc': 'TextToon - это метод создания управляемого мультяшного аватара на основе короткого видео и текстового описания стиля. Он использует условное встраивание Tri-plane для реалистичного и стилизованного представления лица в гауссовом поле деформации. Модель улучшает возможности стилизации 3D Gaussian Splatting с помощью адаптивной нейронной сети пиксельного преобразования и контрастного обучения с учетом патчей. TextToon работает в режиме реального времени и превосходит существующие методы по качеству и анимации.'}, 'en': {'title': '"Animate Your Avatar: Real-Time Toonification from Text and Video"', 'desc': 'TextToon is a novel method for creating animated avatars from a single video and a text description of the desired style. Unlike previous methods that require multiple camera angles and result in static models, TextToon uses a conditional embedding Tri-plane to create dynamic and stylized facial representations. The approach enhances 3D Gaussian Splatting with an adaptive pixel-translation neural network and patch-aware contrastive learning, resulting in high-quality, real-time animations. The system is efficient, running at 48 FPS on GPUs and 15-18 FPS on mobile devices, making it suitable for consumer applications.'}, 'zh': {'title': 'TextToon：实时生成可驱动卡通化头像的创新方法', 'desc': '这篇论文介绍了一种名为TextToon的方法，可以生成可驱动的卡通化头像。通过一个短的单目视频序列和关于头像风格的书面指令，模型能够生成高保真度的卡通化头像，并能通过另一个视频实时驱动。与现有方法不同，TextToon不依赖多视角建模，而是采用条件嵌入三平面来学习真实和风格化的面部表示。通过引入自适应像素翻译神经网络和利用补丁感知对比学习，TextToon实现了高质量的图像生成。'}}, 'hash': '425ed10e0d4ec2cd', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}}, {'id': 'https://huggingface.co/papers/2410.05873', 'title': 'MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment', 'url': 'https://huggingface.co/papers/2410.05873', 'abstract': 'English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.', 'score': 2, 'issue_id': 45, 'pub_date': '2024-10-08', 'pub_date_ru': '8 октября', 'data': {'categories': ['#benchmark', '#multilingual', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'MEXA: Новый способ оценки многоязычности LLM через параллельные предложения', 'desc': 'MEXA - это новый метод оценки многоязычных возможностей предобученных англоцентричных языковых моделей (LLM), использующий параллельные предложения. Метод основан на том, что англоцентричные LLM используют английский как своего рода язык-посредник в промежуточных слоях. MEXA вычисляет выравнивание между английским и другими языками, чтобы оценить перенос понимания языка с английского на другие языки. Результаты показывают высокую корреляцию (0,90) с существующими задачами на нескольких моделях и наборах данных.'}, 'en': {'title': 'Unlocking Multilingual Potential in English-Centric Models', 'desc': 'The paper introduces MEXA, a method to evaluate the multilingual capabilities of English-centric large language models (LLMs) using parallel sentences. MEXA leverages the use of English as a pivot language in LLMs to assess language understanding transfer to non-English languages. The method shows a high correlation with established multilingual benchmarks, indicating its reliability in estimating model performance across languages. This approach provides insights into the multilingual potential and internal mechanisms of LLMs.'}, 'zh': {'title': 'MEXA：揭示英语中心语言模型的多语言潜力', 'desc': '这篇论文介绍了一种名为MEXA的方法，用于评估以英语为中心的大型语言模型的多语言能力。MEXA利用平行句子来计算英语与非英语语言之间的对齐，从而评估语言理解的转移效果。研究表明，MEXA在默认设置下，与三个已建立的下游任务的平均皮尔逊相关系数达到0.90，显示出其在多语言能力评估中的可靠性。通过这种方法，可以更清晰地了解这些模型的多语言潜力和内部工作机制。'}}, 'hash': '98dd8fcc39110307', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}}, {'id': 'https://huggingface.co/papers/2410.07112', 'title': 'VHELM: A Holistic Evaluation of Vision Language Models', 'url': 'https://huggingface.co/papers/2410.07112', 'abstract': 'Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.', 'score': 1, 'issue_id': 47, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#benchmark', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'VHELM: Комплексная оценка мультимодальных моделей по 9 ключевым аспектам', 'desc': 'VHELM - это расширение фреймворка HELM для комплексной оценки моделей компьютерного зрения и языка (VLM). Он охватывает 9 аспектов, включая визуальное восприятие, знания, рассуждения, предвзятость, справедливость, мультиязычность, надежность, токсичность и безопасность. VHELM стандартизирует параметры вывода, методы промптинга и метрики оценки для обеспечения справедливых сравнений между моделями. Исследование выявило интересные результаты, например, что модели, ориентированные на эффективность, значительно хуже справляются с тестами на предвзятость.'}, 'en': {'title': '"VHELM: A New Standard for Fair and Comprehensive VLM Evaluation"', 'desc': 'The paper introduces VHELM, a comprehensive framework for evaluating vision-language models (VLMs) across nine critical aspects, including fairness, multilinguality, and toxicity. By standardizing evaluation procedures and metrics, VHELM allows for fair comparisons between different models. The initial evaluation of 22 VLMs reveals that efficiency-focused models perform worse on bias benchmarks compared to their full counterparts. VHELM aims to be an evolving benchmark, continuously incorporating new datasets and models to provide a holistic view of VLM capabilities.'}, 'zh': {'title': 'VHELM：全面评估视觉语言模型的新标准', 'desc': '这篇论文提出了一种新的评估框架VHELM，用于全面评估视觉语言模型（VLMs）的多方面能力。VHELM通过整合多个数据集，涵盖视觉感知、知识、推理、公平性、多语言性、鲁棒性、毒性和安全性等九个方面。该框架标准化了推理参数、提示方法和评估指标，使得模型之间的比较更加公平。初步评估显示，注重效率的模型在偏见测试中表现不佳，但在其他方面没有明显差距。'}}, 'hash': 'f9db5fd76663d260', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.06468', 'title': 'Does Spatial Cognition Emerge in Frontier Models?', 'url': 'https://huggingface.co/papers/2410.06468', 'abstract': 'Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.', 'score': 1, 'issue_id': 42, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'SPACE: новый рубеж в оценке пространственного интеллекта ИИ', 'desc': 'SPACE - это новый бенчмарк для оценки пространственного мышления у передовых моделей искусственного интеллекта. Он основан на многолетних исследованиях в когнитивной науке и оценивает способности к крупномасштабному картографированию, мелкомасштабному рассуждению о формах и расположении объектов, а также когнитивную инфраструктуру, такую как пространственное внимание и память. Бенчмарк позволяет тестировать как языковые, так и мультимодальные модели. Результаты показывают, что современные модели значительно уступают животным в пространственном интеллекте.'}, 'en': {'title': "Bridging the Gap: Evaluating AI's Spatial Smarts", 'desc': "The paper introduces SPACE, a benchmark designed to assess spatial cognition in advanced machine learning models. It draws from cognitive science to evaluate models' abilities in large-scale mapping, object shape reasoning, and spatial attention and memory. The benchmark uses both text and images to test large language and multimodal models. Findings indicate that current models struggle with spatial tasks, performing poorly compared to animals."}, 'zh': {'title': 'SPACE：评估模型空间认知的新基准', 'desc': '这篇论文介绍了一个名为SPACE的基准，用于系统地评估前沿模型的空间认知能力。该基准基于认知科学的多年研究，评估模型在大规模地图构建、小规模物体形状和布局推理以及空间注意力和记忆等方面的能力。通过文本和图像的平行展示，SPACE可以同时评估大型语言模型和多模态模型。结果表明，当前的前沿模型在空间智能方面仍不如动物，在许多经典的动物认知测试中表现接近随机水平。'}}, 'hash': 'b568df3a7211e7fa', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.07095', 'title': 'MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering', 'url': 'https://huggingface.co/papers/2410.07095', 'abstract': "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.", 'score': 1, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'MLE-bench: измеряем инженерные навыки ИИ в машинном обучении', 'desc': 'MLE-bench - это новый бенчмарк для оценки способностей ИИ-агентов в области инженерии машинного обучения. Он включает 75 соревнований с Kaggle, охватывающих различные аспекты ML-инженерии. Авторы установили базовые показатели человеческой производительности и протестировали несколько языковых моделей, используя открытые фреймворки для агентов. Лучший результат показала модель OpenAI o1-preview с AIDE, достигнув уровня бронзовой медали Kaggle в 16.9% соревнований.'}, 'en': {'title': "Testing AI's Engineering Prowess with MLE-bench", 'desc': "MLE-bench is a new benchmark designed to evaluate how well AI agents can perform machine learning engineering tasks. It includes 75 competitions from Kaggle that test skills like model training, dataset preparation, and experiment execution. The benchmark uses human performance baselines from Kaggle leaderboards and evaluates AI models, finding that OpenAI's o1-preview with AIDE scaffolding performs at a bronze medal level in 16.9% of tasks. The study also explores how resource scaling affects AI performance and the influence of pre-training data contamination, with the benchmark code available for public use."}, 'zh': {'title': '评估AI在机器学习工程中的表现', 'desc': '我们推出了MLE-bench，这是一个用于评估AI代理在机器学习工程中表现的基准。我们从Kaggle上精选了75个与ML工程相关的竞赛，涵盖了训练模型、准备数据集和运行实验等实际技能。通过使用Kaggle的公开排行榜，我们为每个竞赛建立了人类基线。我们发现，使用OpenAI的o1-preview与AIDE框架的组合在16.9%的竞赛中达到了Kaggle铜牌水平。'}}, 'hash': '103d4f2b02c940a9', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}, {'id': 'https://huggingface.co/papers/2410.05160', 'title': 'VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks', 'url': 'https://huggingface.co/papers/2410.05160', 'abstract': "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. Our results show that \\model achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Универсальные мультимодальные эмбеддинги: новый подход и бенчмарк', 'desc': 'Статья представляет новый подход к созданию универсальных мультимодальных эмбеддингов. Авторы предлагают MMEB - масштабный бенчмарк для оценки мультимодальных эмбеддингов, охватывающий 36 датасетов и 4 мета-задачи. Также представлен VLM2Vec - фреймворк для преобразования любой современной vision-language модели в модель эмбеддингов путем обучения на MMEB. Результаты показывают, что VLM2Vec достигает улучшения на 10-20% по сравнению с существующими мультимодальными моделями эмбеддингов.'}, 'en': {'title': 'VLM2Vec: Bridging Vision and Language for Universal Embeddings', 'desc': 'This paper introduces a new approach to creating universal multimodal embedding models that can handle a variety of tasks. The authors present MMEB, a comprehensive benchmark covering multiple tasks and datasets, and VLM2Vec, a framework that transforms vision-language models into versatile embedding models. VLM2Vec stands out by generating fixed-dimensional vectors from any combination of images and text, outperforming existing models like CLIP and BLIP. The results demonstrate significant improvements in performance across different datasets, highlighting the potential of VLM2Vec in advancing multimodal embeddings.'}, 'zh': {'title': '通用嵌入模型：跨任务的多模态处理', 'desc': '这篇论文探讨了构建通用嵌入模型的可能性，这种模型可以处理多种下游任务。作者提出了一个名为MMEB的大规模多模态嵌入基准，涵盖了四个元任务和36个数据集。论文还介绍了VLM2Vec，一种对比训练框架，可以将任何先进的视觉-语言模型转化为嵌入模型。实验结果表明，VLM2Vec在MMEB的评估中比现有的多模态嵌入模型提高了10%到20%。'}}, 'hash': '36a094d66cdd6a3e', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}}, {'id': 'https://huggingface.co/papers/2410.07145', 'title': 'Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling', 'url': 'https://huggingface.co/papers/2410.07145', 'abstract': "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.", 'score': 0, 'issue_id': 46, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'categories': ['#inference', '#architecture', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'RNN покоряют длинные последовательности: прорыв в обработке контекста в миллион токенов', 'desc': "В статье исследуются причины неспособности рекуррентных нейронных сетей (RNN) обрабатывать длинные последовательности и предлагаются способы решения этой проблемы. Авторы выявляют явление 'схлопывания состояния' (state collapse), вызывающее резкое падение производительности на последовательностях длиннее тренировочных. Предложены три метода для смягчения этого эффекта, позволяющие модели Mamba-2 обрабатывать более миллиона токенов без схлопывания. Также эмпирически показано, что емкость рекуррентного состояния для задачи извлечения паролей масштабируется экспоненциально с размером состояния."}, 'en': {'title': 'Unlocking RNNs for Long Sequences: Overcoming State Collapse', 'desc': "Recurrent neural networks (RNNs) are faster than transformer models for long sequences due to their linear computational complexity, but they struggle with sequences longer than those they were trained on. This paper identifies two main issues: the inability to handle longer sequences than trained on, and limited memory capacity. The authors explore 'state collapse,' where performance drops for longer sequences, and attribute it to overfitting. They propose three methods to improve RNNs' ability to handle longer sequences, showing that RNNs can be effective for long-context tasks with proper adjustments."}, 'zh': {'title': 'RNN长序列处理的未来潜力', 'desc': '递归神经网络（RNN）在处理长序列时比基于Transformer的语言模型更快，因为它们的计算复杂度与序列长度呈线性关系。然而，大多数公开的RNN在处理超过1万标记的长序列时效果不佳。本文研究了RNN在长上下文处理中的不足，并提出了关键的改进措施。通过实验，我们发现过拟合和状态崩溃是主要问题，并提出了三种方法来提高Mamba-2模型的长度泛化能力。'}}, 'hash': '955fb73ec92505eb', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (8)', '#agi', '#alignment (3)', '#architecture (5)', '#audio (2)', '#benchmark (19)', '#cv (12)', '#data (3)', '#dataset (10)', '#diffusion (9)', '#edge_computing', '#ethics (1)', '#games', '#graph', '#graphs', '#hallucinations (1)', '#inference (3)', '#interpretability (2)', '#math', '#medicine (1)', '#multilingual (2)', '#multimodal (20)', '#optimization (2)', '#plp (2)', '#quantum', '#rag (3)', '#reasoning (3)', '#rl (6)', '#rlhf (1)', '#robotics', '#security (2)', '#story_generation (2)', '#survey', '#training (13)', '#transfer_learning (1)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-10 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-10 20:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    