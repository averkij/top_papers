{
    "date": {
        "ru": "19 августа",
        "en": "August 19",
        "zh": "8月19日"
    },
    "time_utc": "2025-08-19 02:28",
    "weekday": 1,
    "issue_id": 5416,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.13154",
            "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
            "url": "https://huggingface.co/papers/2508.13154",
            "abstract": "4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.",
            "score": 1,
            "issue_id": 5416,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 августа",
                "en": "August 18",
                "zh": "8月18日"
            },
            "hash": "cd93b5ad64365be7",
            "authors": [
                "Zhaoxi Chen",
                "Tianqi Liu",
                "Long Zhuo",
                "Jiawei Ren",
                "Zeng Tao",
                "He Zhu",
                "Fangzhou Hong",
                "Liang Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13154.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#3d",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🌟",
                "ru": {
                    "title": "От 2D к 4D: революция в генерации динамических 3D-сцен",
                    "desc": "4DNeX - это новый фреймворк для создания динамических 3D-представлений сцен из одного изображения. Он использует предобученную модель диффузии видео, адаптированную для 4D-моделирования. 4DNeX превосходит существующие методы по эффективности и обобщаемости, генерируя высококачественные динамические облака точек. Метод включает создание большого набора данных 4DNeX-10M и использование унифицированного 6D-представления видео для совместного моделирования RGB и XYZ последовательностей."
                },
                "en": {
                    "title": "Transforming Images into Dynamic 3D Worlds Efficiently",
                    "desc": "4DNeX is a novel framework that generates dynamic 3D scene representations from a single image using a fine-tuned video diffusion model. Unlike traditional methods that require multiple frames or extensive computational resources, 4DNeX streamlines the process into an efficient, end-to-end image-to-4D generation. It introduces a large-scale dataset, 4DNeX-10M, with high-quality 4D annotations to address data scarcity and employs a unified 6D video representation for better learning of both visual appearance and spatial geometry. The framework demonstrates superior performance in generating high-quality dynamic point clouds, paving the way for advanced generative models that can simulate evolving scenes."
                },
                "zh": {
                    "title": "4DNeX：高效生成动态3D场景的创新框架",
                    "desc": "4DNeX是一种新型的框架，可以从单张图像生成高质量的动态3D场景表示。与传统方法不同，4DNeX利用经过微调的预训练视频扩散模型，能够高效地实现图像到4D的生成。为了克服4D数据稀缺的问题，研究者们构建了一个包含高质量4D注释的大规模数据集。实验结果表明，4DNeX在效率和通用性方面优于现有的4D生成方法，为图像到4D建模提供了可扩展的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12466",
            "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
            "url": "https://huggingface.co/papers/2508.12466",
            "abstract": "Inverse-LLaVA eliminates alignment pre-training by mapping text embeddings into continuous visual representation space, improving reasoning tasks while reducing computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional multimodal learning approaches require expensive alignment pre-training to bridge vision and language modalities, typically projecting visual features into discrete text token spaces. We challenge both fundamental assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel approach that eliminates alignment pre-training entirely while inverting the conventional mapping direction. Rather than projecting visual features to text space, our method maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. Through selective additive components in attention mechanisms, we enable dynamic integration of visual and textual representations without requiring massive image-text alignment datasets. Comprehensive experiments across nine multimodal benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing expected decreases in perception tasks requiring memorized visual-text associations (celebrity recognition: -49.5%, OCR: -21.3%). These results provide the first empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. Our work establishes the feasibility of a new paradigm that reduces computational requirements by 45%, challenges conventional wisdom about modality fusion, and opens new research directions for efficient multimodal architectures that preserve modality-specific characteristics. Our project website with code and additional resources is available at https://inverse-llava.github.io.",
            "score": 1,
            "issue_id": 5416,
            "pub_date": "2025-08-17",
            "pub_date_card": {
                "ru": "17 августа",
                "en": "August 17",
                "zh": "8月17日"
            },
            "hash": "18214c417cbd148a",
            "authors": [
                "Xuhui Zhan",
                "Tyler Derr"
            ],
            "affiliations": [
                "Computer Science Department Vanderbilt University",
                "Data Science Institute Vanderbilt University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12466.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#reasoning",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Инверсия модальностей для эффективного мультимодального обучения",
                    "desc": "Inverse-LLaVA - это новый подход к мультимодальному обучению, который устраняет необходимость в предварительном обучении выравниванию модальностей. Вместо проецирования визуальных признаков в пространство текстовых токенов, метод отображает текстовые эмбеддинги в непрерывное пространство визуальных представлений. Эксперименты показывают улучшение производительности на задачах рассуждений и когнитивных задачах, но ожидаемое снижение в задачах восприятия. Подход сокращает вычислительные требования на 45% и открывает новые направления исследований эффективных мультимодальных архитектур."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning: No More Alignment Pre-Training!",
                    "desc": "Inverse-LLaVA introduces a new method for multimodal learning that eliminates the need for alignment pre-training, which is typically required to connect visual and textual data. Instead of converting visual features into discrete text tokens, this approach maps text embeddings directly into a continuous visual representation space. By integrating visual and textual information within transformer layers using selective attention mechanisms, Inverse-LLaVA enhances performance on reasoning tasks while reducing the need for large datasets. The results show significant improvements in cognitive reasoning tasks, suggesting that traditional alignment methods may not be necessary for effective multimodal learning."
                },
                "zh": {
                    "title": "逆向LLaVA：无需对齐预训练的多模态学习新方法",
                    "desc": "Inverse-LLaVA是一种新颖的多模态学习方法，它通过将文本嵌入映射到连续的视觉表示空间，完全消除了对对齐预训练的需求。这种方法在推理任务上表现出显著的改进，同时减少了计算资源的需求。通过在变换器的中间层进行视觉和文本表示的动态融合，Inverse-LLaVA能够在不依赖大量图像-文本对齐数据集的情况下实现有效的多模态学习。实验结果表明，该方法在复杂推理任务上取得了显著的性能提升，同时在需要记忆视觉-文本关联的感知任务上表现有所下降。"
                }
            }
        }
    ],
    "link_prev": "2025-08-18.html",
    "link_next": "2025-08-20.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "18.08",
        "en": "08/18",
        "zh": "8月18日"
    },
    "short_date_next": {
        "ru": "20.08",
        "en": "08/20",
        "zh": "8月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}