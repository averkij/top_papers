{
    "date": {
        "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 19",
        "zh": "8æœˆ19æ—¥"
    },
    "time_utc": "2025-08-19 07:11",
    "weekday": 1,
    "issue_id": 5421,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.11737",
            "title": "Ovis2.5 Technical Report",
            "url": "https://huggingface.co/papers/2508.11737",
            "abstract": "Ovis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional \"thinking mode\" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the \"small model, big performance\" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.",
            "score": 60,
            "issue_id": 5417,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 15",
                "zh": "8æœˆ15æ—¥"
            },
            "hash": "ccd30a02aad117a4",
            "authors": [
                "Shiyin Lu",
                "Yang Li",
                "Yu Xia",
                "Yuwei Hu",
                "Shanshan Zhao",
                "Yanqing Ma",
                "Zhichao Wei",
                "Yinglun Li",
                "Lunhao Duan",
                "Jianshan Zhao",
                "Yuxuan Han",
                "Haijun Li",
                "Wanying Chen",
                "Junke Tang",
                "Chengkun Hou",
                "Zhixing Du",
                "Tianli Zhou",
                "Wenjie Zhang",
                "Huping Ding",
                "Jiahe Li",
                "Wen Li",
                "Gui Hu",
                "Yiliang Gu",
                "Siran Yang",
                "Jiamang Wang",
                "Hailong Sun",
                "Yibo Wang",
                "Hui Sun",
                "Jinlong Huang",
                "Yuping He",
                "Shengze Shi",
                "Weihong Zhang",
                "Guodong Zheng",
                "Junpeng Jiang",
                "Sensen Gao",
                "Yi-Feng Wu",
                "Sijia Chen",
                "Yuhui Chen",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11737.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#low_resource",
                    "#open_source",
                    "#science",
                    "#interpretability",
                    "#benchmark",
                    "#small_models",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğvis2.5: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ",
                    "desc": "Ovis2.5 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ ĞºĞ°Ğº Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ 'Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸Ñ„Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO Ğ¸ GRPO. Ovis2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… STEM Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼."
                },
                "en": {
                    "title": "Ovis2.5: Revolutionizing Visual Perception with Multimodal Reasoning",
                    "desc": "Ovis2.5 is a cutting-edge vision transformer designed for high-resolution visual perception and advanced multimodal reasoning. It processes images at their native resolutions, which helps maintain detail and layout, making it effective for complex visual data. The model features a unique 'thinking mode' that allows for self-reflection and revision during inference, enhancing accuracy on challenging tasks. Trained through a structured five-phase curriculum, Ovis2.5 demonstrates significant performance improvements on various benchmarks, particularly in STEM and complex chart analysis, while also being optimized for efficiency and scalability."
                },
                "zh": {
                    "title": "Ovis2.5ï¼šåŸç”Ÿåˆ†è¾¨ç‡ä¸å¤šæ¨¡æ€æ¨ç†çš„ç»“åˆ",
                    "desc": "Ovis2.5æ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰å˜æ¢å™¨ï¼Œä¸“æ³¨äºåŸç”Ÿåˆ†è¾¨ç‡çš„è§†è§‰æ„ŸçŸ¥å’Œå¤šæ¨¡æ€æ¨ç†ã€‚å®ƒèƒ½å¤Ÿå¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œé¿å…äº†å›ºå®šåˆ†è¾¨ç‡åˆ‡ç‰‡å¸¦æ¥çš„ç»†èŠ‚æŸå¤±ï¼Œé€‚åˆå¤æ‚å›¾è¡¨ç­‰è§†è§‰å¯†é›†å†…å®¹ã€‚è¯¥æ¨¡å‹é€šè¿‡äº”ä¸ªé˜¶æ®µçš„è®­ç»ƒè¯¾ç¨‹é€æ­¥æå‡èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†åæ€æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶é€‰æ‹©æ›´é«˜çš„å‡†ç¡®æ€§ã€‚Ovis2.5åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨STEMé¢†åŸŸå’Œå¤æ‚å›¾è¡¨åˆ†æä¸­å–å¾—äº†é¢†å…ˆçš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10419",
            "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
            "url": "https://huggingface.co/papers/2508.10419",
            "abstract": "ComoRAG, an iterative retrieval-based approach, enhances long-context narrative comprehension by dynamically updating memory and generating probing queries, outperforming traditional RAG methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG",
            "score": 34,
            "issue_id": 5417,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 14",
                "zh": "8æœˆ14æ—¥"
            },
            "hash": "5b8e1dc52a233ce3",
            "authors": [
                "Juyuan Wang",
                "Rongchen Zhao",
                "Wei Wei",
                "Yufeng Wang",
                "Mo Yu",
                "Jie Zhou",
                "Jin Xu",
                "Liyan Xu"
            ],
            "affiliations": [
                "Independent Researcher",
                "Pattern Recognition Center, WeChat AI, Tencent",
                "Pazhou Lab, Guangzhou",
                "School of Future Technology, South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10419.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#long_context",
                    "#rag"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "ComoRAG: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "ComoRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑĞ¶ĞµÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² RAG, ComoRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ComoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "ComoRAG: Enhancing Narrative Comprehension through Dynamic Memory and Iterative Retrieval",
                    "desc": "ComoRAG is a novel approach that improves understanding of long narratives by using an iterative retrieval process. Unlike traditional Retrieval-Augmented Generation (RAG) methods, which retrieve information in a single step, ComoRAG dynamically updates its memory and generates probing queries to better capture complex relationships in the text. This method mimics human cognitive processes, allowing for a more effective integration of new information with existing knowledge. As a result, ComoRAG shows significant performance improvements on long-context narrative tasks, achieving up to 11% better results than existing RAG models."
                },
                "zh": {
                    "title": "ComoRAGï¼šåŠ¨æ€è®°å¿†ä¸å™äº‹æ¨ç†çš„ç»“åˆ",
                    "desc": "ComoRAGæ˜¯ä¸€ç§åŸºäºæ£€ç´¢çš„è¿­ä»£æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¯¹é•¿ç¯‡å™äº‹çš„ç†è§£èƒ½åŠ›ã€‚å®ƒé€šè¿‡åŠ¨æ€æ›´æ–°è®°å¿†å’Œç”Ÿæˆæ¢æµ‹æŸ¥è¯¢ï¼Œå…‹æœäº†ä¼ ç»ŸRAGæ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œå¼ºè°ƒå™äº‹æ¨ç†æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€ä¸æ–­æ¼”å˜çš„è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒComoRAGåœ¨å¤šä¸ªé•¿ç¯‡å™äº‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡äº†æŸ¥è¯¢è§£æçš„è¿è´¯æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12811",
            "title": "Next Visual Granularity Generation",
            "url": "https://huggingface.co/papers/2508.12811",
            "abstract": "A novel Next Visual Granularity (NVG) framework generates images by iteratively refining a sequence of visual granularities, outperforming existing methods in class-conditional image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel approach to image generation by decomposing an image into a structured sequence, where each element in the sequence shares the same spatial resolution but differs in the number of unique tokens used, capturing different level of visual granularity. Image generation is carried out through our newly introduced Next Visual Granularity (NVG) generation framework, which generates a visual granularity sequence beginning from an empty image and progressively refines it, from global layout to fine details, in a structured manner. This iterative process encodes a hierarchical, layered representation that offers fine-grained control over the generation process across multiple granularity levels. We train a series of NVG models for class-conditional image generation on the ImageNet dataset and observe clear scaling behavior. Compared to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30 -> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to showcase the capability and potential of the NVG framework. Our code and models will be released.",
            "score": 32,
            "issue_id": 5419,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "712aeeb22b4ceba5",
            "authors": [
                "Yikai Wang",
                "Zhouxia Wang",
                "Zhonghua Wu",
                "Qingyi Tao",
                "Kang Liao",
                "Chen Change Loy"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12811.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸĞ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Next Visual Granularity (NVG). ĞĞ½ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ½Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. NVG Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿ÑƒÑÑ‚Ğ¾Ğ³Ğ¾ Ñ…Ğ¾Ğ»ÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ°Ğ¼ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet."
                },
                "en": {
                    "title": "Refining Images with Next Visual Granularity",
                    "desc": "The paper introduces a new framework called Next Visual Granularity (NVG) for generating images by refining visual details in a structured way. It breaks down the image generation process into a sequence of visual granularities, where each step maintains the same resolution but varies in the number of unique tokens. This method allows for a hierarchical representation, enabling fine control over the image generation from broad layouts to intricate details. The NVG framework shows significant improvements in class-conditional image generation, outperforming previous models in quality metrics like FID scores."
                },
                "zh": {
                    "title": "ä¸‹ä¸€è§†è§‰ç²’åº¦ï¼šå›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºä¸‹ä¸€è§†è§‰ç²’åº¦ï¼ˆNVGï¼‰ï¼Œé€šè¿‡é€æ­¥ç»†åŒ–è§†è§‰ç²’åº¦åºåˆ—æ¥ç”Ÿæˆå›¾åƒã€‚è¯¥æ–¹æ³•å°†å›¾åƒåˆ†è§£ä¸ºç»“æ„åŒ–åºåˆ—ï¼Œæ¯ä¸ªå…ƒç´ å…·æœ‰ç›¸åŒçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½†ä½¿ç”¨ä¸åŒæ•°é‡çš„ç‹¬ç‰¹æ ‡è®°ï¼Œä»è€Œæ•æ‰ä¸åŒå±‚æ¬¡çš„è§†è§‰ç²’åº¦ã€‚NVGæ¡†æ¶ä»ç©ºç™½å›¾åƒå¼€å§‹ï¼Œé€æ­¥ç²¾ç»†åŒ–ï¼Œä»å…¨å±€å¸ƒå±€åˆ°ç»†èŠ‚ï¼Œæä¾›äº†å¯¹ç”Ÿæˆè¿‡ç¨‹çš„ç»†ç²’åº¦æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNVGåœ¨ç±»æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„FIDè¯„åˆ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09834",
            "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
            "url": "https://huggingface.co/papers/2508.09834",
            "abstract": "This survey examines innovative architectures for large language models to enhance efficiency, covering linear and sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid models, and diffusion LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.",
            "score": 25,
            "issue_id": 5417,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 13",
                "zh": "8æœˆ13æ—¥"
            },
            "hash": "97c57497d6821327",
            "authors": [
                "Weigao Sun",
                "Jiaxi Hu",
                "Yucheng Zhou",
                "Jusen Du",
                "Disen Lan",
                "Kexin Wang",
                "Tong Zhu",
                "Xiaoye Qu",
                "Yu Zhang",
                "Xiaoyu Mo",
                "Daizong Liu",
                "Yuxuan Liang",
                "Wenliang Chen",
                "Guoqi Li",
                "Yu Cheng"
            ],
            "affiliations": [
                "HKUST (GZ)",
                "Institute of Automation, Chinese Academy of Sciences",
                "KTH Royal Institute of Technology",
                "Peking University",
                "Shanghai AI Laboratory",
                "Soochow University",
                "The Chinese University of Hong Kong",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09834.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#architecture",
                    "#agi",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ LLM: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM. ĞĞ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Innovating Efficiency in Large Language Models",
                    "desc": "This survey explores new architectures for large language models (LLMs) aimed at improving their efficiency. It discusses various techniques such as linear and sparse sequence modeling, efficient attention mechanisms, and sparse mixture-of-experts. The paper highlights the limitations of traditional transformer models and presents innovative solutions to enhance scalability and reduce computational demands. By categorizing recent advancements, it provides a roadmap for future research in developing more efficient AI systems."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æ¶æ„",
                    "desc": "æœ¬è°ƒæŸ¥ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ›æ–°æ¶æ„ï¼Œä»¥æé«˜å…¶æ•ˆç‡ã€‚æˆ‘ä»¬æ¢è®¨äº†çº¿æ€§å’Œç¨€ç–åºåˆ—å»ºæ¨¡ã€é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ã€ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ã€æ··åˆæ¨¡å‹ä»¥åŠæ‰©æ•£LLMç­‰æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„å˜æ¢å™¨æ¶æ„è™½ç„¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤§è§„æ¨¡è®­ç»ƒå’Œå®é™…éƒ¨ç½²ä¸­é¢ä¸´è®¡ç®—é‡å¤§çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æè¿™äº›æ–°æ¶æ„ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæœªæ¥çš„é«˜æ•ˆã€çµæ´»çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿç ”ç©¶æä¾›è“å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13154",
            "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
            "url": "https://huggingface.co/papers/2508.13154",
            "abstract": "4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.",
            "score": 24,
            "issue_id": 5416,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "cd93b5ad64365be7",
            "authors": [
                "Zhaoxi Chen",
                "Tianqi Liu",
                "Long Zhuo",
                "Jiawei Ren",
                "Zeng Tao",
                "He Zhu",
                "Fangzhou Hong",
                "Liang Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13154.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#synthetic",
                    "#3d",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŸ",
                "ru": {
                    "title": "ĞÑ‚ 2D Ğº 4D: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½",
                    "desc": "4DNeX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ 4D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. 4DNeX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 4DNeX-10M Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 6D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RGB Ğ¸ XYZ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Images into Dynamic 3D Worlds Efficiently",
                    "desc": "4DNeX is a novel framework that generates dynamic 3D scene representations from a single image using a fine-tuned video diffusion model. Unlike traditional methods that require multiple frames or extensive computational resources, 4DNeX streamlines the process into an efficient, end-to-end image-to-4D generation. It introduces a large-scale dataset, 4DNeX-10M, with high-quality 4D annotations to address data scarcity and employs a unified 6D video representation for better learning of both visual appearance and spatial geometry. The framework demonstrates superior performance in generating high-quality dynamic point clouds, paving the way for advanced generative models that can simulate evolving scenes."
                },
                "zh": {
                    "title": "4DNeXï¼šé«˜æ•ˆç”ŸæˆåŠ¨æ€3Dåœºæ™¯çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "4DNeXæ˜¯ä¸€ç§æ–°å‹çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€3Dåœºæ™¯è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œ4DNeXåˆ©ç”¨ç»è¿‡å¾®è°ƒçš„é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å®ç°å›¾åƒåˆ°4Dçš„ç”Ÿæˆã€‚ä¸ºäº†å…‹æœ4Dæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«é«˜è´¨é‡4Dæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ4DNeXåœ¨æ•ˆç‡å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„4Dç”Ÿæˆæ–¹æ³•ï¼Œä¸ºå›¾åƒåˆ°4Då»ºæ¨¡æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13142",
            "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
            "url": "https://huggingface.co/papers/2508.13142",
            "abstract": "Recent multi-modal models, including GPT-5, show significant progress in spatial intelligence but still lag behind human performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models.",
            "score": 13,
            "issue_id": 5420,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "4ef4affb6449d3d7",
            "authors": [
                "Zhongang Cai",
                "Yubo Wang",
                "Qingping Sun",
                "Ruisi Wang",
                "Chenyang Gu",
                "Wanqi Yin",
                "Zhiqian Lin",
                "Zhitao Yang",
                "Chen Wei",
                "Xuanke Shi",
                "Kewang Deng",
                "Xiaoyang Han",
                "Zukai Chen",
                "Jiaqi Li",
                "Xiangyu Fan",
                "Hanming Deng",
                "Lewei Lu",
                "Bo Li",
                "Ziwei Liu",
                "Quan Wang",
                "Dahua Lin",
                "Lei Yang"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13142.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#agi",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GPT-5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸Ğ»Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ, Ğ½Ğ¾ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: GPT-5's Journey Towards Human-Level Spatial Intelligence",
                    "desc": "This paper examines the advancements of recent multi-modal models, particularly GPT-5, in the area of spatial intelligence. Despite significant improvements, these models still do not match human performance on various spatial reasoning tasks. The authors propose a new taxonomy for spatial tasks to better evaluate and compare model performance. Their findings indicate that while GPT-5 shows strong capabilities, it struggles with complex spatial challenges, and proprietary models do not necessarily outperform open-source ones in these scenarios."
                },
                "zh": {
                    "title": "ç©ºé—´æ™ºèƒ½ï¼šAIçš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ€è¿‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5ï¼Œåœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ä»ç„¶è½åäºäººç±»è¡¨ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„ç©ºé—´ä»»åŠ¡åˆ†ç±»æ³•ï¼Œç»Ÿä¸€äº†ç°æœ‰çš„åŸºå‡†ï¼Œå¹¶è®¨è®ºäº†ç¡®ä¿å…¬å¹³è¯„ä¼°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹æœ€æ–°çš„ä¸“æœ‰å’Œå¼€æºæ¨¡å‹åœ¨å…«ä¸ªå…³é”®åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°GPT-5åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„å¼ºåº¦ï¼Œä½†åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­ä»æœªè¾¾åˆ°äººç±»çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯†åˆ«äº†å¤šæ¨¡æ€æ¨¡å‹é¢ä¸´çš„æ›´å…·æŒ‘æˆ˜æ€§çš„ç©ºé—´æ™ºèƒ½é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13009",
            "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model",
            "url": "https://huggingface.co/papers/2508.13009",
            "abstract": "Matrix-Game 2.0 generates real-time interactive videos using few-step auto-regressive diffusion, addressing the limitations of lengthy inference in existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.",
            "score": 11,
            "issue_id": 5417,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "ee608d0de07a48cb",
            "authors": [
                "Xianglong He",
                "Chunli Peng",
                "Zexiang Liu",
                "Boyang Wang",
                "Yifan Zhang",
                "Qi Cui",
                "Fei Kang",
                "Biao Jiang",
                "Mengyin An",
                "Yangyang Ren",
                "Baixin Xu",
                "Hao-Xiang Guo",
                "Kaixiong Gong",
                "Cyrus Wu",
                "Wei Li",
                "Xuchen Song",
                "Yang Liu",
                "Eric Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13009.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#data",
                    "#open_source",
                    "#diffusion",
                    "#dataset",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ…: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Matrix-Game 2.0",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Matrix-Game 2.0 - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°. Matrix-Game 2.0 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ 25 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ."
                },
                "en": {
                    "title": "Real-Time Interactive Video Generation with Matrix-Game 2.0",
                    "desc": "Matrix-Game 2.0 is a novel framework for generating interactive videos in real-time using a few-step auto-regressive diffusion approach. This model overcomes the limitations of traditional interactive world models that rely on lengthy inference and bidirectional attention, which hinder real-time performance. By incorporating a scalable data production pipeline and an action injection module, it allows for dynamic interactions based on user inputs. The result is the ability to produce high-quality videos at an impressive speed of 25 frames per second, making it suitable for simulating real-world dynamics effectively."
                },
                "zh": {
                    "title": "å®æ—¶äº’åŠ¨è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Matrix-Game 2.0 æ˜¯ä¸€ç§å®æ—¶ç”Ÿæˆäº’åŠ¨è§†é¢‘çš„æ¨¡å‹ï¼Œé‡‡ç”¨å°‘æ­¥è‡ªå›å½’æ‰©æ•£æ–¹æ³•ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹æ¨ç†æ—¶é—´è¿‡é•¿çš„é™åˆ¶ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤æ‚çš„ç‰©ç†åŠ¨æ€å’Œäº’åŠ¨è¡Œä¸ºï¼Œé€‚ç”¨äºå®æ—¶æ€§èƒ½è¦æ±‚é«˜çš„åœºæ™¯ã€‚å®ƒçš„æ¡†æ¶åŒ…æ‹¬å¯æ‰©å±•çš„æ•°æ®ç”Ÿäº§ç®¡é“ã€åŠ¨ä½œæ³¨å…¥æ¨¡å—å’ŒåŸºäºå› æœæ¶æ„çš„å°‘æ­¥è’¸é¦æŠ€æœ¯ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼ŒMatrix-Game 2.0 å¯ä»¥ä»¥æ¯ç§’25å¸§çš„é€Ÿåº¦ç”Ÿæˆé«˜è´¨é‡çš„åˆ†é’Ÿçº§è§†é¢‘ï¼Œæ¨åŠ¨äº’åŠ¨ä¸–ç•Œå»ºæ¨¡çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12466",
            "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
            "url": "https://huggingface.co/papers/2508.12466",
            "abstract": "Inverse-LLaVA eliminates alignment pre-training by mapping text embeddings into continuous visual representation space, improving reasoning tasks while reducing computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional multimodal learning approaches require expensive alignment pre-training to bridge vision and language modalities, typically projecting visual features into discrete text token spaces. We challenge both fundamental assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel approach that eliminates alignment pre-training entirely while inverting the conventional mapping direction. Rather than projecting visual features to text space, our method maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. Through selective additive components in attention mechanisms, we enable dynamic integration of visual and textual representations without requiring massive image-text alignment datasets. Comprehensive experiments across nine multimodal benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing expected decreases in perception tasks requiring memorized visual-text associations (celebrity recognition: -49.5%, OCR: -21.3%). These results provide the first empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. Our work establishes the feasibility of a new paradigm that reduces computational requirements by 45%, challenges conventional wisdom about modality fusion, and opens new research directions for efficient multimodal architectures that preserve modality-specific characteristics. Our project website with code and additional resources is available at https://inverse-llava.github.io.",
            "score": 5,
            "issue_id": 5416,
            "pub_date": "2025-08-17",
            "pub_date_card": {
                "ru": "17 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 17",
                "zh": "8æœˆ17æ—¥"
            },
            "hash": "18214c417cbd148a",
            "authors": [
                "Xuhui Zhan",
                "Tyler Derr"
            ],
            "affiliations": [
                "Computer Science Department Vanderbilt University",
                "Data Science Institute Vanderbilt University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12466.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#reasoning",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ˜Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Inverse-LLaVA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 45% Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Learning: No More Alignment Pre-Training!",
                    "desc": "Inverse-LLaVA introduces a new method for multimodal learning that eliminates the need for alignment pre-training, which is typically required to connect visual and textual data. Instead of converting visual features into discrete text tokens, this approach maps text embeddings directly into a continuous visual representation space. By integrating visual and textual information within transformer layers using selective attention mechanisms, Inverse-LLaVA enhances performance on reasoning tasks while reducing the need for large datasets. The results show significant improvements in cognitive reasoning tasks, suggesting that traditional alignment methods may not be necessary for effective multimodal learning."
                },
                "zh": {
                    "title": "é€†å‘LLaVAï¼šæ— éœ€å¯¹é½é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "Inverse-LLaVAæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†æ–‡æœ¬åµŒå…¥æ˜ å°„åˆ°è¿ç»­çš„è§†è§‰è¡¨ç¤ºç©ºé—´ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹å¯¹é½é¢„è®­ç»ƒçš„éœ€æ±‚ã€‚è¿™ç§æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚é€šè¿‡åœ¨å˜æ¢å™¨çš„ä¸­é—´å±‚è¿›è¡Œè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„åŠ¨æ€èåˆï¼ŒInverse-LLaVAèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹é½æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶åœ¨éœ€è¦è®°å¿†è§†è§‰-æ–‡æœ¬å…³è”çš„æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°æœ‰æ‰€ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11598",
            "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
            "url": "https://huggingface.co/papers/2508.11598",
            "abstract": "AuriStream, a biologically inspired two-stage model, encodes speech using cochlear tokens and an autoregressive sequence model, achieving state-of-the-art performance on speech tasks and generating interpretable audio continuations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AuriStream, a biologically inspired model for encoding speech via a two-stage framework inspired by the human auditory processing hierarchy. The first stage transforms raw audio into a time-frequency representation based on the human cochlea, from which we extract discrete cochlear tokens. The second stage applies an autoregressive sequence model over the cochlear tokens. AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics. AuriStream shows competitive performance on diverse downstream SUPERB speech tasks. Complementing AuriStream's strong representational capabilities, it generates continuations of audio which can be visualized in a spectrogram space and decoded back into audio, providing insights into the model's predictions. In summary, we present a two-stage framework for speech representation learning to advance the development of more human-like models that efficiently handle a range of speech-based tasks.",
            "score": 5,
            "issue_id": 5417,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 15",
                "zh": "8æœˆ15æ—¥"
            },
            "hash": "6421a301f871081b",
            "authors": [
                "Greta Tuckute",
                "Klemen Kotar",
                "Evelina Fedorenko",
                "Daniel L. K. Yamins"
            ],
            "affiliations": [
                "Department of Brain and Cognitive Sciences & McGovern Institute for Brain Research, MIT, USA",
                "Department of Computer Science & Wu Tsai Neurosciences Institute, Stanford University, USA",
                "Department of Psychology, Stanford University, USA",
                "Program in Speech and Hearing Bioscience and Technology, Harvard University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11598.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#audio",
                    "#interpretability"
                ],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "Ğ‘Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸",
                    "desc": "AuriStream - ÑÑ‚Ğ¾ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‹Ñ€Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑƒĞ»Ğ¸Ñ‚ĞºĞ¸, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ñ…Ğ»ĞµĞ°Ñ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº ÑÑ‚Ğ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. AuriStream Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾."
                },
                "en": {
                    "title": "AuriStream: Advancing Speech Processing with Human-Inspired Learning",
                    "desc": "AuriStream is a two-stage model designed for encoding speech, inspired by how humans process sound. The first stage converts raw audio into a time-frequency representation, extracting discrete cochlear tokens that represent sound features. The second stage uses an autoregressive sequence model to analyze these tokens, enabling the model to learn meaningful phoneme and word representations. AuriStream achieves state-of-the-art results on various speech tasks and can generate audio continuations that are interpretable, enhancing our understanding of its predictions."
                },
                "zh": {
                    "title": "AuriStreamï¼šç”Ÿç‰©å¯å‘çš„è¯­éŸ³ç¼–ç æ–°æ¨¡å‹",
                    "desc": "AuriStream æ˜¯ä¸€ä¸ªå—ç”Ÿç‰©å¯å‘çš„ä¸¤é˜¶æ®µæ¨¡å‹ï¼Œç”¨äºç¼–ç è¯­éŸ³ã€‚ç¬¬ä¸€é˜¶æ®µå°†åŸå§‹éŸ³é¢‘è½¬æ¢ä¸ºåŸºäºäººç±»è€³èœ—çš„æ—¶é¢‘è¡¨ç¤ºï¼Œå¹¶æå–ç¦»æ•£çš„è€³èœ—æ ‡è®°ã€‚ç¬¬äºŒé˜¶æ®µåœ¨è€³èœ—æ ‡è®°ä¸Šåº”ç”¨è‡ªå›å½’åºåˆ—æ¨¡å‹ï¼Œå­¦ä¹ æœ‰æ„ä¹‰çš„éŸ³ç´ å’Œå•è¯è¡¨ç¤ºã€‚AuriStream åœ¨å¤šç§è¯­éŸ³ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆå¯è§†åŒ–çš„éŸ³é¢‘å»¶ç»­ï¼Œæä¾›å¯¹æ¨¡å‹é¢„æµ‹çš„æ·±å…¥ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12880",
            "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2508.12880",
            "abstract": "S^2-Guidance, a novel method using stochastic block-dropping, improves sample quality and prompt adherence in diffusion models by refining suboptimal predictions, outperforming Classifier-free Guidance and other advanced strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released.",
            "score": 4,
            "issue_id": 5417,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "8d70a9abdc48a41a",
            "authors": [
                "Chubin Chen",
                "Jiashu Zhu",
                "Xiaokun Feng",
                "Nisha Huang",
                "Meiqi Wu",
                "Fangyuan Mao",
                "Jiahong Wu",
                "Xiangxiang Chu",
                "Xiu Li"
            ],
            "affiliations": [
                "AMAP, Alibaba Group",
                "CASIA",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12880.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#open_source",
                    "#diffusion",
                    "#training",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "S^2-Guidance: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "S^2-Guidance - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Classifier-free Guidance Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. S^2-Guidance Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞµÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ°, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ S^2-Guidance Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Refining Predictions for Superior Sample Quality in Diffusion Models",
                    "desc": "The paper introduces S^2-Guidance, a new method that enhances the performance of diffusion models by refining their predictions. It addresses the limitations of Classifier-free Guidance (CFG), which often leads to low-quality outputs due to reliance on suboptimal predictions. By using stochastic block-dropping, S^2-Guidance creates sub-networks that help the model focus on generating higher quality samples. Experimental results show that this method significantly outperforms CFG and other existing strategies in tasks like text-to-image and text-to-video generation."
                },
                "zh": {
                    "title": "S^2-Guidanceï¼šæå‡æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ä¸æç¤ºéµå¾ªæ€§",
                    "desc": "S^2-Guidanceæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡éšæœºå—ä¸¢å¼ƒæŠ€æœ¯æ¥æé«˜æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡å’Œæç¤ºéµå¾ªæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ”¹è¿›æ¬¡ä¼˜é¢„æµ‹ï¼Œå…‹æœäº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­‰ä¼ ç»ŸæŠ€æœ¯çš„ä¸è¶³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„æ¬¡ä¼˜é¢„æµ‹å¯ä»¥é€šè¿‡è‡ªèº«çš„å­ç½‘ç»œæœ‰æ•ˆåœ°è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå‡å°‘è¯­ä¹‰ä¸ä¸€è‡´å’Œä½è´¨é‡è¾“å‡ºçš„é—®é¢˜ã€‚é€šè¿‡åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒS^2-Guidanceå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†CFGå’Œå…¶ä»–å…ˆè¿›çš„å¼•å¯¼ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13104",
            "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
            "url": "https://huggingface.co/papers/2508.13104",
            "abstract": "Visual action prompts, using visual skeletons, enable precise action control in video generation while maintaining cross-domain transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present visual action prompts, a unified action representation for action-to-video generation of complex high-DoF interactions while maintaining transferable visual dynamics across domains. Action-driven video generation faces a precision-generality trade-off: existing methods using text, primitive actions, or coarse masks offer generality but lack precision, while agent-centric action signals provide precision at the cost of cross-domain transferability. To balance action precision and dynamic transferability, we propose to \"render\" actions into precise visual prompts as domain-agnostic representations that preserve both geometric precision and cross-domain adaptability for complex actions; specifically, we choose visual skeletons for their generality and accessibility. We propose robust pipelines to construct skeletons from two interaction-rich data sources - human-object interactions (HOI) and dexterous robotic manipulation - enabling cross-domain training of action-driven generative models. By integrating visual skeletons into pretrained video generation models via lightweight fine-tuning, we enable precise action control of complex interaction while preserving the learning of cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the effectiveness of our proposed approach. Project page: https://zju3dv.github.io/VAP/.",
            "score": 3,
            "issue_id": 5420,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "3492b8aeb00265b2",
            "authors": [
                "Yuang Wang",
                "Chao Wen",
                "Haoyu Guo",
                "Sida Peng",
                "Minghan Qin",
                "Hujun Bao",
                "Xiaowei Zhou",
                "Ruizhen Hu"
            ],
            "affiliations": [
                "Fudan University",
                "Shenzhen University",
                "Tsinghua University",
                "Xiangjiang Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13104.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#transfer_learning",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¦´",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞµĞ»ĞµÑ‚Ñ‹: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞµĞ»ĞµÑ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Visual Skeletons: Bridging Precision and Transferability in Video Generation",
                    "desc": "This paper introduces visual action prompts, which are visual skeletons used to enhance action control in video generation. The authors address the challenge of balancing precision and generality in action-driven video generation, where traditional methods often compromise one for the other. By using visual skeletons as domain-agnostic representations, they achieve both precise action representation and the ability to transfer learned dynamics across different domains. The proposed method is validated through experiments on various datasets, demonstrating its effectiveness in generating complex interactions in videos."
                },
                "zh": {
                    "title": "è§†è§‰éª¨æ¶ï¼šç²¾ç¡®æ§åˆ¶ä¸è·¨é¢†åŸŸè½¬ç§»çš„å¹³è¡¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰åŠ¨ä½œæç¤ºçš„æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰éª¨æ¶å®ç°è§†é¢‘ç”Ÿæˆä¸­çš„ç²¾ç¡®åŠ¨ä½œæ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒè·¨é¢†åŸŸçš„å¯è½¬ç§»æ€§ã€‚ä¼ ç»Ÿçš„åŠ¨ä½œé©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨ç²¾åº¦å’Œé€šç”¨æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†åŠ¨ä½œæ¸²æŸ“ä¸ºç²¾ç¡®çš„è§†è§‰æç¤ºï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬é€‰æ‹©è§†è§‰éª¨æ¶ä½œä¸ºé€šç”¨ä¸”æ˜“äºè·å–çš„è¡¨ç¤ºï¼Œæ„å»ºäº†ä»äºº-ç‰©ä½“äº¤äº’å’Œçµå·§æœºå™¨äººæ“ä½œä¸­æå–éª¨æ¶çš„å¼ºå¤§ç®¡é“ã€‚é€šè¿‡è½»é‡çº§å¾®è°ƒå°†è§†è§‰éª¨æ¶é›†æˆåˆ°é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¤æ‚äº¤äº’çš„ç²¾ç¡®åŠ¨ä½œæ§åˆ¶ï¼ŒåŒæ—¶ä¿ç•™äº†è·¨é¢†åŸŸåŠ¨æ€çš„å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.12945",
            "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models",
            "url": "https://huggingface.co/papers/2508.12945",
            "abstract": "Lumen is an end-to-end video relighting framework that uses a large-scale dataset of realistic and synthetic videos to achieve consistent lighting and foreground preservation in edited videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Video relighting is a challenging yet valuable task, aiming to replace the background in videos while correspondingly adjusting the lighting in the foreground with harmonious blending. During translation, it is essential to preserve the original properties of the foreground, e.g., albedo, and propagate consistent relighting among temporal frames. In this paper, we propose Lumen, an end-to-end video relighting framework developed on large-scale video generative models, receiving flexible textual description for instructing the control of lighting and background. Considering the scarcity of high-qualified paired videos with the same foreground in various lighting conditions, we construct a large-scale dataset with a mixture of realistic and synthetic videos. For the synthetic domain, benefiting from the abundant 3D assets in the community, we leverage advanced 3D rendering engine to curate video pairs in diverse environments. For the realistic domain, we adapt a HDR-based lighting simulation to complement the lack of paired in-the-wild videos. Powered by the aforementioned dataset, we design a joint training curriculum to effectively unleash the strengths of each domain, i.e., the physical consistency in synthetic videos, and the generalized domain distribution in realistic videos. To implement this, we inject a domain-aware adapter into the model to decouple the learning of relighting and domain appearance distribution. We construct a comprehensive benchmark to evaluate Lumen together with existing methods, from the perspectives of foreground preservation and video consistency assessment. Experimental results demonstrate that Lumen effectively edit the input into cinematic relighted videos with consistent lighting and strict foreground preservation. Our project page: https://lumen-relight.github.io/",
            "score": 3,
            "issue_id": 5417,
            "pub_date": "2025-08-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 18",
                "zh": "8æœˆ18æ—¥"
            },
            "hash": "c5be2dd35a23538c",
            "authors": [
                "Jianshu Zeng",
                "Yuxuan Liu",
                "Yutong Feng",
                "Chenxuan Miao",
                "Zixiang Gao",
                "Jiwang Qu",
                "Jianzhang Zhang",
                "Bin Wang",
                "Kun Yuan"
            ],
            "affiliations": [
                "Hangzhou Normal University",
                "Kunbyte AI",
                "Peking University",
                "University of Chinese Academy of Sciences",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.12945.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Lumen: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²ĞºĞ° Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°",
                    "desc": "Lumen - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ² Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ HDR-Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Lumen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²ĞºĞµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°."
                },
                "en": {
                    "title": "Lumen: Mastering Video Relighting with Consistent Foreground Preservation",
                    "desc": "Lumen is a novel framework designed for video relighting, which focuses on changing the background of videos while ensuring that the lighting on the foreground remains consistent. It utilizes a large-scale dataset of both realistic and synthetic videos to train its model, addressing the challenge of preserving foreground properties like albedo during the relighting process. The framework employs a joint training approach that leverages the strengths of synthetic videos for physical consistency and realistic videos for generalization. By incorporating a domain-aware adapter, Lumen effectively separates the learning of relighting from the appearance distribution of different domains, resulting in high-quality, edited videos that maintain both lighting consistency and foreground integrity."
                },
                "zh": {
                    "title": "Lumenï¼šå®ç°ä¸€è‡´å…‰ç…§ä¸å‰æ™¯ä¿ç•™çš„è§†é¢‘é‡å…‰æ¡†æ¶",
                    "desc": "Lumenæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†é¢‘é‡å…‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨å¤§è§„æ¨¡çš„çœŸå®å’Œåˆæˆè§†é¢‘æ•°æ®é›†ï¼Œå®ç°ç¼–è¾‘è§†é¢‘ä¸­çš„ä¸€è‡´å…‰ç…§å’Œå‰æ™¯ä¿ç•™ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®çµæ´»çš„æ–‡æœ¬æè¿°æ¥æ§åˆ¶å…‰ç…§å’ŒèƒŒæ™¯ï¼Œè§£å†³äº†è§†é¢‘é‡å…‰ä¸­å‰æ™¯å±æ€§ï¼ˆå¦‚åç…§ç‡ï¼‰çš„ä¿ç•™é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹é«˜è´¨é‡é…å¯¹è§†é¢‘çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§ç¯å¢ƒçš„åˆæˆè§†é¢‘å’ŒçœŸå®è§†é¢‘çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLumenèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è¾“å…¥è§†é¢‘ç¼–è¾‘ä¸ºå…·æœ‰ä¸€è‡´å…‰ç…§å’Œä¸¥æ ¼å‰æ™¯ä¿ç•™çš„ç”µå½±çº§é‡å…‰è§†é¢‘ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-18.html",
    "link_next": "2025-08-20.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "18.08",
        "en": "08/18",
        "zh": "8æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.08",
        "en": "08/20",
        "zh": "8æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 3,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 2,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}