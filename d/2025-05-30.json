{
    "date": {
        "ru": "30 Ğ¼Ğ°Ñ",
        "en": "May 30",
        "zh": "5æœˆ30æ—¥"
    },
    "time_utc": "2025-05-30 10:12",
    "weekday": 4,
    "issue_id": 4043,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.23747",
            "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
            "url": "https://huggingface.co/papers/2505.23747",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.",
            "score": 39,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "5ee23f045f054465",
            "authors": [
                "Diankun Wu",
                "Fangfu Liu",
                "Yi-Hsin Hung",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23747.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¸Ğ· 2D Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Spatial-MLLM - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 3D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Spatial-MLLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Spatial-MLLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking 3D Spatial Reasoning from 2D Inputs",
                    "desc": "This paper introduces Spatial-MLLM, a new framework designed to enhance spatial reasoning using only 2D inputs like images and videos. Unlike traditional 3D models that require additional 3D data, Spatial-MLLM utilizes a dual-encoder architecture that combines a pretrained 2D visual encoder for semantic features with a spatial encoder for 3D structure features. The model integrates these features into unified visual tokens, improving its ability to understand spatial relationships. Additionally, a novel space-aware frame sampling strategy is employed to prioritize important frames during inference, leading to superior performance in various spatial reasoning tasks."
                },
                "zh": {
                    "title": "ä»2Dåˆ°3Dçš„ç©ºé—´æ¨ç†æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºSpatial-MLLMï¼Œæ—¨åœ¨ä»çº¯2Dè§‚å¯Ÿä¸­è¿›è¡Œè§†è§‰åŸºç¡€çš„ç©ºé—´æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºCLIPè§†è§‰ç¼–ç å™¨çš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨äº†è§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹çš„å¼ºç»“æ„å…ˆéªŒã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨å’Œç©ºé—´ç¼–ç å™¨ï¼Œä»¥æå–è¯­ä¹‰ç‰¹å¾å’Œ3Dç»“æ„ç‰¹å¾ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶é‡‡ç”¨ç©ºé—´æ„ŸçŸ¥çš„å¸§é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23621",
            "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
            "url": "https://huggingface.co/papers/2505.23621",
            "abstract": "Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.",
            "score": 38,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "a359c304b423ff99",
            "authors": [
                "Zheyuan Yang",
                "Lyuhao Chen",
                "Arman Cohan",
                "Yilun Zhao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.23621.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#rl",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ· Ñ‚Ñ€Ğ°ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Table-R1-Zero, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ GPT-4.1, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Scaling Table Reasoning with Fewer Parameters!",
                    "desc": "This paper introduces two innovative strategies, distillation and reinforcement learning with verifiable rewards (RLVR), to enhance the performance of models in table reasoning tasks. The authors develop a new model, Table-R1-Zero, which achieves performance comparable to GPT-4.1 while utilizing significantly fewer parameters. By leveraging a large dataset of reasoning traces for distillation and applying task-specific reward functions in RLVR, the model demonstrates strong generalization capabilities across various reasoning tasks. The findings highlight the importance of instruction tuning and model architecture in achieving effective table reasoning skills."
                },
                "zh": {
                    "title": "è¡¨æ ¼æ¨ç†ä»»åŠ¡çš„æ¨ç†æ—¶é—´æ‰©å±•æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­çš„æ¨ç†æ—¶é—´æ‰©å±•ï¼Œæå‡ºäº†ä¸¤ç§åè®­ç»ƒç­–ç•¥ï¼šè’¸é¦å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚é€šè¿‡è’¸é¦ï¼Œæˆ‘ä»¬åˆ©ç”¨DeepSeek-R1ç”Ÿæˆçš„å¤§è§„æ¨¡æ¨ç†è½¨è¿¹æ•°æ®é›†ï¼Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå½¢æˆTable-R1-SFTæ¨¡å‹ã€‚RLVRåˆ™é€šè¿‡ç‰¹å®šä»»åŠ¡çš„å¯éªŒè¯å¥–åŠ±å‡½æ•°ï¼Œåº”ç”¨GRPOç®—æ³•ï¼Œå¾—åˆ°Table-R1-Zeroæ¨¡å‹ã€‚æœ€ç»ˆï¼ŒTable-R1-Zeroæ¨¡å‹åœ¨å¤šä¸ªè¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‚æ•°é‡ä»…ä¸º7Bï¼Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºGPT-4.1ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22653",
            "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
            "url": "https://huggingface.co/papers/2505.22653",
            "abstract": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
            "score": 36,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "7d2f43b6997c2647",
            "authors": [
                "Ang Lv",
                "Ruobing Xie",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Rui Yan"
            ],
            "affiliations": [
                "GSAI, Renmin University of China",
                "Large Language Model Department, Tencent",
                "School of Computer Science, Wuhan University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22653.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº ÑˆÑƒĞ¼Ñƒ: Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RPR) Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen-2.5-7B ÑĞ¼Ğ¾Ğ³Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 5% Ğ´Ğ¾ 72%, Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° 40% Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Robust LLMs: Thriving Amid Reward Noise with Reasoning Patterns",
                    "desc": "This paper explores how large language models (LLMs) can effectively handle noise in reward signals during post-training, particularly in reinforcement learning scenarios. The authors demonstrate that even with significant reward noise, such as flipping 40% of reward outputs, LLMs like Qwen-2.5-7B can still achieve impressive performance improvements on math tasks. They introduce a novel approach called reasoning pattern rewards (RPR), which focuses on rewarding the presence of key reasoning phrases rather than the correctness of answers, leading to high accuracy. The study emphasizes the importance of enhancing foundational skills during pre-training and offers insights for refining post-training methods in real-world applications."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹é²æ£’æ€§ï¼Œå…‹æœå¥–åŠ±å™ªå£°",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åè®­ç»ƒé˜¶æ®µï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¥–åŠ±å™ªå£°çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨å¥–åŠ±å‡½æ•°è¾“å‡ºä¸­æ‰‹åŠ¨ç¿»è½¬40%çš„ç»“æœï¼Œæ¨¡å‹ä»èƒ½å¿«é€Ÿæ”¶æ•›ï¼Œæ•°å­¦ä»»åŠ¡çš„å‡†ç¡®ç‡ä»5%æå‡è‡³72%ã€‚é€šè¿‡ä»…å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­çš„å‡ºç°ï¼ˆå³æ¨ç†æ¨¡å¼å¥–åŠ±RPRï¼‰ï¼Œæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°äº†è¶…è¿‡70%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¸ä¸¥æ ¼éªŒè¯çš„æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒé˜¶æ®µæå‡æ¨¡å‹åŸºç¡€èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºåè®­ç»ƒæŠ€æœ¯çš„è¿›æ­¥æä¾›äº†æ–°æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23693",
            "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
            "url": "https://huggingface.co/papers/2505.23693",
            "abstract": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
            "score": 34,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "1a301b9c565967e9",
            "authors": [
                "Tingyu Song",
                "Tongyan Hu",
                "Guo Gan",
                "Yilun Zhao"
            ],
            "affiliations": [
                "National University of Singapore",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Yale University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23693.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VF-Eval: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VF-Eval Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4.1, ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MLLM Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Evaluating MLLMs: Bridging AI-Generated Videos and Human Feedback",
                    "desc": "The paper introduces VF-Eval, a new benchmark designed to assess the performance of Multimodal Language Models (MLLMs) in interpreting AI-generated content videos. It focuses on four specific tasks: coherence validation, error awareness, error type detection, and reasoning evaluation, which are crucial for understanding synthetic videos. The study evaluates 13 advanced MLLMs, revealing that even the top model, GPT-4.1, faces challenges in consistently performing well across these tasks. Furthermore, the paper explores how aligning MLLMs with human feedback can enhance the quality of video generation, showcasing the practical implications of the benchmark."
                },
                "zh": {
                    "title": "è¯„ä¼°AIç”Ÿæˆè§†é¢‘çš„èƒ½åŠ›æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•VF-Evalï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§£è¯»AIç”Ÿæˆå†…å®¹è§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«å››ä¸ªä»»åŠ¡ï¼šè¿è´¯æ€§éªŒè¯ã€é”™è¯¯æ„è¯†ã€é”™è¯¯ç±»å‹æ£€æµ‹å’Œæ¨ç†è¯„ä¼°ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsåœ¨å¤„ç†åˆæˆè§†é¢‘æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹GPT-4.1ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä¹Ÿéš¾ä»¥ä¿æŒä¸€è‡´çš„è‰¯å¥½è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºåŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒRePromptï¼Œç ”ç©¶è¡¨æ˜å°†MLLMsä¸äººç±»åé¦ˆæ›´ç´§å¯†å¯¹é½å¯ä»¥æ”¹å–„è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23762",
            "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
            "url": "https://huggingface.co/papers/2505.23762",
            "abstract": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.",
            "score": 33,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "d159a7e52c608567",
            "authors": [
                "Chenyu Yang",
                "Shiqian Su",
                "Shi Liu",
                "Xuan Dong",
                "Yue Yu",
                "Weijie Su",
                "Xuehui Wang",
                "Zhaoyang Liu",
                "Jinguo Zhu",
                "Hao Li",
                "Wenhai Wang",
                "Yu Qiao",
                "Xizhou Zhu",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23762.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#games",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "ZeroGUI - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ZeroGUI Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ“ĞŸĞ˜-ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering GUI Agents with Zero Human Cost",
                    "desc": "ZeroGUI is an innovative online learning framework that leverages Vision-Language Models (VLMs) to enhance the training of GUI Agents with minimal human input. It addresses the challenges of traditional offline learning methods, which require extensive manual annotations and struggle to adapt to changing environments. By utilizing VLMs for automatic task generation and reward estimation, ZeroGUI enables continuous learning and interaction with GUI systems. Experiments show that this approach significantly improves the performance of advanced GUI Agents in various environments."
                },
                "zh": {
                    "title": "ZeroGUIï¼šæ— äººå·¥æˆæœ¬çš„GUIä»£ç†è®­ç»ƒæ¡†æ¶",
                    "desc": "ZeroGUIæ˜¯ä¸€ä¸ªåœ¨çº¿å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œä»»åŠ¡ç”Ÿæˆå’Œå¥–åŠ±è¯„ä¼°ï¼Œä»è€Œåœ¨æœ€å°äººåŠ›å¹²é¢„ä¸‹æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿç¦»çº¿å­¦ä¹ æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šå¯¹é«˜è´¨é‡æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–å’Œå¯¹åŠ¨æ€äº¤äº’ç¯å¢ƒçš„é€‚åº”æ€§ä¸è¶³ã€‚ZeroGUIé€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„è®­ç»ƒç›®æ ‡å’Œè‡ªåŠ¨è¯„ä¼°ä»»åŠ¡æˆåŠŸç‡ï¼Œæ¥å®ç°æ— äººå·¥æˆæœ¬çš„GUIä»£ç†è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroGUIåœ¨OSWorldå’ŒAndroidLabç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ä¸¤ç§å…ˆè¿›GUIä»£ç†çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23604",
            "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
            "url": "https://huggingface.co/papers/2505.23604",
            "abstract": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.",
            "score": 18,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "3c092222b4cf7a3d",
            "authors": [
                "Guangtao Zeng",
                "Maohao Shen",
                "Delin Chen",
                "Zhenting Qi",
                "Subhro Das",
                "Dan Gutfreund",
                "David Cox",
                "Gregory Wornell",
                "Wei Lu",
                "Zhang-Wei Hong",
                "Chuang Gan"
            ],
            "affiliations": [
                "Department of EECS, MIT",
                "Harvard",
                "MIT-IBM Watson AI Lab, IBM Research",
                "Singapore University of Technology and Design",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23604.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "EvoScale - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¼ĞµÑ‰Ğ°Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. EvoScale Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ 32B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²Ğ¾Ğ¸Ñ… ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "EvoScale: Evolving Small Models for Big Performance",
                    "desc": "EvoScale is a novel method that combines evolutionary strategies and reinforcement learning to enhance the performance of small language models on real-world software engineering tasks. It addresses the limitations of existing approaches that rely heavily on supervised fine-tuning and expensive data curation. By treating the output generation as an evolutionary process, EvoScale iteratively refines model outputs through selection and mutation, significantly reducing the number of samples needed for effective solutions. This approach allows smaller models to achieve performance levels comparable to much larger models, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "è¿›åŒ–æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼šå°æ¨¡å‹çš„å¼ºå¤§æå‡",
                    "desc": "EvoScaleæ˜¯ä¸€ç§åŸºäºè¿›åŒ–å’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å°å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£æ”¹è¿›å’Œä¼˜åŒ–è¾“å‡ºï¼Œå‡å°‘äº†å¯»æ‰¾æ­£ç¡®è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒEvoScaleé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è‡ªæˆ‘æå‡ç”Ÿæˆç»“æœçš„è¯„åˆ†ã€‚ç»è¿‡è¯„ä¼°ï¼ŒEvoScaleä½¿å¾—32Bå‚æ•°çš„æ¨¡å‹Satori-SWE-32Båœ¨æ€§èƒ½ä¸Šèƒ½å¤ŸåŒ¹æ•Œæˆ–è¶…è¿‡100Bå‚æ•°çš„æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡æ›´å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23359",
            "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
            "url": "https://huggingface.co/papers/2505.23359",
            "abstract": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on \"test-time scaling\" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench.",
            "score": 18,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "291558dd32e84247",
            "authors": [
                "Yuanxin Liu",
                "Kun Ouyang",
                "Haoning Wu",
                "Yi Liu",
                "Lin Sui",
                "Xinhao Li",
                "Yan Zhong",
                "Y. Charles",
                "Xinyu Zhou",
                "Xu Sun"
            ],
            "affiliations": [
                "Moonshot AI",
                "Nanjing University",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
                "School of Mathematical Sciences, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23359.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "VideoReasonBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, GPT-4o Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 6.9%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° VideoReasonBench, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Video Reasoning with Extended Thinking Budgets",
                    "desc": "The paper introduces VideoReasonBench, a new benchmark for evaluating complex video reasoning that emphasizes the importance of extended thinking budgets. It highlights that traditional benchmarks often lack the depth of reasoning needed to fully assess video understanding capabilities. The benchmark includes tasks that require models to recall visual information, infer latent states, and predict beyond the video content, thus testing their reasoning skills comprehensively. Results show that most state-of-the-art multimodal language models struggle with these tasks, but those with enhanced reasoning capabilities, like Gemini-2.5-Pro, perform significantly better."
                },
                "zh": {
                    "title": "å»¶é•¿æ€è€ƒæ—¶é—´ï¼Œæå‡è§†é¢‘æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºVideoReasonBenchï¼Œæ—¨åœ¨è¯„ä¼°å¤æ‚çš„è§†è§‰è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå»¶é•¿æ€è€ƒæ—¶é—´å¯¹äºæé«˜æ¨¡å‹åœ¨æ­¤åŸºå‡†ä¸Šçš„è¡¨ç°è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ã€‚VideoReasonBenchçš„ä»»åŠ¡è®¾è®¡è¦æ±‚æ¨¡å‹åœ¨è§†é¢‘ä¸­å›å¿†å¤šä¸ªæ“ä½œï¼Œå¹¶è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»¥å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚é€šè¿‡å¯¹18ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†æ€è€ƒå¢å¼ºçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23716",
            "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
            "url": "https://huggingface.co/papers/2505.23716",
            "abstract": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/",
            "score": 17,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "0f88880e118194a0",
            "authors": [
                "Lihan Jiang",
                "Yucheng Mao",
                "Linning Xu",
                "Tao Lu",
                "Kerui Ren",
                "Yichen Jin",
                "Xudong Xu",
                "Mulin Yu",
                "Jiangmiao Pang",
                "Feng Zhao",
                "Dahua Lin",
                "Bo Dai"
            ],
            "affiliations": [
                "Brown University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "The University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23716.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğ¼ĞµÑ€",
                    "desc": "AnySplat - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ ÑÑ†ĞµĞ½Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€. AnySplat ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ·Ğ°Ñ… ĞºĞ°Ğ¼ĞµÑ€, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "AnySplat: Real-Time Novel View Synthesis Without Camera Poses",
                    "desc": "AnySplat is a novel feed forward network designed for synthesizing new views from uncalibrated image collections without needing camera poses. Unlike traditional methods that require detailed scene optimization and known camera positions, AnySplat efficiently predicts scene geometry and appearance in a single forward pass. It utilizes 3D Gaussian primitives to represent the scene, allowing it to handle both sparse and dense datasets seamlessly. The model not only matches the quality of pose-aware methods but also significantly reduces rendering time, making real-time novel view synthesis feasible in casual capture scenarios."
                },
                "zh": {
                    "title": "AnySplatï¼šæ— ä½å§¿æ–°è§†è§’åˆæˆçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "AnySplatæ˜¯ä¸€ç§å‰é¦ˆç½‘ç»œï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç›¸æœºä½å§¿çš„æƒ…å†µä¸‹è¿›è¡Œæ–°è§†è§’åˆæˆã€‚ä¸ä¼ ç»Ÿçš„ç¥ç»æ¸²æŸ“ç®¡é“ä¸åŒï¼ŒAnySplatä¸éœ€è¦å·²çŸ¥çš„ç›¸æœºä½å§¿å’Œé€åœºæ™¯ä¼˜åŒ–ï¼Œè€Œæ˜¯é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ç”Ÿæˆ3Dé«˜æ–¯åŸè¯­ï¼Œç¼–ç åœºæ™¯å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè½»æ¾æ‰©å±•åˆ°å¤šè§†è§’æ•°æ®é›†ï¼Œä¸”æ— éœ€ä½å§¿æ³¨é‡Šã€‚åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒAnySplatåœ¨ç¨€ç–å’Œå¯†é›†è§†å›¾åœºæ™¯ä¸­ä¸åŸºäºä½å§¿çš„æ–¹æ³•è´¨é‡ç›¸å½“ï¼ŒåŒæ—¶åœ¨æ¸²æŸ“å»¶è¿Ÿä¸Šå¤§å¹…é™ä½ï¼Œé€‚ç”¨äºå®æ—¶æ–°è§†è§’åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23660",
            "title": "D-AR: Diffusion via Autoregressive Models",
            "url": "https://huggingface.co/papers/2505.23660",
            "abstract": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at https://github.com/showlab/D-AR",
            "score": 17,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "fb75799d968366cb",
            "authors": [
                "Ziteng Gao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23660.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#diffusion",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "D-AR: Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Diffusion via Autoregressive models (D-AR). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. D-AR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Autoregressive Diffusion",
                    "desc": "This paper introduces Diffusion via Autoregressive models (D-AR), which reformulates the image diffusion process as a standard autoregressive task. It involves designing a tokenizer that transforms images into sequences of discrete tokens, allowing for different diffusion denoising steps to be decoded from these tokens. The method leverages the natural coarse-to-fine order of the tokens, enabling effective next-token prediction without altering existing autoregressive frameworks. The results demonstrate high-quality image generation with consistent previews and layout control, achieving a notable FID score on the ImageNet benchmark."
                },
                "zh": {
                    "title": "è‡ªå›å½’æ¨¡å‹ï¼šå›¾åƒç”Ÿæˆçš„æ–°è§†è§’",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ‰©æ•£æ–¹æ³•ï¼Œç§°ä¸ºè‡ªå›å½’æ¨¡å‹æ‰©æ•£ï¼ˆD-ARï¼‰ï¼Œå°†å›¾åƒæ‰©æ•£è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºæ ‡å‡†çš„è‡ªå›å½’ä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†è¯å™¨ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—ï¼Œè¿™äº›æ ‡è®°åœ¨ä¸åŒä½ç½®å¯ä»¥è§£ç ä¸ºä¸åŒçš„å»å™ªæ­¥éª¤ã€‚å¾—ç›Šäºæ‰©æ•£ç‰¹æ€§ï¼Œè¿™äº›æ ‡è®°è‡ªç„¶éµå¾ªç²—åˆ°ç»†çš„é¡ºåºï¼Œé€‚åˆè‡ªå›å½’å»ºæ¨¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†2.09çš„FIDï¼Œå±•ç¤ºäº†åœ¨è§†è§‰åˆæˆä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23646",
            "title": "Are Reasoning Models More Prone to Hallucination?",
            "url": "https://huggingface.co/papers/2505.23646",
            "abstract": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.",
            "score": 17,
            "issue_id": 4039,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "af43e4853b84c9fe",
            "authors": [
                "Zijun Yao",
                "Yantao Liu",
                "Yanxu Chen",
                "Jianhui Chen",
                "Junfeng Fang",
                "Lei Hou",
                "Juanzi Li",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23646.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LRM: Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Understanding and Reducing Hallucination in Large Reasoning Models",
                    "desc": "This paper investigates how large reasoning models (LRMs) can produce incorrect information, known as hallucination, and how this varies based on their training methods. It finds that certain post-training techniques, like supervised fine-tuning and reinforcement learning, can reduce hallucination, while others may worsen it. The authors identify two key cognitive behaviorsâ€”Flaw Repetition and Think-Answer Mismatchâ€”that contribute to these inaccuracies. Additionally, they explore how the model's uncertainty can misalign with factual correctness, leading to increased hallucination in LRMs."
                },
                "zh": {
                    "title": "ç†è§£å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨åè®­ç»ƒæµç¨‹ä¸­å¯¹å¹»è§‰çš„ä¸åŒæ•æ„Ÿæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç»è¿‡å†·å¯åŠ¨ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæµç¨‹å¯ä»¥å‡è½»å¹»è§‰ç°è±¡ï¼Œè€Œå•ç‹¬çš„è’¸é¦æˆ–æ²¡æœ‰å†·å¯åŠ¨å¾®è°ƒçš„å¼ºåŒ–å­¦ä¹ åˆ™ä¼šå¼•å…¥æ›´å¤šå¤æ‚çš„å¹»è§‰ã€‚è®ºæ–‡è¿˜åˆ†æäº†å½±å“LRMsäº‹å®æ€§çš„ä¸¤ç§å…³é”®è®¤çŸ¥è¡Œä¸ºï¼šç¼ºé™·é‡å¤å’Œæ€è€ƒ-å›ç­”ä¸åŒ¹é…ã€‚æœ€åï¼Œç ”ç©¶è¡¨æ˜ï¼ŒLRMsçš„å¹»è§‰å¢åŠ é€šå¸¸ä¸æ¨¡å‹ä¸ç¡®å®šæ€§å’Œäº‹å®å‡†ç¡®æ€§ä¹‹é—´çš„ä¸ä¸€è‡´æœ‰å…³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22914",
            "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.22914",
            "abstract": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.",
            "score": 16,
            "issue_id": 4040,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "23df6ba515b10bc5",
            "authors": [
                "Maksim Kolodiazhnyi",
                "Denis Tarasov",
                "Dmitrii Zhemchuzhnikov",
                "Alexander Nikulin",
                "Ilya Zisman",
                "Anna Vorontsova",
                "Anton Konushin",
                "Vladislav Kurenkov",
                "Danila Rukhovich"
            ],
            "affiliations": [
                "AIRI Institute",
                "ETH Zurich",
                "Innopolis University",
                "Lomonosov Moscow State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22914.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#3d",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ CAD: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞµÑ‘ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ."
                },
                "en": {
                    "title": "Revolutionizing CAD with Multi-Modal Learning and Reinforcement Techniques",
                    "desc": "This paper presents a multi-modal Computer-Aided Design (CAD) reconstruction model that integrates vision-language models and reinforcement learning to enhance performance across various datasets. Unlike traditional methods that rely on a single input type, this model processes point clouds, images, and text simultaneously, improving its versatility and robustness. The authors employ a two-stage training approach, starting with supervised fine-tuning on large datasets, followed by reinforcement learning to refine the model using real-time feedback. Their results show that this innovative approach achieves state-of-the-art performance, particularly in challenging real-world scenarios, surpassing existing single-modal techniques."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼šè¶…è¶Šå•ä¸€è¾“å…¥çš„åˆ›æ–°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰é‡å»ºæ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åŒæ—¶å¤„ç†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬ä¸‰ç§è¾“å…¥æ¨¡æ€ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–å•ä¸€è¾“å…¥çš„å±€é™æ€§ã€‚é€šè¿‡é‡‡ç”¨ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡ç”Ÿæˆæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶ååˆ©ç”¨åœ¨çº¿åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè®¾ç«‹äº†æ–°çš„æ€§èƒ½æ ‡æ†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20088",
            "title": "Multi-Domain Explainability of Preferences",
            "url": "https://huggingface.co/papers/2505.20088",
            "abstract": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.",
            "score": 15,
            "issue_id": 4039,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "8f1f413918ce2151",
            "authors": [
                "Nitay Calderon",
                "Liat Ein-Dor",
                "Roi Reichart"
            ],
            "affiliations": [
                "Faculty of Data and Decision Sciences, Technion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20088.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#interpretability",
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ñ‚Ñ€ÑƒĞ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Enhancing Preference Predictions with Concept-Based Explanations in LLMs",
                    "desc": "This paper presents a new automated approach that enhances how we explain and predict preferences in large language models (LLMs) using concept-based vectors and a Hierarchical Multi-Domain Regression model. The method identifies key concepts that differentiate between preferred and non-preferred responses, allowing for better understanding of the underlying preferences. By employing a white-box regression model, it captures both general and specific influences across various domains, leading to improved prediction accuracy. The results demonstrate that this approach not only outperforms existing methods but also provides clear explanations, paving the way for better alignment of LLMs with human preferences."
                },
                "zh": {
                    "title": "æ–°æ–¹æ³•æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½è§£é‡Šä¸é¢„æµ‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºæ¦‚å¿µçš„å‘é‡å’Œå±‚æ¬¡å¤šé¢†åŸŸå›å½’æ¨¡å‹ï¼Œæ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½è§£é‡Šå’Œé¢„æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«åŒºåˆ†é€‰æ‹©å’Œæ‹’ç»å“åº”çš„æ¦‚å¿µï¼Œå¹¶ç”¨åŸºäºæ¦‚å¿µçš„å‘é‡è¡¨ç¤ºè¿™äº›æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºçš„ç™½ç›’å±‚æ¬¡å¤šé¢†åŸŸå›å½’æ¨¡å‹èƒ½å¤Ÿæ•æ‰é¢†åŸŸé€šç”¨å’Œé¢†åŸŸç‰¹å®šçš„æ•ˆåº”ï¼Œä»è€Œå»ºæ¨¡æ¦‚å¿µä¸åå¥½ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡åœ¨å…«ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åå¥½é¢„æµ‹æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºçº¿ï¼ŒåŒæ—¶ä¹Ÿå…·å¤‡è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23380",
            "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.23380",
            "abstract": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.",
            "score": 14,
            "issue_id": 4037,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "48e60d93b0069d54",
            "authors": [
                "Weijia Mao",
                "Zhenheng Yang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23380.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "UniRL - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. UniRL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ supervised fine-tuning Ğ¸ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Self-Improving Multimodal Learning with UniRL",
                    "desc": "UniRL is a novel post-training method designed for unified multimodal large language models, allowing them to improve their performance without needing external data. It generates images from text prompts and uses these images as training data, facilitating a self-improving cycle that enhances both generation and understanding tasks. The approach leverages supervised fine-tuning and Group Relative Policy Optimization to optimize model performance, ensuring that the tasks support each other. UniRL demonstrates significant advantages, including independence from external datasets, improved task performance, and minimal additional training requirements."
                },
                "zh": {
                    "title": "UniRLï¼šè‡ªæˆ‘æ”¹è¿›çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ–¹æ³•",
                    "desc": "UniRLæ˜¯ä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„åè®­ç»ƒæ–¹æ³•ï¼Œä¸“ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡ç”Ÿæˆå›¾åƒä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¢å¼ºäº†ç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„æ€§èƒ½ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿å¾—ç”Ÿæˆçš„å›¾åƒå¯ä»¥ç”¨äºç†è§£ä»»åŠ¡ï¼ŒåŒæ—¶ç†è§£çš„ç»“æœåˆå¯ä»¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œå®ç°ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚UniRLçš„ä¼˜åŠ¿åœ¨äºä¸éœ€è¦å¤–éƒ¨å›¾åƒæ•°æ®ã€æå‡äº†ä»»åŠ¡æ€§èƒ½å¹¶å‡å°‘äº†ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23419",
            "title": "SWE-bench Goes Live!",
            "url": "https://huggingface.co/papers/2505.23419",
            "abstract": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.",
            "score": 13,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "56fc88c3eb857590",
            "authors": [
                "Linghao Zhang",
                "Shilin He",
                "Chaoyun Zhang",
                "Yu Kang",
                "Bowen Li",
                "Chengxing Xie",
                "Junhao Wang",
                "Maoquan Wang",
                "Yufan Huang",
                "Shengyu Fu",
                "Elsie Nallipogu",
                "Qingwei Lin",
                "Yingnong Dang",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23419.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "SWE-bench-Live: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SWE-bench-Live - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², SWE-bench-Live Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1319 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 93 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Docker-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° SWE-bench-Live Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "SWE-bench-Live: A Dynamic Benchmark for Evaluating LLMs in Bug Fixing",
                    "desc": "This paper introduces SWE-bench-Live, a new benchmark for evaluating large language models (LLMs) in the task of generating patches for real-world software bugs. Unlike previous benchmarks like SWE-bench, SWE-bench-Live is continuously updated and includes a broader range of tasks derived from recent GitHub issues, ensuring relevance and diversity. The benchmark features an automated curation pipeline that simplifies the process of creating tasks and setting up environments, reducing manual effort and enhancing scalability. The authors demonstrate that LLMs perform significantly better on SWE-bench-Live compared to static benchmarks, highlighting the importance of using dynamic and up-to-date datasets for evaluation."
                },
                "zh": {
                    "title": "å®æ—¶æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæå‡æ¨¡å‹è¯„ä¼°èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†SWE-bench-Liveï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å®æ—¶æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚ç°æœ‰çš„SWE-benchåŠå…¶å˜ä½“æœªèƒ½åŠæ—¶æ›´æ–°ï¼Œä¸”ä¾èµ–äºæ‰‹åŠ¨æ„å»ºå®ä¾‹å’Œç¯å¢ƒè®¾ç½®ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚SWE-bench-LiveåŒ…å«æ¥è‡ª2024å¹´åçœŸå®GitHubé—®é¢˜çš„1,319ä¸ªä»»åŠ¡ï¼Œå¹¶æä¾›ä¸“ç”¨çš„Dockeré•œåƒä»¥ç¡®ä¿å¯é‡å¤æ‰§è¡Œã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›çš„ä»£ç†æ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°SWE-bench-Liveåœ¨åŠ¨æ€è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­æä¾›äº†æ›´ä¸ºä¸¥è°¨å’ŒæŠ—æ±¡æŸ“çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22618",
            "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
            "url": "https://huggingface.co/papers/2505.22618",
            "abstract": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6times throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.",
            "score": 11,
            "issue_id": 4038,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "7b3d07e1309cacdc",
            "authors": [
                "Chengyue Wu",
                "Hao Zhang",
                "Shuchen Xue",
                "Zhijian Liu",
                "Shizhe Diao",
                "Ligeng Zhu",
                "Ping Luo",
                "Song Han",
                "Enze Xie"
            ],
            "affiliations": [
                "Independent Researcher",
                "MIT",
                "NVIDIA",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22618.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ KV-ĞºÑÑˆ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Boosting Speed and Quality in Diffusion LLMs!",
                    "desc": "This paper presents a new method to enhance the speed of diffusion-based large language models (Diffusion LLMs) during text generation. It introduces a block-wise approximate Key-Value (KV) Cache that allows for efficient reuse of cached information, which helps maintain performance while speeding up inference. Additionally, the authors propose a confidence-aware parallel decoding strategy that addresses quality issues caused by token dependency disruptions during simultaneous decoding. The results show significant improvements in processing speed with minimal loss in accuracy, making Diffusion LLMs more competitive with traditional autoregressive models."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å—çŠ¶è¿‘ä¼¼KVç¼“å­˜æœºåˆ¶å’ŒåŸºäºç½®ä¿¡åº¦çš„å¹¶è¡Œè§£ç ç­–ç•¥ï¼Œä»¥æé«˜æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½è´¨é‡ã€‚æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹åœ¨éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºç¼ºä¹KVç¼“å­˜å’Œå¹¶è¡Œè§£ç æ—¶çš„è´¨é‡ä¸‹é™ï¼Œå®é™…æ¨ç†é€Ÿåº¦å¸¸å¸¸è½åäºè‡ªå›å½’æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å—çŠ¶è¿‘ä¼¼KVç¼“å­˜æœºåˆ¶ï¼Œå…è®¸ç¼“å­˜é‡ç”¨ï¼Œå‡ ä¹ä¸å½±å“æ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºçš„åŸºäºç½®ä¿¡åº¦çš„å¹¶è¡Œè§£ç ç­–ç•¥å¯ä»¥é€‰æ‹©æ€§åœ°è§£ç è¶…è¿‡ç½®ä¿¡åº¦é˜ˆå€¼çš„æ ‡è®°ï¼Œä»è€Œå‡è½»ä¾èµ–æ€§è¿åï¼Œä¿æŒç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23606",
            "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
            "url": "https://huggingface.co/papers/2505.23606",
            "abstract": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.",
            "score": 10,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "2bbc295244e31cb2",
            "authors": [
                "Qingyu Shi",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Wenhao Chai",
                "Kaidong Yu",
                "Jianzong Wu",
                "Shuangyong Song",
                "Yunhai Tong",
                "Xiangtai Li",
                "Xuelong Li",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Princeton University",
                "TeleAI, China Telecom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23606.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Muddit - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Muddit Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Muddit: Fast and High-Quality Unified Generation for Text and Images",
                    "desc": "Muddit is a new model that combines text and image generation in a single framework, making it faster and more efficient. It uses a discrete diffusion approach, which allows for parallel processing, unlike traditional models that generate outputs one step at a time. By incorporating pretrained visual knowledge from existing models, Muddit enhances the quality of its outputs while maintaining a lightweight structure for text decoding. The results show that Muddit performs as well or better than larger models, demonstrating the effectiveness of using strong visual priors in unified generation tasks."
                },
                "zh": {
                    "title": "Mudditï¼šå¿«é€Ÿé«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆ",
                    "desc": "Mudditæ˜¯ä¸€ç§ç»Ÿä¸€çš„ç¦»æ•£æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¸­å®ç°å¿«é€Ÿä¸”é«˜è´¨é‡çš„ç”Ÿæˆã€‚å®ƒé€šè¿‡å°†é¢„è®­ç»ƒçš„è§†è§‰å…ˆéªŒä¸è½»é‡çº§æ–‡æœ¬è§£ç å™¨ç›¸ç»“åˆï¼Œå…‹æœäº†è‡ªå›å½’æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢å’Œéè‡ªå›å½’æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚Mudditåœ¨ç»Ÿä¸€æ¶æ„ä¸‹å®ç°äº†çµæ´»çš„å¤šæ¨¡æ€ç”Ÿæˆï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šä¼˜äºè®¸å¤šæ›´å¤§çš„è‡ªå›å½’æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å½“é…å¤‡å¼ºå¤§çš„è§†è§‰å…ˆéªŒæ—¶ï¼Œçº¯ç¦»æ•£æ‰©æ•£æ¨¡å‹ä½œä¸ºç»Ÿä¸€ç”Ÿæˆçš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„åŸºç¡€æ¶æ„çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23585",
            "title": "On-Policy RL with Optimal Reward Baseline",
            "url": "https://huggingface.co/papers/2505.23585",
            "abstract": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.",
            "score": 9,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "8a27563b11999352",
            "authors": [
                "Yaru Hao",
                "Li Dong",
                "Xun Wu",
                "Shaohan Huang",
                "Zewen Chi",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OPO: Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ OPO (On-Policy RL with Optimal reward baseline). OPO Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ OPO - Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ on-policy Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ»Ğ¸Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ OPO Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning for Language Models with OPO",
                    "desc": "This paper introduces a new reinforcement learning algorithm called On-Policy RL with Optimal reward baseline (OPO) to improve the training of large language models. OPO focuses on exact on-policy training, which helps stabilize the learning process and encourages better exploration of solutions. The algorithm also incorporates an optimal reward baseline that reduces gradient variance, leading to more reliable updates during training. Evaluations show that OPO outperforms existing methods in terms of stability and diversity of responses, making it a strong candidate for aligning language models with human preferences."
                },
                "zh": {
                    "title": "OPOï¼šç¨³å®šé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹å‘",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç§°ä¸ºæœ€ä¼˜å¥–åŠ±åŸºçº¿çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç®—æ³•åœ¨è®­ç»ƒä¸ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„é—®é¢˜ã€‚OPOå¼ºè°ƒç²¾ç¡®çš„åœ¨çº¿è®­ç»ƒï¼Œè¿™åœ¨å®è·µä¸­ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹å¹¶å¢å¼ºäº†æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOPOå¼•å…¥äº†æœ€ä¼˜å¥–åŠ±åŸºçº¿ï¼Œç†è®ºä¸Šå¯ä»¥æœ€å°åŒ–æ¢¯åº¦æ–¹å·®ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¯„ä¼°ï¼ŒOPOå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–çš„æ¨¡å‹æˆ–æ­£åˆ™åŒ–é¡¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22421",
            "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
            "url": "https://huggingface.co/papers/2505.22421",
            "abstract": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.",
            "score": 9,
            "issue_id": 4036,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "2a5e22b54a576dad",
            "authors": [
                "Anthony Chen",
                "Wenzhao Zheng",
                "Yida Wang",
                "Xueyang Zhang",
                "Kun Zhan",
                "Peng Jia",
                "Kurt Keutzer",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Li Auto Inc.",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22421.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#agents",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "GeoDrive: 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "GeoDrive - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ 2D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ñ. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GeoDrive Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Autonomous Navigation with Robust 3D Geometry",
                    "desc": "GeoDrive is a novel approach that enhances autonomous driving by integrating robust 3D geometry into world models, improving spatial awareness and action control. It addresses limitations in current models, such as maintaining 3D geometric consistency and handling occlusions, which are crucial for safety assessments. By extracting 3D representations and dynamically editing vehicle positions during training, GeoDrive enables more accurate action predictions and realistic scene modeling. The method shows significant improvements in action accuracy and adaptability, making it a promising solution for safer autonomous navigation."
                },
                "zh": {
                    "title": "GeoDriveï¼šæå‡è‡ªä¸»é©¾é©¶çš„ç©ºé—´ç†è§£ä¸å®‰å…¨æ€§",
                    "desc": "GeoDrive æ˜¯ä¸€ç§å°†ç¨³å¥çš„ä¸‰ç»´å‡ ä½•ä½“é›†æˆåˆ°é©¾é©¶ä¸–ç•Œæ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‡ªä¸»å¯¼èˆªä¸­çš„ç©ºé—´ç†è§£å’ŒåŠ¨ä½œå¯æ§æ€§ï¼Œä»è€Œå¢å¼ºå®‰å…¨æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»è¾“å…¥å¸§ä¸­æå–ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„è‡ªè½¦è½¨è¿¹ç”ŸæˆäºŒç»´æ¸²æŸ“ï¼Œæ¥å®ç°åŠ¨æ€å»ºæ¨¡ã€‚GeoDrive è¿˜å¼•å…¥äº†åŠ¨æ€ç¼–è¾‘æ¨¡å—ï¼Œä»¥å¢å¼ºæ¸²æŸ“æ•ˆæœï¼Œå…è®¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼–è¾‘è½¦è¾†ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoDrive åœ¨åŠ¨ä½œå‡†ç¡®æ€§å’Œä¸‰ç»´ç©ºé—´æ„è¯†æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æ›´çœŸå®ã€é€‚åº”æ€§å¼ºä¸”å¯é çš„åœºæ™¯å»ºæ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23758",
            "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
            "url": "https://huggingface.co/papers/2505.23758",
            "abstract": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.",
            "score": 8,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "383f00e023ee6237",
            "authors": [
                "Yusuf Dalva",
                "Hidir Yesiltepe",
                "Pinar Yanardag"
            ],
            "affiliations": [
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23758.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#story_generation",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "LoRAShop - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LoRA. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° Flux Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹. LoRAShop ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ° Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²ĞµÑĞ° LoRA Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹."
                },
                "en": {
                    "title": "Seamless Multi-Concept Image Editing with LoRAShop",
                    "desc": "LoRAShop is a novel framework designed for multi-concept image editing using LoRA models. It utilizes the unique feature activation patterns in Flux-style diffusion transformers to create a disentangled latent mask for each concept, allowing for precise blending of different styles or subjects. This method ensures that the edits maintain the original scene's global context, lighting, and intricate details. The framework enhances identity preservation in images and simplifies the editing process by removing the need for retraining, making it a powerful tool for creative visual storytelling."
                },
                "zh": {
                    "title": "LoRAShopï¼šä¸ªæ€§åŒ–å›¾åƒç¼–è¾‘çš„æ–°å·¥å…·",
                    "desc": "LoRAShopæ˜¯ä¸€ä¸ªç”¨äºå¤šæ¦‚å¿µå›¾åƒç¼–è¾‘çš„æ¡†æ¶ï¼Œåˆ©ç”¨LoRAæ¨¡å‹ã€‚å®ƒåŸºäºFluxé£æ ¼æ‰©æ•£å˜æ¢å™¨ä¸­çš„ç‰¹å¾äº¤äº’æ¨¡å¼ï¼Œèƒ½å¤Ÿåœ¨å»å™ªè¿‡ç¨‹ä¸­æ—©æœŸæ¿€æ´»ç©ºé—´ä¸€è‡´çš„ç‰¹å¾åŒºåŸŸã€‚é€šè¿‡ä¸ºæ¯ä¸ªæ¦‚å¿µç”Ÿæˆè§£è€¦çš„æ½œåœ¨æ©ç ï¼ŒLoRAShopå¯ä»¥åœ¨ç‰¹å®šåŒºåŸŸå†…æ··åˆç›¸åº”çš„LoRAæƒé‡ï¼Œä»è€Œå®ç°å¤šä¸ªä¸»é¢˜æˆ–é£æ ¼çš„æ— ç¼æ•´åˆã€‚å®éªŒè¡¨æ˜ï¼ŒLoRAShopåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œæˆä¸ºä¸€ç§å®ç”¨çš„ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23735",
            "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
            "url": "https://huggingface.co/papers/2505.23735",
            "abstract": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark.",
            "score": 8,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "e9763dde29798ac1",
            "authors": [
                "Ali Behrouz",
                "Zeman Li",
                "Praneeth Kacham",
                "Majid Daliri",
                "Yuan Deng",
                "Peilin Zhong",
                "Meisam Razaviyayn",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23735.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ATLAS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ATLAS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ATLAS Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ±Ğ°Ğ·Ğµ ATLAS Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ DeepTransformers, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Transformer. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ATLAS Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "ATLAS: Revolutionizing Long-Term Memory in Transformers",
                    "desc": "This paper introduces ATLAS, a long-term memory module designed to enhance the performance of sequence modeling tasks. It addresses the limitations of traditional architectures, particularly in handling long contexts, by optimizing memory based on both current and past inputs. The authors propose DeepTransformers, a new family of architectures that generalize the original Transformer model while improving memory capacity and management. Experimental results demonstrate that ATLAS outperforms existing models, achieving significant accuracy improvements in tasks requiring long-context understanding."
                },
                "zh": {
                    "title": "ATLASï¼šè¶…è¶Šä¼ ç»Ÿçš„é•¿æ—¶è®°å¿†æ¨¡å—",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é•¿æ—¶è®°å¿†æ¨¡å—ATLASï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å±€é™æ€§ã€‚ATLASé€šè¿‡ä¼˜åŒ–è®°å¿†ç»“æ„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å­˜å‚¨å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿé•¿æ—¶è®°å¿†æ¨¡å‹çš„åœ¨çº¿æ›´æ–°é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒATLASåœ¨è¯­è¨€å»ºæ¨¡ã€å¸¸è¯†æ¨ç†å’Œé•¿ä¸Šä¸‹æ–‡ç†è§£ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„Transformerå’Œçº¿æ€§é€’å½’æ¨¡å‹ã€‚æœ€ç»ˆï¼ŒATLASåœ¨BABILongåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†10Mä¸Šä¸‹æ–‡é•¿åº¦çš„+80\\%å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23416",
            "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
            "url": "https://huggingface.co/papers/2505.23416",
            "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.",
            "score": 8,
            "issue_id": 4038,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "6ba9403ec041f5ac",
            "authors": [
                "Jang-Hyun Kim",
                "Jinuk Kim",
                "Sangwoo Kwon",
                "Jae W. Lee",
                "Sangdoo Yun",
                "Hyun Oh Song"
            ],
            "affiliations": [
                "NAVER AI Lab",
                "Neural Processing Research Center",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23416.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "KVzip: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ KVzip - Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ (KV) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². KVzip Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€ KV, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KVzip ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ KV-ĞºÑÑˆĞ° Ğ² 3-4 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ FlashAttention Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient KV Cache Management with KVzip",
                    "desc": "This paper presents KVzip, a novel method for managing key-value (KV) caches in transformer-based large language models (LLMs) during inference. As the context length increases, traditional KV caches become large and slow, but KVzip allows for efficient reuse of compressed caches across different queries. It evaluates the importance of KV pairs using the LLM's ability to reconstruct contexts, allowing less important pairs to be evicted. The results show that KVzip can reduce cache size significantly while improving decoding speed and maintaining performance across various tasks and models."
                },
                "zh": {
                    "title": "KVzipï¼šé«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKVzipçš„ç¼“å­˜é©±é€æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ã€‚éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼ŒKVç¼“å­˜çš„å¤§å°ä¹Ÿéšä¹‹æ‰©å¤§ï¼Œå¯¼è‡´å†…å­˜å¼€é”€å’Œæ³¨æ„åŠ›å»¶è¿Ÿæ˜¾è‘—å¢åŠ ã€‚KVzipé€šè¿‡é‡åŒ–KVå¯¹çš„é‡è¦æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡ç”¨å‹ç¼©çš„KVç¼“å­˜ï¼Œä»è€Œåœ¨å¤šç§æŸ¥è¯¢ä¸­å®ç°æ›´é«˜æ•ˆçš„ç¼“å­˜ç®¡ç†ã€‚å®éªŒè¯æ˜ï¼ŒKVzipå¯ä»¥å°†KVç¼“å­˜å¤§å°å‡å°‘3-4å€ï¼Œå¹¶å°†FlashAttentionè§£ç å»¶è¿Ÿé™ä½çº¦2å€ï¼ŒåŒæ—¶åœ¨é—®ç­”ã€æ£€ç´¢ã€æ¨ç†å’Œä»£ç ç†è§£ä»»åŠ¡ä¸­å‡ ä¹æ²¡æœ‰æ€§èƒ½æŸå¤±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23559",
            "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
            "url": "https://huggingface.co/papers/2505.23559",
            "abstract": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce SafeScientist, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose SciSafetyBench, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. red{Warning: this paper contains example data that may be offensive or harmful.}",
            "score": 7,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "47b1655c578567ee",
            "authors": [
                "Kunlun Zhu",
                "Jiaxun Zhang",
                "Ziheng Qi",
                "Nuoxing Shang",
                "Zijia Liu",
                "Peixuan Han",
                "Yue Su",
                "Haofei Yu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23559.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#healthcare",
                    "#ethics",
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#security"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "SafeScientist - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². ĞĞ½ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ°ĞºÑ†ĞµĞ½Ñ‚ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. SafeScientist Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° SciSafetyBench."
                },
                "en": {
                    "title": "Enhancing Safety in AI-Driven Science with SafeScientist",
                    "desc": "SafeScientist is an AI framework designed to improve safety in AI-driven scientific research by implementing various defensive mechanisms. It proactively avoids engaging in ethically questionable or high-risk tasks, ensuring a focus on safety throughout the research process. The framework includes features like prompt monitoring and agent collaboration oversight, which help maintain ethical standards. Additionally, SafeScientist is validated using the SciSafetyBench benchmark, demonstrating a 35% improvement in safety performance compared to traditional AI frameworks while maintaining high-quality scientific output."
                },
                "zh": {
                    "title": "å®‰å…¨ç§‘å­¦å®¶ï¼šæå‡AIç§‘å­¦ç ”ç©¶çš„å®‰å…¨æ€§",
                    "desc": "SafeScientistæ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šç§é˜²å¾¡æœºåˆ¶æé«˜AIé©±åŠ¨ç§‘å­¦ç ”ç©¶çš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä¸»åŠ¨æ‹’ç»ä¸é“å¾·æˆ–é«˜é£é™©çš„ä»»åŠ¡ï¼Œå¹¶åœ¨æ•´ä¸ªç ”ç©¶è¿‡ç¨‹ä¸­å¼ºè°ƒå®‰å…¨æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†SciSafetyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç§‘å­¦é¢†åŸŸä¸­AIçš„å®‰å…¨æ€§ï¼Œæ¶µç›–240ä¸ªé«˜é£é™©ç§‘å­¦ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeScientiståœ¨å®‰å…¨æ€§èƒ½ä¸Šæ¯”ä¼ ç»ŸAIç§‘å­¦å®¶æ¡†æ¶æé«˜äº†35%ï¼ŒåŒæ—¶ä¸å½±å“ç§‘å­¦è¾“å‡ºçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22961",
            "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
            "url": "https://huggingface.co/papers/2505.22961",
            "abstract": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.",
            "score": 7,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "13c778698d292f73",
            "authors": [
                "Peixuan Han",
                "Zijia Liu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22961.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ToMAP: Ğ˜Ğ˜-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ToMAP - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ°. ToMAP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ Ğ¼Ñ‹ÑĞ»ÑÑ… Ğ¸ Ğ¼Ğ½ĞµĞ½Ğ¸ÑÑ… Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ToMAP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµÑĞµĞ´ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Persuasion with Theory of Mind",
                    "desc": "This paper presents Theory of Mind Augmented Persuader (ToMAP), a new method that enhances large language models (LLMs) for persuasive tasks by integrating Theory of Mind (ToM) modules. These modules allow the persuader to better understand and anticipate the thoughts and objections of opponents, leading to improved argument quality and diversity. By employing a reinforcement learning approach, ToMAP trains the persuader to analyze opponent-related information effectively, resulting in more logical and nuanced arguments. Experimental results demonstrate that ToMAP significantly outperforms larger models, showcasing its potential for creating advanced persuasive agents."
                },
                "zh": {
                    "title": "æå‡è¯´æœåŠ›çš„ç†è®ºå¿ƒæ™ºå¢å¼ºæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºToMAPçš„ç†è®ºå¿ƒæ™ºå¢å¼ºè¯´æœè€…æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯´æœè¿‡ç¨‹ä¸­çš„å¯¹æ‰‹æ„è¯†å’Œè®ºè¯è´¨é‡ã€‚é€šè¿‡å¼•å…¥ä¸¤ä¸ªç†è®ºå¿ƒæ™ºæ¨¡å—ï¼ŒToMAPèƒ½å¤Ÿæ›´å¥½åœ°åˆ†æå¯¹æ‰‹çš„å¿ƒç†çŠ¶æ€ï¼Œä»è€Œç”Ÿæˆæ›´æœ‰æ•ˆçš„è®ºç‚¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ToMAPçš„å‚æ•°é‡ä»…ä¸º30äº¿ï¼Œä½†åœ¨å¤šä¸ªè¯´æœæ¨¡å‹å’Œä¸åŒè¯­æ–™åº“ä¸Šï¼Œå…¶è¡¨ç°è¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œæå‡å¹…åº¦è¾¾åˆ°39.4%ã€‚æ­¤å¤–ï¼ŒToMAPåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å±•ç°å‡ºå¤æ‚çš„æ¨ç†é“¾å’Œå‡å°‘é‡å¤çš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨é•¿æ—¶é—´å¯¹è¯ä¸­æ›´å…·é€»è¾‘æ€§å’Œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20755",
            "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
            "url": "https://huggingface.co/papers/2505.20755",
            "abstract": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a theory-driven framework which we name the \\emph{Uni-Instruct}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the f-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded f-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded f-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \\emph{1.46} for unconditional generation and \\emph{1.38} for conditional generation. On the ImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \\emph{1.02}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.",
            "score": 6,
            "issue_id": 4037,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "099d5bf1fb188745",
            "authors": [
                "Yifei Wang",
                "Weimin Bai",
                "Colin Zhang",
                "Debing Zhang",
                "Weijian Luo",
                "He Sun"
            ],
            "affiliations": [
                "Academy for Advanced Interdisciplinary Studies, Peking University",
                "College of Future Technology, Peking University",
                "National Biomedical Imaging Center, Peking University",
                "Xiaohongshu Inc",
                "Yuanpei College, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20755.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#transfer_learning",
                    "#cv",
                    "#diffusion",
                    "#dataset",
                    "#math",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Uni-Instruct: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Uni-Instruct - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸. Uni-Instruct Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ FID Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CIFAR10 Ğ¸ ImageNet-64x64. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Uni-Instruct: Unifying Diffusion for Superior Image and 3D Generation",
                    "desc": "The paper presents Uni-Instruct, a framework that integrates over ten existing one-step diffusion distillation methods into a cohesive theory-driven approach. It introduces a novel diffusion expansion theory based on the f-divergence family, which addresses the challenges of training one-step diffusion models. By minimizing an equivalent and tractable loss derived from this theory, Uni-Instruct achieves state-of-the-art performance in both unconditional and conditional image generation tasks. Additionally, it demonstrates effectiveness in text-to-3D generation, surpassing previous methods in quality and diversity."
                },
                "zh": {
                    "title": "Uni-Instructï¼šå•æ­¥æ‰©æ•£è’¸é¦çš„æ–°ç»Ÿä¸€ç†è®º",
                    "desc": "æœ¬æ–‡æå‡ºäº†Uni-Instructï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€å’Œå¢å¼ºå•æ­¥æ‰©æ•£è’¸é¦æ–¹æ³•çš„æ–°ç†è®ºæ¡†æ¶ã€‚é€šè¿‡å¼•å…¥æ‰©æ•£æ‰©å±•ç†è®ºï¼ŒUni-InstructæˆåŠŸåœ°æ•´åˆäº†åå¤šç§ç°æœ‰çš„å•æ­¥æ‰©æ•£è’¸é¦æ–¹æ³•ï¼Œå¹¶è§£å†³äº†åŸå§‹æ‰©å±•fæ•£åº¦çš„ä¸å¯å¤„ç†æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ— æ¡ä»¶å’Œæ¡ä»¶å›¾åƒç”Ÿæˆä»¥åŠæ–‡æœ¬åˆ°3Dç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨CIFAR10å’ŒImageNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚Uni-Instructçš„ç†è®ºå’Œå®è¯è´¡çŒ®å°†ä¸ºæœªæ¥çš„å•æ­¥æ‰©æ•£è’¸é¦å’Œæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç ”ç©¶æä¾›é‡è¦æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23742",
            "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
            "url": "https://huggingface.co/papers/2505.23742",
            "abstract": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF",
            "score": 5,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "7234edb0d22029a4",
            "authors": [
                "Yufan Deng",
                "Xun Guo",
                "Yuanyang Yin",
                "Jacob Zhiyuan Fang",
                "Yiding Yang",
                "Yizhi Wang",
                "Shenghai Yuan",
                "Angtian Wang",
                "Bo Liu",
                "Haibin Huang",
                "Chongyang Ma"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.23742.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MAGREF: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MAGREF - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¾Ğ½Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. MAGREF Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MAGREF: Mastering Multi-Subject Video Generation with Flexibility and Precision",
                    "desc": "This paper presents MAGREF, a new framework for generating videos that can include multiple subjects from various reference images and text prompts. It introduces a masked guidance technique that allows the model to adaptively focus on different subjects like people and objects without needing to change its structure. Additionally, it employs a pixel-wise channel concatenation method to maintain the visual features of the subjects during generation. The results show that MAGREF achieves superior video quality and consistency compared to existing methods, making it a significant advancement in multi-subject video synthesis."
                },
                "zh": {
                    "title": "MAGREFï¼šé«˜è´¨é‡å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMAGREFçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåŸºäºä»»æ„å‚è€ƒå›¾åƒçš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ©è”½å¼•å¯¼æœºåˆ¶ï¼Œä»¥å®ç°å¤šä¸»ä½“è§†é¢‘åˆæˆï¼Œç¡®ä¿ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œé«˜è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„åŠ¨æ€æ©è”½æœºåˆ¶èƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒçš„ä¸»ä½“æ¨æ–­ï¼Œè€Œåƒç´ çº§é€šé“è¿æ¥æœºåˆ¶åˆ™æ›´å¥½åœ°ä¿ç•™å¤–è§‚ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGREFåœ¨å¤šä¸»ä½“è§†é¢‘åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23754",
            "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.23754",
            "abstract": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.  \t\t\t\t\tAI-generated summary \t\t\t\t Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.",
            "score": 4,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "8ef15301bb840f49",
            "authors": [
                "Ziyin Zhang",
                "Jiahao Xu",
                "Zhiwei He",
                "Tian Liang",
                "Qiuzhi Liu",
                "Yansi Li",
                "Linfeng Song",
                "Zhengwen Liang",
                "Zhuosheng Zhang",
                "Rui Wang",
                "Zhaopeng Tu",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23754.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "DeepTheorem - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 121 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ IMO. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL-Zero) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM. DeepTheorem Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with DeepTheorem",
                    "desc": "DeepTheorem is a new framework that improves how large language models (LLMs) prove theorems using natural language. It addresses the gap between traditional automated theorem proving methods and the informal reasoning strengths of LLMs. The framework includes a large dataset of 121,000 informal theorems and proofs, which are carefully annotated and designed to enhance mathematical reasoning. By employing a specialized reinforcement learning strategy, DeepTheorem significantly boosts the performance of LLMs in theorem proving tasks, achieving top results in accuracy and reasoning quality."
                },
                "zh": {
                    "title": "DeepTheoremï¼šæå‡å®šç†è¯æ˜çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DeepTheorem æ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®šç†è¯æ˜èƒ½åŠ›çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€æ•°æ®é›†å’Œå®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚å®ƒåŒ…å«121,000ä¸ªé«˜è´¨é‡çš„éæ­£å¼å®šç†å’Œè¯æ˜ï¼Œè¦†ç›–å¤šä¸ªæ•°å­¦é¢†åŸŸï¼Œå¹¶ç»è¿‡ä¸¥æ ¼æ ‡æ³¨ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆRL-Zeroï¼‰ï¼ŒDeepTheorem èƒ½å¤Ÿæœ‰æ•ˆæå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepTheorem åœ¨å®šç†è¯æ˜çš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23387",
            "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
            "url": "https://huggingface.co/papers/2505.23387",
            "abstract": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.",
            "score": 4,
            "issue_id": 4043,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "1538cb2eddf848da",
            "authors": [
                "Mingzhe Du",
                "Luu Tuan Tuan",
                "Yue Liu",
                "Yuhao Qing",
                "Dong Huang",
                "Xinyi He",
                "Qian Liu",
                "Zejun Ma",
                "See-kiong Ng"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Singapore",
                "The University of Hong Kong",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23387.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ĞºĞ»ÑÑ‡ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM), Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³Ğ´Ğµ LLM Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Supervised Fine-Tuning, Direct Preference Optimization Ğ¸ Group Relative Policy Optimization. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ GRPO, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Reinforcement Learning: The Key to Self-Improving Code Efficiency",
                    "desc": "This paper presents a new method for improving the efficiency of code generated by large language models (LLMs) using reinforcement learning (RL). The proposed framework allows LLMs to iteratively refine their code based on real-time performance feedback from a testing environment. The authors compare different training strategies, finding that while some methods quickly reach a limit in efficiency gains, the RL approach continues to enhance performance over time. The results show significant improvements in both the success rate of code generation and the ability to surpass human-generated code in efficiency."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡ä»£ç æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æµ‹è¯•æ—¶è¿­ä»£ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸æ–­æå‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç çš„æ•ˆç‡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨ä»£ç æ•ˆç‡ä¸Šå¸¸å¸¸å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹å®é™…åº”ç”¨æ„æˆäº†ç“¶é¢ˆã€‚æˆ‘ä»¬é‡‡ç”¨é—­ç¯ç³»ç»Ÿï¼Œè®©LLMsæ ¹æ®æ‰§è¡Œæ²™ç®±çš„åé¦ˆè¿­ä»£ä¼˜åŒ–ä»£ç ï¼Œå¹¶æ¢ç´¢äº†ä¸‰ç§è®­ç»ƒç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPOé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸æ‰§è¡Œåé¦ˆçš„ç»“åˆï¼Œèƒ½å¤ŸæŒç»­ä¼˜åŒ–ä»£ç æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†é€šè¿‡ç‡å’Œè¶…è¶Šäººç±»æäº¤çš„æ•ˆç‡å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21114",
            "title": "Differentiable Solver Search for Fast Diffusion Sampling",
            "url": "https://huggingface.co/papers/2505.21114",
            "abstract": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.",
            "score": 4,
            "issue_id": 4040,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "7c5213dc05c3e3e4",
            "authors": [
                "Shuai Wang",
                "Zexian Li",
                "Qipeng zhang",
                "Tianhui Song",
                "Xubin Li",
                "Tiezheng Ge",
                "Bo Zheng",
                "Limin Wang"
            ],
            "affiliations": [
                "Shanghai AI Lab, Shanghai, China",
                "State Key Lab of Novel Software Technology, Nanjing University, Nanjing, China",
                "Taobao & Tmall Group of Alibaba, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21114.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#data",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ°, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼, Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¸Ğ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸. ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Optimizing Diffusion Models with a Smart Solver Search",
                    "desc": "This paper introduces a new algorithm for optimizing solver efficiency in diffusion models used for generating images. The authors identify that traditional t-related Lagrange interpolation methods are not the best choice for these models, leading to a compact search space for time steps and solver coefficients. They propose a differentiable solver search algorithm that finds more effective solvers, resulting in improved image generation quality with fewer computational steps. The new solvers significantly enhance performance across different model architectures and sizes, achieving lower FID scores compared to conventional methods."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„æ±‚è§£å™¨æœç´¢ç®—æ³•",
                    "desc": "ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯å¾®åˆ†æ±‚è§£å™¨æœç´¢ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œè´¨é‡ã€‚æ‰©æ•£æ¨¡å‹è™½ç„¶ç”Ÿæˆè´¨é‡å‡ºè‰²ï¼Œä½†éœ€è¦å¤§é‡çš„å‡½æ•°è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºæ—¶é—´ç›¸å…³çš„æ‹‰æ ¼æœ—æ—¥æ’å€¼åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªç”±æ—¶é—´æ­¥å’Œæ±‚è§£å™¨ç³»æ•°ç»„æˆçš„ç´§å‡‘æœç´¢ç©ºé—´ã€‚åŸºäºæˆ‘ä»¬çš„åˆ†æï¼Œæå‡ºçš„å¯å¾®åˆ†æ±‚è§£å™¨æœç´¢ç®—æ³•èƒ½å¤Ÿè¯†åˆ«æ›´ä¼˜çš„æ±‚è§£å™¨ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17818",
            "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
            "url": "https://huggingface.co/papers/2505.17818",
            "abstract": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.",
            "score": 4,
            "issue_id": 4036,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "affe6411f2c6037a",
            "authors": [
                "Daeun Kyung",
                "Hyunseung Chung",
                "Seongsu Bae",
                "Jiho Kim",
                "Jae Ho Sohn",
                "Taerim Kim",
                "Soo Kyung Kim",
                "Edward Choi"
            ],
            "affiliations": [
                "Ewha Womans University",
                "KAIST",
                "Samsung Medical Center",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17818.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#healthcare",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ",
                    "desc": "PatientSim - ÑÑ‚Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ· MIMIC-ED Ğ¸ MIMIC-IV, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞµĞ¹: Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ»Ğ¸Ğ½Ğ¸Ñ†Ğ¸ÑÑ‚Ğ°Ğ¼Ğ¸. PatientSim Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "PatientSim: Realistic Patient Personas for Better Medical Dialogue",
                    "desc": "PatientSim is a patient simulator designed to create realistic and diverse patient personas using clinical data, which is essential for training and evaluating large language models (LLMs) in medical dialogue. It utilizes real-world clinical profiles from the MIMIC-ED and MIMIC-IV datasets and defines personas based on personality traits, language skills, medical history recall, and cognitive confusion, resulting in 37 unique combinations. The framework was tested with eight LLMs for their factual accuracy and consistency with the generated personas, with Llama 3.3 emerging as the top performer validated by clinicians. As an open-source platform, PatientSim offers a customizable and privacy-compliant environment for evaluating medical dialogue systems, making it a valuable tool for both training and educational purposes in healthcare."
                },
                "zh": {
                    "title": "PatientSimï¼šå¤šæ ·åŒ–æ‚£è€…è§’è‰²çš„ç”Ÿæˆä¸è¯„ä¼°",
                    "desc": "PatientSim æ˜¯ä¸€ä¸ªç”Ÿæˆå¤šæ ·åŒ–å’ŒçœŸå®æ‚£è€…è§’è‰²çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨è¯„ä¼°åŒ»ç–—å¯¹è¯ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚å®ƒåˆ©ç”¨çœŸå®çš„ä¸´åºŠæ•°æ®ï¼Œåˆ›å»ºåŸºäºç—‡çŠ¶å’Œç—…å²çš„æ‚£è€…æ¡£æ¡ˆï¼Œå¹¶é€šè¿‡ä¸ªæ€§ã€è¯­è¨€èƒ½åŠ›ã€ç—…å²å›å¿†æ°´å¹³å’Œè®¤çŸ¥æ··æ·†æ°´å¹³å››ä¸ªç»´åº¦å®šä¹‰æ‚£è€…è§’è‰²ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPatientSim ç”Ÿæˆäº† 37 ç§ç‹¬ç‰¹çš„æ‚£è€…ç»„åˆï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ ä¸´åºŠå®è·µä¸­çš„å¤šæ ·æ€§ã€‚è¯¥å¹³å°ä¸ä»…ä¸ºåŒ»ç–—å¯¹è¯ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯é‡å¤å’Œå¯æ‰©å±•çš„æµ‹è¯•ç¯å¢ƒï¼Œè¿˜ä¸ºåŒ»ç–—æ•™è‚²æä¾›äº†æ½œåœ¨çš„å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23745",
            "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
            "url": "https://huggingface.co/papers/2505.23745",
            "abstract": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.",
            "score": 3,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "55ac97c649fb77b4",
            "authors": [
                "Hao Dong",
                "Moru Liu",
                "Jian Liang",
                "Eleni Chatzi",
                "Olga Fink"
            ],
            "affiliations": [
                "EPFL",
                "ETH ZÃ¼rich",
                "NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Technical University of Munich",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23745.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#architecture",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "TrustVLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞµĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TrustVLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 17 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Trust Your Vision-Language Model with TrustVLM!",
                    "desc": "TrustVLM is a framework that enhances the reliability of Vision-Language Models (VLMs) by estimating the trustworthiness of their predictions without the need for retraining. It addresses the issue of misclassification in VLMs, which can produce confident but incorrect outputs, especially in critical applications. By introducing a novel confidence-scoring function that utilizes the image embedding space, TrustVLM improves the detection of misclassifications. The framework has been rigorously tested across multiple datasets and architectures, showing significant performance improvements in reliability metrics."
                },
                "zh": {
                    "title": "æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦",
                    "desc": "TrustVLM æ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯é æ€§çš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¯„ä¼°é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ç½®ä¿¡è¯„åˆ†å‡½æ•°ï¼Œåˆ©ç”¨å›¾åƒåµŒå…¥ç©ºé—´æ¥æ”¹å–„é”™è¯¯åˆ†ç±»çš„æ£€æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTrustVLM åœ¨ 17 ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šç§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„å¯é æ€§ï¼ŒTrustVLM ä¸º VLM åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23253",
            "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
            "url": "https://huggingface.co/papers/2505.23253",
            "abstract": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.",
            "score": 3,
            "issue_id": 4038,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "1882fe9c03da1e58",
            "authors": [
                "Yixun Liang",
                "Kunming Luo",
                "Xiao Chen",
                "Rui Chen",
                "Hongyu Yan",
                "Weiyu Li",
                "Jiarui Liu",
                "Ping Tan"
            ],
            "affiliations": [
                "HKUST",
                "Light Illusion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23253.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: UniTEX Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸",
                    "desc": "UniTEX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¢ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (TF) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ UV-Ğ¼Ğ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ‘Ğ¾Ğ»ÑŒÑˆÑƒÑ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¢ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (LTM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ TF Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. UniTEX Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ."
                },
                "en": {
                    "title": "Revolutionizing 3D Texture Generation with UniTEX",
                    "desc": "UniTEX is a new framework designed for generating high-quality 3D textures without the need for UV mapping. It uses Texture Functions to create a continuous representation of textures based on 3D spatial data, allowing for better consistency and quality. The framework employs a transformer-based model to predict these textures directly from images and geometry, enhancing the process of texture synthesis. By avoiding traditional UV-based methods, UniTEX addresses common challenges in 3D texture generation, resulting in superior visual quality and integrity."
                },
                "zh": {
                    "title": "UniTEXï¼šæ— UVæ˜ å°„çš„é«˜è´¨é‡ä¸‰ç»´çº¹ç†ç”Ÿæˆ",
                    "desc": "UniTEXæ˜¯ä¸€ç§æ–°é¢–çš„ä¸‰ç»´çº¹ç†ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»å›¾åƒå’Œå‡ ä½•å½¢çŠ¶ç”Ÿæˆé«˜è´¨é‡ã€ä¸€è‡´çš„ä¸‰ç»´çº¹ç†ï¼Œè€Œæ— éœ€ä½¿ç”¨UVæ˜ å°„ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¹ç†å‡½æ•°ï¼ˆTexture Functionsï¼‰å°†çº¹ç†ç”Ÿæˆæå‡åˆ°ä¸‰ç»´ç©ºé—´ï¼Œé¿å…äº†ä¼ ç»ŸUVæ˜ å°„å¸¦æ¥çš„æ‹“æ‰‘æ¨¡ç³Šé—®é¢˜ã€‚UniTEXåˆ©ç”¨åŸºäºå˜æ¢å™¨çš„å¤§å‹çº¹ç†æ¨¡å‹ï¼ˆLarge Texturing Modelï¼‰ç›´æ¥é¢„æµ‹çº¹ç†å‡½æ•°ï¼Œä»è€Œæé«˜çº¹ç†è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniTEXåœ¨è§†è§‰è´¨é‡å’Œçº¹ç†å®Œæ•´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–ä¸‰ç»´çº¹ç†ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18087",
            "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
            "url": "https://huggingface.co/papers/2505.18087",
            "abstract": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
            "score": 3,
            "issue_id": 4035,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 Ğ¼Ğ°Ñ",
                "en": "May 23",
                "zh": "5æœˆ23æ—¥"
            },
            "hash": "af9bc7f06b3e9889",
            "authors": [
                "Hyungyung Lee",
                "Geon Choi",
                "Jung-Oh Lee",
                "Hangyul Yoon",
                "Hyuk Gi Hong",
                "Edward Choi"
            ],
            "affiliations": [
                "KAIST",
                "Seoul Medical Center",
                "Seoul National University Hospital"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18087.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#science",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "ğŸ©»",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ",
                    "desc": "CheXStruct Ğ¸ CXReasonBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MIMIC-CXR-JPG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. CheXStruct Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸. CXReasonBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Clinical Diagnosis with Structured Reasoning in AI",
                    "desc": "The paper introduces CheXStruct and CXReasonBench, benchmarks designed to evaluate Large Vision-Language Models (LVLMs) in clinical diagnosis using the MIMIC-CXR-JPG dataset. These tools focus on assessing structured reasoning, visual grounding, and the models' ability to generalize across various diagnostic tasks. CheXStruct automates the extraction of intermediate reasoning steps from chest X-rays, while CXReasonBench tests the models' capacity to perform clinically valid reasoning and learn from structured guidance. The findings reveal that even the best LVLMs struggle with structured reasoning and connecting abstract knowledge to visual data, highlighting the need for improved model capabilities in medical contexts."
                },
                "zh": {
                    "title": "è¯„ä¼°ä¸´åºŠè¯Šæ–­ä¸­çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›",
                    "desc": "CheXStructå’ŒCXReasonBenchæ˜¯ç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„å·¥å…·ã€‚å®ƒä»¬é€šè¿‡åˆ†æèƒ¸éƒ¨Xå…‰å›¾åƒï¼Œè‡ªåŠ¨ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¸®åŠ©æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æ¨ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«18,988ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–12ä¸ªè¯Šæ–­ä»»åŠ¡ï¼Œæ”¯æŒå¤šè·¯å¾„å’Œå¤šé˜¶æ®µè¯„ä¼°ã€‚å°½ç®¡æœ‰10ä¸ªæ¨¡å‹å‚ä¸è¯„ä¼°ï¼Œä½†å®ƒä»¬åœ¨ç»“æ„åŒ–æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14321",
            "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
            "url": "https://huggingface.co/papers/2505.14321",
            "abstract": "Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating a model's temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that obscure whether higher scores truly indicate stronger understanding of the dynamic content in videos: (1) strong language priors, where models can answer questions without watching the video; and (2) shuffling invariance, where models maintain similar performance on certain questions even when video frames are temporally shuffled. To alleviate these issues, we propose VBenchComp, an automated pipeline that categorizes questions into different domains: LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions can be answered without viewing the video; Semantic questions remain answerable even when the video frames are shuffled; and Temporal questions require understanding the correct temporal order of frames. The rest of the questions are labeled as Others. This can enable fine-grained evaluation of different capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that are hidden by traditional overall scores, and we offer insights and recommendations for designing future benchmarks that more accurately assess video LLMs.",
            "score": 3,
            "issue_id": 4041,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "fd7b12c6173122fa",
            "authors": [
                "Bo Feng",
                "Zhengfeng Lai",
                "Shiyu Li",
                "Zizhen Wang",
                "Simon Wang",
                "Ping Huang",
                "Meng Cao"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14321.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VBenchComp - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ²Ğ¸Ğ´Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ°."
                },
                "en": {
                    "title": "Isolating Temporal Reasoning in Video Understanding",
                    "desc": "This paper addresses the shortcomings of existing video understanding benchmarks that mix different types of questions, making it hard to evaluate a model's ability to reason about video content over time. The authors identify two main issues: models can sometimes answer questions based on language knowledge alone, and they perform similarly on questions even when video frames are shuffled. To tackle these problems, they introduce VBenchComp, a new evaluation framework that classifies questions into categories based on their reliance on temporal reasoning. This approach allows for a more detailed assessment of video language models (LLMs) and highlights specific areas where models may struggle, leading to better benchmark designs in the future."
                },
                "zh": {
                    "title": "ç²¾å‡†è¯„ä¼°è§†é¢‘ç†è§£èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "ç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†å¾€å¾€å°†åŸºäºçŸ¥è¯†å’Œçº¯å›¾åƒçš„é—®é¢˜æ··ä¸ºä¸€è°ˆï¼Œæœªèƒ½æ¸…æ™°åœ°éš”ç¦»æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¯è§†é¢‘ç†è§£ä¸å…¶ä»–æ¨¡æ€çš„å…³é”®åŒºåˆ«ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼Œå½±å“äº†é«˜åˆ†æ˜¯å¦çœŸæ­£åæ˜ äº†å¯¹è§†é¢‘åŠ¨æ€å†…å®¹çš„ç†è§£ï¼šä¸€æ˜¯å¼ºè¯­è¨€å…ˆéªŒï¼Œæ¨¡å‹å¯ä»¥åœ¨ä¸è§‚çœ‹è§†é¢‘çš„æƒ…å†µä¸‹å›ç­”é—®é¢˜ï¼›äºŒæ˜¯æ´—ç‰Œä¸å˜æ€§ï¼Œæ¨¡å‹åœ¨æŸäº›é—®é¢˜ä¸Šå³ä½¿è§†é¢‘å¸§è¢«æ‰“ä¹±ä¹Ÿèƒ½ä¿æŒç›¸ä¼¼çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VBenchCompï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œå°†é—®é¢˜åˆ†ç±»ä¸ºä¸åŒé¢†åŸŸï¼šLLMå¯å›ç­”ã€è¯­ä¹‰å’Œæ—¶é—´ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¼ ç»Ÿæ•´ä½“è¯„åˆ†æ©ç›–çš„æ¨¡å‹å¼±ç‚¹ï¼Œå¹¶ä¸ºè®¾è®¡æ›´å‡†ç¡®è¯„ä¼°è§†é¢‘LLMçš„æœªæ¥åŸºå‡†æä¾›äº†è§è§£å’Œå»ºè®®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23761",
            "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
            "url": "https://huggingface.co/papers/2505.23761",
            "abstract": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information.",
            "score": 2,
            "issue_id": 4040,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "df30792f37655462",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#alignment",
                    "#synthetic",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ DPO Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ DPO Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ğ°ÑÑƒ, Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Optimal Learning with Differential Information in DPO",
                    "desc": "This paper provides a theoretical analysis of Direct Preference Optimization (DPO), focusing on the log-ratio reward parameterization, which is shown to be optimal for learning target policies through preference optimization. It introduces the concept of Differential Information Distribution (DID), which captures the information gained during policy updates and links preference labels to the transformation of reference policies into target policies. The study reveals that the effectiveness of DPO is tied to log-margin ordered policies, an important but previously overlooked aspect of preference optimization. Additionally, it characterizes how different levels of entropy in differential information influence policy reinforcement and smoothing, offering insights into effective instruction-following and question answering tasks."
                },
                "zh": {
                    "title": "ç›´æ¥åå¥½ä¼˜åŒ–ï¼šå·®åˆ†ä¿¡æ¯çš„å…³é”®è§’è‰²",
                    "desc": "æœ¬æ–‡å¯¹ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†å¯¹æ•°æ¯”å¥–åŠ±å‚æ•°åŒ–åœ¨é€šè¿‡åå¥½ä¼˜åŒ–å­¦ä¹ ç›®æ ‡ç­–ç•¥æ—¶çš„æœ€ä½³æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å·®åˆ†ä¿¡æ¯åˆ†å¸ƒï¼ˆDIDï¼‰æ¥è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œå¹¶å±•ç¤ºäº†åå¥½æ ‡ç­¾å¦‚ä½•ç¼–ç æ‰€éœ€çš„å·®åˆ†ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—DPOä¸­çš„å¯¹æ•°æ¯”å¥–åŠ±æˆä¸ºå­¦ä¹ ç›®æ ‡ç­–ç•¥çš„å”¯ä¸€æœ€ä½³å½¢å¼ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåå¥½ç¼–ç å·®åˆ†ä¿¡æ¯çš„æ¡ä»¶ä¸éšå«çš„å¯¹æ•°è¾¹é™…æœ‰åºç­–ç•¥å‡è®¾å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä¸€å‡è®¾åœ¨åå¥½ä¼˜åŒ–ä¸­å¹¿æ³›ä½¿ç”¨ä½†ä¹‹å‰æœªè¢«è®¤è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æDIDçš„ç†µï¼Œé˜æ˜äº†ä½ç†µå’Œé«˜ç†µå·®åˆ†ä¿¡æ¯å¯¹ç­–ç•¥åˆ†å¸ƒçš„ä¸åŒå½±å“ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23625",
            "title": "ZeroSep: Separate Anything in Audio with Zero Training",
            "url": "https://huggingface.co/papers/2505.23625",
            "abstract": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "21979b2379717385",
            "authors": [
                "Chao Huang",
                "Yuesheng Ma",
                "Junxuan Huang",
                "Susan Liang",
                "Yunlong Tang",
                "Jing Bi",
                "Wenqiang Liu",
                "Nima Mesgarani",
                "Chenliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Tencent America",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23625.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#audio",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº",
                    "desc": "ZeroSep - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ. ZeroSep Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼."
                },
                "en": {
                    "title": "ZeroSep: Revolutionizing Audio Separation with Zero-Shot Learning",
                    "desc": "ZeroSep is a novel audio source separation model that utilizes pre-trained text-guided audio diffusion techniques to achieve zero-shot performance. Unlike traditional supervised methods that require extensive labeled data, ZeroSep leverages generative models to separate audio sources without any task-specific training. By inverting mixed audio into the model's latent space and applying text conditioning, it effectively guides the denoising process to isolate individual sound sources. This approach not only enhances separation accuracy but also allows for flexibility in handling diverse and unpredictable acoustic environments."
                },
                "zh": {
                    "title": "ZeroSepï¼šæ— ç›‘ç£éŸ³é¢‘æºåˆ†ç¦»çš„æ–°çªç ´",
                    "desc": "ZeroSepæ˜¯ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„éŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æºåˆ†ç¦»ã€‚å®ƒé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹å’Œæ–‡æœ¬æ¡ä»¶åŒ–ï¼Œå…‹æœäº†ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚ZeroSepé€šè¿‡å°†æ··åˆéŸ³é¢‘åè½¬åˆ°æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æŒ‡å¯¼å»å™ªè¿‡ç¨‹ï¼ŒæˆåŠŸæ¢å¤ä¸ªåˆ«éŸ³æºã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨å¼€æ”¾åœºæ™¯ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22918",
            "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
            "url": "https://huggingface.co/papers/2505.22918",
            "abstract": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1\\% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: https://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
            "score": 2,
            "issue_id": 4041,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "0d7f4111cbc74b76",
            "authors": [
                "Ruichen Chen",
                "Keith G. Mills",
                "Liyao Jiang",
                "Chao Gao",
                "Di Niu"
            ],
            "affiliations": [
                "ECE Department University of Alberta",
                "Huawei Technologies Edmonton, Alberta, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22918.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#video",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Re-ttention Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Re-ttention Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ 3.1% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Re-ttention: Sparsity Meets Quality in Visual Generation",
                    "desc": "Re-ttention is a novel approach that enhances the efficiency of diffusion models in visual generation by utilizing temporal redundancy to achieve high sparse attention. This method addresses the computational challenges posed by traditional attention mechanisms, which become increasingly complex with higher resolution and longer video lengths. By reshaping attention scores based on historical softmax distributions, Re-ttention maintains visual quality even at extreme levels of sparsity. Experimental results show that it significantly reduces the number of tokens needed during inference while also improving latency, outperforming existing sparse attention techniques."
                },
                "zh": {
                    "title": "Re-ttentionï¼šé«˜æ•ˆç¨€ç–æ³¨æ„åŠ›çš„è§†è§‰ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "Re-ttentionæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´å†—ä½™ï¼Œå®ç°é«˜ç¨€ç–æ³¨æ„åŠ›çš„è§†è§‰ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å’Œé•¿è§†é¢‘æ—¶ï¼Œè®¡ç®—å¤æ‚åº¦å‘ˆå¹³æ–¹çº§å¢é•¿ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚Re-ttentioné€šè¿‡é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°ï¼ŒåŸºäºå…ˆå‰çš„softmaxåˆ†å¸ƒå†å²ï¼Œè§£å†³äº†åœ¨æé«˜ç¨€ç–åº¦ä¸‹çš„è§†è§‰è´¨é‡ä¿æŒé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-ttentionåœ¨æ¨ç†æ—¶åªéœ€ä½¿ç”¨3.1%çš„æ ‡è®°ï¼Œä¸”åœ¨å»¶è¿Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22765",
            "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
            "url": "https://huggingface.co/papers/2505.22765",
            "abstract": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.  \t\t\t\t\tAI-generated summary \t\t\t\t Sentence stress refers to emphasis, placed on specific words within a spoken utterance to highlight or contrast an idea, or to introduce new information. It is often used to imply an underlying intention that is not explicitly stated. Recent advances in speech-aware language models (SLMs) have enabled direct processing of audio, allowing models to bypass transcription and access the full richness of the speech signal and perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and speaker intent, it remains largely overlooked in evaluation and development of such models. In this work, we address this gap by introducing StressTest, a benchmark specifically designed to evaluate a model's ability to distinguish between interpretations of spoken sentences based on the stress pattern. We assess the performance of several leading SLMs and find that, despite their overall capabilities, they perform poorly on such tasks. To overcome this limitation, we propose a novel synthetic data generation pipeline, and create Stress17k, a training set that simulates change of meaning implied by stress variation. Then, we empirically show that optimizing models with this synthetic dataset aligns well with real-world recordings and enables effective finetuning of SLMs. Results suggest, that our finetuned model, StresSLM, significantly outperforms existing models on both sentence stress reasoning and detection tasks. Code, models, data, and audio samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.",
            "score": 2,
            "issue_id": 4040,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "601fd00657c1e4f1",
            "authors": [
                "Iddo Yosha",
                "Gallil Maimon",
                "Yossi Adi"
            ],
            "affiliations": [
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22765.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº StressTest Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Stress17k Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ğµ Ğ² ÑƒÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸ĞµĞ¼, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Stress17k, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ StresSLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Speech Models with Sentence Stress Understanding",
                    "desc": "This paper introduces the StressTest benchmark and the synthetic Stress17k dataset to enhance speech-aware language models' understanding of sentence stress in spoken language. Sentence stress is crucial for conveying meaning and speaker intent, yet it has been largely ignored in the evaluation of these models. The authors evaluate several leading speech-aware language models and find that they struggle with tasks involving sentence stress interpretation. By training a model called StresSLM on the new dataset, the authors demonstrate significant improvements in both reasoning and detection of sentence stress, showcasing the importance of this feature in natural language processing."
                },
                "zh": {
                    "title": "æå‡è¯­éŸ³æ¨¡å‹ç†è§£å¥å­é‡éŸ³çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºStressTestçš„åŸºå‡†æµ‹è¯•å’Œåˆæˆæ•°æ®é›†Stress17kï¼Œæ—¨åœ¨æé«˜è¯­éŸ³æ„ŸçŸ¥è¯­è¨€æ¨¡å‹å¯¹å£è¯­ä¸­å¥å­é‡éŸ³çš„ç†è§£èƒ½åŠ›ã€‚å¥å­é‡éŸ³æ˜¯æŒ‡åœ¨å£è¯­ä¸­å¯¹ç‰¹å®šå•è¯çš„å¼ºè°ƒï¼Œç”¨ä»¥çªå‡ºæˆ–å¯¹æ¯”æŸä¸ªè§‚ç‚¹ã€‚å°½ç®¡å¥å­é‡éŸ³åœ¨ä¼ è¾¾æ„ä¹‰å’Œè¯´è¯è€…æ„å›¾ä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œä½†åœ¨ç°æœ‰æ¨¡å‹çš„è¯„ä¼°å’Œå¼€å‘ä¸­å´è¢«å¿½è§†ã€‚é€šè¿‡å¼•å…¥StressTeståŸºå‡†å’ŒStress17kæ•°æ®é›†ï¼Œç ”ç©¶è¡¨æ˜ä¼˜åŒ–æ¨¡å‹å¯ä»¥æ˜¾è‘—æå‡å…¶åœ¨å¥å­é‡éŸ³æ¨ç†å’Œæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20282",
            "title": "One-shot Entropy Minimization",
            "url": "https://huggingface.co/papers/2505.20282",
            "abstract": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "9064c4451edd439a",
            "authors": [
                "Zitian Gao",
                "Lynx Chen",
                "Joey Zhou",
                "Bryan Dai"
            ],
            "affiliations": [
                "Ubiquant"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20282.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ° 13,440 Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ LLM."
                },
                "en": {
                    "title": "Revolutionizing Language Model Training with Minimal Data",
                    "desc": "This paper presents a novel approach to improving large language models through entropy minimization using only one sample and minimal optimization. The authors trained 13,440 models and discovered that this method can achieve performance enhancements similar to or better than traditional methods that rely on extensive datasets and complex reward systems in reinforcement learning. This finding suggests that the reliance on large amounts of labeled data may be reconsidered in the context of post-training strategies for language models. The research highlights the potential for more efficient training processes in machine learning applications."
                },
                "zh": {
                    "title": "ç†µæœ€å°åŒ–ï¼šç”¨ä¸€ä¸ªæ ·æœ¬å®ç°æ€§èƒ½é£è·ƒ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ç†µæœ€å°åŒ–åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…éœ€ä¸€ä¸ªæ— æ ‡ç­¾æ ·æœ¬å’Œ10æ­¥ä¼˜åŒ–ï¼Œå°±èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸ªç»“æœä¸ä½¿ç”¨æˆåƒä¸Šä¸‡çš„æ•°æ®å’Œç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸åª²ç¾ï¼Œç”šè‡³æ›´ä¼˜ã€‚æ­¤å‘ç°å¯èƒ½ä¼šä¿ƒä½¿äººä»¬é‡æ–°æ€è€ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒèŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19360",
            "title": "ChartLens: Fine-grained Visual Attribution in Charts",
            "url": "https://huggingface.co/papers/2505.19360",
            "abstract": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "1b792efe04b5f610",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19360.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#hallucinations",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ChartLens",
                    "desc": "ChartLens - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ChartVA-Eval Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ChartLens ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ½Ğ° 26-66%."
                },
                "en": {
                    "title": "ChartLens: Enhancing Chart Understanding with Visual Attributions",
                    "desc": "ChartLens is a novel approach that enhances multimodal large language models (MLLMs) by providing fine-grained visual attributions for better chart understanding. It addresses the issue of hallucinations in MLLMs, where the generated text may not align with the visual data. By using segmentation techniques to identify specific chart elements and employing set-of-marks prompting, ChartLens improves the accuracy of interpreting charts significantly. The introduction of ChartVA-Eval, a benchmark for evaluating these attributions, further supports the effectiveness of ChartLens across various domains."
                },
                "zh": {
                    "title": "ChartLensï¼šæå‡å›¾è¡¨ç†è§£çš„ç»†ç²’åº¦è§†è§‰å½’å› ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºChartLensçš„æ–°ç®—æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾è¡¨ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç»†ç²’åº¦çš„è§†è§‰å½’å› ï¼ŒChartLensèƒ½å¤Ÿè¯†åˆ«å›¾è¡¨ä¸­çš„å…·ä½“å…ƒç´ ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå‡å°‘ç”Ÿæˆæ–‡æœ¬ä¸è§†è§‰æ•°æ®ä¹‹é—´çš„çŸ›ç›¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ChartVA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«åˆæˆå’ŒçœŸå®ä¸–ç•Œå›¾è¡¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–é‡‘èã€æ”¿ç­–å’Œç»æµç­‰å¤šä¸ªé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChartLensåœ¨ç»†ç²’åº¦å½’å› æ–¹é¢çš„è¡¨ç°æé«˜äº†26%åˆ°66%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19286",
            "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.19286",
            "abstract": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "a42b79a0269a396c",
            "authors": [
                "Utkarsh Sahu",
                "Zhisheng Qi",
                "Yongjia Lei",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Nesreen K. Ahmed",
                "Mahantesh M Halappanavar",
                "Yao Ma",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Pacific Northwest National Laboratory",
                "Rensselaer Polytechnic Institute",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19286.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#graphs",
                    "#architecture",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unveiling Knowledge Patterns in Language Models through Graphs",
                    "desc": "This paper investigates how knowledge is structured in large language models (LLMs) by viewing it through a graph lens. It identifies a phenomenon called knowledge homophily, where entities that are closely connected in the graph tend to have similar knowledge levels. The authors develop graph machine learning models to estimate the knowledge of entities based on their neighboring connections. Their findings suggest that fine-tuning LLMs with carefully selected triplets can significantly enhance their performance."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ç»“æ„ä¸åŒè´¨æ€§",
                    "desc": "æœ¬ç ”ç©¶ä»å›¾çš„è§’åº¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ç»“æ„æ¨¡å¼ï¼Œæ­ç¤ºäº†çŸ¥è¯†åŒè´¨æ€§ï¼Œå¹¶å¼€å‘äº†å›¾æœºå™¨å­¦ä¹ æ¨¡å‹æ¥ä¼°è®¡å®ä½“çŸ¥è¯†ã€‚æˆ‘ä»¬é‡åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œåˆ†æäº†å…¶ä¸å›¾ç»“æ„å±æ€§ï¼ˆå¦‚èŠ‚ç‚¹åº¦ï¼‰çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œæ‹“æ‰‘ä¸Šç›¸è¿‘çš„å®ä½“è¡¨ç°å‡ºç›¸ä¼¼çš„çŸ¥è¯†æ°´å¹³ï¼Œè¿™æ¿€åŠ±æˆ‘ä»¬åŸºäºå±€éƒ¨é‚»å±…å¼€å‘æ¨¡å‹æ¥ä¼°è®¡å®ä½“çŸ¥è¯†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é€‰å®šçš„ä¸‰å…ƒç»„è¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23183",
            "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
            "url": "https://huggingface.co/papers/2505.23183",
            "abstract": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.",
            "score": 1,
            "issue_id": 4041,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 Ğ¼Ğ°Ñ",
                "en": "May 29",
                "zh": "5æœˆ29æ—¥"
            },
            "hash": "015c1dbfc16cb8cb",
            "authors": [
                "Gabriele Sarti",
                "VilÃ©m Zouhar",
                "Malvina Nissim",
                "Arianna Bisazza"
            ],
            "affiliations": [
                "CLCG, University of Groningen",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23183.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#machine_translation",
                    "#data",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°: Ğ¾Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ supervised Ğ¸ unsupervised Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 14 Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ 12 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» unsupervised Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ supervised Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°."
                },
                "en": {
                    "title": "Unlocking Translation Quality: The Power of Unsupervised Metrics",
                    "desc": "This paper explores word-level quality estimation (WQE) techniques that automatically detect translation errors in machine-generated text. It emphasizes the importance of model interpretability and uncertainty quantification to improve the identification of these errors. The study evaluates 14 different metrics across 12 translation directions, focusing on how variations in human labeling affect the performance of these metrics. The findings reveal the advantages of unsupervised methods and the limitations of supervised approaches, particularly in situations with uncertain labels."
                },
                "zh": {
                    "title": "æ¢ç´¢é«˜æ•ˆçš„è¯çº§è´¨é‡è¯„ä¼°æŠ€æœ¯",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è¯çº§è´¨é‡è¯„ä¼°æŠ€æœ¯åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•åˆ©ç”¨æ¨¡å‹å¯è§£é‡Šæ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–æ¥è¯†åˆ«ç¿»è¯‘é”™è¯¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„WQEæŠ€æœ¯é€šå¸¸éœ€è¦å¤§é‡äººç±»æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†14ç§æŒ‡æ ‡åœ¨12ä¸ªç¿»è¯‘æ–¹å‘ä¸Šçš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†äººç±»æ ‡ç­¾å˜å¼‚å¯¹æŒ‡æ ‡æ€§èƒ½çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ— ç›‘ç£æŒ‡æ ‡å…·æœ‰æœªè¢«å……åˆ†åˆ©ç”¨çš„æ½œåŠ›ï¼Œè€Œç›‘ç£æ–¹æ³•åœ¨æ ‡ç­¾ä¸ç¡®å®šæ€§é¢å‰å­˜åœ¨ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22943",
            "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
            "url": "https://huggingface.co/papers/2505.22943",
            "abstract": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "0e150bd219867ad7",
            "authors": [
                "Jaewoo Ahn",
                "Heeseung Yun",
                "Dayoon Ko",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22943.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#security",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Multimodal Adversarial Compositionality (MAC) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. MAC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AI Resilience Against Deceptive Text in Multimodal Models",
                    "desc": "This paper introduces a benchmark called Multimodal Adversarial Compositionality (MAC) to assess the weaknesses in multimodal representations, such as those used in AI models like CLIP. It highlights how these models can make incorrect judgments when faced with deceptive text samples generated by large language models. The authors propose a self-training method that improves zero-shot learning by enhancing the success of attacks and increasing the diversity of the samples used. Their approach, which utilizes smaller language models, shows better performance in identifying these vulnerabilities across different types of media, including images, videos, and audio."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ¨¡æ€è¡¨ç¤ºçš„ç»„åˆè„†å¼±æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æ¬ºéª—æ€§æ–‡æœ¬æ ·æœ¬æ¥è¯„ä¼°å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ç»„åˆè„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å¯¹æŠ—ç»„åˆæ€§ï¼ˆMACï¼‰ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¬ºéª—æ€§æ–‡æœ¬æ ·æœ¬ï¼Œåˆ©ç”¨è¿™äº›è„†å¼±æ€§è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†æ”¹å–„é›¶æ ·æœ¬æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘è®­ç»ƒçš„æ–¹æ³•ï¼Œç»“åˆæ‹’ç»é‡‡æ ·å¾®è°ƒå’Œå¤šæ ·æ€§ä¿ƒè¿›è¿‡æ»¤ï¼Œä»è€Œæé«˜æ”»å‡»æˆåŠŸç‡å’Œæ ·æœ¬å¤šæ ·æ€§ã€‚ä½¿ç”¨è¾ƒå°çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚Llama-3.1-8Bï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ­ç¤ºå„ç§å¤šæ¨¡æ€è¡¨ç¤ºï¼ˆåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼‰çš„ç»„åˆè„†å¼±æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22810",
            "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
            "url": "https://huggingface.co/papers/2505.22810",
            "abstract": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments.",
            "score": 1,
            "issue_id": 4041,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "965d2d2ccc7a3cba",
            "authors": [
                "Zhoufaran Yang",
                "Yan Shu",
                "Zhifei Yang",
                "Yan Zhang",
                "Yu Li",
                "Keyang Lu",
                "Gangyan Zeng",
                "Shaohui Liu",
                "Yu Zhou",
                "Nicu Sebe"
            ],
            "affiliations": [
                "BUAA",
                "HIT",
                "IIE, CAS",
                "NJUST",
                "NKU",
                "PKU",
                "UCAS",
                "UNITN"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22810.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VidText: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "VidText - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ»Ğ¸Ğ¿Ğ° Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Video Text Understanding with VidText",
                    "desc": "VidText is a new benchmark designed to evaluate how well models understand text in videos, focusing on both overall summaries and specific details. It addresses the lack of attention to textual information in existing video benchmarks and the limitations of OCR benchmarks that only deal with static images. The benchmark includes a variety of real-world scenarios and tasks that assess models at different levels, from video-wide summaries to specific instances of text retrieval. Experiments show that current multimodal models face challenges in these tasks, indicating a need for further development in video text understanding."
                },
                "zh": {
                    "title": "VidTextï¼šè§†é¢‘æ–‡æœ¬ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "VidTextæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘æ–‡æœ¬ç†è§£çš„èƒ½åŠ›ï¼Œæ¶µç›–å…¨çƒæ‘˜è¦å’Œå±€éƒ¨æ£€ç´¢ç­‰å¤šç§ä»»åŠ¡ã€‚è§†é¢‘ä¸­çš„è§†è§‰æ–‡æœ¬åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¹æ•´ä½“è§†é¢‘ç†è§£å’Œç»†è‡´çš„äººç±»è¡Œä¸ºæ¨ç†è‡³å…³é‡è¦ã€‚ç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†å¤§å¤šå¿½è§†æ–‡æœ¬ä¿¡æ¯ï¼Œè€ŒOCRç‰¹å®šåŸºå‡†åˆå±€é™äºé™æ€å›¾åƒï¼Œæ— æ³•æ•æ‰æ–‡æœ¬ä¸åŠ¨æ€è§†è§‰ç¯å¢ƒä¹‹é—´çš„äº’åŠ¨ã€‚VidTexté€šè¿‡å¼•å…¥å¤šå±‚æ¬¡çš„è¯„ä¼°æ¡†æ¶å’Œå¤šç§çœŸå®åœºæ™¯ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22126",
            "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
            "url": "https://huggingface.co/papers/2505.22126",
            "abstract": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.",
            "score": 1,
            "issue_id": 4039,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 Ğ¼Ğ°Ñ",
                "en": "May 28",
                "zh": "5æœˆ28æ—¥"
            },
            "hash": "6401cdf5b0a13760",
            "authors": [
                "Yifan Chang",
                "Yukang Feng",
                "Jianwen Sun",
                "Jiaxin Ai",
                "Chuanhao Li",
                "S. Kevin Zhou",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22126.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "SridBench: Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ˜Ğ˜ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "SridBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1120 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 13 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾ 6 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o-image, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "SridBench: Elevating AI in Scientific Figure Generation",
                    "desc": "SridBench is a new benchmark designed to evaluate the generation of scientific figures by AI models. It highlights that current leading models, such as GPT-4o-image, struggle with achieving the same level of semantic and structural accuracy as human-generated illustrations. The benchmark includes 1,120 carefully selected examples from various scientific fields, assessing models on criteria like semantic fidelity and structural accuracy. The results indicate a significant gap in performance, emphasizing the necessity for improved multimodal reasoning in visual generation tasks."
                },
                "zh": {
                    "title": "ç§‘å­¦å›¾å½¢ç”Ÿæˆçš„æ–°åŸºå‡†ï¼šSridBench",
                    "desc": "SridBenchæ˜¯ä¸€ä¸ªç”¨äºç§‘å­¦å›¾å½¢ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰AIæ¨¡å‹åœ¨ç”Ÿæˆç§‘å­¦æ’å›¾æ—¶çš„è¡¨ç°ã€‚å°½ç®¡åƒGPT-4o-imageè¿™æ ·çš„é¡¶å°–æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨è¯­ä¹‰å’Œç»“æ„å‡†ç¡®æ€§ä¸Šä»ç„¶æ— æ³•ä¸äººç±»ç›¸åª²ç¾ã€‚ç§‘å­¦æ’å›¾ç”Ÿæˆéœ€è¦å¯¹æŠ€æœ¯å†…å®¹è¿›è¡Œå‡†ç¡®è§£è¯»ï¼Œå¹¶å°†æŠ½è±¡æ¦‚å¿µè½¬åŒ–ä¸ºæ¸…æ™°çš„æ ‡å‡†åŒ–è§†è§‰æ•ˆæœï¼Œè¿™ä¸€è¿‡ç¨‹çŸ¥è¯†å¯†é›†ä¸”è€—æ—¶ã€‚SridBenché€šè¿‡æä¾›1,120ä¸ªå®ä¾‹ï¼Œå¡«è¡¥äº†è¿™ä¸€è¯„ä¼°ç©ºç™½ï¼Œå¼ºè°ƒäº†å¯¹æ›´å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†é©±åŠ¨çš„è§†è§‰ç”Ÿæˆèƒ½åŠ›çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20199",
            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
            "url": "https://huggingface.co/papers/2505.20199",
            "abstract": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.",
            "score": 1,
            "issue_id": 4036,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 Ğ¼Ğ°Ñ",
                "en": "May 26",
                "zh": "5æœˆ26æ—¥"
            },
            "hash": "9386cae0db1bfc6c",
            "authors": [
                "Pengxiang Li",
                "Shilin Yan",
                "Joey Tsai",
                "Renrui Zhang",
                "Ruichuan An",
                "Ziyu Guo",
                "Xiaowei Gao"
            ],
            "affiliations": [
                "CUHK",
                "FDU",
                "ICL",
                "PKU",
                "PolyU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20199.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ‘ĞµÑĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğµ Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (A-CFG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. A-CFG Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ CFG Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Dynamic Guidance for Better Language Generation",
                    "desc": "Adaptive Classifier-Free Guidance (A-CFG) enhances language generation by adjusting guidance based on the model's confidence levels. Unlike traditional Classifier-Free Guidance (CFG), which uses a fixed unconditional input, A-CFG dynamically re-masks tokens where the model is uncertain. This targeted approach allows for more effective corrections during the generation process, improving overall performance. Experiments show that A-CFG significantly outperforms standard CFG, demonstrating the importance of adapting guidance to model uncertainty."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´å¼•å¯¼ï¼Œæå‡ç”Ÿæˆæ•ˆæœ",
                    "desc": "è‡ªé€‚åº”æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆA-CFGï¼‰é€šè¿‡å…³æ³¨æ¨¡å‹ä¿¡å¿ƒè¾ƒä½çš„åŒºåŸŸï¼ŒåŠ¨æ€è°ƒæ•´æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„å¼•å¯¼ï¼Œä»è€Œæ˜¾è‘—æé«˜è¯­è¨€ç”Ÿæˆæ€§èƒ½ã€‚ä¼ ç»Ÿçš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ä½¿ç”¨é™æ€çš„æ— æ¡ä»¶è¾“å…¥ï¼Œè¿™åœ¨æ¨¡å‹ä¸ç¡®å®šæ€§åŠ¨æ€å˜åŒ–çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½æ•ˆæœä¸ä½³ã€‚A-CFGæ–¹æ³•é€šè¿‡åˆ©ç”¨æ¨¡å‹çš„å³æ—¶é¢„æµ‹ä¿¡å¿ƒï¼Œå®šåˆ¶æ— æ¡ä»¶è¾“å…¥ï¼Œåœ¨æ¯ä¸€æ­¥è¿­ä»£ä¸­è¯†åˆ«å½“å‰ç”Ÿæˆåºåˆ—ä¸­æ¨¡å‹ä¿¡å¿ƒä½çš„æ ‡è®°ã€‚é€šè¿‡ä¸´æ—¶é‡æ–°æ©è”½è¿™äº›æ ‡è®°ï¼ŒA-CFGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é›†ä¸­å¼•å¯¼åœ¨æ¨¡ç³ŠåŒºåŸŸï¼Œä»è€Œæå‡ç”Ÿæˆæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19236",
            "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
            "url": "https://huggingface.co/papers/2505.19236",
            "abstract": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.  \t\t\t\t\tAI-generated summary \t\t\t\t Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.",
            "score": 1,
            "issue_id": 4037,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ",
                "en": "May 25",
                "zh": "5æœˆ25æ—¥"
            },
            "hash": "3f76ece8cb945378",
            "authors": [
                "Qian Cao",
                "Xiting Wang",
                "Yuzhuo Yuan",
                "Yahui Liu",
                "Fang Luo",
                "Ruihua Song"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19236.jpg",
            "data": {
                "categories": [
                    "#creativity",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "CrEval: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CreataSet, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CrEval - Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. CrEval Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Creativity Evaluation with CrEval",
                    "desc": "This paper introduces a new framework for evaluating creativity in text using a pairwise-comparison method. It utilizes a dataset called CreataSet, which contains over 100,000 human-generated and 1 million synthetic creative instruction-response pairs. The authors developed an LLM-based evaluator named CrEval, which shows significant improvement in aligning with human judgments compared to existing evaluation methods. The study highlights the importance of combining human and synthetic data to create robust evaluators that can enhance the creativity of large language models."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ›æ„è¯„ä¼°çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æˆå¯¹æ¯”è¾ƒæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ›æ„ï¼Œåˆ©ç”¨CreataSetæ•°æ®é›†è®­ç»ƒäº†ä¸€ä¸ªåä¸ºCrEvalçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å™¨ã€‚è¯¥è¯„ä¼°å™¨æ˜¾è‘—æé«˜äº†ä¸äººç±»åˆ¤æ–­ä¸€è‡´çš„æ–‡æœ¬åˆ›æ„è¯„ä¼°æ•ˆæœï¼Œè§£å†³äº†å½“å‰è¯„ä¼°æ–¹æ³•ä¾èµ–äººç±»åˆ¤æ–­çš„ä½æ•ˆé—®é¢˜ã€‚CreataSetæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡ä¸ªäººå·¥åˆ›æ„å’Œè¶…è¿‡100ä¸‡ä¸ªåˆæˆåˆ›æ„çš„æŒ‡ä»¤-å“åº”å¯¹ï¼Œæ¶µç›–å¤šç§å¼€æ”¾é¢†åŸŸä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆäººç±»ç”Ÿæˆå’Œåˆæˆæ•°æ®å¯¹äºè®­ç»ƒå¼ºå¤§çš„è¯„ä¼°å™¨è‡³å…³é‡è¦ï¼ŒCrEvalåœ¨æå‡å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ„èƒ½åŠ›æ–¹é¢å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-29.html",
    "link_next": "2025-06-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "29.05",
        "en": "05/29",
        "zh": "5æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "02.06",
        "en": "06/02",
        "zh": "6æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 5,
        "#benchmark": 24,
        "#agents": 4,
        "#cv": 9,
        "#rl": 11,
        "#rlhf": 7,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 6,
        "#audio": 2,
        "#video": 5,
        "#multimodal": 14,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 3,
        "#training": 24,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 9,
        "#reasoning": 16,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 19,
        "#survey": 2,
        "#diffusion": 11,
        "#alignment": 5,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 2,
        "#synthetic": 4,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 4,
        "#low_resource": 0,
        "#creativity": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨å¥–åŠ±å™ªéŸ³ï¼ˆreward noiseï¼‰å­˜åœ¨çš„æƒ…å†µä¸‹ä»èƒ½è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚å³ä½¿åœ¨æ•°å­¦ä»»åŠ¡ä¸­æ‰‹åŠ¨ç¿»è½¬40%çš„å¥–åŠ±å‡½æ•°è¾“å‡ºï¼Œæ¨¡å‹ä»èƒ½å¿«é€Ÿæ”¶æ•›ï¼Œæé«˜å…¶æ•°å­¦ä»»åŠ¡çš„è¡¨ç°ã€‚é€šè¿‡å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­ï¼ˆRPRï¼‰çš„å‡ºç°ï¼Œæ¨¡å‹åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿå¾—åˆ°äº†æå‡ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ”¹è¿›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µçš„åŸºç¡€èƒ½åŠ›çš„é‡è¦æ€§ã€‚",
        "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
        "pinyin": "è¿™ç¯‡æ–‡ç« ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsåœ¨å¥–åŠ±å™ªéŸ³ï¼ˆreward noiseï¼‰å­˜åœ¨çš„æƒ…å†µä¸‹ä»èƒ½è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚å³ä½¿åœ¨æ•°å­¦ä»»åŠ¡ä¸­æ‰‹åŠ¨ç¿»è½¬40%çš„å¥–åŠ±å‡½æ•°è¾“å‡ºï¼Œæ¨¡å‹ä»èƒ½å¿«é€Ÿæ”¶æ•›ï¼Œæé«˜å…¶æ•°å­¦ä»»åŠ¡çš„è¡¨ç°ã€‚é€šè¿‡å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­ï¼ˆRPRï¼‰çš„å‡ºç°ï¼Œæ¨¡å‹åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿå¾—åˆ°äº†æå‡ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ”¹è¿›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µçš„åŸºç¡€èƒ½åŠ›çš„é‡è¦æ€§ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng yÃ¡njiÅ« le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i qiÃ¡ngzhÃ¹ xuÃ©xÃ­ zhÅng de biÇoxiÃ n. YÃ¡njiÅ« fÄxiÃ n, LLMs zÃ i jiÇnglÃ¬ zÃ oyÄ«n (reward noise) cÃºnzÃ i de qÃ­ngkuÃ ng xiÃ  rÃ©ng nÃ©ng biÇoxiÃ n chÅ« qiÃ¡ngdÃ  de lÇ”cÇ”xÃ¬ng. JÃ­shÇ zÃ i shÃ¹xuÃ© rÃ¨nwÃ¹ zhÅng shÇ’udÃ²ng fÄnzhuÇn 40% de jiÇnglÃ¬ hÃ¡nshÃ¹ shÅ«chÅ«, mÃ³xÃ­ng rÃ©ng nÃ©ng kuÃ isÃ¹ shÅuliÇn, tÃ­gÄo qÃ­ shÃ¹xuÃ© rÃ¨nwÃ¹ de biÇoxiÃ n. TÅngguÃ² jiÇnglÃ¬ guÇnjiÃ n tuÄ«lÇ duÇnyÇ” (RPR) de chÅ«xiÃ n, mÃ³xÃ­ng zÃ i kÄifÃ ngshÃ¬ rÃ¨nwÃ¹ zhÅng de biÇoxiÃ n yÄ› dÃ©dÃ o le tÃ­shÄ“ng. ZhÃ¨xiÄ“ fÄxiÃ n qiÃ¡ngdiÃ o le gÇijÃ¬n mÃ³xÃ­ng zÃ i yÃ¹xÃ¹n jiÄ“duÃ n de jÄ«chÇ” nÃ©nglÃ¬ de zhÃ²ngyÃ oxÃ¬ng.",
        "vocab": "[\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å¼ºåŒ–å­¦ä¹ \", \"pinyin\": \"qiÃ¡ng huÃ  xuÃ©xÃ­\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å¥–åŠ±å™ªéŸ³\", \"pinyin\": \"jiÇng lÃ¬ zÃ o yÄ«n\", \"trans\": \"reward noise\"},\n    {\"word\": \"é²æ£’æ€§\", \"pinyin\": \"lÇ” bÄng xÃ¬ng\", \"trans\": \"robustness\"},\n    {\"word\": \"æ‰‹åŠ¨ç¿»è½¬\", \"pinyin\": \"shÇ’u dÃ²ng fÄn zhuÇn\", \"trans\": \"manually flip\"},\n    {\"word\": \"å¥–åŠ±å‡½æ•°\", \"pinyin\": \"jiÇng lÃ¬ hÃ¡n shÃ¹\", \"trans\": \"reward function\"},\n    {\"word\": \"è¾“å‡º\", \"pinyin\": \"shÅ« chÅ«\", \"trans\": \"output\"},\n    {\"word\": \"å¿«é€Ÿæ”¶æ•›\", \"pinyin\": \"kuÃ i sÃ¹ shÅu liÇn\", \"trans\": \"rapid convergence\"},\n    {\"word\": \"æé«˜\", \"pinyin\": \"tÃ­ gÄo\", \"trans\": \"improve\"},\n    {\"word\": \"å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­\", \"pinyin\": \"jiÇng lÃ¬ guÇn jiÃ n tuÄ« lÇ duÇn yÇ”\", \"trans\": \"reward key reasoning phrases\"},\n    {\"word\": \"å¼€æ”¾å¼ä»»åŠ¡\", \"pinyin\": \"kÄi fÃ ng shÃ¬ rÃ¨n wÃ¹\", \"trans\": \"open-ended tasks\"},\n    {\"word\": \"å‘ç°\", \"pinyin\": \"fÄ xiÃ n\", \"trans\": \"findings\"},\n    {\"word\": \"å¼ºè°ƒ\", \"pinyin\": \"qiÃ¡ng diÃ o\", \"trans\": \"emphasize\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"é¢„è®­ç»ƒé˜¶æ®µ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n jiÄ“ duÃ n\", \"trans\": \"pre-training stage\"},\n    {\"word\": \"åŸºç¡€èƒ½åŠ›\", \"pinyin\": \"jÄ« chÇ” nÃ©ng lÃ¬\", \"trans\": \"foundational capabilities\"}\n]",
        "trans": "This article investigates the performance of large language models (LLMs) in reinforcement learning. The study found that LLMs exhibit strong robustness even in the presence of reward noise. Even when manually flipping 40% of the reward function outputs in mathematical tasks, the model can still converge quickly and improve its performance on these tasks. By rewarding the appearance of key reasoning phrases (RPR), the model's performance on open-ended tasks is also enhanced. These findings underscore the importance of improving the model's foundational capabilities during the pre-training phase.",
        "update_ts": "2025-05-30 09:12"
    }
}