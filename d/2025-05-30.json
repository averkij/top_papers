{
    "date": {
        "ru": "30 мая",
        "en": "May 30",
        "zh": "5月30日"
    },
    "time_utc": "2025-05-30 10:12",
    "weekday": 4,
    "issue_id": 4043,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.23747",
            "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
            "url": "https://huggingface.co/papers/2505.23747",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.",
            "score": 39,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "5ee23f045f054465",
            "authors": [
                "Diankun Wu",
                "Fangfu Liu",
                "Yi-Hsin Hung",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23747.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Пространственный интеллект из 2D наблюдений",
                    "desc": "Статья представляет Spatial-MLLM - новую модель для пространственного анализа на основе только 2D изображений и видео. В отличие от существующих 3D мультимодальных языковых моделей, Spatial-MLLM использует двойной энкодер: семантический и пространственный, объединяя их результаты для улучшенного понимания 3D структуры. Модель также применяет стратегию выборки кадров с учетом пространственной информации при выводе. Тестирование на различных наборах данных показало превосходство Spatial-MLLM в задачах пространственного анализа и рассуждений."
                },
                "en": {
                    "title": "Unlocking 3D Spatial Reasoning from 2D Inputs",
                    "desc": "This paper introduces Spatial-MLLM, a new framework designed to enhance spatial reasoning using only 2D inputs like images and videos. Unlike traditional 3D models that require additional 3D data, Spatial-MLLM utilizes a dual-encoder architecture that combines a pretrained 2D visual encoder for semantic features with a spatial encoder for 3D structure features. The model integrates these features into unified visual tokens, improving its ability to understand spatial relationships. Additionally, a novel space-aware frame sampling strategy is employed to prioritize important frames during inference, leading to superior performance in various spatial reasoning tasks."
                },
                "zh": {
                    "title": "从2D到3D的空间推理新突破",
                    "desc": "本论文提出了一种新的框架，称为Spatial-MLLM，旨在从纯2D观察中进行视觉基础的空间推理。与传统的依赖于CLIP视觉编码器的视频多模态大语言模型不同，我们的模型利用了视觉几何基础模型的强结构先验。我们设计了一个双编码器架构，结合了预训练的2D视觉编码器和空间编码器，以提取语义特征和3D结构特征。通过在推理时采用空间感知的帧采样策略，我们的模型在多种视觉空间理解和推理任务中实现了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23621",
            "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
            "url": "https://huggingface.co/papers/2505.23621",
            "abstract": "Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training.",
            "score": 38,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "a359c304b423ff99",
            "authors": [
                "Zheyuan Yang",
                "Lyuhao Chen",
                "Arman Cohan",
                "Yilun Zhao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.23621.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#rl",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное масштабирование для рассуждений над таблицами",
                    "desc": "В этой статье представлено исследование масштабирования во время вывода для задач рассуждения над таблицами. Авторы разработали две стратегии пост-обучения: дистилляция из трасс рассуждений передовых моделей и обучение с подкреплением с верифицируемыми наградами (RLVR). Модель Table-R1-Zero, полученная с помощью этих методов, достигает производительности GPT-4.1, используя меньше параметров. Она также демонстрирует сильную обобщающую способность на задачах вне обучающего распределения."
                },
                "en": {
                    "title": "Scaling Table Reasoning with Fewer Parameters!",
                    "desc": "This paper introduces two innovative strategies, distillation and reinforcement learning with verifiable rewards (RLVR), to enhance the performance of models in table reasoning tasks. The authors develop a new model, Table-R1-Zero, which achieves performance comparable to GPT-4.1 while utilizing significantly fewer parameters. By leveraging a large dataset of reasoning traces for distillation and applying task-specific reward functions in RLVR, the model demonstrates strong generalization capabilities across various reasoning tasks. The findings highlight the importance of instruction tuning and model architecture in achieving effective table reasoning skills."
                },
                "zh": {
                    "title": "表格推理任务的推理时间扩展新策略",
                    "desc": "本文研究了表格推理任务中的推理时间扩展，提出了两种后训练策略：蒸馏和可验证奖励的强化学习（RLVR）。通过蒸馏，我们利用DeepSeek-R1生成的大规模推理轨迹数据集，微调大型语言模型（LLM），形成Table-R1-SFT模型。RLVR则通过特定任务的可验证奖励函数，应用GRPO算法，得到Table-R1-Zero模型。最终，Table-R1-Zero模型在多个表格推理任务中表现出色，参数量仅为7B，且在泛化能力上优于GPT-4.1。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22653",
            "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
            "url": "https://huggingface.co/papers/2505.22653",
            "abstract": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
            "score": 36,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "7d2f43b6997c2647",
            "authors": [
                "Ang Lv",
                "Ruobing Xie",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Rui Yan"
            ],
            "affiliations": [
                "GSAI, Renmin University of China",
                "Large Language Model Department, Tencent",
                "School of Computer Science, Wuhan University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22653.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM устойчивы к шуму: вознаграждение за процесс важнее результата",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) демонстрируют устойчивость к шуму в функции вознаграждения при пост-обучении. Модели достигают высокой производительности, используя вознаграждения за паттерны рассуждений (RPR) в сочетании с зашумленными моделями вознаграждения. Например, модель Qwen-2.5-7B смогла улучшить свою точность в математических задачах с 5% до 72%, даже когда 40% выходов функции вознаграждения были намеренно искажены. Результаты подчеркивают важность улучшения базовых способностей моделей на этапе предварительного обучения."
                },
                "en": {
                    "title": "Robust LLMs: Thriving Amid Reward Noise with Reasoning Patterns",
                    "desc": "This paper explores how large language models (LLMs) can effectively handle noise in reward signals during post-training, particularly in reinforcement learning scenarios. The authors demonstrate that even with significant reward noise, such as flipping 40% of reward outputs, LLMs like Qwen-2.5-7B can still achieve impressive performance improvements on math tasks. They introduce a novel approach called reasoning pattern rewards (RPR), which focuses on rewarding the presence of key reasoning phrases rather than the correctness of answers, leading to high accuracy. The study emphasizes the importance of enhancing foundational skills during pre-training and offers insights for refining post-training methods in real-world applications."
                },
                "zh": {
                    "title": "提升模型鲁棒性，克服奖励噪声",
                    "desc": "本研究探讨了在后训练阶段，大型语言模型（LLMs）对奖励噪声的鲁棒性。我们发现，即使在奖励函数输出中手动翻转40%的结果，模型仍能快速收敛，数学任务的准确率从5%提升至72%。通过仅奖励关键推理短语的出现（即推理模式奖励RPR），模型在下游任务中达到了超过70%的准确率，表现与严格验证的模型相当。我们的研究强调了在预训练阶段提升模型基础能力的重要性，并为后训练技术的进步提供了新思路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23693",
            "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
            "url": "https://huggingface.co/papers/2505.23693",
            "abstract": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
            "score": 34,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "1a301b9c565967e9",
            "authors": [
                "Tingyu Song",
                "Tongyan Hu",
                "Guo Gan",
                "Yilun Zhao"
            ],
            "affiliations": [
                "National University of Singapore",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Yale University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23693.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VF-Eval: новый рубеж в оценке ИИ-видео мультимодальными моделями",
                    "desc": "Новый бенчмарк VF-Eval оценивает способности мультимодальных языковых моделей (MLLM) в интерпретации видео, созданных искусственным интеллектом. Бенчмарк включает четыре задачи: проверку согласованности, обнаружение ошибок, определение типов ошибок и оценку рассуждений. Эксперименты показали, что даже лучшие модели, такие как GPT-4.1, сталкиваются с трудностями при выполнении всех задач. Исследование также демонстрирует, что согласование MLLM с обратной связью от людей может улучшить генерацию видео."
                },
                "en": {
                    "title": "Evaluating MLLMs: Bridging AI-Generated Videos and Human Feedback",
                    "desc": "The paper introduces VF-Eval, a new benchmark designed to assess the performance of Multimodal Language Models (MLLMs) in interpreting AI-generated content videos. It focuses on four specific tasks: coherence validation, error awareness, error type detection, and reasoning evaluation, which are crucial for understanding synthetic videos. The study evaluates 13 advanced MLLMs, revealing that even the top model, GPT-4.1, faces challenges in consistently performing well across these tasks. Furthermore, the paper explores how aligning MLLMs with human feedback can enhance the quality of video generation, showcasing the practical implications of the benchmark."
                },
                "zh": {
                    "title": "评估AI生成视频的能力新基准",
                    "desc": "本文提出了一个新的基准测试VF-Eval，用于评估多模态大语言模型（MLLMs）在解读AI生成内容视频方面的能力。该基准包含四个任务：连贯性验证、错误意识、错误类型检测和推理评估，旨在全面评估MLLMs在处理合成视频时的表现。研究发现，即使是表现最好的模型GPT-4.1，在所有任务中也难以保持一致的良好表现，显示出基准测试的挑战性。此外，通过实验RePrompt，研究表明将MLLMs与人类反馈更紧密对齐可以改善视频生成的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23762",
            "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
            "url": "https://huggingface.co/papers/2505.23762",
            "abstract": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.",
            "score": 33,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "d159a7e52c608567",
            "authors": [
                "Chenyu Yang",
                "Shiqian Su",
                "Shi Liu",
                "Xuan Dong",
                "Yue Yu",
                "Weijie Su",
                "Xuehui Wang",
                "Zhaoyang Liu",
                "Jinguo Zhu",
                "Hao Li",
                "Wenhai Wang",
                "Yu Qiao",
                "Xizhou Zhu",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23762.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#games",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Автоматизация обучения ГПИ-агентов без участия человека",
                    "desc": "ZeroGUI - это фреймворк онлайн-обучения, использующий визуально-языковые модели для генерации задач и оценки вознаграждений, что улучшает производительность ГПИ-агентов с минимальным вмешательством человека. Он решает проблемы зависимости от ручной разметки и ограниченной адаптивности в динамических средах. ZeroGUI включает автоматическую генерацию задач, оценку вознаграждений и двухэтапное обучение с подкреплением для взаимодействия с ГПИ-средами. Эксперименты показывают значительное повышение производительности агентов в различных средах."
                },
                "en": {
                    "title": "Empowering GUI Agents with Zero Human Cost",
                    "desc": "ZeroGUI is an innovative online learning framework that leverages Vision-Language Models (VLMs) to enhance the training of GUI Agents with minimal human input. It addresses the challenges of traditional offline learning methods, which require extensive manual annotations and struggle to adapt to changing environments. By utilizing VLMs for automatic task generation and reward estimation, ZeroGUI enables continuous learning and interaction with GUI systems. Experiments show that this approach significantly improves the performance of advanced GUI Agents in various environments."
                },
                "zh": {
                    "title": "ZeroGUI：无人工成本的GUI代理训练框架",
                    "desc": "ZeroGUI是一个在线学习框架，利用视觉-语言模型（VLM）进行任务生成和奖励评估，从而在最小人力干预下提升图形用户界面（GUI）代理的性能。该框架解决了传统离线学习方法的两个主要限制：对高质量手动标注的依赖和对动态交互环境的适应性不足。ZeroGUI通过自动生成多样化的训练目标和自动评估任务成功率，来实现无人工成本的GUI代理训练。实验结果表明，ZeroGUI在OSWorld和AndroidLab环境中显著提升了两种先进GUI代理的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23604",
            "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
            "url": "https://huggingface.co/papers/2505.23604",
            "abstract": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.",
            "score": 18,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "3c092222b4cf7a3d",
            "authors": [
                "Guangtao Zeng",
                "Maohao Shen",
                "Delin Chen",
                "Zhenting Qi",
                "Subhro Das",
                "Dan Gutfreund",
                "David Cox",
                "Gregory Wornell",
                "Wei Lu",
                "Zhang-Wei Hong",
                "Chuang Gan"
            ],
            "affiliations": [
                "Department of EECS, MIT",
                "Harvard",
                "MIT-IBM Watson AI Lab, IBM Research",
                "Singapore University of Technology and Design",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23604.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Эволюционное масштабирование для повышения эффективности малых языковых моделей",
                    "desc": "EvoScale - это метод, сочетающий эволюционное обучение и обучение с подкреплением для улучшения работы малых языковых моделей на реальных задачах разработки программного обеспечения. Он итеративно улучшает и уточняет выходные данные модели, смещая распределение в сторону более высоких оценок. EvoScale позволяет 32B модели достичь производительности моделей с более чем 100B параметров, используя меньше образцов. Метод обучает модель самостоятельно улучшать оценки своих собственных генераций без использования внешних верификаторов на этапе вывода."
                },
                "en": {
                    "title": "EvoScale: Evolving Small Models for Big Performance",
                    "desc": "EvoScale is a novel method that combines evolutionary strategies and reinforcement learning to enhance the performance of small language models on real-world software engineering tasks. It addresses the limitations of existing approaches that rely heavily on supervised fine-tuning and expensive data curation. By treating the output generation as an evolutionary process, EvoScale iteratively refines model outputs through selection and mutation, significantly reducing the number of samples needed for effective solutions. This approach allows smaller models to achieve performance levels comparable to much larger models, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "进化测试时间缩放：小模型的强大提升",
                    "desc": "EvoScale是一种基于进化和强化学习的方法，旨在提高小型语言模型在实际软件工程任务中的表现。该方法通过迭代改进和优化输出，减少了寻找正确解决方案所需的样本数量。与传统的监督微调方法相比，EvoScale采用自我进化的方式，使模型能够在推理时自我提升生成结果的评分。经过评估，EvoScale使得32B参数的模型Satori-SWE-32B在性能上能够匹敌或超过100B参数的模型，同时使用的样本数量更少。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23359",
            "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
            "url": "https://huggingface.co/papers/2505.23359",
            "abstract": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on \"test-time scaling\" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench.",
            "score": 18,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "291558dd32e84247",
            "authors": [
                "Yuanxin Liu",
                "Kun Ouyang",
                "Haoning Wu",
                "Yi Liu",
                "Lin Sui",
                "Xinhao Li",
                "Yan Zhong",
                "Y. Charles",
                "Xinyu Zhou",
                "Xu Sun"
            ],
            "affiliations": [
                "Moonshot AI",
                "Nanjing University",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
                "School of Mathematical Sciences, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23359.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Глубокое рассуждение - ключ к пониманию видео искусственным интеллектом",
                    "desc": "VideoReasonBench - это новый бенчмарк для оценки сложных задач рассуждения на основе видео. Он требует от моделей точного запоминания множества операций в видео и пошагового рассуждения для получения правильных ответов. Эксперименты показали, что большинство современных мультимодальных языковых моделей плохо справляются с такими задачами, например, GPT-4o достигает точности всего 6.9%. Исследование выявило, что увеличение времени на обдумывание критически важно для улучшения производительности на VideoReasonBench, в отличие от существующих видео-бенчмарков."
                },
                "en": {
                    "title": "Unlocking Video Reasoning with Extended Thinking Budgets",
                    "desc": "The paper introduces VideoReasonBench, a new benchmark for evaluating complex video reasoning that emphasizes the importance of extended thinking budgets. It highlights that traditional benchmarks often lack the depth of reasoning needed to fully assess video understanding capabilities. The benchmark includes tasks that require models to recall visual information, infer latent states, and predict beyond the video content, thus testing their reasoning skills comprehensively. Results show that most state-of-the-art multimodal language models struggle with these tasks, but those with enhanced reasoning capabilities, like Gemini-2.5-Pro, perform significantly better."
                },
                "zh": {
                    "title": "延长思考时间，提升视频推理能力！",
                    "desc": "本文介绍了一个新的基准测试，名为VideoReasonBench，旨在评估复杂的视觉视频推理能力。研究发现，延长思考时间对于提高模型在此基准上的表现至关重要，尤其是与现有基准相比。VideoReasonBench的任务设计要求模型在视频中回忆多个操作，并进行逐步推理，以得出正确答案。通过对18个最先进的多模态大语言模型的评估，发现大多数模型在复杂视频推理上表现不佳，强调了思考增强的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23716",
            "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
            "url": "https://huggingface.co/papers/2505.23716",
            "abstract": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/",
            "score": 17,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "0f88880e118194a0",
            "authors": [
                "Lihan Jiang",
                "Yucheng Mao",
                "Linning Xu",
                "Tao Lu",
                "Kerui Ren",
                "Yichen Jin",
                "Xudong Xu",
                "Mulin Yu",
                "Jiangmiao Pang",
                "Feng Zhao",
                "Dahua Lin",
                "Bo Dai"
            ],
            "affiliations": [
                "Brown University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "The University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23716.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Синтез новых ракурсов без калибровки камер",
                    "desc": "AnySplat - это нейронная сеть прямого распространения для синтеза новых ракурсов на основе неоткалиброванных наборов изображений. Она предсказывает 3D гауссовы примитивы, кодирующие геометрию и внешний вид сцены, а также параметры камер для входных изображений за один проход. Модель эффективно работает как на разреженных, так и на плотных наборах данных без аннотаций положения камер. AnySplat сопоставима по качеству с методами, использующими информацию о позах камер, и превосходит существующие подходы без использования этой информации."
                },
                "en": {
                    "title": "AnySplat: Real-Time Novel View Synthesis Without Camera Poses",
                    "desc": "AnySplat is a novel feed forward network designed for synthesizing new views from uncalibrated image collections without needing camera poses. Unlike traditional methods that require detailed scene optimization and known camera positions, AnySplat efficiently predicts scene geometry and appearance in a single forward pass. It utilizes 3D Gaussian primitives to represent the scene, allowing it to handle both sparse and dense datasets seamlessly. The model not only matches the quality of pose-aware methods but also significantly reduces rendering time, making real-time novel view synthesis feasible in casual capture scenarios."
                },
                "zh": {
                    "title": "AnySplat：无位姿新视角合成的高效解决方案",
                    "desc": "AnySplat是一种前馈网络，能够在没有相机位姿的情况下进行新视角合成。与传统的神经渲染管道不同，AnySplat不需要已知的相机位姿和逐场景优化，而是通过一次前向传播生成3D高斯原语，编码场景几何和外观信息。该模型能够轻松扩展到多视角数据集，且无需位姿注释。在零样本评估中，AnySplat在稀疏和密集视图场景中与基于位姿的方法质量相当，同时在渲染延迟上大幅降低，适用于实时新视角合成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23660",
            "title": "D-AR: Diffusion via Autoregressive Models",
            "url": "https://huggingface.co/papers/2505.23660",
            "abstract": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at https://github.com/showlab/D-AR",
            "score": 17,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "fb75799d968366cb",
            "authors": [
                "Ziteng Gao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23660.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#diffusion",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "D-AR: Диффузия изображений через авторегрессию",
                    "desc": "Статья представляет новый подход к генерации изображений, называемый Diffusion via Autoregressive models (D-AR). Этот метод преобразует процесс диффузии изображений в стандартную авторегрессионную задачу предсказания следующего токена. D-AR использует токенизатор для преобразования изображений в последовательности дискретных токенов, которые соответствуют различным шагам диффузионного шума. Модель достигает высокого качества генерации изображений с возможностью предварительного просмотра и контроля компоновки, используя архитектуру большой языковой модели."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Autoregressive Diffusion",
                    "desc": "This paper introduces Diffusion via Autoregressive models (D-AR), which reformulates the image diffusion process as a standard autoregressive task. It involves designing a tokenizer that transforms images into sequences of discrete tokens, allowing for different diffusion denoising steps to be decoded from these tokens. The method leverages the natural coarse-to-fine order of the tokens, enabling effective next-token prediction without altering existing autoregressive frameworks. The results demonstrate high-quality image generation with consistent previews and layout control, achieving a notable FID score on the ImageNet benchmark."
                },
                "zh": {
                    "title": "自回归模型：图像生成的新视角",
                    "desc": "本文提出了一种新的图像扩散方法，称为自回归模型扩散（D-AR），将图像扩散过程重新构建为标准的自回归任务。我们设计了一种分词器，将图像转换为离散的标记序列，这些标记在不同位置可以解码为不同的去噪步骤。得益于扩散特性，这些标记自然遵循粗到细的顺序，适合自回归建模。我们的实验表明，该方法在ImageNet基准测试中取得了2.09的FID，展示了在视觉合成中使用大型语言模型的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23646",
            "title": "Are Reasoning Models More Prone to Hallucination?",
            "url": "https://huggingface.co/papers/2505.23646",
            "abstract": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs.",
            "score": 17,
            "issue_id": 4039,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "af43e4853b84c9fe",
            "authors": [
                "Zijun Yao",
                "Yantao Liu",
                "Yanxu Chen",
                "Jianhui Chen",
                "Junfeng Fang",
                "Lei Hou",
                "Juanzi Li",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "School of Computing, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23646.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Галлюцинации в больших моделях рассуждений: причины и решения",
                    "desc": "Это исследование изучает склонность к галлюцинациям у больших моделей рассуждений (LRM) в зависимости от различных методов пост-обучения. Авторы обнаружили, что холодный старт с контролируемой тонкой настройкой и обучение с подкреплением на основе проверяемых наград обычно уменьшают галлюцинации. Выявлены два ключевых когнитивных поведения, влияющих на фактическую точность LRM: повторение ошибок и несоответствие между рассуждениями и ответом. Исследование также показало связь между повышенными галлюцинациями и несоответствием между неопределенностью модели и фактической точностью."
                },
                "en": {
                    "title": "Understanding and Reducing Hallucination in Large Reasoning Models",
                    "desc": "This paper investigates how large reasoning models (LRMs) can produce incorrect information, known as hallucination, and how this varies based on their training methods. It finds that certain post-training techniques, like supervised fine-tuning and reinforcement learning, can reduce hallucination, while others may worsen it. The authors identify two key cognitive behaviors—Flaw Repetition and Think-Answer Mismatch—that contribute to these inaccuracies. Additionally, they explore how the model's uncertainty can misalign with factual correctness, leading to increased hallucination in LRMs."
                },
                "zh": {
                    "title": "理解大型推理模型中的幻觉现象",
                    "desc": "这篇论文探讨了大型推理模型（LRMs）在后训练流程中对幻觉的不同敏感性。研究发现，经过冷启动监督微调和可验证奖励强化学习的后训练流程可以减轻幻觉现象，而单独的蒸馏或没有冷启动微调的强化学习则会引入更多复杂的幻觉。论文还分析了影响LRMs事实性的两种关键认知行为：缺陷重复和思考-回答不匹配。最后，研究表明，LRMs的幻觉增加通常与模型不确定性和事实准确性之间的不一致有关。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22914",
            "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.22914",
            "abstract": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.",
            "score": 16,
            "issue_id": 4040,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "23df6ba515b10bc5",
            "authors": [
                "Maksim Kolodiazhnyi",
                "Denis Tarasov",
                "Dmitrii Zhemchuzhnikov",
                "Alexander Nikulin",
                "Ilya Zisman",
                "Anna Vorontsova",
                "Anton Konushin",
                "Vladislav Kurenkov",
                "Danila Rukhovich"
            ],
            "affiliations": [
                "AIRI Institute",
                "ETH Zurich",
                "Innopolis University",
                "Lomonosov Moscow State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22914.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#3d",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Мультимодальная реконструкция CAD: объединяя зрение, язык и обучение с подкреплением",
                    "desc": "Эта статья представляет новую мультимодальную модель для реконструкции CAD-моделей, использующую визуально-языковые модели и обучение с подкреплением. Модель обрабатывает одновременно облака точек, изображения и текст, что повышает её универсальность и надёжность. Авторы применяют двухэтапный подход: сначала модель обучается на процедурно сгенерированных данных, а затем дообучается с помощью онлайн-алгоритмов обучения с подкреплением. Результаты показывают, что предложенная модель превосходит существующие одномодальные подходы на нескольких наборах данных, включая реальные."
                },
                "en": {
                    "title": "Revolutionizing CAD with Multi-Modal Learning and Reinforcement Techniques",
                    "desc": "This paper presents a multi-modal Computer-Aided Design (CAD) reconstruction model that integrates vision-language models and reinforcement learning to enhance performance across various datasets. Unlike traditional methods that rely on a single input type, this model processes point clouds, images, and text simultaneously, improving its versatility and robustness. The authors employ a two-stage training approach, starting with supervised fine-tuning on large datasets, followed by reinforcement learning to refine the model using real-time feedback. Their results show that this innovative approach achieves state-of-the-art performance, particularly in challenging real-world scenarios, surpassing existing single-modal techniques."
                },
                "zh": {
                    "title": "多模态CAD重建模型：超越单一输入的创新",
                    "desc": "本文提出了一种多模态计算机辅助设计（CAD）重建模型，结合了视觉-语言模型和强化学习，能够在多个数据集上实现最先进的性能。该模型同时处理点云、图像和文本三种输入模态，克服了传统方法仅依赖单一输入的局限性。通过采用两阶段的训练流程，首先在大规模生成数据上进行监督微调，然后利用在线反馈进行强化学习微调，显著提升了模型的表现。实验结果表明，该模型在DeepCAD基准测试中超越了现有的单模态方法，并在多个具有挑战性的数据集上设立了新的性能标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20088",
            "title": "Multi-Domain Explainability of Preferences",
            "url": "https://huggingface.co/papers/2505.20088",
            "abstract": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.",
            "score": 15,
            "issue_id": 4039,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "8f1f413918ce2151",
            "authors": [
                "Nitay Calderon",
                "Liat Ein-Dor",
                "Roi Reichart"
            ],
            "affiliations": [
                "Faculty of Data and Decision Sciences, Technion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20088.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#interpretability",
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны предпочтений в больших языковых моделях",
                    "desc": "Этот научный труд представляет новый автоматизированный метод для объяснения и прогнозирования предпочтений в больших языковых моделях (LLM). Метод использует векторы на основе концепций и иерархическую мультидоменную регрессионную модель. Авторы предлагают способ генерации локальных и глобальных объяснений предпочтений на основе концепций в различных доменах. Исследование демонстрирует высокую эффективность метода в предсказании предпочтений и улучшении выходных данных LLM."
                },
                "en": {
                    "title": "Enhancing Preference Predictions with Concept-Based Explanations in LLMs",
                    "desc": "This paper presents a new automated approach that enhances how we explain and predict preferences in large language models (LLMs) using concept-based vectors and a Hierarchical Multi-Domain Regression model. The method identifies key concepts that differentiate between preferred and non-preferred responses, allowing for better understanding of the underlying preferences. By employing a white-box regression model, it captures both general and specific influences across various domains, leading to improved prediction accuracy. The results demonstrate that this approach not only outperforms existing methods but also provides clear explanations, paving the way for better alignment of LLMs with human preferences."
                },
                "zh": {
                    "title": "新方法提升大型语言模型的偏好解释与预测",
                    "desc": "本文提出了一种新的自动化方法，利用基于概念的向量和层次多领域回归模型，来改善大型语言模型的偏好解释和预测。该方法通过大型语言模型识别区分选择和拒绝响应的概念，并用基于概念的向量表示这些概念。我们提出的白盒层次多领域回归模型能够捕捉领域通用和领域特定的效应，从而建模概念与偏好之间的关系。通过在八个不同领域的数据集上进行评估，我们的方法在偏好预测性能上超越了基线，同时也具备良好的可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23380",
            "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.23380",
            "abstract": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.",
            "score": 14,
            "issue_id": 4037,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "48e60d93b0069d54",
            "authors": [
                "Weijia Mao",
                "Zhenheng Yang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "ByteDance",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23380.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Самосовершенствование мультимодальных ИИ-моделей без внешних данных",
                    "desc": "UniRL - это метод пост-обучения для универсальных мультимодальных языковых моделей, который использует сгенерированные изображения в качестве обучающих данных. Он улучшает как генерацию, так и понимание без использования внешних данных. UniRL позволяет модели генерировать изображения из текстовых запросов и использовать их для обучения в каждой итерации. Метод применяет supervised fine-tuning и Group Relative Policy Optimization для оптимизации моделей."
                },
                "en": {
                    "title": "Self-Improving Multimodal Learning with UniRL",
                    "desc": "UniRL is a novel post-training method designed for unified multimodal large language models, allowing them to improve their performance without needing external data. It generates images from text prompts and uses these images as training data, facilitating a self-improving cycle that enhances both generation and understanding tasks. The approach leverages supervised fine-tuning and Group Relative Policy Optimization to optimize model performance, ensuring that the tasks support each other. UniRL demonstrates significant advantages, including independence from external datasets, improved task performance, and minimal additional training requirements."
                },
                "zh": {
                    "title": "UniRL：自我改进的多模态模型训练方法",
                    "desc": "UniRL是一种自我改进的后训练方法，专为统一的多模态大型语言模型设计。它通过生成图像作为训练数据，增强了生成和理解任务的性能，而无需依赖外部数据。该方法使得生成的图像可以用于理解任务，同时理解的结果又可以指导生成过程，从而实现任务之间的相互促进。UniRL的优势在于不需要外部图像数据、提升了任务性能并减少了生成与理解之间的不平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23419",
            "title": "SWE-bench Goes Live!",
            "url": "https://huggingface.co/papers/2505.23419",
            "abstract": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.",
            "score": 13,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "56fc88c3eb857590",
            "authors": [
                "Linghao Zhang",
                "Shilin He",
                "Chaoyun Zhang",
                "Yu Kang",
                "Bowen Li",
                "Chengxing Xie",
                "Junhao Wang",
                "Maoquan Wang",
                "Yufan Huang",
                "Shengyu Fu",
                "Elsie Nallipogu",
                "Qingwei Lin",
                "Yingnong Dang",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23419.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "SWE-bench-Live: Динамичный эталон для оценки ИИ в реальной разработке ПО",
                    "desc": "Статья представляет SWE-bench-Live - новый эталонный тест для оценки возможностей больших языковых моделей в решении реальных программных ошибок. В отличие от существующих статических тестов, SWE-bench-Live обновляется в реальном времени, охватывает широкий спектр репозиториев и использует автоматизированный процесс создания задач. Тест включает 1319 задач из 93 репозиториев, каждая с собственным Docker-образом для воспроизводимости. Оценка современных агентов и языковых моделей на SWE-bench-Live показала значительный разрыв в производительности по сравнению со статическими тестами."
                },
                "en": {
                    "title": "SWE-bench-Live: A Dynamic Benchmark for Evaluating LLMs in Bug Fixing",
                    "desc": "This paper introduces SWE-bench-Live, a new benchmark for evaluating large language models (LLMs) in the task of generating patches for real-world software bugs. Unlike previous benchmarks like SWE-bench, SWE-bench-Live is continuously updated and includes a broader range of tasks derived from recent GitHub issues, ensuring relevance and diversity. The benchmark features an automated curation pipeline that simplifies the process of creating tasks and setting up environments, reducing manual effort and enhancing scalability. The authors demonstrate that LLMs perform significantly better on SWE-bench-Live compared to static benchmarks, highlighting the importance of using dynamic and up-to-date datasets for evaluation."
                },
                "zh": {
                    "title": "实时更新的基准测试，提升模型评估能力",
                    "desc": "本论文提出了SWE-bench-Live，这是一个可实时更新的基准测试，旨在解决现有基准测试的局限性。现有的SWE-bench及其变体未能及时更新，且依赖于手动构建实例和环境设置，限制了其可扩展性。SWE-bench-Live包含来自2024年后真实GitHub问题的1,319个任务，并提供专用的Docker镜像以确保可重复执行。通过对多种先进的代理框架和大型语言模型进行评估，发现SWE-bench-Live在动态软件开发环境中提供了更为严谨和抗污染的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22618",
            "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
            "url": "https://huggingface.co/papers/2505.22618",
            "abstract": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6times throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.",
            "score": 11,
            "issue_id": 4038,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "7b3d07e1309cacdc",
            "authors": [
                "Chengyue Wu",
                "Hao Zhang",
                "Shuchen Xue",
                "Zhijian Liu",
                "Shizhe Diao",
                "Ligeng Zhu",
                "Ping Luo",
                "Song Han",
                "Enze Xie"
            ],
            "affiliations": [
                "Independent Researcher",
                "MIT",
                "NVIDIA",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22618.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение диффузионных ЯМ без потери качества",
                    "desc": "Статья представляет новые методы для улучшения скорости вывода диффузионных языковых моделей. Авторы предлагают блочный приближенный KV-кэш, адаптированный для двунаправленных диффузионных моделей. Также вводится стратегия параллельного декодирования с учетом уверенности, которая выборочно декодирует токены, превышающие пороговое значение. Эксперименты показывают значительное увеличение пропускной способности при минимальной потере точности."
                },
                "en": {
                    "title": "Boosting Speed and Quality in Diffusion LLMs!",
                    "desc": "This paper presents a new method to enhance the speed of diffusion-based large language models (Diffusion LLMs) during text generation. It introduces a block-wise approximate Key-Value (KV) Cache that allows for efficient reuse of cached information, which helps maintain performance while speeding up inference. Additionally, the authors propose a confidence-aware parallel decoding strategy that addresses quality issues caused by token dependency disruptions during simultaneous decoding. The results show significant improvements in processing speed with minimal loss in accuracy, making Diffusion LLMs more competitive with traditional autoregressive models."
                },
                "zh": {
                    "title": "提升扩散型大语言模型推理速度的创新策略",
                    "desc": "本文提出了一种新颖的块状近似KV缓存机制和基于置信度的并行解码策略，以提高扩散型大语言模型的推理速度，而不会显著降低质量。扩散型大语言模型在非自回归文本生成中表现出色，但由于缺乏KV缓存和并行解码时的质量下降，实际推理速度常常落后于自回归模型。我们通过引入块状近似KV缓存机制，允许缓存重用，几乎不影响性能。同时，我们提出的基于置信度的并行解码策略可以选择性地解码超过置信度阈值的标记，从而减轻依赖性违反，保持生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23606",
            "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
            "url": "https://huggingface.co/papers/2505.23606",
            "abstract": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.",
            "score": 10,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "2bbc295244e31cb2",
            "authors": [
                "Qingyu Shi",
                "Jinbin Bai",
                "Zhuoran Zhao",
                "Wenhao Chai",
                "Kaidong Yu",
                "Jianzong Wu",
                "Shuangyong Song",
                "Yunhai Tong",
                "Xiangtai Li",
                "Xuelong Li",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Princeton University",
                "TeleAI, China Telecom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23606.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Унифицированная мультимодальная генерация с помощью дискретной диффузии",
                    "desc": "Muddit - это унифицированный дискретный диффузионный трансформер, объединяющий предобученные визуальные приоры с легковесным текстовым декодером. Он позволяет быстро и качественно генерировать как текст, так и изображения в рамках единой архитектуры. Muddit превосходит по эффективности и качеству значительно более крупные авторегрессионные модели. Это исследование демонстрирует потенциал чисто дискретной диффузии с сильными визуальными приорами как масштабируемой и эффективной основы для унифицированной генерации."
                },
                "en": {
                    "title": "Muddit: Fast and High-Quality Unified Generation for Text and Images",
                    "desc": "Muddit is a new model that combines text and image generation in a single framework, making it faster and more efficient. It uses a discrete diffusion approach, which allows for parallel processing, unlike traditional models that generate outputs one step at a time. By incorporating pretrained visual knowledge from existing models, Muddit enhances the quality of its outputs while maintaining a lightweight structure for text decoding. The results show that Muddit performs as well or better than larger models, demonstrating the effectiveness of using strong visual priors in unified generation tasks."
                },
                "zh": {
                    "title": "Muddit：快速高效的多模态生成",
                    "desc": "Muddit是一种统一的离散扩散变换器，能够在文本和图像模态中实现快速且高质量的生成。它通过将预训练的视觉先验与轻量级文本解码器相结合，克服了自回归模型推理速度慢和非自回归模型泛化能力弱的问题。Muddit在统一架构下实现了灵活的多模态生成，实验结果表明其在质量和效率上优于许多更大的自回归模型。该研究展示了当配备强大的视觉先验时，纯离散扩散模型作为统一生成的可扩展和有效的基础架构的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23585",
            "title": "On-Policy RL with Optimal Reward Baseline",
            "url": "https://huggingface.co/papers/2505.23585",
            "abstract": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.",
            "score": 9,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "8a27563b11999352",
            "authors": [
                "Yaru Hao",
                "Li Dong",
                "Xun Wu",
                "Shaohan Huang",
                "Zewen Chi",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "OPO: Стабильное обучение с подкреплением для улучшения языковых моделей",
                    "desc": "Статья представляет новый алгоритм обучения с подкреплением под названием OPO (On-Policy RL with Optimal reward baseline). OPO решает проблемы нестабильности и неэффективности существующих алгоритмов при обучении больших языковых моделей. Основные особенности OPO - точное обучение on-policy и оптимальная базовая линия вознаграждения, что улучшает стабильность и исследование. Эксперименты показывают превосходство OPO в задачах математических рассуждений, а также более разнообразные и менее повторяющиеся ответы модели."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning for Language Models with OPO",
                    "desc": "This paper introduces a new reinforcement learning algorithm called On-Policy RL with Optimal reward baseline (OPO) to improve the training of large language models. OPO focuses on exact on-policy training, which helps stabilize the learning process and encourages better exploration of solutions. The algorithm also incorporates an optimal reward baseline that reduces gradient variance, leading to more reliable updates during training. Evaluations show that OPO outperforms existing methods in terms of stability and diversity of responses, making it a strong candidate for aligning language models with human preferences."
                },
                "zh": {
                    "title": "OPO：稳定高效的强化学习新方向",
                    "desc": "本研究提出了一种新的强化学习算法，称为最优奖励基线的在线强化学习（OPO），旨在解决当前算法在训练不稳定性和计算效率方面的问题。OPO强调精确的在线训练，这在实践中稳定了训练过程并增强了探索能力。此外，OPO引入了最优奖励基线，理论上可以最小化梯度方差。通过在数学推理基准上的评估，OPO展示了其优越的性能和训练稳定性，且无需额外的模型或正则化项。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22421",
            "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
            "url": "https://huggingface.co/papers/2505.22421",
            "abstract": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.",
            "score": 9,
            "issue_id": 4036,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "2a5e22b54a576dad",
            "authors": [
                "Anthony Chen",
                "Wenzhao Zheng",
                "Yida Wang",
                "Xueyang Zhang",
                "Kun Zhan",
                "Peng Jia",
                "Kurt Keutzer",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Li Auto Inc.",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22421.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#agents",
                    "#3d",
                    "#training"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "GeoDrive: 3D-геометрия для безопасного автономного вождения",
                    "desc": "GeoDrive - это новый подход к моделированию мира для автономного вождения, который интегрирует надежную 3D-геометрию для улучшения пространственного понимания и управляемости действий. Модель извлекает 3D-представление из входного кадра и получает его 2D-рендеринг на основе заданной траектории автомобиля. Во время обучения используется модуль динамического редактирования для улучшения рендеринга путем изменения положения транспортных средств. Эксперименты показывают, что GeoDrive превосходит существующие модели по точности действий и пространственному пониманию, обеспечивая более реалистичное и надежное моделирование сцен для безопасного автономного вождения."
                },
                "en": {
                    "title": "Enhancing Autonomous Navigation with Robust 3D Geometry",
                    "desc": "GeoDrive is a novel approach that enhances autonomous driving by integrating robust 3D geometry into world models, improving spatial awareness and action control. It addresses limitations in current models, such as maintaining 3D geometric consistency and handling occlusions, which are crucial for safety assessments. By extracting 3D representations and dynamically editing vehicle positions during training, GeoDrive enables more accurate action predictions and realistic scene modeling. The method shows significant improvements in action accuracy and adaptability, making it a promising solution for safer autonomous navigation."
                },
                "zh": {
                    "title": "GeoDrive：提升自主驾驶的空间理解与安全性",
                    "desc": "GeoDrive 是一种将稳健的三维几何体集成到驾驶世界模型中的方法，旨在提高自主导航中的空间理解和动作可控性，从而增强安全性和可靠性。该方法通过从输入帧中提取三维表示，并根据用户指定的自车轨迹生成二维渲染，来实现动态建模。GeoDrive 还引入了动态编辑模块，以增强渲染效果，允许在训练过程中编辑车辆位置。实验结果表明，GeoDrive 在动作准确性和三维空间意识方面显著优于现有模型，能够实现更真实、适应性强且可靠的场景建模。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23758",
            "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
            "url": "https://huggingface.co/papers/2505.23758",
            "abstract": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.",
            "score": 8,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "383f00e023ee6237",
            "authors": [
                "Yusuf Dalva",
                "Hidir Yesiltepe",
                "Pinar Yanardag"
            ],
            "affiliations": [
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23758.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#story_generation",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Умное редактирование изображений с помощью ИИ",
                    "desc": "LoRAShop - это новая система для редактирования изображений с использованием нескольких концепций на основе моделей LoRA. Она использует пространственно согласованную активацию признаков в диффузионных трансформерах типа Flux для плавной интеграции множественных объектов или стилей. LoRAShop создает разделенную латентную маску для каждого концепта и смешивает соответствующие веса LoRA только в нужных областях. Это позволяет сохранить глобальный контекст, освещение и мелкие детали исходной сцены."
                },
                "en": {
                    "title": "Seamless Multi-Concept Image Editing with LoRAShop",
                    "desc": "LoRAShop is a novel framework designed for multi-concept image editing using LoRA models. It utilizes the unique feature activation patterns in Flux-style diffusion transformers to create a disentangled latent mask for each concept, allowing for precise blending of different styles or subjects. This method ensures that the edits maintain the original scene's global context, lighting, and intricate details. The framework enhances identity preservation in images and simplifies the editing process by removing the need for retraining, making it a powerful tool for creative visual storytelling."
                },
                "zh": {
                    "title": "LoRAShop：个性化图像编辑的新工具",
                    "desc": "LoRAShop是一个用于多概念图像编辑的框架，利用LoRA模型。它基于Flux风格扩散变换器中的特征交互模式，能够在去噪过程中早期激活空间一致的特征区域。通过为每个概念生成解耦的潜在掩码，LoRAShop可以在特定区域内混合相应的LoRA权重，从而实现多个主题或风格的无缝整合。实验表明，LoRAShop在保持身份一致性方面优于其他基线方法，成为一种实用的个性化扩散模型工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23735",
            "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
            "url": "https://huggingface.co/papers/2505.23735",
            "abstract": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark.",
            "score": 8,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "e9763dde29798ac1",
            "authors": [
                "Ali Behrouz",
                "Zeman Li",
                "Praneeth Kacham",
                "Majid Daliri",
                "Yuan Deng",
                "Peilin Zhong",
                "Meisam Razaviyayn",
                "Vahab Mirrokni"
            ],
            "affiliations": [
                "Google"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23735.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ATLAS: Революция в долговременной памяти нейросетей",
                    "desc": "Статья представляет ATLAS - новый модуль долговременной памяти для нейронных сетей. ATLAS преодолевает ограничения современных рекуррентных нейронных сетей, оптимизируя память на основе текущих и прошлых токенов. На базе ATLAS авторы создали семейство архитектур DeepTransformers, обобщающих оригинальную архитектуру Transformer. Эксперименты показывают превосходство ATLAS над трансформерами и линейными рекуррентными моделями в задачах языкового моделирования и понимания длинного контекста."
                },
                "en": {
                    "title": "ATLAS: Revolutionizing Long-Term Memory in Transformers",
                    "desc": "This paper introduces ATLAS, a long-term memory module designed to enhance the performance of sequence modeling tasks. It addresses the limitations of traditional architectures, particularly in handling long contexts, by optimizing memory based on both current and past inputs. The authors propose DeepTransformers, a new family of architectures that generalize the original Transformer model while improving memory capacity and management. Experimental results demonstrate that ATLAS outperforms existing models, achieving significant accuracy improvements in tasks requiring long-context understanding."
                },
                "zh": {
                    "title": "ATLAS：超越传统的长时记忆模块",
                    "desc": "本文介绍了一种新的长时记忆模块ATLAS，旨在解决现有模型在处理长序列时的局限性。ATLAS通过优化记忆结构，能够更好地存储和利用上下文信息，从而克服了传统长时记忆模型的在线更新问题。研究表明，ATLAS在语言建模、常识推理和长上下文理解等任务中表现优于现有的Transformer和线性递归模型。最终，ATLAS在BABILong基准测试中实现了10M上下文长度的+80\\%准确率，展示了其强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23416",
            "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
            "url": "https://huggingface.co/papers/2505.23416",
            "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.",
            "score": 8,
            "issue_id": 4038,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "6ba9403ec041f5ac",
            "authors": [
                "Jang-Hyun Kim",
                "Jinuk Kim",
                "Sangwoo Kwon",
                "Jae W. Lee",
                "Sangdoo Yun",
                "Hyun Oh Song"
            ],
            "affiliations": [
                "NAVER AI Lab",
                "Neural Processing Research Center",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23416.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "KVzip: Эффективное сжатие кэша для ускорения языковых моделей",
                    "desc": "Статья представляет KVzip - метод сжатия кэша ключ-значение (KV) для больших языковых моделей на основе трансформеров. KVzip оценивает важность пар KV, используя саму модель для реконструкции исходного контекста, и удаляет менее важные пары. Эмпирические исследования показывают, что KVzip уменьшает размер KV-кэша в 3-4 раза и латентность декодирования FlashAttention примерно в 2 раза при незначительной потере производительности. Метод значительно превосходит существующие подходы к сжатию KV-кэша, особенно в сценариях с несколькими запросами."
                },
                "en": {
                    "title": "Efficient KV Cache Management with KVzip",
                    "desc": "This paper presents KVzip, a novel method for managing key-value (KV) caches in transformer-based large language models (LLMs) during inference. As the context length increases, traditional KV caches become large and slow, but KVzip allows for efficient reuse of compressed caches across different queries. It evaluates the importance of KV pairs using the LLM's ability to reconstruct contexts, allowing less important pairs to be evicted. The results show that KVzip can reduce cache size significantly while improving decoding speed and maintaining performance across various tasks and models."
                },
                "zh": {
                    "title": "KVzip：高效的KV缓存管理方法",
                    "desc": "本文介绍了一种名为KVzip的缓存驱逐方法，旨在优化基于Transformer的大型语言模型（LLM）中的键值（KV）缓存。随着上下文长度的增加，KV缓存的大小也随之扩大，导致内存开销和注意力延迟显著增加。KVzip通过量化KV对的重要性，能够有效地重用压缩的KV缓存，从而在多种查询中实现更高效的缓存管理。实验证明，KVzip可以将KV缓存大小减少3-4倍，并将FlashAttention解码延迟降低约2倍，同时在问答、检索、推理和代码理解任务中几乎没有性能损失。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23559",
            "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
            "url": "https://huggingface.co/papers/2505.23559",
            "abstract": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce SafeScientist, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose SciSafetyBench, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. red{Warning: this paper contains example data that may be offensive or harmful.}",
            "score": 7,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "47b1655c578567ee",
            "authors": [
                "Kunlun Zhu",
                "Jiaxun Zhang",
                "Ziheng Qi",
                "Nuoxing Shang",
                "Zijia Liu",
                "Peixuan Han",
                "Yue Su",
                "Haofei Yu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23559.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#healthcare",
                    "#ethics",
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#security"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Безопасный ИИ-ученый: этичные исследования без компромиссов",
                    "desc": "SafeScientist - это фреймворк искусственного интеллекта, который повышает безопасность научных исследований, проводимых с помощью ИИ, используя несколько защитных механизмов. Он отказывается от этически неприемлемых или высокорисковых задач и делает акцент на безопасности на протяжении всего исследовательского процесса. SafeScientist включает в себя мониторинг промптов, взаимодействия агентов, использования инструментов, а также компонент этической проверки. Эффективность фреймворка подтверждена с помощью специально разработанного бенчмарка SciSafetyBench."
                },
                "en": {
                    "title": "Enhancing Safety in AI-Driven Science with SafeScientist",
                    "desc": "SafeScientist is an AI framework designed to improve safety in AI-driven scientific research by implementing various defensive mechanisms. It proactively avoids engaging in ethically questionable or high-risk tasks, ensuring a focus on safety throughout the research process. The framework includes features like prompt monitoring and agent collaboration oversight, which help maintain ethical standards. Additionally, SafeScientist is validated using the SciSafetyBench benchmark, demonstrating a 35% improvement in safety performance compared to traditional AI frameworks while maintaining high-quality scientific output."
                },
                "zh": {
                    "title": "安全科学家：提升AI科学研究的安全性",
                    "desc": "SafeScientist是一个人工智能框架，旨在通过多种防御机制提高AI驱动科学研究的安全性。该框架能够主动拒绝不道德或高风险的任务，并在整个研究过程中强调安全性。我们还提出了SciSafetyBench，这是一个专门设计的基准，用于评估科学领域中AI的安全性，涵盖240个高风险科学任务。实验结果表明，SafeScientist在安全性能上比传统AI科学家框架提高了35%，同时不影响科学输出的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22961",
            "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
            "url": "https://huggingface.co/papers/2505.22961",
            "abstract": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.",
            "score": 7,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "13c778698d292f73",
            "authors": [
                "Peixuan Han",
                "Zijia Liu",
                "Jiaxuan You"
            ],
            "affiliations": [
                "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22961.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ToMAP: ИИ-убеждающий с пониманием оппонента",
                    "desc": "Статья представляет ToMAP - новый подход к созданию более гибких агентов-убеждающих с использованием модулей теории разума. ToMAP улучшает осведомленность ИИ-убеждающего о мыслях и мнениях оппонента, что позволяет генерировать более эффективные аргументы. Эксперименты показывают, что ToMAP превосходит более крупные базовые модели, демонстрируя сложные цепочки рассуждений и меньшую повторяемость. Этот метод особенно эффективен для длительных бесед и использования более логичных и ориентированных на оппонента стратегий убеждения."
                },
                "en": {
                    "title": "Empowering Persuasion with Theory of Mind",
                    "desc": "This paper presents Theory of Mind Augmented Persuader (ToMAP), a new method that enhances large language models (LLMs) for persuasive tasks by integrating Theory of Mind (ToM) modules. These modules allow the persuader to better understand and anticipate the thoughts and objections of opponents, leading to improved argument quality and diversity. By employing a reinforcement learning approach, ToMAP trains the persuader to analyze opponent-related information effectively, resulting in more logical and nuanced arguments. Experimental results demonstrate that ToMAP significantly outperforms larger models, showcasing its potential for creating advanced persuasive agents."
                },
                "zh": {
                    "title": "提升说服力的理论心智增强模型",
                    "desc": "本论文提出了一种名为ToMAP的理论心智增强说服者模型，旨在提高大型语言模型（LLM）在说服过程中的对手意识和论证质量。通过引入两个理论心智模块，ToMAP能够更好地分析对手的心理状态，从而生成更有效的论点。研究表明，尽管ToMAP的参数量仅为30亿，但在多个说服模型和不同语料库上，其表现超过了更大规模的基线模型，如GPT-4o，提升幅度达到39.4%。此外，ToMAP在训练过程中展现出复杂的推理链和减少重复的能力，使其在长时间对话中更具逻辑性和灵活性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20755",
            "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
            "url": "https://huggingface.co/papers/2505.20755",
            "abstract": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a theory-driven framework which we name the \\emph{Uni-Instruct}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the f-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded f-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded f-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \\emph{1.46} for unconditional generation and \\emph{1.38} for conditional generation. On the ImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \\emph{1.02}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.",
            "score": 6,
            "issue_id": 4037,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "099d5bf1fb188745",
            "authors": [
                "Yifei Wang",
                "Weimin Bai",
                "Colin Zhang",
                "Debing Zhang",
                "Weijian Luo",
                "He Sun"
            ],
            "affiliations": [
                "Academy for Advanced Interdisciplinary Studies, Peking University",
                "College of Future Technology, Peking University",
                "National Biomedical Imaging Center, Peking University",
                "Xiaohongshu Inc",
                "Yuanpei College, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20755.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#transfer_learning",
                    "#cv",
                    "#diffusion",
                    "#dataset",
                    "#math",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Uni-Instruct: Революция в одношаговой дистилляции диффузионных моделей",
                    "desc": "Статья представляет Uni-Instruct - новый подход к одношаговой дистилляции диффузионных моделей. Авторы объединяют более 10 существующих методов в единую теоретическую структуру, основанную на теории диффузионного расширения f-дивергенции. Uni-Instruct достигает рекордных показателей FID в задачах генерации изображений на наборах данных CIFAR10 и ImageNet-64x64. Метод также демонстрирует улучшенные результаты в генерации 3D-объектов по текстовому описанию."
                },
                "en": {
                    "title": "Uni-Instruct: Unifying Diffusion for Superior Image and 3D Generation",
                    "desc": "The paper presents Uni-Instruct, a framework that integrates over ten existing one-step diffusion distillation methods into a cohesive theory-driven approach. It introduces a novel diffusion expansion theory based on the f-divergence family, which addresses the challenges of training one-step diffusion models. By minimizing an equivalent and tractable loss derived from this theory, Uni-Instruct achieves state-of-the-art performance in both unconditional and conditional image generation tasks. Additionally, it demonstrates effectiveness in text-to-3D generation, surpassing previous methods in quality and diversity."
                },
                "zh": {
                    "title": "Uni-Instruct：单步扩散蒸馏的新统一理论",
                    "desc": "本文提出了Uni-Instruct，这是一种统一和增强单步扩散蒸馏方法的新理论框架。通过引入扩散扩展理论，Uni-Instruct成功地整合了十多种现有的单步扩散蒸馏方法，并解决了原始扩展f散度的不可处理性问题。该方法在无条件和条件图像生成以及文本到3D生成任务中，均取得了最先进的性能，特别是在CIFAR10和ImageNet基准测试中表现优异。Uni-Instruct的理论和实证贡献将为未来的单步扩散蒸馏和扩散模型的知识转移研究提供重要支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23742",
            "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
            "url": "https://huggingface.co/papers/2505.23742",
            "abstract": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF",
            "score": 5,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "7234edb0d22029a4",
            "authors": [
                "Yufan Deng",
                "Xun Guo",
                "Yuanyang Yin",
                "Jacob Zhiyuan Fang",
                "Yiding Yang",
                "Yizhi Wang",
                "Shenghai Yuan",
                "Angtian Wang",
                "Bo Liu",
                "Haibin Huang",
                "Chongyang Ma"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.23742.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MAGREF: Революция в генерации видео с несколькими объектами",
                    "desc": "Статья представляет MAGREF - новую систему для генерации видео на основе нескольких референсных изображений и текстового описания. Авторы предлагают механизм динамического маскирования для гибкой обработки различных объектов и фона, а также механизм конкатенации каналов для лучшего сохранения признаков внешнего вида. MAGREF превосходит существующие методы в качестве генерации видео с несколькими объектами, обеспечивая согласованный синтез и точный контроль над отдельными субъектами. Авторы также представляют новый набор данных для оценки качества генерации видео с несколькими объектами."
                },
                "en": {
                    "title": "MAGREF: Mastering Multi-Subject Video Generation with Flexibility and Precision",
                    "desc": "This paper presents MAGREF, a new framework for generating videos that can include multiple subjects from various reference images and text prompts. It introduces a masked guidance technique that allows the model to adaptively focus on different subjects like people and objects without needing to change its structure. Additionally, it employs a pixel-wise channel concatenation method to maintain the visual features of the subjects during generation. The results show that MAGREF achieves superior video quality and consistency compared to existing methods, making it a significant advancement in multi-subject video synthesis."
                },
                "zh": {
                    "title": "MAGREF：高质量多主体视频生成的新框架",
                    "desc": "本论文提出了一种名为MAGREF的统一框架，用于基于任意参考图像的视频生成。该框架引入了掩蔽引导机制，以实现多主体视频合成，确保生成的一致性和高质量。我们提出的动态掩蔽机制能够灵活处理不同的主体推断，而像素级通道连接机制则更好地保留外观特征。实验结果表明，MAGREF在多主体视频合成中表现出色，超越了现有的开源和商业基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23754",
            "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.23754",
            "abstract": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.  \t\t\t\t\tAI-generated summary \t\t\t\t Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.",
            "score": 4,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "8ef15301bb840f49",
            "authors": [
                "Ziyin Zhang",
                "Jiahao Xu",
                "Zhiwei He",
                "Tian Liang",
                "Qiuzhi Liu",
                "Yansi Li",
                "Linfeng Song",
                "Zhengwen Liang",
                "Zhuosheng Zhang",
                "Rui Wang",
                "Zhaopeng Tu",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23754.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в автоматическом доказательстве теорем с помощью естественного языка",
                    "desc": "DeepTheorem - это комплексная система для неформального доказательства теорем с использованием больших языковых моделей (LLM). Она включает в себя масштабный набор данных из 121 тысячи высококачественных неформальных теорем и доказательств уровня IMO. Система использует специально разработанную стратегию обучения с подкреплением (RL-Zero) для улучшения математических рассуждений LLM. DeepTheorem демонстрирует значительное улучшение производительности LLM в доказательстве теорем по сравнению с существующими методами."
                },
                "en": {
                    "title": "Revolutionizing Theorem Proving with DeepTheorem",
                    "desc": "DeepTheorem is a new framework that improves how large language models (LLMs) prove theorems using natural language. It addresses the gap between traditional automated theorem proving methods and the informal reasoning strengths of LLMs. The framework includes a large dataset of 121,000 informal theorems and proofs, which are carefully annotated and designed to enhance mathematical reasoning. By employing a specialized reinforcement learning strategy, DeepTheorem significantly boosts the performance of LLMs in theorem proving tasks, achieving top results in accuracy and reasoning quality."
                },
                "zh": {
                    "title": "DeepTheorem：提升定理证明的创新框架",
                    "desc": "DeepTheorem 是一个增强大型语言模型（LLM）定理证明能力的框架，利用大规模自然语言数据集和定制的强化学习策略。它包含121,000个高质量的非正式定理和证明，覆盖多个数学领域，并经过严格标注。通过引入专门针对非正式定理证明的强化学习策略（RL-Zero），DeepTheorem 能够有效提升数学推理能力。实验结果表明，DeepTheorem 在定理证明的准确性和推理质量上达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23387",
            "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
            "url": "https://huggingface.co/papers/2505.23387",
            "abstract": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.",
            "score": 4,
            "issue_id": 4043,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "1538cb2eddf848da",
            "authors": [
                "Mingzhe Du",
                "Luu Tuan Tuan",
                "Yue Liu",
                "Yuhao Qing",
                "Dong Huang",
                "Xinyi He",
                "Qian Liu",
                "Zejun Ma",
                "See-kiong Ng"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University",
                "National University of Singapore",
                "The University of Hong Kong",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23387.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Обучение с подкреплением: ключ к непрерывному улучшению эффективности кода",
                    "desc": "Эта статья представляет новую систему оптимизации кода, генерируемого большими языковыми моделями (LLM), с использованием обучения с подкреплением. Авторы предлагают итеративный подход, где LLM постоянно улучшает код на основе обратной связи о его производительности. Сравниваются три стратегии обучения: Supervised Fine-Tuning, Direct Preference Optimization и Group Relative Policy Optimization. Результаты показывают, что метод GRPO, основанный на обучении с подкреплением, значительно повышает эффективность кода и превосходит другие подходы."
                },
                "en": {
                    "title": "Reinforcement Learning: The Key to Self-Improving Code Efficiency",
                    "desc": "This paper presents a new method for improving the efficiency of code generated by large language models (LLMs) using reinforcement learning (RL). The proposed framework allows LLMs to iteratively refine their code based on real-time performance feedback from a testing environment. The authors compare different training strategies, finding that while some methods quickly reach a limit in efficiency gains, the RL approach continues to enhance performance over time. The results show significant improvements in both the success rate of code generation and the ability to surpass human-generated code in efficiency."
                },
                "zh": {
                    "title": "利用强化学习提升代码效率的创新框架",
                    "desc": "本文提出了一种新颖的测试时迭代优化框架，利用强化学习不断提升大型语言模型生成代码的效率。虽然大型语言模型（LLMs）能够生成功能正确的解决方案，但在代码效率上常常存在不足，这对实际应用构成了瓶颈。我们采用闭环系统，让LLMs根据执行沙箱的反馈迭代优化代码，并探索了三种训练策略：监督微调（SFT）、直接偏好优化（DPO）和组相对策略优化（GRPO）。实验结果表明，GRPO通过强化学习与执行反馈的结合，能够持续优化代码性能，显著提高了通过率和超越人类提交的效率可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21114",
            "title": "Differentiable Solver Search for Fast Diffusion Sampling",
            "url": "https://huggingface.co/papers/2505.21114",
            "abstract": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.",
            "score": 4,
            "issue_id": 4040,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 мая",
                "en": "May 27",
                "zh": "5月27日"
            },
            "hash": "7c5213dc05c3e3e4",
            "authors": [
                "Shuai Wang",
                "Zexian Li",
                "Qipeng zhang",
                "Tianhui Song",
                "Xubin Li",
                "Tiezheng Ge",
                "Bo Zheng",
                "Limin Wang"
            ],
            "affiliations": [
                "Shanghai AI Lab, Shanghai, China",
                "State Key Lab of Novel Software Technology, Nanjing University, Nanjing, China",
                "Taobao & Tmall Group of Alibaba, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.21114.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#data",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективный поиск решателя для ускорения диффузионных моделей",
                    "desc": "Исследователи предлагают новый алгоритм дифференцируемого поиска решателя для оптимизации вычислительной эффективности и качества диффузионных моделей в задачах генерации изображений. Они показывают, что интерполяция Лагранжа, связанная со временем, неоптимальна для диффузионных моделей, и раскрывают компактное пространство поиска, состоящее из временных шагов и коэффициентов решателя. Предложенный алгоритм позволяет идентифицировать более оптимальный решатель, который значительно превосходит традиционные решатели. Найденный решатель демонстрирует универсальность для различных архитектур моделей, разрешений и размеров моделей."
                },
                "en": {
                    "title": "Optimizing Diffusion Models with a Smart Solver Search",
                    "desc": "This paper introduces a new algorithm for optimizing solver efficiency in diffusion models used for generating images. The authors identify that traditional t-related Lagrange interpolation methods are not the best choice for these models, leading to a compact search space for time steps and solver coefficients. They propose a differentiable solver search algorithm that finds more effective solvers, resulting in improved image generation quality with fewer computational steps. The new solvers significantly enhance performance across different model architectures and sizes, achieving lower FID scores compared to conventional methods."
                },
                "zh": {
                    "title": "优化扩散模型的求解器搜索算法",
                    "desc": "研究人员提出了一种新颖的可微分求解器搜索算法，旨在优化图像生成任务中扩散模型的计算效率和质量。扩散模型虽然生成质量出色，但需要大量的函数评估。我们发现，基于时间相关的拉格朗日插值在扩散模型中并不是最优的，并揭示了一个由时间步和求解器系数组成的紧凑搜索空间。基于我们的分析，提出的可微分求解器搜索算法能够识别更优的求解器，从而显著提高模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17818",
            "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
            "url": "https://huggingface.co/papers/2505.17818",
            "abstract": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.",
            "score": 4,
            "issue_id": 4036,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "affe6411f2c6037a",
            "authors": [
                "Daeun Kyung",
                "Hyunseung Chung",
                "Seongsu Bae",
                "Jiho Kim",
                "Jae Ho Sohn",
                "Taerim Kim",
                "Soo Kyung Kim",
                "Edward Choi"
            ],
            "affiliations": [
                "Ewha Womans University",
                "KAIST",
                "Samsung Medical Center",
                "UCSF"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17818.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#healthcare",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Реалистичная симуляция пациентов для обучения ИИ в медицине",
                    "desc": "PatientSim - это симулятор, генерирующий разнообразные и реалистичные профили пациентов для оценки языковых моделей в медицинских диалогах. Он использует клинические данные из реальных баз MIMIC-ED и MIMIC-IV, а также создает персонажей на основе четырех осей: личность, владение языком, уровень памяти о медицинской истории и когнитивная путаница. Система была протестирована на восьми языковых моделях и валидирована клиницистами. PatientSim предлагает масштабируемое решение для обучения и оценки медицинских диалоговых систем."
                },
                "en": {
                    "title": "PatientSim: Realistic Patient Personas for Better Medical Dialogue",
                    "desc": "PatientSim is a patient simulator designed to create realistic and diverse patient personas using clinical data, which is essential for training and evaluating large language models (LLMs) in medical dialogue. It utilizes real-world clinical profiles from the MIMIC-ED and MIMIC-IV datasets and defines personas based on personality traits, language skills, medical history recall, and cognitive confusion, resulting in 37 unique combinations. The framework was tested with eight LLMs for their factual accuracy and consistency with the generated personas, with Llama 3.3 emerging as the top performer validated by clinicians. As an open-source platform, PatientSim offers a customizable and privacy-compliant environment for evaluating medical dialogue systems, making it a valuable tool for both training and educational purposes in healthcare."
                },
                "zh": {
                    "title": "PatientSim：多样化患者角色的生成与评估",
                    "desc": "PatientSim 是一个生成多样化和真实患者角色的模拟器，旨在评估医疗对话中的大型语言模型（LLMs）。它利用真实的临床数据，创建基于症状和病史的患者档案，并通过个性、语言能力、病史回忆水平和认知混淆水平四个维度定义患者角色。通过这种方式，PatientSim 生成了 37 种独特的患者组合，能够更好地反映临床实践中的多样性。该平台不仅为医疗对话系统提供了一个可重复和可扩展的测试环境，还为医疗教育提供了潜在的工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23745",
            "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
            "url": "https://huggingface.co/papers/2505.23745",
            "abstract": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.",
            "score": 3,
            "issue_id": 4035,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "55ac97c649fb77b4",
            "authors": [
                "Hao Dong",
                "Moru Liu",
                "Jian Liang",
                "Eleni Chatzi",
                "Olga Fink"
            ],
            "affiliations": [
                "EPFL",
                "ETH Zürich",
                "NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Technical University of Munich",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23745.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#architecture",
                    "#security"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение надежности мультимодальных моделей без переобучения",
                    "desc": "TrustVLM - это новый подход к повышению надежности мультимодальных моделей машинного обучения, работающих с изображениями и текстом. Он позволяет оценивать достоверность предсказаний модели без необходимости ее переобучения. TrustVLM использует особенности пространства эмбеддингов изображений для улучшения обнаружения ошибочных классификаций. Метод показал значительное улучшение по сравнению с существующими базовыми подходами на 17 различных наборах данных."
                },
                "en": {
                    "title": "Trust Your Vision-Language Model with TrustVLM!",
                    "desc": "TrustVLM is a framework that enhances the reliability of Vision-Language Models (VLMs) by estimating the trustworthiness of their predictions without the need for retraining. It addresses the issue of misclassification in VLMs, which can produce confident but incorrect outputs, especially in critical applications. By introducing a novel confidence-scoring function that utilizes the image embedding space, TrustVLM improves the detection of misclassifications. The framework has been rigorously tested across multiple datasets and architectures, showing significant performance improvements in reliability metrics."
                },
                "zh": {
                    "title": "提升视觉-语言模型的可信度",
                    "desc": "TrustVLM 是一种增强视觉-语言模型（VLM）可靠性的框架，它可以在不重新训练的情况下评估预测的可信度。该方法通过引入一种新的置信评分函数，利用图像嵌入空间来改善错误分类的检测。研究表明，TrustVLM 在 17 个不同的数据集上进行了严格评估，显示出在多种指标上显著优于现有基线。通过提高模型的可靠性，TrustVLM 为 VLM 在实际应用中的安全部署铺平了道路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23253",
            "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
            "url": "https://huggingface.co/papers/2505.23253",
            "abstract": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.",
            "score": 3,
            "issue_id": 4038,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "1882fe9c03da1e58",
            "authors": [
                "Yixun Liang",
                "Kunming Luo",
                "Xiao Chen",
                "Rui Chen",
                "Hongyu Yan",
                "Weiyu Li",
                "Jiarui Liu",
                "Ping Tan"
            ],
            "affiliations": [
                "HKUST",
                "Light Illusion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23253.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в 3D-текстурировании: UniTEX объединяет функциональное пространство и нейросети",
                    "desc": "UniTEX - это новая двухэтапная система для генерации высококачественных и согласованных 3D-текстур. Она использует Текстурные Функции (TF) для работы непосредственно в трехмерном пространстве, избегая ограничений UV-маппинга. Система включает Большую Модель Текстурирования (LTM) на основе трансформеров для предсказания TF из изображений и геометрии. UniTEX также применяет адаптированные Диффузионные Трансформеры (DiT) для улучшения качества текстур на первом этапе."
                },
                "en": {
                    "title": "Revolutionizing 3D Texture Generation with UniTEX",
                    "desc": "UniTEX is a new framework designed for generating high-quality 3D textures without the need for UV mapping. It uses Texture Functions to create a continuous representation of textures based on 3D spatial data, allowing for better consistency and quality. The framework employs a transformer-based model to predict these textures directly from images and geometry, enhancing the process of texture synthesis. By avoiding traditional UV-based methods, UniTEX addresses common challenges in 3D texture generation, resulting in superior visual quality and integrity."
                },
                "zh": {
                    "title": "UniTEX：无UV映射的高质量三维纹理生成",
                    "desc": "UniTEX是一种新颖的三维纹理生成框架，能够直接从图像和几何形状生成高质量、一致的三维纹理，而无需使用UV映射。该方法通过纹理函数（Texture Functions）将纹理生成提升到三维空间，避免了传统UV映射带来的拓扑模糊问题。UniTEX利用基于变换器的大型纹理模型（Large Texturing Model）直接预测纹理函数，从而提高纹理质量。实验结果表明，UniTEX在视觉质量和纹理完整性方面优于现有方法，提供了一种可扩展的自动化三维纹理生成解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.18087",
            "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
            "url": "https://huggingface.co/papers/2505.18087",
            "abstract": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
            "score": 3,
            "issue_id": 4035,
            "pub_date": "2025-05-23",
            "pub_date_card": {
                "ru": "23 мая",
                "en": "May 23",
                "zh": "5月23日"
            },
            "hash": "af9bc7f06b3e9889",
            "authors": [
                "Hyungyung Lee",
                "Geon Choi",
                "Jung-Oh Lee",
                "Hangyul Yoon",
                "Hyuk Gi Hong",
                "Edward Choi"
            ],
            "affiliations": [
                "KAIST",
                "Seoul Medical Center",
                "Seoul National University Hospital"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.18087.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#science",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🩻",
                "ru": {
                    "title": "Структурированное рассуждение в медицинском ИИ: новый подход к оценке",
                    "desc": "CheXStruct и CXReasonBench - это новые инструменты для оценки крупных визуально-языковых моделей в клинической диагностике. Они используют набор данных MIMIC-CXR-JPG для оценки структурированного рассуждения, визуальной привязки и обобщения. CheXStruct автоматически выводит последовательность промежуточных шагов рассуждения непосредственно из рентгеновских снимков грудной клетки. CXReasonBench использует этот конвейер для оценки способности моделей выполнять клинически обоснованные шаги рассуждения и учиться на структурированных рекомендациях."
                },
                "en": {
                    "title": "Enhancing Clinical Diagnosis with Structured Reasoning in AI",
                    "desc": "The paper introduces CheXStruct and CXReasonBench, benchmarks designed to evaluate Large Vision-Language Models (LVLMs) in clinical diagnosis using the MIMIC-CXR-JPG dataset. These tools focus on assessing structured reasoning, visual grounding, and the models' ability to generalize across various diagnostic tasks. CheXStruct automates the extraction of intermediate reasoning steps from chest X-rays, while CXReasonBench tests the models' capacity to perform clinically valid reasoning and learn from structured guidance. The findings reveal that even the best LVLMs struggle with structured reasoning and connecting abstract knowledge to visual data, highlighting the need for improved model capabilities in medical contexts."
                },
                "zh": {
                    "title": "评估临床诊断中的结构化推理能力",
                    "desc": "CheXStruct和CXReasonBench是用于评估大型视觉语言模型在临床诊断中的工具。它们通过分析胸部X光图像，自动生成中间推理步骤，帮助模型进行结构化推理。该基准测试包含18,988个问答对，涵盖12个诊断任务，支持多路径和多阶段评估。尽管有10个模型参与评估，但它们在结构化推理和泛化能力上仍然面临挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14321",
            "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
            "url": "https://huggingface.co/papers/2505.14321",
            "abstract": "Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating a model's temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that obscure whether higher scores truly indicate stronger understanding of the dynamic content in videos: (1) strong language priors, where models can answer questions without watching the video; and (2) shuffling invariance, where models maintain similar performance on certain questions even when video frames are temporally shuffled. To alleviate these issues, we propose VBenchComp, an automated pipeline that categorizes questions into different domains: LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions can be answered without viewing the video; Semantic questions remain answerable even when the video frames are shuffled; and Temporal questions require understanding the correct temporal order of frames. The rest of the questions are labeled as Others. This can enable fine-grained evaluation of different capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that are hidden by traditional overall scores, and we offer insights and recommendations for designing future benchmarks that more accurately assess video LLMs.",
            "score": 3,
            "issue_id": 4041,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "fd7b12c6173122fa",
            "authors": [
                "Bo Feng",
                "Zhengfeng Lai",
                "Shiyu Li",
                "Zizhen Wang",
                "Simon Wang",
                "Ping Huang",
                "Meng Cao"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14321.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Детальная оценка понимания видео языковыми моделями",
                    "desc": "Статья представляет VBenchComp - автоматизированный конвейер для категоризации вопросов в задачах понимания видео. Авторы выделяют три основных домена вопросов: отвечаемые языковой моделью без просмотра видео, семантические и временные. Это позволяет более точно оценивать различные аспекты работы видео-ориентированных языковых моделей. Анализ выявляет скрытые недостатки моделей, которые не видны при традиционной оценке общего балла."
                },
                "en": {
                    "title": "Isolating Temporal Reasoning in Video Understanding",
                    "desc": "This paper addresses the shortcomings of existing video understanding benchmarks that mix different types of questions, making it hard to evaluate a model's ability to reason about video content over time. The authors identify two main issues: models can sometimes answer questions based on language knowledge alone, and they perform similarly on questions even when video frames are shuffled. To tackle these problems, they introduce VBenchComp, a new evaluation framework that classifies questions into categories based on their reliance on temporal reasoning. This approach allows for a more detailed assessment of video language models (LLMs) and highlights specific areas where models may struggle, leading to better benchmark designs in the future."
                },
                "zh": {
                    "title": "精准评估视频理解能力的新基准",
                    "desc": "现有的视频理解基准往往将基于知识和纯图像的问题混为一谈，未能清晰地隔离模型的时间推理能力，这是视频理解与其他模态的关键区别。我们识别出两个主要限制，影响了高分是否真正反映了对视频动态内容的理解：一是强语言先验，模型可以在不观看视频的情况下回答问题；二是洗牌不变性，模型在某些问题上即使视频帧被打乱也能保持相似的表现。为了解决这些问题，我们提出了VBenchComp，一个自动化的管道，将问题分类为不同领域：LLM可回答、语义和时间。我们的分析揭示了传统整体评分掩盖的模型弱点，并为设计更准确评估视频LLM的未来基准提供了见解和建议。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23761",
            "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
            "url": "https://huggingface.co/papers/2505.23761",
            "abstract": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information.",
            "score": 2,
            "issue_id": 4040,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "df30792f37655462",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#alignment",
                    "#synthetic",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Теоретическое обоснование DPO через призму дифференциальной информации",
                    "desc": "Данная статья представляет теоретический анализ метода Direct Preference Optimization (DPO) для обучения языковых моделей. Авторы показывают, что параметризация вознаграждения через логарифмическое отношение является оптимальной для обучения целевой политики путем оптимизации предпочтений. Исследование связывает DPO с политиками, упорядоченными по логарифмическому запасу, и объясняет эффекты усиления и сглаживания политики на основе дифференциальной информационной энтропии. Результаты подтверждаются как на синтетических, так и на реальных наборах данных для выполнения инструкций."
                },
                "en": {
                    "title": "Unlocking Optimal Learning with Differential Information in DPO",
                    "desc": "This paper provides a theoretical analysis of Direct Preference Optimization (DPO), focusing on the log-ratio reward parameterization, which is shown to be optimal for learning target policies through preference optimization. It introduces the concept of Differential Information Distribution (DID), which captures the information gained during policy updates and links preference labels to the transformation of reference policies into target policies. The study reveals that the effectiveness of DPO is tied to log-margin ordered policies, an important but previously overlooked aspect of preference optimization. Additionally, it characterizes how different levels of entropy in differential information influence policy reinforcement and smoothing, offering insights into effective instruction-following and question answering tasks."
                },
                "zh": {
                    "title": "直接偏好优化：差分信息的关键角色",
                    "desc": "本文对直接偏好优化（DPO）进行了理论分析，揭示了对数比奖励参数化在通过偏好优化学习目标策略时的最佳性。我们利用差分信息分布（DID）来解释这一现象，并展示了偏好标签如何编码所需的差分信息，从而使得DPO中的对数比奖励成为学习目标策略的唯一最佳形式。研究还发现，偏好编码差分信息的条件与隐含的对数边际有序策略假设密切相关，这一假设在偏好优化中广泛使用但之前未被认识。最后，我们通过分析DID的熵，阐明了低熵和高熵差分信息对策略分布的不同影响，进一步验证了我们的理论发现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23625",
            "title": "ZeroSep: Separate Anything in Audio with Zero Training",
            "url": "https://huggingface.co/papers/2505.23625",
            "abstract": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "21979b2379717385",
            "authors": [
                "Chao Huang",
                "Yuesheng Ma",
                "Junxuan Huang",
                "Susan Liang",
                "Yunlong Tang",
                "Jing Bi",
                "Wenqiang Liu",
                "Nima Mesgarani",
                "Chenliang Xu"
            ],
            "affiliations": [
                "Columbia University",
                "Tencent America",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23625.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#audio",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Разделение аудиоисточников без обучения с помощью текстовых подсказок",
                    "desc": "ZeroSep - это модель разделения аудиоисточников на основе диффузии, управляемой текстом. Она достигает разделения источников без предварительного обучения, используя предобученные модели и текстовое условие. ZeroSep работает путем инвертирования смешанного аудио в латентное пространство модели диффузии, а затем использует текстовое условие для управления процессом удаления шума для восстановления отдельных источников. Модель превосходит методы с учителем на различных эталонных тестах и поддерживает сценарии открытого множества благодаря богатым текстовым приорам."
                },
                "en": {
                    "title": "ZeroSep: Revolutionizing Audio Separation with Zero-Shot Learning",
                    "desc": "ZeroSep is a novel audio source separation model that utilizes pre-trained text-guided audio diffusion techniques to achieve zero-shot performance. Unlike traditional supervised methods that require extensive labeled data, ZeroSep leverages generative models to separate audio sources without any task-specific training. By inverting mixed audio into the model's latent space and applying text conditioning, it effectively guides the denoising process to isolate individual sound sources. This approach not only enhances separation accuracy but also allows for flexibility in handling diverse and unpredictable acoustic environments."
                },
                "zh": {
                    "title": "ZeroSep：无监督音频源分离的新突破",
                    "desc": "ZeroSep是一种文本引导的音频扩散模型，能够在没有特定训练的情况下实现源分离。它通过预训练模型和文本条件化，克服了传统监督学习方法对大量标注数据的依赖。ZeroSep通过将混合音频反转到扩散模型的潜在空间，并利用文本指导去噪过程，成功恢复个别音源。该方法在多个基准测试中表现优异，超越了现有的监督方法，展现了其在开放场景中的强大能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22918",
            "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
            "url": "https://huggingface.co/papers/2505.22918",
            "abstract": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1\\% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU at negligible overhead cost.   Code available online here: https://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
            "score": 2,
            "issue_id": 4041,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "0d7f4111cbc74b76",
            "authors": [
                "Ruichen Chen",
                "Keith G. Mills",
                "Liyao Jiang",
                "Chao Gao",
                "Di Niu"
            ],
            "affiliations": [
                "ECE Department University of Alberta",
                "Huawei Technologies Edmonton, Alberta, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22918.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#video",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективное разреженное внимание для генерации видео и изображений",
                    "desc": "Статья представляет новый метод под названием Re-ttention для оптимизации механизма внимания в диффузионных моделях для генерации визуального контента. Re-ttention использует временную избыточность для реализации сильно разреженного внимания, сохраняя при этом качество генерации. Метод позволяет использовать всего 3.1% токенов при выводе, превосходя современные аналоги. Эксперименты показывают значительное снижение латентности без существенных накладных расходов."
                },
                "en": {
                    "title": "Re-ttention: Sparsity Meets Quality in Visual Generation",
                    "desc": "Re-ttention is a novel approach that enhances the efficiency of diffusion models in visual generation by utilizing temporal redundancy to achieve high sparse attention. This method addresses the computational challenges posed by traditional attention mechanisms, which become increasingly complex with higher resolution and longer video lengths. By reshaping attention scores based on historical softmax distributions, Re-ttention maintains visual quality even at extreme levels of sparsity. Experimental results show that it significantly reduces the number of tokens needed during inference while also improving latency, outperforming existing sparse attention techniques."
                },
                "zh": {
                    "title": "Re-ttention：高效稀疏注意力的视觉生成新方法",
                    "desc": "Re-ttention是一种新方法，利用扩散模型中的时间冗余，实现高稀疏注意力的视觉生成，同时保持高质量并减少计算开销。传统的注意力机制在处理高分辨率和长视频时，计算复杂度呈平方级增长，导致效率低下。Re-ttention通过重塑注意力分数，基于先前的softmax分布历史，解决了在极高稀疏度下的视觉质量保持问题。实验结果表明，Re-ttention在推理时只需使用3.1%的标记，且在延迟方面显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22765",
            "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
            "url": "https://huggingface.co/papers/2505.22765",
            "abstract": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.  \t\t\t\t\tAI-generated summary \t\t\t\t Sentence stress refers to emphasis, placed on specific words within a spoken utterance to highlight or contrast an idea, or to introduce new information. It is often used to imply an underlying intention that is not explicitly stated. Recent advances in speech-aware language models (SLMs) have enabled direct processing of audio, allowing models to bypass transcription and access the full richness of the speech signal and perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and speaker intent, it remains largely overlooked in evaluation and development of such models. In this work, we address this gap by introducing StressTest, a benchmark specifically designed to evaluate a model's ability to distinguish between interpretations of spoken sentences based on the stress pattern. We assess the performance of several leading SLMs and find that, despite their overall capabilities, they perform poorly on such tasks. To overcome this limitation, we propose a novel synthetic data generation pipeline, and create Stress17k, a training set that simulates change of meaning implied by stress variation. Then, we empirically show that optimizing models with this synthetic dataset aligns well with real-world recordings and enables effective finetuning of SLMs. Results suggest, that our finetuned model, StresSLM, significantly outperforms existing models on both sentence stress reasoning and detection tasks. Code, models, data, and audio samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.",
            "score": 2,
            "issue_id": 4040,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "601fd00657c1e4f1",
            "authors": [
                "Iddo Yosha",
                "Gallil Maimon",
                "Yossi Adi"
            ],
            "affiliations": [
                "The Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22765.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Новый подход к пониманию фразового ударения в речевых ИИ-моделях",
                    "desc": "Представлен бенчмарк StressTest и синтетический набор данных Stress17k для улучшения способности речевых языковых моделей интерпретировать фразовое ударение в устной речи. Исследователи обнаружили, что существующие модели плохо справляются с задачами, связанными с фразовым ударением, несмотря на их общие возможности. Для решения этой проблемы был разработан конвейер генерации синтетических данных и создан набор Stress17k, имитирующий изменение смысла при вариации ударения. Дообученная модель StresSLM значительно превзошла существующие модели в задачах рассуждения и обнаружения фразового ударения."
                },
                "en": {
                    "title": "Enhancing Speech Models with Sentence Stress Understanding",
                    "desc": "This paper introduces the StressTest benchmark and the synthetic Stress17k dataset to enhance speech-aware language models' understanding of sentence stress in spoken language. Sentence stress is crucial for conveying meaning and speaker intent, yet it has been largely ignored in the evaluation of these models. The authors evaluate several leading speech-aware language models and find that they struggle with tasks involving sentence stress interpretation. By training a model called StresSLM on the new dataset, the authors demonstrate significant improvements in both reasoning and detection of sentence stress, showcasing the importance of this feature in natural language processing."
                },
                "zh": {
                    "title": "提升语音模型理解句子重音的能力",
                    "desc": "本文介绍了一个名为StressTest的基准测试和合成数据集Stress17k，旨在提高语音感知语言模型对口语中句子重音的理解能力。句子重音是指在口语中对特定单词的强调，用以突出或对比某个观点。尽管句子重音在传达意义和说话者意图中起着重要作用，但在现有模型的评估和开发中却被忽视。通过引入StressTest基准和Stress17k数据集，研究表明优化模型可以显著提升其在句子重音推理和检测任务上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20282",
            "title": "One-shot Entropy Minimization",
            "url": "https://huggingface.co/papers/2505.20282",
            "abstract": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "9064c4451edd439a",
            "authors": [
                "Zitian Gao",
                "Lynx Chen",
                "Joey Zhou",
                "Bryan Dai"
            ],
            "affiliations": [
                "Ubiquant"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20282.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в обучении языковых моделей: максимальный эффект при минимальных затратах",
                    "desc": "Исследователи обнаружили, что минимизация энтропии с использованием всего одного образца данных и минимальной оптимизации может значительно улучшить производительность больших языковых моделей (LLM). Этот метод показал результаты, сравнимые или даже превосходящие те, что достигаются при использовании тысяч образцов данных и тщательно разработанных наград в обучении с подкреплением. Эксперименты были проведены на 13,440 больших языковых моделях. Результаты исследования могут привести к переосмыслению парадигм пост-обучения для LLM."
                },
                "en": {
                    "title": "Revolutionizing Language Model Training with Minimal Data",
                    "desc": "This paper presents a novel approach to improving large language models through entropy minimization using only one sample and minimal optimization. The authors trained 13,440 models and discovered that this method can achieve performance enhancements similar to or better than traditional methods that rely on extensive datasets and complex reward systems in reinforcement learning. This finding suggests that the reliance on large amounts of labeled data may be reconsidered in the context of post-training strategies for language models. The research highlights the potential for more efficient training processes in machine learning applications."
                },
                "zh": {
                    "title": "熵最小化：用一个样本实现性能飞跃",
                    "desc": "本研究探讨了熵最小化在大型语言模型中的应用。我们发现，仅需一个无标签样本和10步优化，就能显著提升模型性能。这个结果与使用成千上万的数据和精心设计的奖励的强化学习方法相媲美，甚至更优。此发现可能会促使人们重新思考大型语言模型的后训练范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19360",
            "title": "ChartLens: Fine-grained Visual Attribution in Charts",
            "url": "https://huggingface.co/papers/2505.19360",
            "abstract": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "1b792efe04b5f610",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19360.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#multimodal",
                    "#hallucinations",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Точное понимание графиков с помощью ChartLens",
                    "desc": "ChartLens - это новый алгоритм для улучшения понимания графиков мультимодальными языковыми моделями. Он использует сегментацию для идентификации объектов на графиках и применяет специальные промпты для точной визуальной атрибуции. Авторы также представили бенчмарк ChartVA-Eval с синтетическими и реальными графиками из разных областей. Результаты показывают, что ChartLens улучшает точность атрибуции на 26-66%."
                },
                "en": {
                    "title": "ChartLens: Enhancing Chart Understanding with Visual Attributions",
                    "desc": "ChartLens is a novel approach that enhances multimodal large language models (MLLMs) by providing fine-grained visual attributions for better chart understanding. It addresses the issue of hallucinations in MLLMs, where the generated text may not align with the visual data. By using segmentation techniques to identify specific chart elements and employing set-of-marks prompting, ChartLens improves the accuracy of interpreting charts significantly. The introduction of ChartVA-Eval, a benchmark for evaluating these attributions, further supports the effectiveness of ChartLens across various domains."
                },
                "zh": {
                    "title": "ChartLens：提升图表理解的细粒度视觉归因",
                    "desc": "本论文介绍了一种名为ChartLens的新算法，旨在增强多模态语言模型（MLLMs）在图表理解方面的能力。通过细粒度的视觉归因，ChartLens能够识别图表中的具体元素，从而提高模型的准确性，减少生成文本与视觉数据之间的矛盾。我们还提出了ChartVA-Eval，这是一个包含合成和真实世界图表的基准测试，涵盖金融、政策和经济等多个领域。实验结果表明，ChartLens在细粒度归因方面的表现提高了26%到66%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19286",
            "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.19286",
            "abstract": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.",
            "score": 2,
            "issue_id": 4036,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "a42b79a0269a396c",
            "authors": [
                "Utkarsh Sahu",
                "Zhisheng Qi",
                "Yongjia Lei",
                "Ryan A. Rossi",
                "Franck Dernoncourt",
                "Nesreen K. Ahmed",
                "Mahantesh M Halappanavar",
                "Yao Ma",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco AI Research",
                "Pacific Northwest National Laboratory",
                "Rensselaer Polytechnic Institute",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19286.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#interpretability",
                    "#graphs",
                    "#architecture",
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Графовый анализ раскрывает структуру знаний в языковых моделях",
                    "desc": "Исследование изучает структурные паттерны знаний в больших языковых моделях с точки зрения графов. Авторы обнаружили явление гомофилии знаний, где топологически близкие сущности демонстрируют схожий уровень знаний. На основе этого были разработаны модели машинного обучения на графах для оценки знаний сущностей. Результаты показывают, что использование отобранных триплетов для дообучения приводит к улучшению производительности модели."
                },
                "en": {
                    "title": "Unveiling Knowledge Patterns in Language Models through Graphs",
                    "desc": "This paper investigates how knowledge is structured in large language models (LLMs) by viewing it through a graph lens. It identifies a phenomenon called knowledge homophily, where entities that are closely connected in the graph tend to have similar knowledge levels. The authors develop graph machine learning models to estimate the knowledge of entities based on their neighboring connections. Their findings suggest that fine-tuning LLMs with carefully selected triplets can significantly enhance their performance."
                },
                "zh": {
                    "title": "揭示大型语言模型的知识结构与同质性",
                    "desc": "本研究从图的角度探讨了大型语言模型中的知识结构模式，揭示了知识同质性，并开发了图机器学习模型来估计实体知识。我们量化了大型语言模型的知识，分析了其与图结构属性（如节点度）的关系。研究发现，拓扑上相近的实体表现出相似的知识水平，这激励我们基于局部邻居开发模型来估计实体知识。实证结果表明，使用选定的三元组进行微调可以显著提高模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23183",
            "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
            "url": "https://huggingface.co/papers/2505.23183",
            "abstract": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.",
            "score": 1,
            "issue_id": 4041,
            "pub_date": "2025-05-29",
            "pub_date_card": {
                "ru": "29 мая",
                "en": "May 29",
                "zh": "5月29日"
            },
            "hash": "015c1dbfc16cb8cb",
            "authors": [
                "Gabriele Sarti",
                "Vilém Zouhar",
                "Malvina Nissim",
                "Arianna Bisazza"
            ],
            "affiliations": [
                "CLCG, University of Groningen",
                "ETH Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23183.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#machine_translation",
                    "#data",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Эффективная оценка качества перевода: от интерпретируемости к точности",
                    "desc": "Данная статья исследует методы оценки качества перевода на уровне слов, используя интерпретируемость моделей и квантификацию неопределенности. Авторы анализируют влияние вариативности разметки на производительность метрик и сравнивают supervised и unsupervised подходы. Исследование охватывает 14 метрик для 12 направлений перевода, выявляя потенциал unsupervised методов и недостатки supervised подходов при неопределенности разметки. Результаты подчеркивают ненадежность оценки на основе разметки одного аннотатора."
                },
                "en": {
                    "title": "Unlocking Translation Quality: The Power of Unsupervised Metrics",
                    "desc": "This paper explores word-level quality estimation (WQE) techniques that automatically detect translation errors in machine-generated text. It emphasizes the importance of model interpretability and uncertainty quantification to improve the identification of these errors. The study evaluates 14 different metrics across 12 translation directions, focusing on how variations in human labeling affect the performance of these metrics. The findings reveal the advantages of unsupervised methods and the limitations of supervised approaches, particularly in situations with uncertain labels."
                },
                "zh": {
                    "title": "探索高效的词级质量评估技术",
                    "desc": "本文探讨了词级质量评估技术在机器翻译中的应用，重点关注如何利用模型可解释性和不确定性量化来识别翻译错误。研究表明，现有的WQE技术通常需要大量人类标注数据，成本较高。我们评估了14种指标在12个翻译方向上的表现，并分析了人类标签变异对指标性能的影响。结果显示，无监督指标具有未被充分利用的潜力，而监督方法在标签不确定性面前存在不足。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22943",
            "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
            "url": "https://huggingface.co/papers/2505.22943",
            "abstract": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.",
            "score": 1,
            "issue_id": 4035,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "0e150bd219867ad7",
            "authors": [
                "Jaewoo Ahn",
                "Heeseung Yun",
                "Dayoon Ko",
                "Gunhee Kim"
            ],
            "affiliations": [
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22943.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#security",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Раскрытие слабых мест мультимодальных моделей с помощью обманчивых текстов",
                    "desc": "Статья представляет новый бенчмарк Multimodal Adversarial Compositionality (MAC) для оценки уязвимостей в мультимодальных представлениях. MAC использует большие языковые модели для генерации обманчивых текстовых образцов, эксплуатирующих эти уязвимости. Авторы предлагают метод самообучения на основе отбора образцов и фильтрации для улучшения атак с нулевым обучением. Эксперименты показывают, что этот подход эффективно выявляет композиционные уязвимости в различных мультимодальных представлениях."
                },
                "en": {
                    "title": "Enhancing AI Resilience Against Deceptive Text in Multimodal Models",
                    "desc": "This paper introduces a benchmark called Multimodal Adversarial Compositionality (MAC) to assess the weaknesses in multimodal representations, such as those used in AI models like CLIP. It highlights how these models can make incorrect judgments when faced with deceptive text samples generated by large language models. The authors propose a self-training method that improves zero-shot learning by enhancing the success of attacks and increasing the diversity of the samples used. Their approach, which utilizes smaller language models, shows better performance in identifying these vulnerabilities across different types of media, including images, videos, and audio."
                },
                "zh": {
                    "title": "揭示多模态表示的组合脆弱性",
                    "desc": "本文介绍了一种基准测试，利用欺骗性文本样本来评估多模态表示中的组合脆弱性。我们提出了多模态对抗组合性（MAC），通过大型语言模型生成欺骗性文本样本，利用这些脆弱性进行评估。为了改善零样本方法，我们提出了一种自我训练的方法，结合拒绝采样微调和多样性促进过滤，从而提高攻击成功率和样本多样性。使用较小的语言模型，如Llama-3.1-8B，我们的方法在揭示各种多模态表示（包括图像、视频和音频）的组合脆弱性方面表现优越。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22810",
            "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
            "url": "https://huggingface.co/papers/2505.22810",
            "abstract": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments.",
            "score": 1,
            "issue_id": 4041,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "965d2d2ccc7a3cba",
            "authors": [
                "Zhoufaran Yang",
                "Yan Shu",
                "Zhifei Yang",
                "Yan Zhang",
                "Yu Li",
                "Keyang Lu",
                "Gangyan Zeng",
                "Shaohui Liu",
                "Yu Zhou",
                "Nicu Sebe"
            ],
            "affiliations": [
                "BUAA",
                "HIT",
                "IIE, CAS",
                "NJUST",
                "NKU",
                "PKU",
                "UCAS",
                "UNITN"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22810.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VidText: Новый рубеж в понимании текста в видео",
                    "desc": "VidText - это новый бенчмарк для оценки понимания текста в видео, охватывающий различные задачи от глобального обобщения до локального поиска. Он включает многоязычный контент и разнообразные сценарии, где естественным образом появляется текст в видео. Бенчмарк предлагает иерархическую структуру оценки с задачами на уровне видео, клипа и отдельных элементов, а также набор задач на восприятие и рассуждение. Эксперименты показали, что современные мультимодальные модели (LMM) испытывают трудности с большинством задач, что открывает широкое поле для улучшений."
                },
                "en": {
                    "title": "Unlocking Video Text Understanding with VidText",
                    "desc": "VidText is a new benchmark designed to evaluate how well models understand text in videos, focusing on both overall summaries and specific details. It addresses the lack of attention to textual information in existing video benchmarks and the limitations of OCR benchmarks that only deal with static images. The benchmark includes a variety of real-world scenarios and tasks that assess models at different levels, from video-wide summaries to specific instances of text retrieval. Experiments show that current multimodal models face challenges in these tasks, indicating a need for further development in video text understanding."
                },
                "zh": {
                    "title": "VidText：视频文本理解的新基准",
                    "desc": "VidText是一个新的基准，旨在评估视频文本理解的能力，涵盖全球摘要和局部检索等多种任务。视频中的视觉文本包含丰富的语义信息，对整体视频理解和细致的人类行为推理至关重要。现有的视频理解基准大多忽视文本信息，而OCR特定基准又局限于静态图像，无法捕捉文本与动态视觉环境之间的互动。VidText通过引入多层次的评估框架和多种真实场景，填补了这一空白，推动多模态推理的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.22126",
            "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
            "url": "https://huggingface.co/papers/2505.22126",
            "abstract": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.",
            "score": 1,
            "issue_id": 4039,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "6401cdf5b0a13760",
            "authors": [
                "Yifan Chang",
                "Yukang Feng",
                "Jianwen Sun",
                "Jiaxin Ai",
                "Chuanhao Li",
                "S. Kevin Zhou",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.22126.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#interpretability",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "SridBench: вызов ИИ в создании научных иллюстраций",
                    "desc": "SridBench - это новый эталонный тест для оценки генерации научных иллюстраций искусственным интеллектом. Он включает 1120 примеров из 13 научных дисциплин, оцениваемых по 6 параметрам, включая семантическую точность и структурную корректность. Результаты показывают, что даже передовые модели, такие как GPT-4o-image, значительно уступают человеку в этой задаче. Это подчеркивает необходимость разработки более продвинутых моделей мультимодального рассуждения для генерации визуального контента."
                },
                "en": {
                    "title": "SridBench: Elevating AI in Scientific Figure Generation",
                    "desc": "SridBench is a new benchmark designed to evaluate the generation of scientific figures by AI models. It highlights that current leading models, such as GPT-4o-image, struggle with achieving the same level of semantic and structural accuracy as human-generated illustrations. The benchmark includes 1,120 carefully selected examples from various scientific fields, assessing models on criteria like semantic fidelity and structural accuracy. The results indicate a significant gap in performance, emphasizing the necessity for improved multimodal reasoning in visual generation tasks."
                },
                "zh": {
                    "title": "科学图形生成的新基准：SridBench",
                    "desc": "SridBench是一个用于科学图形生成的基准测试，旨在评估当前AI模型在生成科学插图时的表现。尽管像GPT-4o-image这样的顶尖模型在图像生成方面取得了进展，但它们在语义和结构准确性上仍然无法与人类相媲美。科学插图生成需要对技术内容进行准确解读，并将抽象概念转化为清晰的标准化视觉效果，这一过程知识密集且耗时。SridBench通过提供1,120个实例，填补了这一评估空白，强调了对更先进的多模态推理驱动的视觉生成能力的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.20199",
            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
            "url": "https://huggingface.co/papers/2505.20199",
            "abstract": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.",
            "score": 1,
            "issue_id": 4036,
            "pub_date": "2025-05-26",
            "pub_date_card": {
                "ru": "26 мая",
                "en": "May 26",
                "zh": "5月26日"
            },
            "hash": "9386cae0db1bfc6c",
            "authors": [
                "Pengxiang Li",
                "Shilin Yan",
                "Joey Tsai",
                "Renrui Zhang",
                "Ruichuan An",
                "Ziyu Guo",
                "Xiaowei Gao"
            ],
            "affiliations": [
                "CUHK",
                "FDU",
                "ICL",
                "PKU",
                "PolyU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.20199.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Динамическая адаптация руководства для повышения качества генерации текста",
                    "desc": "Статья представляет новый метод под названием Адаптивное Бесклассовое Руководство (A-CFG) для улучшения генерации текста в маскированных диффузионных языковых моделях. A-CFG динамически корректирует руководство, фокусируясь на областях с низкой уверенностью модели. Метод временно ремаскирует токены с низкой уверенностью, создавая локализованный безусловный вход. Эксперименты показывают значительное улучшение производительности по сравнению со стандартным CFG на различных языковых задачах."
                },
                "en": {
                    "title": "Dynamic Guidance for Better Language Generation",
                    "desc": "Adaptive Classifier-Free Guidance (A-CFG) enhances language generation by adjusting guidance based on the model's confidence levels. Unlike traditional Classifier-Free Guidance (CFG), which uses a fixed unconditional input, A-CFG dynamically re-masks tokens where the model is uncertain. This targeted approach allows for more effective corrections during the generation process, improving overall performance. Experiments show that A-CFG significantly outperforms standard CFG, demonstrating the importance of adapting guidance to model uncertainty."
                },
                "zh": {
                    "title": "动态调整引导，提升生成效果",
                    "desc": "自适应无分类器引导（A-CFG）通过关注模型信心较低的区域，动态调整掩蔽扩散语言模型中的引导，从而显著提高语言生成性能。传统的无分类器引导（CFG）使用静态的无条件输入，这在模型不确定性动态变化的迭代生成过程中可能效果不佳。A-CFG方法通过利用模型的即时预测信心，定制无条件输入，在每一步迭代中识别当前生成序列中模型信心低的标记。通过临时重新掩蔽这些标记，A-CFG能够更有效地集中引导在模糊区域，从而提升生成效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.19236",
            "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
            "url": "https://huggingface.co/papers/2505.19236",
            "abstract": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.  \t\t\t\t\tAI-generated summary \t\t\t\t Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.",
            "score": 1,
            "issue_id": 4037,
            "pub_date": "2025-05-25",
            "pub_date_card": {
                "ru": "25 мая",
                "en": "May 25",
                "zh": "5月25日"
            },
            "hash": "3f76ece8cb945378",
            "authors": [
                "Qian Cao",
                "Xiting Wang",
                "Yuzhuo Yuan",
                "Yahui Liu",
                "Fang Luo",
                "Ruihua Song"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.19236.jpg",
            "data": {
                "categories": [
                    "#creativity",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "CrEval: Революция в автоматической оценке креативности текста",
                    "desc": "Статья представляет новый фреймворк для оценки текстовой креативности, основанный на попарном сравнении. Авторы создали датасет CreataSet, содержащий более 100 тысяч пар инструкций и ответов, созданных людьми, а также более 1 миллиона синтетических пар. На основе этого датасета была обучена модель CrEval - оценщик креативности на базе большой языковой модели. CrEval показывает значительное улучшение в оценке креативности по сравнению с существующими методами, демонстрируя высокую согласованность с человеческими суждениями."
                },
                "en": {
                    "title": "Revolutionizing Creativity Evaluation with CrEval",
                    "desc": "This paper introduces a new framework for evaluating creativity in text using a pairwise-comparison method. It utilizes a dataset called CreataSet, which contains over 100,000 human-generated and 1 million synthetic creative instruction-response pairs. The authors developed an LLM-based evaluator named CrEval, which shows significant improvement in aligning with human judgments compared to existing evaluation methods. The study highlights the importance of combining human and synthetic data to create robust evaluators that can enhance the creativity of large language models."
                },
                "zh": {
                    "title": "提升文本创意评估的创新框架",
                    "desc": "本文提出了一种新颖的成对比较框架，用于评估文本创意，利用CreataSet数据集训练了一个名为CrEval的基于大语言模型的评估器。该评估器显著提高了与人类判断一致的文本创意评估效果，解决了当前评估方法依赖人类判断的低效问题。CreataSet是一个大规模数据集，包含超过10万个人工创意和超过100万个合成创意的指令-响应对，涵盖多种开放领域任务。实验结果表明，结合人类生成和合成数据对于训练强大的评估器至关重要，CrEval在提升大语言模型的创意能力方面具有实际应用价值。"
                }
            }
        }
    ],
    "link_prev": "2025-05-29.html",
    "link_next": "2025-06-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "29.05",
        "en": "05/29",
        "zh": "5月29日"
    },
    "short_date_next": {
        "ru": "02.06",
        "en": "06/02",
        "zh": "6月2日"
    },
    "categories": {
        "#dataset": 12,
        "#data": 5,
        "#benchmark": 24,
        "#agents": 4,
        "#cv": 9,
        "#rl": 11,
        "#rlhf": 7,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 6,
        "#audio": 2,
        "#video": 5,
        "#multimodal": 14,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 3,
        "#training": 24,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 9,
        "#reasoning": 16,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 19,
        "#survey": 2,
        "#diffusion": 11,
        "#alignment": 5,
        "#story_generation": 1,
        "#hallucinations": 3,
        "#long_context": 2,
        "#synthetic": 4,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 4,
        "#low_resource": 0,
        "#creativity": 1
    },
    "zh": {
        "text": "这篇文章研究了大语言模型（LLMs）在强化学习中的表现。研究发现，LLMs在奖励噪音（reward noise）存在的情况下仍能表现出强大的鲁棒性。即使在数学任务中手动翻转40%的奖励函数输出，模型仍能快速收敛，提高其数学任务的表现。通过奖励关键推理短语（RPR）的出现，模型在开放式任务中的表现也得到了提升。这些发现强调了改进模型在预训练阶段的基础能力的重要性。",
        "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
        "pinyin": "这篇文章研究了大语言模型（LLMs）在强化学习中的表现。研究发现，LLMs在奖励噪音（reward noise）存在的情况下仍能表现出强大的鲁棒性。即使在数学任务中手动翻转40%的奖励函数输出，模型仍能快速收敛，提高其数学任务的表现。通过奖励关键推理短语（RPR）的出现，模型在开放式任务中的表现也得到了提升。这些发现强调了改进模型在预训练阶段的基础能力的重要性。\n\nZhè piān wénzhāng yánjiū le dà yǔyán móxíng (LLMs) zài qiángzhù xuéxí zhōng de biǎoxiàn. Yánjiū fāxiàn, LLMs zài jiǎnglì zàoyīn (reward noise) cúnzài de qíngkuàng xià réng néng biǎoxiàn chū qiángdà de lǔcǔxìng. Jíshǐ zài shùxué rènwù zhōng shǒudòng fānzhuǎn 40% de jiǎnglì hánshù shūchū, móxíng réng néng kuàisù shōuliǎn, tígāo qí shùxué rènwù de biǎoxiàn. Tōngguò jiǎnglì guǎnjiàn tuīlǐ duǎnyǔ (RPR) de chūxiàn, móxíng zài kāifàngshì rènwù zhōng de biǎoxiàn yě dédào le tíshēng. Zhèxiē fāxiàn qiángdiào le gǎijìn móxíng zài yùxùn jiēduàn de jīchǔ nénglì de zhòngyàoxìng.",
        "vocab": "[\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔyán móxíng\", \"trans\": \"large language model\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"奖励噪音\", \"pinyin\": \"jiǎng lì zào yīn\", \"trans\": \"reward noise\"},\n    {\"word\": \"鲁棒性\", \"pinyin\": \"lǔ bāng xìng\", \"trans\": \"robustness\"},\n    {\"word\": \"手动翻转\", \"pinyin\": \"shǒu dòng fān zhuǎn\", \"trans\": \"manually flip\"},\n    {\"word\": \"奖励函数\", \"pinyin\": \"jiǎng lì hán shù\", \"trans\": \"reward function\"},\n    {\"word\": \"输出\", \"pinyin\": \"shū chū\", \"trans\": \"output\"},\n    {\"word\": \"快速收敛\", \"pinyin\": \"kuài sù shōu liǎn\", \"trans\": \"rapid convergence\"},\n    {\"word\": \"提高\", \"pinyin\": \"tí gāo\", \"trans\": \"improve\"},\n    {\"word\": \"奖励关键推理短语\", \"pinyin\": \"jiǎng lì guǎn jiàn tuī lǐ duǎn yǔ\", \"trans\": \"reward key reasoning phrases\"},\n    {\"word\": \"开放式任务\", \"pinyin\": \"kāi fàng shì rèn wù\", \"trans\": \"open-ended tasks\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"findings\"},\n    {\"word\": \"强调\", \"pinyin\": \"qiáng diào\", \"trans\": \"emphasize\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improve\"},\n    {\"word\": \"预训练阶段\", \"pinyin\": \"yù xùn liàn jiē duàn\", \"trans\": \"pre-training stage\"},\n    {\"word\": \"基础能力\", \"pinyin\": \"jī chǔ néng lì\", \"trans\": \"foundational capabilities\"}\n]",
        "trans": "This article investigates the performance of large language models (LLMs) in reinforcement learning. The study found that LLMs exhibit strong robustness even in the presence of reward noise. Even when manually flipping 40% of the reward function outputs in mathematical tasks, the model can still converge quickly and improve its performance on these tasks. By rewarding the appearance of key reasoning phrases (RPR), the model's performance on open-ended tasks is also enhanced. These findings underscore the importance of improving the model's foundational capabilities during the pre-training phase.",
        "update_ts": "2025-05-30 09:12"
    }
}