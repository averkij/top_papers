
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 30 papers. March 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 марта</span> | <span id="title-articles-count">30 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-18.html">⬅️ <span id="prev-date">18.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-20.html">➡️ <span id="next-date">20.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'};
        let feedDateNext = {'ru': '20.03', 'en': '03/20', 'zh': '3月20日'};
        let feedDatePrev = {'ru': '18.03', 'en': '03/18', 'zh': '3月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.14456', 'title': 'RWKV-7 "Goose" with Expressive Dynamic State Evolution', 'url': 'https://huggingface.co/papers/2503.14456', 'abstract': 'We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7\'s language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.', 'score': 90, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '0cd796cef6fa6475', 'authors': ['Bo Peng', 'Ruichong Zhang', 'Daniel Goldstein', 'Eric Alcaide', 'Haowen Hou', 'Janna Lu', 'William Merrill', 'Guangyu Song', 'Kaifeng Tan', 'Saiteja Utpala', 'Nathan Wilce', 'Johan S. Wind', 'Tianyi Wu', 'Daniel Wuttke', 'Christian Zhou-Zheng'], 'affiliations': ['Beijing Normal University', 'Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'Denigma', 'EleutherAI', 'George Mason University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'New York University', 'RWKV Project (under Linux Foundation AI & Data)', 'Recursal AI', 'Shenzhen University', 'Tano Labs', 'Tsinghua University', 'University of Oslo'], 'pdf_title_img': 'assets/pdf/title_img/2503.14456.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#dataset', '#multilingual'], 'emoji': '🦢', 'ru': {'title': 'RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей', 'desc': "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая устанавливает новый state-of-the-art в производительности при 3 миллиардах параметров на многоязычных задачах. Модель требует постоянного использования памяти и времени вывода на токен, вводит обобщенную формулировку правила дельты с векторным гейтингом и обучением в контексте. RWKV-7 способна отслеживать состояния и распознавать все регулярные языки, превосходя возможности трансформеров. Авторы также представляют многоязычный корпус из 3,1 триллиона токенов и обучают на нем четыре модели RWKV-7 размером от 0,19 до 2,9 миллиардов параметров."}, 'en': {'title': 'RWKV-7: Efficient Multilingual Mastery with Fewer Parameters', 'desc': 'RWKV-7 "Goose" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development.'}, 'zh': {'title': 'RWKV-7：多语言任务的新突破', 'desc': 'RWKV-7 "Goose" 是一种新的序列建模架构，具有3亿参数的预训练语言模型，在多语言任务中达到了新的最先进水平。与其他顶级3B模型相比，RWKV-7在训练时使用的标记数量显著减少，但仍能与当前的英语语言性能相匹配。该模型采用了新的广义增量规则，结合了向量值门控和上下文学习率，同时保持了训练的并行性。RWKV-7能够进行状态跟踪并识别所有正规语言，超越了标准复杂性猜想下的Transformer的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14378', 'title': 'Impossible Videos', 'url': 'https://huggingface.co/papers/2503.14378', 'abstract': "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.", 'score': 47, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '3334aa74b743ac8d', 'authors': ['Zechen Bai', 'Hai Ci', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.14378.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#synthetic', '#video'], 'emoji': '🎬', 'ru': {'title': 'IPV-Bench: Новый рубеж в понимании и генерации невозможных видео', 'desc': 'Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксономии, охватывающей 4 домена и 14 категорий, и включает разнообразные сцены, нарушающие физические, биологические, географические или социальные законы. Бенчмарк содержит набор промптов для оценки моделей генерации видео и видеоданные для оценки способности Video-LLM понимать невозможные видео. Комплексная оценка выявляет ограничения и предоставляет идеи для будущих направлений развития видеомоделей.'}, 'en': {'title': 'Exploring the Impossible: Advancing Video Generation and Understanding', 'desc': 'This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field.'}, 'zh': {'title': '探索不可能视频的生成与理解', 'desc': '本论文探讨了合成视频在数据稀缺和多样性方面的应用。当前的合成数据集主要复制现实场景，而对不可能、反事实和反现实的视频概念研究不足。我们提出了IPV-Bench，这是一个新颖的基准，旨在评估和促进视频理解与生成的进展。该基准涵盖了多种违反物理、生物、地理或社会法则的场景，并通过构建提示套件来挑战视频生成模型的创造力和提示跟随能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14476', 'title': 'DAPO: An Open-Source LLM Reinforcement Learning System at Scale', 'url': 'https://huggingface.co/papers/2503.14476', 'abstract': 'Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.', 'score': 41, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5b4841d2845817e8', 'authors': ['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Weinan Dai', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.14476.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая система RL для улучшения рассуждений больших языковых моделей', 'desc': 'Статья представляет алгоритм DAPO для обучения с подкреплением крупномасштабных языковых моделей. Авторы открыто публикуют систему, достигающую 50 баллов на AIME 2024 с использованием базовой модели Qwen2.5-32B. В работе раскрываются четыре ключевые техники, делающие успешным RL для больших ЯМ. Исследователи также предоставляют код обучения и подготовленный набор данных для воспроизводимости результатов.'}, 'en': {'title': 'Unlocking LLM Potential with Open-Source Reinforcement Learning', 'desc': 'This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.'}, 'zh': {'title': '解锁大型语言模型的推理潜力', 'desc': '本论文提出了一种新的算法，称为解耦剪辑和动态采样策略优化（DAPO），旨在提升大型语言模型（LLM）的推理能力。我们通过强化学习技术，成功实现了在AIME 2024上获得50分的成绩，使用的是Qwen2.5-32B基础模型。与以往研究不同，我们公开了四个关键技术细节，帮助社区更好地理解和复现我们的训练结果。此外，我们还开源了训练代码和经过精心处理的数据集，以促进大型LLM强化学习的可重复性和未来研究。'}}}, {'id': 'https://huggingface.co/papers/2503.14478', 'title': 'Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM', 'url': 'https://huggingface.co/papers/2503.14478', 'abstract': "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.", 'score': 37, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '120f1d8ec2eb88a8', 'authors': ['Xinyu Fang', 'Zhijian Chen', 'Kai Lan', 'Shengyuan Ding', 'Yingji Liang', 'Xiangyu Zhao', 'Farong Wen', 'Zicheng Zhang', 'Guofeng Zhang', 'Haodong Duan', 'Kai Chen', 'Dahua Lin'], 'affiliations': ['East China Normal University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14478.jpg', 'data': {'categories': ['#creativity', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Измеряя творчество искусственного интеллекта: новый бенчмарк для мультимодальных моделей', 'desc': 'Статья представляет Creation-MMBench - новый мультимодальный бенчмарк для оценки творческих способностей мультимодальных больших языковых моделей (MLLM) в задачах, основанных на изображениях. Бенчмарк содержит 765 тестовых примеров, охватывающих 51 детализированную задачу, с критериями оценки для каждого случая. Эксперименты показали, что открытые MLLM значительно уступают проприетарным моделям в творческих задачах. Исследование также выявило, что визуальная дообучение может негативно влиять на творческие способности базовой языковой модели.'}, 'en': {'title': 'Unlocking Creativity in Multimodal AI with Creation-MMBench', 'desc': 'This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.'}, 'zh': {'title': '评估多模态大型语言模型的创造力', 'desc': '创造力是智能的一个基本方面，涉及在不同情境中生成新颖且适当的解决方案。虽然大型语言模型（LLMs）在创造能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）的评估仍然相对缺乏。为了解决这一问题，我们推出了Creation-MMBench，这是一个专门设计用于评估MLLMs在基于图像的实际任务中创造能力的多模态基准。实验结果表明，当前的开源MLLMs在创造性任务中显著低于专有模型，而视觉微调可能会对基础LLM的创造能力产生负面影响。'}}}, {'id': 'https://huggingface.co/papers/2503.12797', 'title': 'DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding', 'url': 'https://huggingface.co/papers/2503.12797', 'abstract': 'Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.', 'score': 24, 'issue_id': 2777, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c682a086aaac0fa1', 'authors': ['Xinyu Ma', 'Ziyang Ding', 'Zhicong Luo', 'Chi Chen', 'Zonghao Guo', 'Derek F. Wong', 'Xiaoyi Feng', 'Maosong Sun'], 'affiliations': ['Northwestern Polytechnical University', 'Shandong University', 'Tsinghua University', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.12797.jpg', 'data': {'categories': ['#multimodal', '#data', '#dataset', '#reasoning', '#synthetic', '#benchmark', '#transfer_learning', '#training'], 'emoji': '🔬', 'ru': {'title': 'DeepPerception: Улучшение визуального восприятия ИИ через интеграцию знаний', 'desc': 'Эта статья представляет новый подход к улучшению визуального восприятия мультимодальных языковых моделей (MLLM). Авторы вводят задачу knowledge-intensive visual grounding (KVG), требующую тонкого визуального различения и интеграции специализированных знаний. Предлагается модель DeepPerception, использующая автоматическую генерацию обучающих данных и двухэтапное обучение для улучшения когнитивных способностей MLLM. Эксперименты на созданном датасете KVG-Bench показывают значительное улучшение точности и обобщающей способности модели.'}, 'en': {'title': 'Enhancing Visual Perception in MLLMs with DeepPerception', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.'}, 'zh': {'title': '提升视觉感知的认知整合能力', 'desc': '人类专家在细粒度视觉辨别方面表现出色，能够利用领域知识来优化感知特征，而当前的多模态大型语言模型（MLLMs）在这方面仍显不足。尽管拥有丰富的专家级知识，MLLMs在视觉感知中整合推理的能力较弱，常常直接生成响应而缺乏深入分析。为了解决这一问题，我们提出了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，要求同时具备细粒度感知和领域特定知识的整合。我们提出的DeepPerception模型增强了认知视觉感知能力，通过自动化数据合成和两阶段训练框架，显著提高了在KVG-Bench数据集上的准确性和跨领域泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.12329', 'title': 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era', 'url': 'https://huggingface.co/papers/2503.12329', 'abstract': 'Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.', 'score': 19, 'issue_id': 2778, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'c97a8c730bfcbfa8', 'authors': ['Kanzhi Cheng', 'Wenpo Song', 'Jiaxin Fan', 'Zheng Ma', 'Qiushi Sun', 'Fangzhi Xu', 'Chenyang Yan', 'Nuo Chen', 'Jianbing Zhang', 'Jiajun Chen'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12329.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#cv'], 'emoji': '📸', 'ru': {'title': 'Новый рубеж в генерации подписей к изображениям: ИИ догоняет человека', 'desc': 'Статья посвящена проблеме оценки качества генерации подписей к изображениям с помощью современных моделей компьютерного зрения и обработки естественного языка (VLM). Авторы создали платформу CapArena для сравнения подписей, сгенерированных моделями и людьми, показав, что некоторые модели (например, GPT-4o) достигают или превосходят человеческий уровень. Исследование также оценивает эффективность автоматических метрик для оценки качества подписей, выявляя их ограничения. На основе полученных результатов авторы разработали CapArena-Auto - автоматизированный бенчмарк для оценки детальных подписей к изображениям.'}, 'en': {'title': 'Elevating Image Captioning: Human-Level Performance and Reliable Metrics', 'desc': 'This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking.'}, 'zh': {'title': '图像描述的新标准：VLM的崛起与评估', 'desc': '图像描述一直是视觉与语言研究中的一个重要挑战。随着大型语言模型（LLMs）的发展，现代视觉-语言模型（VLMs）能够生成详细且全面的图像描述。本文通过建立CapArena平台，评估当前VLM在图像描述任务上的表现，发现领先模型如GPT-4o的表现甚至超过了人类。我们还分析了自动评估指标的可靠性，结果表明VLM作为评判者在描述质量评估中表现出色，提供了一种新的高效基准方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13424', 'title': 'Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation', 'url': 'https://huggingface.co/papers/2503.13424', 'abstract': 'Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility', 'score': 18, 'issue_id': 2776, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '20793013b58dba36', 'authors': ['Xinyu Lian', 'Zichao Yu', 'Ruiming Liang', 'Yitong Wang', 'Li Ray Luo', 'Kaixu Chen', 'Yuanzhen Zhou', 'Qihong Tang', 'Xudong Xu', 'Zhaoyang Lyu', 'Bo Dai', 'Jiangmiao Pang'], 'affiliations': ['Fudan University', 'Harbin Institute of Technology, Shenzhen', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'South China University of Technology', 'The University of Hong Kong', 'Tongji University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.13424.jpg', 'data': {'categories': ['#3d', '#open_source', '#dataset', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Процедурная генерация сочлененных объектов для воплощенного ИИ', 'desc': 'Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью процедурной генерации. Авторы утверждают, что их подход превосходит современные методы и сравним с датасетами, размеченными вручную, по физическим свойствам и качеству полигональных сеток. Метод решает проблему ограниченности существующих подходов, основанных на данных или симуляции. Синтетические данные, полученные с помощью Infinite Mobility, могут использоваться для обучения генеративных моделей, что открывает возможности для масштабирования.'}, 'en': {'title': 'Revolutionizing Articulated Object Creation with Infinite Mobility', 'desc': 'This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI.'}, 'zh': {'title': '无限移动：合成高保真关节物体的新方法', 'desc': '在这篇论文中，我们提出了一种名为无限移动（Infinite Mobility）的方法，用于通过程序生成合成高保真度的关节物体。这种方法克服了现有数据驱动或模拟方法在规模和质量上的限制。用户研究和定量评估表明，我们的方法在物理属性和网格质量上超越了当前最先进的方法，并且与人工标注的数据集相当。此外，我们的合成数据可以作为生成模型的训练数据，支持后续的扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.14125', 'title': 'Frac-Connections: Fractional Extension of Hyper-Connections', 'url': 'https://huggingface.co/papers/2503.14125', 'abstract': 'Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.', 'score': 13, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '936ea0aa4972c382', 'authors': ['Defa Zhu', 'Hongzhi Huang', 'Jundong Zhou', 'Zihao Huang', 'Yutao Zeng', 'Banggu Wu', 'Qiyang Min', 'Xun Zhou'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.14125.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний', 'desc': 'Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот метод развивает идею остаточных соединений (residual connections), разделяя скрытые состояния на несколько частей вместо расширения их ширины. Frac-Connections сохраняют преимущества Hyper-Connections, при этом снижая потребление памяти. Эффективность подхода подтверждена масштабными экспериментами на языковых задачах, включая обучение модели MoE размером 7 миллиардов параметров на 3 триллионах токенов.'}, 'en': {'title': 'Frac-Connections: Efficient Memory for Deep Learning', 'desc': 'This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections.'}, 'zh': {'title': 'Frac-Connections：优化深度学习的内存使用', 'desc': '残差连接是现代深度学习架构的核心，能够通过减轻梯度消失问题来训练非常深的网络。超连接最近通过在不同深度引入多个连接强度来推广残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。我们提出了Frac-Connections，这是一种新方法，通过将隐藏状态划分为多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少了内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.14492', 'title': 'Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control', 'url': 'https://huggingface.co/papers/2503.14492', 'abstract': 'We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.', 'score': 12, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5098c043161888fa', 'authors': ['NVIDIA', ':', 'Hassan Abu Alhaija', 'Jose Alvarez', 'Maciej Bala', 'Tiffany Cai', 'Tianshi Cao', 'Liz Cha', 'Joshua Chen', 'Mike Chen', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Yunhao Ge', 'Jinwei Gu', 'Ali Hassani', 'Michael Isaev', 'Pooya Jannaty', 'Shiyi Lan', 'Tobias Lasser', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Fabio Ramos', 'Xuanchi Ren', 'Tianchang Shen', 'Shitao Tang', 'Ting-Chun Wang', 'Jay Wu', 'Jiashu Xu', 'Stella Xu', 'Kevin Xie', 'Yuchong Ye', 'Xiaodong Yang', 'Xiaohui Zeng', 'Yu Zeng'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.14492.jpg', 'data': {'categories': ['#agents', '#robotics', '#transfer_learning', '#open_source', '#inference', '#3d'], 'emoji': '🌌', 'ru': {'title': 'Адаптивная генерация миров с пространственным контролем', 'desc': 'Cosmos-Transfer - это условная модель генерации миров, способная создавать симуляции на основе нескольких пространственных входных данных разных модальностей. Модель позволяет гибко настраивать веса различных условных входов в разных пространственных локациях. Это обеспечивает высоко контролируемую генерацию миров и находит применение в различных сценариях переноса между мирами, включая Sim2Real. Авторы провели обширные оценки модели и продемонстрировали ее применение для физического ИИ, в том числе для робототехники Sim2Real и обогащения данных для беспилотных автомобилей.'}, 'en': {'title': 'Empowering World Generation with Adaptive Control', 'desc': 'Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.'}, 'zh': {'title': '可控的世界生成，助力物理人工智能', 'desc': '我们介绍了Cosmos-Transfer，这是一种条件世界生成模型，可以根据多种空间控制输入（如分割、深度和边缘）生成世界模拟。在设计上，空间条件方案是自适应和可定制的，允许在不同空间位置对不同条件输入进行加权。这使得世界生成具有高度可控性，并在多种世界到世界的转移应用中找到用途，包括Sim2Real。我们进行了广泛的评估，分析了所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。'}}}, {'id': 'https://huggingface.co/papers/2503.14504', 'title': 'Aligning Multimodal LLM with Human Preference: A Survey', 'url': 'https://huggingface.co/papers/2503.14504', 'abstract': 'Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.', 'score': 10, 'issue_id': 2778, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'b33bcba515cfa942', 'authors': ['Tao Yu', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Junkang Wu', 'Jinda Lu', 'Kun Wang', 'Xingyu Lu', 'Yunhang Shen', 'Guibin Zhang', 'Dingjie Song', 'Yibo Yan', 'Tianlong Xu', 'Qingsong Wen', 'Zhang Zhang', 'Yan Huang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Institute of automation, Chinese academy of science', 'Lehigh University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shenzhen International Graduate School, Tsinghua University', 'Squirrel Ai Learning', 'Tencent Youtu Lab', 'The Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14504.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#dataset', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Выравнивание мультимодальных ИИ: путь к безопасности и эффективности', 'desc': 'Эта статья представляет собой всесторонний обзор алгоритмов выравнивания для мультимодальных больших языковых моделей (MLLM). Авторы рассматривают различные сценарии применения этих алгоритмов, включая понимание изображений, видео и аудио. Они анализируют ключевые факторы в создании наборов данных для выравнивания и обсуждают бенчмарки для оценки эффективности алгоритмов. Статья также предлагает потенциальные направления для будущих исследований в этой области.'}, 'en': {'title': 'Aligning MLLMs: Bridging Gaps for Better Understanding', 'desc': 'This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs.'}, 'zh': {'title': '对齐算法助力多模态语言模型的未来', 'desc': '大型语言模型（LLMs）能够通过简单的提示处理各种通用任务，而无需特定任务的训练。多模态大型语言模型（MLLMs）在处理涉及视觉、听觉和文本数据的复杂任务方面展现了令人印象深刻的潜力。然而，关于真实性、安全性、类o1推理和与人类偏好的对齐等关键问题仍未得到充分解决。本文旨在系统性地回顾MLLMs的对齐算法，探讨其应用场景、数据集构建核心因素、评估基准以及未来发展方向。'}}}, {'id': 'https://huggingface.co/papers/2503.14499', 'title': 'Measuring AI Ability to Complete Long Tasks', 'url': 'https://huggingface.co/papers/2503.14499', 'abstract': "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.", 'score': 9, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'c31aeeed7f6139af', 'authors': ['Thomas Kwa', 'Ben West', 'Joel Becker', 'Amy Deng', 'Katharyn Garcia', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Nate Rush', 'Sydney Von Arx', 'Ryan Bloom', 'Thomas Broadley', 'Haoxing Du', 'Brian Goodrich', 'Nikola Jurkovic', 'Luke Harold Miles', 'Seraphina Nix', 'Tao Lin', 'Neev Parikh', 'David Rein', 'Lucas Jun Koba Sato', 'Hjalmar Wijk', 'Daniel M. Ziegler', 'Elizabeth Barnes', 'Lawrence Chan'], 'affiliations': ['Anthropic', 'Model Evaluation & Threat Research (METR)'], 'pdf_title_img': 'assets/pdf/title_img/2503.14499.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#reasoning'], 'emoji': '⏳', 'ru': {'title': 'Время на вашей стороне: ИИ догоняет человека в скорости выполнения задач', 'desc': 'Статья предлагает новую метрику для оценки возможностей ИИ-систем - горизонт времени 50%-ного выполнения задач. Этот показатель измеряет время, которое обычно требуется людям для выполнения задач, которые модели ИИ могут выполнить с 50%-ной вероятностью успеха. Исследование показало, что современные передовые модели ИИ, такие как Claude 3.7 Sonnet, имеют горизонт времени около 50 минут. Авторы отмечают, что горизонт времени ИИ удваивается примерно каждые семь месяцев с 2019 года, и обсуждают потенциальные последствия повышения автономности ИИ-систем.'}, 'en': {'title': 'Measuring AI Progress: The 50%-Task-Completion Time Horizon', 'desc': 'This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.'}, 'zh': {'title': 'AI能力的新度量：50%任务完成时间', 'desc': '尽管人工智能在基准测试上取得了快速进展，但其实际表现的意义仍不明确。我们提出了一种新的度量标准：50%任务完成时间范围，旨在量化AI系统与人类能力的对比。通过对人类专家在多个任务上的完成时间进行测量，我们发现当前的前沿AI模型在这些任务上的50%时间范围约为50分钟。我们的研究表明，AI模型的时间范围自2019年以来大约每七个月翻倍，未来五年内，AI系统可能能够自动化许多目前需要人类一个月才能完成的软件任务。'}}}, {'id': 'https://huggingface.co/papers/2503.12355', 'title': 'Atlas: Multi-Scale Attention Improves Long Context Image Modeling', 'url': 'https://huggingface.co/papers/2503.12355', 'abstract': 'Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas.', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '0b78d0c7af506f20', 'authors': ['Kumar Krishna Agrawal', 'Long Lian', 'Longchao Liu', 'Natalia Harguindeguy', 'Boyi Li', 'Alexander Bick', 'Maggie Chung', 'Trevor Darrell', 'Adam Yala'], 'affiliations': ['University of California San Francisco', 'University of California, Berkeley', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12355.jpg', 'data': {'categories': ['#cv', '#open_source', '#architecture', '#optimization', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'Масштабируемое внимание для эффективной обработки гигантских изображений', 'desc': 'Статья представляет новый подход к эффективному моделированию больших изображений - Multi-Scale Attention (MSA). MSA использует многомасштабные представления и двунаправленную коммуникацию между масштабами, создавая O(log N) уровней для представления изображения. На основе MSA авторы разработали нейросетевую архитектуру Atlas, которая значительно улучшает соотношение производительности и вычислительных затрат при моделировании изображений с большим контекстом. Atlas демонстрирует высокую точность и скорость работы по сравнению с современными моделями на задаче классификации изображений высокого разрешения.'}, 'en': {'title': 'Revolutionizing Image Modeling with Multi-Scale Attention', 'desc': 'This paper presents Multi-Scale Attention (MSA), a method designed to efficiently model large images in machine learning. MSA utilizes multi-scale representations and bi-directional cross-scale communication to enhance image feature extraction. The authors introduce Atlas, a neural network architecture that implements MSA, achieving significant improvements in speed and accuracy for high-resolution image tasks. Atlas outperforms existing models like ConvNext and FasterViT, demonstrating a superior compute-performance tradeoff while maintaining high accuracy on the ImageNet dataset.'}, 'zh': {'title': '高效图像建模的新突破：多尺度注意力', 'desc': '在机器学习中，高效建模大规模图像一直是一个挑战。为此，我们提出了多尺度注意力（MSA），它依赖于多尺度表示和双向跨尺度通信两个关键思想。MSA通过创建O(log N)的尺度来表示图像，并利用交叉注意力在不同尺度之间传播信息。我们还介绍了一种基于MSA的新型神经网络架构Atlas，实验表明Atlas在高分辨率图像建模中显著提高了计算性能的平衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10522', 'title': 'AudioX: Diffusion Transformer for Anything-to-Audio Generation', 'url': 'https://huggingface.co/papers/2503.10522', 'abstract': 'Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c3590459e5737c75', 'authors': ['Zeyue Tian', 'Yizhu Jin', 'Zhaoyang Liu', 'Ruibin Yuan', 'Xu Tan', 'Qifeng Chen', 'Wei Xue', 'Yike Guo'], 'affiliations': ['Hong Kong University of Science and Technology', 'Moonshot AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10522.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#diffusion', '#audio', '#multimodal', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'AudioX: универсальная модель для генерации аудио из чего угодно', 'desc': 'AudioX - это унифицированная модель Diffusion Transformer для генерации аудио и музыки на основе различных входных данных. Модель использует стратегию мультимодального маскированного обучения для создания надежных кросс-модальных представлений. Для обучения были созданы два больших набора данных: vggsound-caps и V2M-caps. Эксперименты показывают, что AudioX не уступает специализированным моделям и обладает высокой универсальностью в обработке различных модальностей входных данных.'}, 'en': {'title': 'AudioX: Unifying Audio and Music Generation Across Modalities', 'desc': 'This paper introduces AudioX, a novel Diffusion Transformer model designed for generating audio and music from various input types. Unlike traditional models that focus on specific tasks, AudioX integrates multiple modalities, allowing it to process text, video, images, and audio seamlessly. The model employs a unique multi-modal masked training strategy, which enhances its ability to learn robust representations by masking inputs across different modalities. Additionally, the authors address the challenge of limited high-quality training data by creating two extensive datasets, enabling AudioX to outperform existing specialized models in versatility and performance.'}, 'zh': {'title': '统一音频与音乐生成的创新模型', 'desc': '音频和音乐生成在许多应用中变得越来越重要，但现有方法存在显著局限性。我们提出了AudioX，这是一种统一的扩散变换器模型，能够实现多种输入到音频和音乐的生成。AudioX的创新之处在于其多模态掩蔽训练策略，使模型能够从不同模态的掩蔽输入中学习，从而生成高质量的音频和音乐。通过构建两个全面的数据集，AudioX在处理多样化输入模态和生成任务方面展现了卓越的灵活性和性能。'}}}, {'id': 'https://huggingface.co/papers/2503.12505', 'title': 'MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification', 'url': 'https://huggingface.co/papers/2503.12505', 'abstract': 'Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.', 'score': 8, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '0ac4fdd3411855ac', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Jiaxin Ai', 'Wangbo Zhao', 'Kai Wang', 'Xiaojiang Peng', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.12505.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#rl'], 'emoji': '🧠', 'ru': {'title': 'MPBench: Комплексная оценка рассуждений языковых моделей', 'desc': 'Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценариях рассуждений. MPBench включает три парадигмы оценки: корректность шагов, агрегацию ответов и поиск процесса рассуждений. Бенчмарк охватывает мультимодальные задачи и позволяет всесторонне оценить PRM в контексте рассуждений языковых моделей. Это помогает улучшить способности больших языковых моделей (LLM) к рассуждениям и выявлению ошибок в процессе.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with MPBench', 'desc': "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."}, 'zh': {'title': '全面评估推理过程的多模态基准', 'desc': '推理是大型语言模型（LLMs）处理复杂任务的重要能力，而识别过程错误对于提升这一能力至关重要。最近提出的过程级奖励模型（PRMs）通过提供逐步奖励，促进了强化学习和数据生成，从而在推理过程中引导LLMs走向正确的步骤，提高推理准确性。然而，现有的PRMs基准主要基于文本，专注于错误检测，忽视了推理搜索等其他场景。为了解决这一问题，我们引入了MPBench，这是一个全面的多任务多模态基准，旨在系统评估PRMs在不同场景中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13265', 'title': 'FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.13265', 'abstract': 'Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg} rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.', 'score': 7, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '8b68e5c333f5fe62', 'authors': ['Luxi Chen', 'Zihan Zhou', 'Min Zhao', 'Yikai Wang', 'Ge Zhang', 'Wenhao Huang', 'Hao Sun', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'ByteDance', 'Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University', 'Gaoling School of AI, Renmin University of China', 'School of Artificial Intelligence, Beijing Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13265.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': '🌐', 'ru': {'title': 'Создание гибких 3D-миров из одного изображения', 'desc': 'FlexWorld - это новая система для создания гибких 3D-сцен из одиночных изображений, включая вращение на 360° и масштабирование. Она состоит из мощной модели диффузии видео-в-видео для генерации высококачественных изображений с новых ракурсов и процесса прогрессивного расширения для построения полной 3D-сцены. FlexWorld использует предобученную видеомодель и точные пары обучающих данных с оценкой глубины для генерации видов при значительных изменениях положения камеры. Эксперименты показывают превосходство FlexWorld над существующими методами в генерации высококачественных видео с новых ракурсов и гибких 3D-сцен из одиночных изображений.'}, 'en': {'title': 'Transforming Single Images into Dynamic 3D Worlds', 'desc': 'FlexWorld is a new framework designed to create flexible 3D scenes from single images, addressing the challenge of limited 3D data. It features a video-to-video (V2V) diffusion model that generates high-quality images from incomplete scenes, allowing for significant camera pose variations. Additionally, it employs a progressive expansion process to build a complete 3D environment, integrating new content through geometry-aware scene fusion. Experiments show that FlexWorld outperforms existing methods in generating high-fidelity scenes with flexible viewing options like 360-degree rotations and zooming.'}, 'zh': {'title': 'FlexWorld：从单图像生成灵活视角3D场景的创新框架', 'desc': 'FlexWorld是一个新颖的框架，旨在从单张图像生成灵活视角的3D场景，包括360度旋转和缩放。该框架包含两个关键组件：一个强大的视频到视频（V2V）扩散模型，用于从粗糙场景渲染的缺失输入中生成高质量的新视图图像，以及一个渐进扩展过程，用于构建完整的3D场景。通过利用先进的预训练视频模型和准确的深度估计训练对，V2V模型能够在大相机姿态变化下生成新视图。实验结果表明，FlexWorld在生成高质量新视图视频和灵活视角3D场景方面表现优越，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13111', 'title': 'MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.13111', 'abstract': 'Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.', 'score': 6, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'ce045e2d94eafd19', 'authors': ['Erik Daxberger', 'Nina Wenzel', 'David Griffiths', 'Haiming Gang', 'Justin Lazarow', 'Gefen Kohavi', 'Kai Kang', 'Marcin Eichner', 'Yinfei Yang', 'Afshin Dehghan', 'Peter Grasch'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.13111.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#3d', '#multimodal', '#benchmark', '#dataset'], 'emoji': '🏠', 'ru': {'title': 'Новый шаг к пониманию 3D-пространства искусственным интеллектом', 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей пониманию трехмерного пространства. Авторы создали набор данных CA-VQA для обучения и оценки моделей на задачах пространственного анализа в интерьерах. На основе этих данных была обучена модель MM-Spatial, показавшая высокие результаты в понимании 3D-пространства. Исследование демонстрирует, что включение метрической глубины и многоракурсных изображений улучшает пространственное понимание моделей.'}, 'en': {'title': 'Enhancing 3D Understanding in MLLMs with CA-VQA', 'desc': 'This paper addresses the limitations of multimodal large language models (MLLMs) in understanding 3D spaces. The authors introduce a new supervised fine-tuning dataset called Cubify Anything VQA (CA-VQA), which includes high-quality 3D scene data with open-set annotations. CA-VQA focuses on various spatial tasks such as predicting spatial relationships and estimating sizes and distances in indoor environments. The study demonstrates that their model, MM-Spatial, achieves state-of-the-art performance in 3D spatial understanding by leveraging the rich data from CA-VQA, enhancing depth perception capabilities.'}, 'zh': {'title': '提升三维空间理解的多模态语言模型', 'desc': '这篇论文介绍了一种新的多模态大型语言模型（MLLM），它在理解二维视觉方面表现出色，但在三维空间推理上仍然有限。研究者利用大规模高质量的三维场景数据，创建了一个新的监督微调数据集和评估基准，专注于室内场景。新数据集Cubify Anything VQA（CA-VQA）涵盖了多种空间任务，包括空间关系预测、度量大小和距离估计以及三维定位。研究表明，通过结合度量深度和多视角输入，模型在三维理解方面的表现得到了显著提升，甚至达到了专用单目深度估计模型的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.12271', 'title': 'Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection', 'url': 'https://huggingface.co/papers/2503.12271', 'abstract': 'The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.', 'score': 6, 'issue_id': 2778, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 марта', 'en': 'March 15', 'zh': '3月15日'}, 'hash': '2f560e2ec0839955', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Arsh Koneru', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2503.12271.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#inference', '#diffusion', '#reasoning'], 'emoji': '🖼️', 'ru': {'title': 'Reflect-DiT: Умное улучшение генерации изображений через анализ и рефлексию', 'desc': 'Эта статья представляет новый метод Reflect-DiT для улучшения генерации изображений по текстовому описанию. В отличие от традиционного подхода best-of-N, Reflect-DiT позволяет моделям Diffusion Transformer анализировать ранее сгенерированные изображения и текстовую обратную связь для улучшения результатов. Метод показывает улучшение производительности на бенчмарке GenEval, достигая нового рекордного результата 0.81. Reflect-DiT демонстрирует эффективность подхода inference-time scaling для повышения качества генерации изображений.'}, 'en': {'title': 'Refining Image Generation with Reflective Learning', 'desc': 'This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models.'}, 'zh': {'title': '反思生成，提升图像质量！', 'desc': '本文提出了一种新的文本到图像生成方法，称为Reflect-DiT。该方法通过在生成过程中引入上下文反思能力，帮助Diffusion Transformers根据之前生成的图像和文本反馈进行改进。与传统的最佳N采样方法不同，Reflect-DiT能够针对特定的改进需求进行调整，从而提高生成质量。实验结果表明，Reflect-DiT在GenEval基准测试中表现优异，达到了新的最先进分数。'}}}, {'id': 'https://huggingface.co/papers/2503.14495', 'title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'url': 'https://huggingface.co/papers/2503.14495', 'abstract': 'Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency', 'score': 5, 'issue_id': 2779, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '590d8cdf2ae26151', 'authors': ['Jiacheng Guo', 'Yue Wu', 'Jiahao Qiu', 'Kaixuan Huang', 'Xinzhe Juan', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14495.jpg', 'data': {'categories': ['#math', '#benchmark', '#optimization', '#reasoning', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Повышение точности верификации математических рассуждений через временную согласованность', 'desc': 'Статья представляет новый метод временной согласованности для верификации математических рассуждений. В отличие от одноразовой проверки или подходов с участием нескольких моделей, этот метод использует последовательность действий самоанализа для повышения точности верификации. Эмпирические оценки на различных эталонных тестах по выявлению ошибок в математических процессах показали стабильное улучшение производительности по сравнению с базовыми методами. При применении к недавно дистиллированным моделям DeepSeek R1 метод продемонстрировал высокую эффективность, позволив моделям 7B/8B превзойти все модели 70B/72B и GPT-4o на тесте ProcessBench.'}, 'en': {'title': 'Enhancing Verification Accuracy through Temporal Consistency', 'desc': 'This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': '提升数学验证的时间一致性方法', 'desc': '本文提出了一种新的时间一致性验证方法，旨在提高数学推理的有效性。该方法通过迭代地根据之前的评估来细化判断，克服了单轮验证和多模型辩论的局限性。通过在多个数学过程错误识别基准（如Mathcheck、ProcessBench和PRM800K）上的实证评估，显示出相较于基线方法的一致性性能提升。应用于最新的DeepSeek R1蒸馏模型时，我们的方法使得7B/8B蒸馏模型在ProcessBench上超越了所有70B/72B模型和GPT-4o。'}}}, {'id': 'https://huggingface.co/papers/2503.14151', 'title': 'Concat-ID: Towards Universal Identity-Preserving Video Synthesis', 'url': 'https://huggingface.co/papers/2503.14151', 'abstract': "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.", 'score': 5, 'issue_id': 2783, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'ba38580b0d116018', 'authors': ['Yong Zhong', 'Zhuoyi Yang', 'Jiayan Teng', 'Xiaotao Gu', 'Chongxuan Li'], 'affiliations': ['Gaoling School of AI, Renmin University of China, Beijing, China', 'Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.14151.jpg', 'data': {'categories': ['#benchmark', '#3d', '#video'], 'emoji': '🎬', 'ru': {'title': 'Concat-ID: Новый стандарт в генерации видео с сохранением идентичности', 'desc': 'Concat-ID - это унифицированная система для генерации видео с сохранением идентичности. Она использует вариационные автоэнкодеры для извлечения признаков изображений и 3D механизмы самовнимания для обработки видеопоследовательностей. Предложена новая стратегия сопоставления видео и многоэтапное обучение для баланса между сохранением идентичности и редактируемостью лиц. Эксперименты показывают превосходство Concat-ID над существующими методами в генерации видео с одним и несколькими персонажами.'}, 'en': {'title': 'Identity-Preserving Video Generation Made Easy with Concat-ID', 'desc': 'Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds.'}, 'zh': {'title': 'Concat-ID：身份一致性视频生成的新标杆', 'desc': 'Concat-ID 是一个统一的框架，用于生成保持身份一致性的视频。它使用变分自编码器提取图像特征，并将这些特征与视频潜在变量在序列维度上进行连接，完全依赖于 3D 自注意力机制，而无需额外模块。该方法引入了一种新颖的跨视频配对策略和多阶段训练方案，以平衡身份一致性和面部可编辑性，同时提高视频的自然性。实验结果表明，Concat-ID 在单一和多身份生成方面优于现有方法，并且能够无缝扩展到多主体场景，包括虚拟试穿和背景可控生成。'}}}, {'id': 'https://huggingface.co/papers/2503.12545', 'title': 'PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.12545', 'abstract': 'In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.', 'score': 4, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '8a908fbc8ce24853', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Weidong Tang', 'Jiaxin Ai', 'Wangbo Zhao', 'Xiaojiang Peng', 'Kai Wang', 'Yang You', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2503.12545.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#security', '#benchmark', '#dataset'], 'emoji': '🔒', 'ru': {'title': 'PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей', 'desc': 'Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU) в мультимодальных больших языковых моделях (MLLM). PEBench включает набор данных с личными сущностями и соответствующими общими сценами событий. Авторы протестировали 6 методов MU, выявив их сильные и слабые стороны. Исследование направлено на продвижение разработки безопасных и конфиденциальных мультимодальных моделей.'}, 'en': {'title': 'Enhancing Privacy in Multimodal Models with Machine Unlearning', 'desc': 'This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems.'}, 'zh': {'title': '推动多模态模型的安全与隐私保护', 'desc': '近年来，多模态大型语言模型（MLLMs）在视觉问答、视觉理解和推理等任务上取得了显著进展。然而，这些进展依赖于从互联网收集的大量数据，这引发了隐私和安全方面的重大担忧。为了解决这些问题，机器遗忘（MU）作为一种有前景的解决方案应运而生，能够在不需要从头开始重新训练的情况下，从已训练的模型中删除特定知识。我们引入了一个基准测试PEBench，旨在全面评估MU在MLLMs中的表现，推动安全和隐私保护的多模态模型研究。'}}}, {'id': 'https://huggingface.co/papers/2503.12303', 'title': 'Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs', 'url': 'https://huggingface.co/papers/2503.12303', 'abstract': "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLM's systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt a structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct a next-generation foundation MLLM with self-improved cognition, SIcog first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through self-consistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both low- and high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SIcog produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmark-leading performance compared to prevalent pre-training approaches.", 'score': 4, 'issue_id': 2787, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'd0f6251740b9c15c', 'authors': ['Xiaoying Zhang', 'Da Peng', 'Yipeng Zhang', 'Zonghao Guo', 'Chengyue Wu', 'Chi Chen', 'Wei Ke', 'Helen Meng', 'Maosong Sun'], 'affiliations': ['The Chinese University of Hong Kong', 'The University of Hong Kong', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12303.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#transfer_learning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствующийся ИИ: новый подход к обучению мультимодальных языковых моделей', 'desc': 'Статья представляет SIcog - фреймворк для улучшения когнитивных способностей мультимодальных больших языковых моделей (MLLM). SIcog использует метод Chain-of-Description для пошагового понимания визуальной информации и структурированное рассуждение по цепочке для интеграции мультимодальных рассуждений. Фреймворк сначала обучает MLLM систематическому восприятию и рассуждению с минимальными внешними аннотациями, затем генерирует детальные подписи и данные для рассуждений. Эксперименты показывают, что SIcog значительно улучшает когнитивные способности MLLM, достигая ведущих результатов на различных бенчмарках.'}, 'en': {'title': 'Enhancing MLLMs with Self-Improving Cognition for Better Perception and Reasoning', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in fine-grained perception and complex reasoning. It introduces Self-Improving cognition (SIcog), a framework that enhances MLLMs by using self-generated data for multimodal pre-training. The proposed Chain-of-Description method allows MLLMs to develop systematic visual understanding, improving the quality of generated captions and reasoning. Through minimal external annotations, SIcog enables MLLMs to achieve superior performance on various benchmarks with significantly fewer training samples.'}, 'zh': {'title': '自我改进认知，提升多模态模型的智能', 'desc': '尽管多模态大型语言模型（MLLMs）具有强大的能力，但在细粒度感知和复杂推理方面仍面临挑战。现有的多模态预训练方法主要通过高质量的图像描述来增强感知，但收集链式思维（CoT）推理数据的成本极高。本文提出了一种自我改进认知框架（SIcog），旨在通过自生成数据的多模态预训练来提升MLLM的系统认知能力。我们提出的描述链方法能够逐步增强视觉理解，确保生成的描述更全面、更准确，同时结合结构化的CoT推理技术，实现深入的多模态推理。'}}}, {'id': 'https://huggingface.co/papers/2503.09443', 'title': 'Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2503.09443', 'abstract': 'Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).', 'score': 4, 'issue_id': 2784, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'c895c390f49ed5c1', 'authors': ['Julian Spravil', 'Sebastian Houben', 'Sven Behnke'], 'affiliations': ['Fraunhofer IAIS, Germany', 'Lamarr Institute for Machine Learning and Artificial Intelligence, Germany', 'University of Applied Sciences Bonn-Rhein-Sieg, Germany', 'University of Bonn, Computer Science Institute VI, Center for Robotics, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.09443.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#multimodal', '#training', '#synthetic', '#machine_translation', '#long_context', '#transfer_learning'], 'emoji': '🌐', 'ru': {'title': 'Монолингвальные модели преодолевают языковые барьеры в мультимодальных задачах', 'desc': 'Статья исследует масштабируемость монолингвальных мультимодальных моделей для решения мультиязычных задач. Авторы предлагают модель Florenz, объединяющую предобученную мультимодальную модель Florence-2 и языковую модель Gemma-2. Модель обучается на синтетическом датасете с неполным языковым покрытием для генерации описаний изображений. Результаты показывают, что способность к описанию изображений на конкретном языке может появиться даже при наличии только данных для задачи перевода.'}, 'en': {'title': 'Florenz: Bridging Languages with Monolingual Vision-Language Models', 'desc': 'This paper introduces Florenz, a monolingual vision-language model (VLM) designed to enhance cross-lingual transfer for vision tasks. Unlike existing multilingual models that struggle with performance due to the complexities of multiple languages, Florenz leverages a single language to achieve systematic generalization across various languages. The model is built on a combination of the pre-trained VLM Florence-2 and the large language model Gemma-2, with a focus on scaling laws related to model size and training data. Results show that Florenz can effectively learn to perform image captioning in new languages, even when trained only on translation tasks, demonstrating strong performance in several downstream applications.'}, 'zh': {'title': '单语模型的跨语言能力新突破', 'desc': '这篇论文研究了跨语言迁移如何使视觉语言模型（VLMs）在仅用一种语言的训练数据下执行视觉任务。当前的方法依赖于大型预训练的多语言模型，但在多语言能力与下游任务性能之间存在权衡。我们提出了Florenz，一个单语编码器-解码器VLM，结合了预训练的Florence-2和大型语言模型Gemma-2，具有0.4B到11.2B的参数。通过在合成数据集上训练，Florenz展示了即使只有翻译任务的数据，也能在特定语言中产生图像描述能力。'}}}, {'id': 'https://huggingface.co/papers/2503.10905', 'title': 'Learning to Inference Adaptively for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.10905', 'abstract': 'Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/.', 'score': 2, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c1a89325eab3a9af', 'authors': ['Zhuoyan Xu', 'Khoi Duc Nguyen', 'Preeti Mukherjee', 'Saurabh Bagchi', 'Somali Chaterji', 'Yingyu Liang', 'Yin Li'], 'affiliations': ['Purdue University', 'The University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.10905.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#optimization', '#hallucinations', '#multimodal', '#inference'], 'emoji': '⚡', 'ru': {'title': 'AdaLLaVA: Адаптивный вывод для эффективных мультимодальных языковых моделей', 'desc': 'AdaLLaVA - это адаптивная система вывода для мультимодальных больших языковых моделей (MLLM), которая динамически реконфигурирует операции во время выполнения с учетом входных данных и ограничений по задержке. Эта система позволяет эффективно использовать MLLM в условиях ограниченных ресурсов, адаптируясь к меняющимся условиям выполнения. Эксперименты показали, что AdaLLaVA успешно соблюдает заданные ограничения по задержке, достигая различных компромиссов между точностью и скоростью работы. Система также демонстрирует способность адаптироваться к входным данным и обобщаться на различные MLLM.'}, 'en': {'title': 'Adaptive Inference for Efficient Multimodal Language Models', 'desc': 'This paper presents AdaLLaVA, an adaptive inference framework designed for Multimodal Large Language Models (MLLMs) to optimize their performance under varying computational resources. It addresses the challenge of maintaining efficiency while responding to different runtime conditions, such as resource contention from other applications. AdaLLaVA dynamically adjusts the operations of MLLMs during inference based on the input data and a specified latency budget, allowing for flexible accuracy and latency trade-offs. The framework has been tested across various benchmarks, demonstrating its ability to adapt to both input characteristics and latency requirements, while also integrating with token selection for improved efficiency.'}, 'zh': {'title': '自适应推理，提升多模态模型效率', 'desc': '多模态大型语言模型（MLLMs）在推理方面表现出色，但其计算成本高，限制了在资源受限环境中的应用。尽管近期对提高MLLMs效率的努力有所增加，但现有解决方案在应对不同运行时条件方面仍显不足，特别是在资源可用性变化时。为了解决这个问题，我们提出了AdaLLaVA，这是一种自适应推理框架，能够在推理过程中动态重新配置MLLM的操作，考虑输入数据和延迟预算。我们的实验表明，AdaLLaVA能够有效遵循输入延迟预算，实现运行时的准确性和延迟之间的权衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10546', 'title': 'KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.10546', 'abstract': 'With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'f856affbe8bcf064', 'authors': ['Zixian Liu', 'Mingtong Zhang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10546.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#robotics', '#open_source', '#agents'], 'emoji': '🤖', 'ru': {'title': 'KUDA: Динамическое планирование для роботов с открытым словарем', 'desc': 'KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное управление через ключевые точки. Она использует модели видео-языкового взаимодействия (VLM) и нейронные модели динамики для планирования траекторий робота. KUDA сначала назначает ключевые точки на RGB-изображении и запрашивает VLM для генерации целевых спецификаций. Затем эти абстрактные представления на основе ключевых точек преобразуются в функции стоимости, которые оптимизируются с помощью обученной модели динамики.'}, 'en': {'title': 'KUDA: Bridging Language and Dynamics for Robotic Manipulation', 'desc': 'This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types.'}, 'zh': {'title': 'KUDA：动态学习与视觉提示的开放词汇操作系统', 'desc': '随着大语言模型（LLMs）和视觉语言模型（VLMs）的快速发展，开放词汇的机器人操作系统取得了显著进展。然而，许多现有方法忽视了物体动态的重要性，限制了它们在更复杂动态任务中的适用性。我们提出了KUDA，一个集成了动态学习和通过关键点进行视觉提示的开放词汇操作系统，利用了VLMs和基于学习的神经动态模型。KUDA通过将关键点分配给RGB图像，并查询VLM生成目标规范，将抽象的关键点表示转换为成本函数，从而优化机器人轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.10410', 'title': 'RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation', 'url': 'https://huggingface.co/papers/2503.10410', 'abstract': 'Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '44150f611040e79d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#3d', '#optimization', '#data', '#dataset', '#cv', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'RoCo-Sim: прорыв в симуляции дорожного восприятия', 'desc': 'RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, разреженности данных и мультиракурсной согласованности путем генерации синтетических данных. RoCo-Sim включает оптимизацию внешних параметров камер, многоракурсный выборщик с учетом окклюзий, моделирование отношений передний план-фон и инструменты постобработки. Система значительно улучшает 3D-детектирование объектов, превосходя современные методы на популярных наборах данных.'}, 'en': {'title': 'Enhancing Roadside Awareness with Collaborative Perception', 'desc': 'This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets.'}, 'zh': {'title': '提升路边感知的协同力量', 'desc': '路边协同感知是一种系统，多个路边单元协作汇聚感知数据，帮助车辆提高环境意识。现有的路边感知方法主要关注模型设计，但忽视了数据问题，如校准误差、信息稀疏和多视图一致性，导致在最新数据集上的表现不佳。为显著提升路边协同感知并解决关键数据问题，我们提出了首个路边协同感知模拟框架RoCo-Sim。RoCo-Sim能够通过动态前景编辑和单图像的全场景风格迁移生成多样化的、多视图一致的模拟路边数据。'}}}, {'id': 'https://huggingface.co/papers/2503.08893', 'title': 'EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees', 'url': 'https://huggingface.co/papers/2503.08893', 'abstract': "An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also propose a weakness profiling method EvalTree. It constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we release our code and an interface that allows practitioners to interactively explore the capability trees built by EvalTree.", 'score': 2, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '8645a737473c8209', 'authors': ['Zhiyuan Zeng', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Pang Wei Koh'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.08893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#data', '#interpretability', '#training', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'EvalTree: точное профилирование слабостей языковых моделей', 'desc': 'Статья представляет метод EvalTree для создания профиля слабостей языковых моделей. EvalTree строит дерево возможностей, где каждый узел связан с подмножеством тестовых примеров, оценивающих конкретную способность модели. Метод превосходит базовые подходы в точности и полноте выявления слабостей на бенчмарках MATH и WildChat. Профилирование слабостей позволяет целенаправленно собирать обучающие данные, что улучшает производительность модели эффективнее других стратегий.'}, 'en': {'title': 'Uncovering Weaknesses for Stronger Language Models', 'desc': 'This paper focuses on improving the evaluation of Language Models (LMs) by creating a weakness profile that identifies specific areas where the model underperforms. The authors introduce EvalTree, a method that organizes model capabilities into a tree structure, linking each capability to benchmark instances that test it. By analyzing these capabilities, EvalTree generates a detailed profile of weaknesses, allowing for targeted improvements in model training. The results demonstrate that using EvalTree leads to better performance in LMs by guiding data collection based on identified weaknesses, surpassing traditional evaluation methods.'}, 'zh': {'title': '识别模型弱点，提升语言模型性能', 'desc': '本文提出了一种新的语言模型评估方法，旨在识别模型的弱点并提供改进建议。我们引入了弱点分析的概念，通过生成弱点档案来描述模型在基准测试中的表现。我们提出的EvalTree方法构建了一个能力树，能够更准确地识别模型的不足之处。通过在MATH和WildChat基准测试上的实验，EvalTree显示出比传统方法更优越的性能，并且能够指导数据收集以提升模型表现。'}}}, {'id': 'https://huggingface.co/papers/2503.14002', 'title': 'MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling', 'url': 'https://huggingface.co/papers/2503.14002', 'abstract': 'Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is a promising perspective for making these models available in these fields. Creating high-quality, domain-specific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains a significant bottleneck. We present MeshFleet, a filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes a pipeline for automated data filtering based on a quality classifier. This classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through a comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling.', 'score': 1, 'issue_id': 2789, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '8affb04cd78c70c9', 'authors': ['Damian Boborzi', 'Phillip Mueller', 'Jonas Emrich', 'Dominik Schmid', 'Sebastian Mueller', 'Lars Mikelsons'], 'affiliations': ['BMW Group', 'University of Augsburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.14002.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#optimization', '#data', '#3d'], 'emoji': '🚗', 'ru': {'title': 'MeshFleet: Автоматизированная фильтрация данных для 3D генеративных моделей транспортных средств', 'desc': 'Статья представляет MeshFleet - отфильтрованный и аннотированный набор данных 3D моделей транспортных средств, извлеченный из Objaverse-XL. Авторы предлагают автоматизированный пайплайн фильтрации данных на основе классификатора качества, обученного на вручную размеченном подмножестве Objaverse с использованием эмбеддингов DINOv2 и SigLIP. Эффективность метода фильтрации демонстрируется путем сравнительного анализа с методами, основанными на подписях и оценках эстетичности изображений. Исследование подчеркивает важность целенаправленного отбора данных для предметно-ориентированного 3D генеративного моделирования.'}, 'en': {'title': 'Enhancing 3D Generative Models with Targeted Data Selection', 'desc': "This paper discusses advancements in generative models for creating 3D objects, particularly focusing on their application in engineering. It highlights the challenges of achieving the necessary accuracy and quality for specific tasks, which can be addressed by fine-tuning these models with high-quality datasets. The authors introduce MeshFleet, a curated 3D vehicle dataset derived from Objaverse-XL, aimed at improving the training process of generative models. They also present a novel automated data filtering method using a quality classifier, which enhances the dataset's relevance and effectiveness for domain-specific applications."}, 'zh': {'title': 'MeshFleet：提升3D生成模型的关键数据集', 'desc': '生成模型在3D物体领域取得了显著进展，但在工程等领域的实际应用仍然有限，因为它们无法提供所需的准确性、质量和可控性。对大型生成模型进行微调是使这些模型在特定领域可用的有前景的方法。创建高质量、特定领域的3D数据集对于微调大型生成模型至关重要，但数据过滤和标注过程仍然是一个重大瓶颈。我们提出了MeshFleet，这是一个从Objaverse-XL提取的经过过滤和标注的3D车辆数据集，展示了基于质量分类器的自动化数据过滤方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13661', 'title': 'Pensez: Less Data, Better Reasoning -- Rethinking French LLM', 'url': 'https://huggingface.co/papers/2503.13661', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios.', 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '248eff76119a7839', 'authors': ['Huy Hoang Ha'], 'affiliations': ['Menlo Research', 'Université Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2503.13661.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#data', '#training', '#transfer_learning', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'Малые данные, большой результат: оптимизация языковых моделей без масштабирования', 'desc': 'Данная статья исследует стратегический подход к улучшению языковых моделей (LLM) путем тонкой настройки на небольшом, но качественном двуязычном наборе данных. Авторы демонстрируют значительное улучшение способностей модели к математическим рассуждениям и владению французским языком после обучения на всего 2000 тщательно отобранных примерах. Модель Pensez 7B показала увеличение точности до 20% на тесте AIME25 и на 12% на французском эквиваленте теста MATH уровня 5. Результаты исследования ставят под сомнение необходимость огромных наборов данных для достижения высокой производительности LLM в специализированных задачах.'}, 'en': {'title': 'Strategic Fine-Tuning: Small Data, Big Gains!', 'desc': 'This paper explores how to improve large language models (LLMs) in specialized areas like math and French without needing huge datasets. It focuses on strategic fine-tuning using a small, high-quality bilingual dataset of only 2,000 samples. The results show that this targeted approach can significantly boost reasoning accuracy, achieving up to a 20% improvement in mathematical tasks. This challenges the idea that only large datasets can lead to strong performance, suggesting that careful data selection and training can be more effective.'}, 'zh': {'title': '小数据集也能提升大型语言模型的推理能力', 'desc': '大型语言模型（LLMs）在自然语言处理任务中表现出色，但在数学推理和非英语语言等专业领域的表现通常需要大量数据的训练。本文探讨了一种不同的方法：通过在小规模高质量的双语（英语-法语）数据集上进行战略性微调，以提高大型语言模型的推理能力和法语水平。我们通过对仅2000个精心挑选的样本进行有针对性的监督微调（SFT），在数学推理上取得了显著的提升，Pensez 7B在AIME25上的准确率提高了20%，在法语MATH 5级基准上提高了12%。这些结果挑战了大规模数据集是LLMs强大推理性能的先决条件的普遍假设，突显了战略数据策划和优化微调在提升专业技能和多语言能力方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.12127', 'title': 'Hyperbolic Safety-Aware Vision-Language Models', 'url': 'https://huggingface.co/papers/2503.12127', 'abstract': "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC.", 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 марта', 'en': 'March 15', 'zh': '3月15日'}, 'hash': 'fe42b5bcd1d5d4be', 'authors': ['Tobia Poppi', 'Tejaswi Kasarla', 'Pascal Mettes', 'Lorenzo Baraldi', 'Rita Cucchiara'], 'affiliations': ['IIT-CNR, Italy', 'University of Amsterdam, Netherlands', 'University of Modena and Reggio Emilia, Italy', 'University of Pisa, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.12127.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#open_source', '#cv', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Гиперболическая иерархия для осознанной безопасности в мультимодальных моделях', 'desc': 'Статья представляет новый подход к обработке небезопасного контента в мультимодальных моделях, таких как CLIP. Вместо удаления знаний о небезопасных концепциях, авторы предлагают использовать гиперболическое пространство для создания иерархии безопасного и небезопасного контента. Модель HySAC (Hyperbolic Safety-Aware CLIP) использует функции потерь для моделирования иерархических отношений между безопасными и небезопасными парами изображение-текст. Эксперименты показывают, что этот подход улучшает распознавание безопасности и создает более гибкую систему модерации контента.'}, 'en': {'title': 'Enhancing Safety Awareness in Vision-Language Models with Hyperbolic Space', 'desc': 'This paper addresses the challenge of retrieving unsafe content from vision-language models like CLIP by introducing a new approach called Hyperbolic Safety-Aware CLIP (HySAC). Instead of using unlearning techniques that erase knowledge of unsafe concepts, HySAC utilizes the hierarchical properties of hyperbolic space to encode safe and unsafe content as an entailment hierarchy. By employing entailment loss functions, the model can better understand the relationships between safe and unsafe image-text pairs, enhancing its ability to classify and moderate content. The results demonstrate that HySAC improves safety recognition and offers a more flexible framework for content moderation in multimodal applications.'}, 'zh': {'title': '提升视觉-语言模型的安全性意识', 'desc': '本论文探讨了如何从视觉-语言模型（如CLIP）中检索不安全内容。当前的方法主要依赖于消除学习技术，但这种方法限制了模型区分安全和不安全内容的能力。我们提出了一种新方法，利用双曲空间的层次特性，将安全和不安全内容编码为蕴含层次结构。我们的HySAC模型通过蕴含损失函数建模安全与不安全图像-文本对之间的层次和不对称关系，从而增强了模型对不安全内容的意识。'}}}, {'id': 'https://huggingface.co/papers/2503.08683', 'title': 'CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving', 'url': 'https://huggingface.co/papers/2503.08683', 'abstract': 'Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver.', 'score': 1, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'ef46cd2c17c64880', 'authors': ['Changxing Liu', 'Genjia Liu', 'Zijun Wang', 'Jinchang Yang', 'Siheng Chen'], 'affiliations': ['Multi-Agent Governance & Intelligence Crew (MAGIC)', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08683.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#rl', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Языковые модели открывают новую эру в кооперативном автономном вождении', 'desc': 'CoLMDriver - это первая система кооперативного вождения на основе языковых моделей (LLM), обеспечивающая эффективные языковые переговоры и управление в реальном времени. Система включает модуль переговоров на основе LLM и генератор путевых точек, управляемый намерениями. Авторы также представили InterDrive - симуляционный бенчмарк на базе CARLA для оценки кооперации между транспортными средствами. Эксперименты показали, что CoLMDriver значительно превосходит существующие подходы, достигая на 11% более высокого уровня успешности в различных сценариях взаимодействия автомобилей.'}, 'en': {'title': 'Revolutionizing V2V Cooperation with CoLMDriver!', 'desc': 'This paper introduces CoLMDriver, a novel system for vehicle-to-vehicle (V2V) cooperative autonomous driving that leverages large language models (LLMs) for improved safety and decision-making. CoLMDriver addresses the limitations of traditional methods by implementing a full-pipeline approach that includes a negotiation module and a waypoint generator, allowing vehicles to communicate and plan collaboratively in real-time. The negotiation module uses an actor-critic framework to adaptively refine cooperation strategies based on feedback from previous interactions. Experimental results show that CoLMDriver outperforms existing systems, achieving a higher success rate in complex driving scenarios, demonstrating its effectiveness in enhancing V2V cooperation.'}, 'zh': {'title': 'CoLMDriver：智能协作驾驶的新突破', 'desc': '本文提出了一种名为CoLMDriver的全流程基于大语言模型（LLM）的合作驾驶系统，旨在提高车辆间的安全性。该系统通过语言基础的协商和实时驾驶控制，克服了传统合作方法的局限性。CoLMDriver包含两个关键组件：基于演员-评论家范式的协商模块和意图引导的路径点生成器，能够有效地将协商结果转化为可执行的路径点。实验结果表明，CoLMDriver在多种高度互动的车辆间驾驶场景中，成功率比现有方法提高了11%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (7)', '#agents (3)', '#agi', '#alignment (1)', '#architecture (5)', '#audio (1)', '#benchmark (16)', '#cv (5)', '#data (5)', '#dataset (12)', '#diffusion (3)', '#ethics (2)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math (1)', '#multilingual (3)', '#multimodal (12)', '#open_source (12)', '#optimization (12)', '#plp', '#rag', '#reasoning (12)', '#rl (3)', '#rlhf', '#robotics (2)', '#science', '#security (2)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (6)', '#training (9)', '#transfer_learning (5)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-19 20:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-19 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-19 20:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    