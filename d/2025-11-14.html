
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. November 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 Ğ½Ğ¾ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-11-13.html">â¬…ï¸ <span id="prev-date">13.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-11-17.html">â¡ï¸ <span id="next-date">17.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-11.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'};
        let feedDateNext = {'ru': '17.11', 'en': '11/17', 'zh': '11æœˆ17æ—¥'};
        let feedDatePrev = {'ru': '13.11', 'en': '11/13', 'zh': '11æœˆ13æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2511.10629', 'title': 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models', 'url': 'https://huggingface.co/papers/2511.10629', 'abstract': "LUA is a lightweight module that performs super-resolution directly in the latent space of diffusion models, improving efficiency without compromising image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.", 'score': 122, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '10517a05b5ad5f99', 'authors': ['Aleksandr Razin', 'Danil Kazantsev', 'Ilya Makarov'], 'affiliations': ['HSE', 'NIUITMO', 'SPbSTU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10629.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#diffusion', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡ÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Latent Upscaler Adapter (LUA) â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LUA Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VAE, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Swin Transformer Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ pixel-shuffle Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² 2Ñ… Ğ¸ 4Ñ… Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… VAE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LUA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Efficient High-Resolution Image Synthesis with LUA', 'desc': 'The paper introduces the Latent Upscaler Adapter (LUA), a module designed to enhance image resolution directly in the latent space of diffusion models. By performing super-resolution before the final decoding step, LUA significantly reduces the time and resources needed for high-resolution image generation. It operates as a drop-in component, requiring no changes to existing models, and achieves comparable image quality with lower latency. Extensive testing shows that LUA effectively generalizes across different variational autoencoders (VAEs), making it a versatile tool for efficient image synthesis.'}, 'zh': {'title': 'LUAï¼šé«˜æ•ˆçš„æ½œåœ¨ç©ºé—´è¶…åˆ†è¾¨ç‡è§£å†³æ–¹æ¡ˆ', 'desc': 'LUAæ˜¯ä¸€ç§è½»é‡çº§æ¨¡å—ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œä»è€Œæé«˜æ•ˆç‡è€Œä¸å½±å“å›¾åƒè´¨é‡ã€‚ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡é‡‡æ ·æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒLUAé€šè¿‡åœ¨æœ€ç»ˆVAEè§£ç æ­¥éª¤ä¹‹å‰å¯¹ç”Ÿæˆå™¨çš„æ½œåœ¨ä»£ç è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚LUAä½œä¸ºä¸€ä¸ªå¯æ’æ‹”ç»„ä»¶ï¼Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼Œæ”¯æŒ2å€å’Œ4å€çš„æ”¾å¤§å› å­ï¼Œå¹¶ä¸”ä¸å›¾åƒç©ºé—´çš„è¶…åˆ†è¾¨ç‡åŸºçº¿å…¼å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒLUAåœ¨ä¿æŒé«˜ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è§£ç å’Œæ”¾å¤§æ—¶é—´ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„é«˜ä¿çœŸå›¾åƒåˆæˆæ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10647', 'title': 'Depth Anything 3: Recovering the Visual Space from Any Views', 'url': 'https://huggingface.co/papers/2511.10647', 'abstract': 'Depth Anything 3 (DA3) uses a plain transformer for geometry prediction from visual inputs, achieving state-of-the-art results in camera pose estimation, any-view geometry, visual rendering, and monocular depth estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.', 'score': 93, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'e50fcd521d3e8098', 'authors': ['Haotong Lin', 'Sili Chen', 'Junhao Liew', 'Donny Y. Chen', 'Zhenyu Li', 'Guang Shi', 'Jiashi Feng', 'Bingyi Kang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10647.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#cv', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Depth Anything 3 (DA3) â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¼Ğµ: Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° DINO Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, Ğ° ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ depth-rays) Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ DA2, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞµÑ‘ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. DA3 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 44.3% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ 25.1% Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Geometry Prediction with Simplicity', 'desc': 'Depth Anything 3 (DA3) is a machine learning model that predicts 3D geometry from various visual inputs, even when the camera positions are unknown. It simplifies the architecture by using a basic transformer model, which proves to be effective without needing complex designs. DA3 introduces a unique depth-ray prediction method that eliminates the requirement for multi-task learning, making the training process more efficient. The model achieves impressive results in tasks like camera pose estimation and depth estimation, outperforming previous models and setting new benchmarks in visual geometry.'}, 'zh': {'title': 'æ·±åº¦é¢„æµ‹çš„æ–°çªç ´ï¼šDA3', 'desc': 'Depth Anything 3ï¼ˆDA3ï¼‰æ˜¯ä¸€ç§ä½¿ç”¨æ™®é€šå˜æ¢å™¨è¿›è¡Œå‡ ä½•é¢„æµ‹çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ä»»æ„æ•°é‡çš„è§†è§‰è¾“å…¥ä¸­é¢„æµ‹ç©ºé—´ä¸€è‡´çš„å‡ ä½•å½¢çŠ¶ã€‚è¯¥æ¨¡å‹åœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€ä»»æ„è§†è§’å‡ ä½•ã€è§†è§‰æ¸²æŸ“å’Œå•ç›®æ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚DA3çš„å…³é”®åœ¨äºä½¿ç”¨å•ä¸€çš„æ™®é€šå˜æ¢å™¨ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¹¶é€šè¿‡æ·±åº¦å…‰çº¿é¢„æµ‹ç›®æ ‡ç®€åŒ–äº†å¤šä»»åŠ¡å­¦ä¹ çš„å¤æ‚æ€§ã€‚é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒèŒƒå¼ï¼ŒDA3åœ¨ç»†èŠ‚å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¸å‰ä¸€ç‰ˆæœ¬DA2ç›¸å½“ï¼Œå¹¶åœ¨æ–°çš„è§†è§‰å‡ ä½•åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰ä»»åŠ¡çš„å…ˆå‰æœ€ä½³è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09057', 'title': 'PAN: A World Model for General, Interactable, and Long-Horizon World Simulation', 'url': 'https://huggingface.co/papers/2511.09057', 'abstract': 'PAN, a general, interactable, and long-horizon world model, predicts future world states using a Generative Latent Prediction (GLP) architecture that combines autoregressive latent dynamics with a video diffusion decoder, enabling detailed, long-term, and coherent video simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.', 'score': 75, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '8491e2c4bf196511', 'authors': ['PAN Team', 'Jiannan Xiang', 'Yi Gu', 'Zihan Liu', 'Zeyu Feng', 'Qiyue Gao', 'Yiyan Hu', 'Benhao Huang', 'Guangyi Liu', 'Yichi Yang', 'Kun Zhou', 'Davit Abrahamyan', 'Arif Ahmad', 'Ganesh Bannur', 'Junrong Chen', 'Kimi Chen', 'Mingkai Deng', 'Ruobing Han', 'Xinqi Huang', 'Haoqiang Kang', 'Zheqi Liu', 'Enze Ma', 'Hector Ren', 'Yashowardhan Shinde', 'Rohan Shingre', 'Ramsundar Tanikella', 'Kaiming Tao', 'Dequan Yang', 'Xinle Yu', 'Cong Zeng', 'Binglin Zhou', 'Zhengzhong Liu', 'Zhiting Hu', 'Eric P. Xing'], 'affiliations': ['Institute of Foundation Models'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09057.jpg', 'data': {'categories': ['#video', '#diffusion', '#agents', '#long_context', '#multimodal', '#training', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'PAN â€” ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Generative Latent Prediction (GLP), ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, PAN Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'PAN: Predicting the Future with Interactive World Models', 'desc': "The paper introduces PAN, a versatile world model that predicts future states of the world using a Generative Latent Prediction (GLP) architecture. This model combines autoregressive latent dynamics with a video diffusion decoder, allowing it to generate detailed and coherent video simulations over long time horizons. Unlike previous models that lack interactivity and causal control, PAN can simulate diverse environments and respond to natural language actions. The results demonstrate PAN's effectiveness in action-conditioned world simulation and long-term forecasting, marking progress towards more generalizable world models for intelligent agents."}, 'zh': {'title': 'PANï¼šé€šç”¨çš„é•¿æ—¶é—´èŒƒå›´ä¸–ç•Œæ¨¡å‹', 'desc': 'PANæ˜¯ä¸€ç§é€šç”¨çš„ã€å¯äº¤äº’çš„ã€é•¿æ—¶é—´èŒƒå›´çš„ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæ½œåœ¨é¢„æµ‹ï¼ˆGLPï¼‰æ¶æ„é¢„æµ‹æœªæ¥çš„ä¸–ç•ŒçŠ¶æ€ã€‚å®ƒç»“åˆäº†è‡ªå›å½’æ½œåœ¨åŠ¨æ€å’Œè§†é¢‘æ‰©æ•£è§£ç å™¨ï¼Œå®ç°äº†è¯¦ç»†ã€é•¿æœŸä¸”è¿è´¯çš„è§†é¢‘æ¨¡æ‹Ÿã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒPANèƒ½å¤Ÿåœ¨å†å²å’Œè‡ªç„¶è¯­è¨€åŠ¨ä½œçš„æ¡ä»¶ä¸‹è¿›è¡Œé«˜è´¨é‡çš„è§†é¢‘æ¨¡æ‹Ÿï¼Œæ”¯æŒå¼€æ”¾é¢†åŸŸçš„åŠ¨ä½œæ¡ä»¶æ¨¡æ‹Ÿã€‚å®éªŒè¡¨æ˜ï¼ŒPANåœ¨åŠ¨ä½œæ¡ä»¶çš„ä¸–ç•Œæ¨¡æ‹Ÿå’Œé•¿æ—¶é—´é¢„æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†é€šç”¨ä¸–ç•Œæ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10643', 'title': 'Black-Box On-Policy Distillation of Large Language Models', 'url': 'https://huggingface.co/papers/2511.10643', 'abstract': "Generative Adversarial Distillation (GAD) enhances black-box distillation by framing the student model as a generator and using a discriminator to provide adaptive feedback, surpassing traditional sequence-level knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.", 'score': 46, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '14d8a22edabc8326', 'authors': ['Tianzhu Ye', 'Li Dong', 'Zewen Chi', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10643.jpg', 'data': {'categories': ['#architecture', '#training'], 'emoji': 'âš”ï¸', 'ru': {'title': 'Ğ¡Ğ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Generative Adversarial Distillation (GAD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞµÑ‘ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑĞ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ, Ğ³Ğ´Ğµ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»ÑƒĞ¶Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ ÑĞ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GAD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ sequence-level Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-14B Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4-Chat.'}, 'en': {'title': 'Revolutionizing Black-Box Distillation with GAD', 'desc': "Generative Adversarial Distillation (GAD) is a novel approach to black-box distillation that treats the student model as a generator while employing a discriminator for adaptive feedback. This method allows the student model to learn from the teacher model's outputs without needing access to its internal workings. By creating a minimax game between the generator and discriminator, GAD provides a stable reward mechanism that enhances the learning process. Experimental results demonstrate that GAD outperforms traditional sequence-level knowledge distillation, making it a significant advancement in the field of large language model training."}, 'zh': {'title': 'ç”Ÿæˆå¯¹æŠ—è’¸é¦ï¼šé»‘ç®±è’¸é¦çš„æ–°çªç ´', 'desc': 'ç”Ÿæˆå¯¹æŠ—è’¸é¦ï¼ˆGADï¼‰é€šè¿‡å°†å­¦ç”Ÿæ¨¡å‹è§†ä¸ºç”Ÿæˆå™¨ï¼Œå¹¶ä½¿ç”¨é‰´åˆ«å™¨æä¾›è‡ªé€‚åº”åé¦ˆï¼Œå¢å¼ºäº†é»‘ç®±è’¸é¦ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åºåˆ—çº§çŸ¥è¯†è’¸é¦ã€‚é»‘ç®±è’¸é¦ä»…é€šè¿‡å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºï¼Œåˆ›å»ºå­¦ç”Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè€Œä¸æ¥è§¦å…¶å†…éƒ¨é€»è¾‘æˆ–å‚æ•°ã€‚GADä½¿å¾—åœ¨ç­–ç•¥å’Œé»‘ç®±è’¸é¦ä¸­ï¼Œå­¦ç”ŸLLMä¸æ•™å¸ˆLLMä¹‹é—´å½¢æˆäº†ä¸€ä¸ªæœ€å°æœ€å¤§åšå¼ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGADåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºå¸¸ç”¨çš„åºåˆ—çº§çŸ¥è¯†è’¸é¦ï¼Œå°¤å…¶æ˜¯ç»è¿‡GADè®­ç»ƒçš„Qwen2.5-14B-Instructåœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ä¸å…¶æ•™å¸ˆGPT-5-Chatç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08521', 'title': 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist', 'url': 'https://huggingface.co/papers/2511.08521', 'abstract': 'UniVA is an open-source multi-agent framework that integrates video understanding, segmentation, editing, and generation into cohesive workflows using a Plan-and-Act architecture and hierarchical memory.  \t\t\t\t\tAI-generated summary \t\t\t\t While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)', 'score': 37, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '8a917acfe391e2de', 'authors': ['Zhengyang Liang', 'Daoan Zhang', 'Huichi Zhou', 'Rui Huang', 'Bobo Li', 'Yuechen Zhang', 'Shengqiong Wu', 'Xiaohan Wang', 'Jiebo Luo', 'Lizi Liao', 'Hao Fei'], 'affiliations': ['National University of Singapore', 'Singapore Management University', 'Stanford University', 'The Chinese University of Hong Kong', 'University College London', 'University of Rochester'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08521.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#agents', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'UniVA â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñæ¡†æ¶Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Plan-and-Act, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ UniVA-Bench â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ÑĞµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'UniVA: Revolutionizing Video Processing with Multi-Agent Intelligence', 'desc': 'UniVA is an innovative open-source framework designed for multi-agent video processing that integrates various tasks such as understanding, segmentation, editing, and generation. It utilizes a Plan-and-Act architecture where a planner agent breaks down user intentions into actionable steps, while executor agents carry out these tasks using modular tool servers. The framework features a hierarchical memory system that supports long-term reasoning and effective communication between agents, allowing for complex and iterative video workflows. UniVA also introduces UniVA-Bench, a benchmark for evaluating multi-step video tasks, promoting advancements in interactive and general-purpose video intelligence.'}, 'zh': {'title': 'UniVAï¼šè§†é¢‘æ™ºèƒ½çš„ä¸‹ä¸€ä»£è§£å†³æ–¹æ¡ˆ', 'desc': 'UniVAæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘ç†è§£ã€åˆ†å‰²ã€ç¼–è¾‘å’Œç”Ÿæˆæ•´åˆä¸ºç»Ÿä¸€çš„å·¥ä½œæµç¨‹ã€‚å®ƒé‡‡ç”¨è®¡åˆ’ä¸æ‰§è¡Œçš„åŒæ™ºèƒ½ä½“æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤„ç†å¤æ‚çš„è§†é¢‘ä»»åŠ¡ã€‚é€šè¿‡å±‚æ¬¡åŒ–çš„å¤šçº§è®°å¿†ï¼ŒUniVAæ”¯æŒé•¿æ—¶é—´çš„æ¨ç†å’Œä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„æ²Ÿé€šã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†UniVA-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ­¥éª¤è§†é¢‘ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ¨åŠ¨äº’åŠ¨å’Œé€šç”¨è§†é¢‘æ™ºèƒ½çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09780', 'title': 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO', 'url': 'https://huggingface.co/papers/2511.09780', 'abstract': 'The study identifies and defends against adversarial attacks in decentralized Group Relative Policy Optimization (GRPO) for Large Language Models (LLMs), demonstrating attack success rates of up to 100% and proposing effective defense mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.', 'score': 27, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '1b2e719f2a2252da', 'authors': ['Nikolay Blagoev', 'OÄŸuzhan Ersoy', 'Lydia Yiyu Chen'], 'affiliations': ['Gensyn', 'TU Delft', 'University of Neuchatel'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09780.jpg', 'data': {'categories': ['#alignment', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· out-of-context Ğ¸ in-context Ğ°Ñ‚Ğ°ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 100% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° 50 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 100%.'}, 'en': {'title': 'Defending Decentralized GRPO: Battling Adversarial Attacks on LLMs', 'desc': 'This paper explores the vulnerabilities of decentralized Group Relative Policy Optimization (GRPO) in training Large Language Models (LLMs) against adversarial attacks. It reveals that attackers can successfully inject harmful tokens into benign models, leading to a complete compromise of the training process with success rates reaching 100%. The study presents two defense strategies tailored to different training scenarios, effectively neutralizing these attacks. By demonstrating the ease of poisoning in both out-of-context and in-context settings, the research highlights the critical need for robust defenses in decentralized machine learning environments.'}, 'zh': {'title': 'é˜²å¾¡å¯¹æŠ—æ”»å‡»ï¼Œä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å»ä¸­å¿ƒåŒ–çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­è¯†åˆ«å’Œé˜²å¾¡å¯¹æŠ—æ€§æ”»å‡»ï¼Œå°¤å…¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¶æ„æ”»å‡»è€…å¯ä»¥é€šè¿‡æ³¨å…¥æ¶æ„æ ‡è®°æ¥ç ´åç³»ç»Ÿï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾100%ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é˜²å¾¡æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜»æ­¢è¿™äº›æ”»å‡»ï¼Œç¡®ä¿æ¨¡å‹çš„å®‰å…¨æ€§ã€‚é€šè¿‡å®è¯ä¾‹å­ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›é˜²å¾¡æ–¹æ³•åœ¨ä¸åŒè®­ç»ƒæ¨¡å‹æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09030', 'title': 'Solving a Million-Step LLM Task with Zero Errors', 'url': 'https://huggingface.co/papers/2511.09030', 'abstract': 'MAKER, a system using microagents with error correction, successfully solves tasks with over a million LLM steps, suggesting a new approach for scaling LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.', 'score': 20, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'da64c7a751cec303', 'authors': ['Elliot Meyerson', 'Giuseppe Paolo', 'Roberto Dailey', 'Hormoz Shahrzad', 'Olivier Francon', 'Conor F. Hayes', 'Xin Qiu', 'Babak Hodjat', 'Risto Miikkulainen'], 'affiliations': ['Cognizant AI Lab', 'UT Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09030.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM', 'desc': 'MAKER â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Scaling LLMs with Error-Free Microagents', 'desc': 'This paper introduces MAKER, a novel system that utilizes microagents to perform tasks with over one million steps of a large language model (LLM) without any errors. The key innovation is the extreme decomposition of complex tasks into smaller, manageable subtasks, allowing each microagent to focus on specific components. This modular approach enables effective error correction through a multi-agent voting mechanism, ensuring accuracy at each step. The findings suggest that rather than solely improving existing LLMs, employing massively decomposed agentic processes (MDAPs) could enhance problem-solving capabilities on a larger scale.'}, 'zh': {'title': 'æè‡´åˆ†è§£ï¼Œé›¶é”™è¯¯è§£å†³æ–¹æ¡ˆï¼', 'desc': 'MAKERæ˜¯ä¸€ä¸ªä½¿ç”¨å¾®ä»£ç†å’Œé”™è¯¯ä¿®æ­£çš„ç³»ç»Ÿï¼ŒæˆåŠŸåœ°è§£å†³äº†è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªLLMæ­¥éª¤çš„ä»»åŠ¡ï¼Œå±•ç¤ºäº†ä¸€ç§æ‰©å±•LLMèƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚å°½ç®¡LLMåœ¨æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å°†è¿™äº›èƒ½åŠ›ä¸²è”æˆå¤§è§„æ¨¡çš„è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚MAKERé€šè¿‡å°†ä»»åŠ¡æåº¦åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œä½¿æ¯ä¸ªå­ä»»åŠ¡ç”±ä¸“æ³¨çš„å¾®ä»£ç†å¤„ç†ï¼Œä»è€Œå®ç°äº†é›¶é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§æç«¯åˆ†è§£å’Œé”™è¯¯ä¿®æ­£çš„ç»“åˆï¼Œä½¿å¾—åœ¨ç»„ç»‡å’Œç¤¾ä¼šå±‚é¢é«˜æ•ˆè§£å†³é—®é¢˜æˆä¸ºå¯èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08522', 'title': 'AlphaResearch: Accelerating New Algorithm Discovery with Language Models', 'url': 'https://huggingface.co/papers/2511.08522', 'abstract': "AlphaResearch, an autonomous research agent, discovers new algorithms in open-ended problems with a dual research environment, achieving competitive performance against human researchers in a benchmark competition.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.", 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'e13cbf21d0122775', 'authors': ['Zhaojian Yu', 'Kaiyue Feng', 'Yilun Zhao', 'Shilin He', 'Xiao-Ping Zhang', 'Arman Cohan'], 'affiliations': ['ByteDance', 'New York University', 'Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08522.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#open_source', '#agents', '#dataset', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ', 'desc': 'AlphaResearch â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ñ‡Ñ‘Ñ‚ĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ ÑÑ€ĞµĞ´Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµĞ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AlphaResearchComp Ñ Ğ²Ğ¾ÑÑŒĞ¼ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Algorithm Discovery with AlphaResearch', 'desc': "AlphaResearch is an autonomous research agent that excels in discovering new algorithms for open-ended problems. It operates within a dual research environment that combines execution-based verification with a simulated peer review process, enhancing both feasibility and innovation. The agent follows a structured approach of proposing ideas, verifying them, and optimizing for performance. In a benchmark competition, AlphaResearch demonstrated competitive results against human researchers, achieving notable success in specific algorithmic challenges, particularly in the 'packing circles' problem."}, 'zh': {'title': 'è‡ªä¸»ç ”ç©¶ä»£ç†ï¼šåŠ é€Ÿç®—æ³•å‘ç°çš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†AlphaResearchï¼Œä¸€ä¸ªè‡ªä¸»ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾æ€§é—®é¢˜å¹¶å‘ç°æ–°ç®—æ³•ã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªåŒé‡ç ”ç©¶ç¯å¢ƒï¼Œç»“åˆæ‰§è¡ŒéªŒè¯å’Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„åŒè¡Œè¯„å®¡ï¼Œæ¥ä¿ƒè¿›å‘ç°è¿‡ç¨‹çš„å¯è¡Œæ€§å’Œåˆ›æ–°æ€§ã€‚AlphaResearché€šè¿‡è¿­ä»£æ­¥éª¤æå‡ºæ–°æƒ³æ³•ã€éªŒè¯è¿™äº›æƒ³æ³•å¹¶ä¼˜åŒ–ç ”ç©¶ææ¡ˆï¼Œä»è€Œå®ç°ç®—æ³•çš„å‘ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaResearchåœ¨ä¸äººç±»ç ”ç©¶è€…çš„å¯¹æ¯”ä¸­å–å¾—äº†2/8çš„èƒœç‡ï¼Œå±•ç¤ºäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åŠ é€Ÿç®—æ³•å‘ç°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01918', 'title': 'Superpositional Gradient Descent: Harnessing Quantum Principles for\n  Model Training', 'url': 'https://huggingface.co/papers/2511.01918', 'abstract': 'Superpositional Gradient Descent, a quantum-inspired optimizer, improves convergence and reduces final loss in large language model training compared to AdamW.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with classical optimization techniques like AdamW to improve convergence and generalization. However, the mechanisms by which quantum-inspired methods enhance classical training remain underexplored. We introduce Superpositional Gradient Descent (SGD), a novel optimizer linking gradient updates with quantum superposition by injecting quantum circuit perturbations. We present a mathematical framework and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On synthetic sequence classification and large-scale LLM fine-tuning, SGD converges faster and yields lower final loss than AdamW. Despite promising results, scalability and hardware constraints limit adoption. Overall, this work provides new insights into the intersection of quantum computing and deep learning, suggesting practical pathways for leveraging quantum principles to control and enhance model behavior.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '27489697380f09cd', 'authors': ['Ahmet Erdem Pamuk', 'Emir Kaan Ã–zdemir', 'Åuayp Talha Kocabay'], 'affiliations': ['Istanbul Erkek High School Istanbul, Turkiye', 'Science High School Ankara, Turkiye', 'UBITAK Science High School Kocaeli, Turkiye'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01918.jpg', 'data': {'categories': [], 'emoji': 'âš›ï¸', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Superpositional Gradient Descent (SGD), Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ PyTorch Ğ¸ Qiskit. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ AdamW Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ fine-tuning LLM. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ².'}, 'en': {'title': 'Harnessing Quantum Principles for Faster Model Training', 'desc': 'This paper introduces Superpositional Gradient Descent (SGD), a new optimizer inspired by quantum mechanics, which aims to improve the training of large language models (LLMs). By incorporating quantum superposition into the gradient update process, SGD enhances convergence speed and reduces final loss compared to the traditional AdamW optimizer. The authors provide a mathematical framework and demonstrate the implementation of hybrid quantum-classical circuits using PyTorch and Qiskit. Although the results are promising, challenges related to scalability and hardware limitations may hinder widespread adoption of this approach.'}, 'zh': {'title': 'é‡å­å¯å‘çš„ä¼˜åŒ–å™¨ï¼šè¶…ä½ç½®æ¢¯åº¦ä¸‹é™', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºè¶…ä½ç½®æ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„æ–°å‹ä¼˜åŒ–å™¨ï¼Œå®ƒå—åˆ°é‡å­è®¡ç®—çš„å¯å‘ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„AdamWä¼˜åŒ–å™¨ç›¸æ¯”ï¼ŒSGDé€šè¿‡å¼•å…¥é‡å­ç”µè·¯æ‰°åŠ¨æ¥åŠ é€Ÿæ”¶æ•›å¹¶é™ä½æœ€ç»ˆæŸå¤±ã€‚æˆ‘ä»¬åœ¨PyTorchå’ŒQiskitä¸­å®ç°äº†æ··åˆé‡å­-ç»å…¸ç”µè·¯ï¼Œå¹¶åœ¨åˆæˆåºåˆ—åˆ†ç±»å’Œå¤§è§„æ¨¡LLMå¾®è°ƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å°½ç®¡ç»“æœä»¤äººé¼“èˆï¼Œä½†å¯æ‰©å±•æ€§å’Œç¡¬ä»¶é™åˆ¶ä»ç„¶æ˜¯å…¶åº”ç”¨çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10289', 'title': 'Music Flamingo: Scaling Music Understanding in Audio Language Models', 'url': 'https://huggingface.co/papers/2511.10289', 'abstract': "Music Flamingo, a large audio-language model, advances music understanding through fine-tuning on a rich dataset and post-training with novel methods, achieving state-of-the-art results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.", 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '8213f0b545744910', 'authors': ['Sreyan Ghosh', 'Arushi Goel', 'Lasha Koroshinadze', 'Sang-gil Lee', 'Zhifeng Kong', 'Joao Felipe Santos', 'Ramani Duraiswami', 'Dinesh Manocha', 'Wei Ping', 'Mohammad Shoeybi', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, CA, USA', 'University of Maryland, College Park, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10289.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#audio', '#multimodal', '#training', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ: Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ', 'desc': 'Music Flamingo â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MF-Skills Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‚ĞµĞ¼Ğ±Ñ€ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Music Flamingo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 10+ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¼Ñƒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Music Understanding with Music Flamingo', 'desc': "Music Flamingo is a large audio-language model that enhances music understanding by utilizing a comprehensive dataset and innovative training techniques. It addresses the challenges of music's complex nature and the lack of quality data by introducing the MF-Skills dataset, which includes detailed annotations on various musical elements. The model is fine-tuned on this dataset and further improved through a unique post-training approach that incorporates reasoning based on music theory. As a result, Music Flamingo achieves top performance on multiple benchmarks, showcasing its ability to understand music in a nuanced, human-like manner."}, 'zh': {'title': 'éŸ³ä¹ç†è§£çš„æ–°æ ‡å‡†', 'desc': 'Music Flamingo æ˜¯ä¸€ç§å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ä¸°å¯Œæ•°æ®é›†çš„å¾®è°ƒå’Œæ–°é¢–çš„åè®­ç»ƒæ–¹æ³•ï¼Œæå‡éŸ³ä¹ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å…‹æœäº†ä»¥å¾€æ¨¡å‹åœ¨éŸ³ä¹ç†è§£ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŠ¨æ€æ€§ã€å±‚æ¬¡æ€§å’Œä¿¡æ¯å¯†é›†æ€§ã€‚é€šè¿‡æ„å»º MF-Skills æ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„éŸ³ä¹æè¿°å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–å’Œå£°ã€ç»“æ„ã€éŸ³è‰²ã€æ­Œè¯å’Œæ–‡åŒ–èƒŒæ™¯ç­‰æ–¹é¢ã€‚Music Flamingo åœ¨å¤šä¸ªéŸ³ä¹ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨éŸ³ä¹ç†è§£å’Œæ¨ç†æ–¹é¢çš„å¹¿æ³›èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07685', 'title': 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents', 'url': 'https://huggingface.co/papers/2511.07685', 'abstract': "ResearchRubrics is a benchmark for evaluating deep research agents, using expert rubrics to assess their factual grounding, reasoning, and clarity across diverse, complex tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.", 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'fe2382a8bb21c8c7', 'authors': ['Manasi Sharma', 'Chen Bo Calvin Zhang', 'Chaithanya Bandi', 'Clinton Wang', 'Ankit Aich', 'Huy Nghiem', 'Tahseen Rabbani', 'Ye Htet', 'Brian Jang', 'Sumana Basu', 'Aishwarya Balwani', 'Denis Peskoff', 'Marcos Ayestaran', 'Sean M. Hendryx', 'Brad Kenstler', 'Bing Liu'], 'affiliations': ['McGill University', 'Scale AI', 'University of California, Berkeley', 'University of Chicago', 'University of Maryland', 'Washington University, St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07685.jpg', 'data': {'categories': ['#rag', '#benchmark', '#reasoning', '#open_source', '#agents', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'ResearchRubrics â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2500 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑˆĞ¸Ñ€Ğ¾Ñ‚Ñƒ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 68% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° ÑƒĞ¿ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating Deep Research Agents with ResearchRubrics', 'desc': "ResearchRubrics is a benchmark designed to evaluate deep research agents that utilize large language models for complex queries. It assesses these agents based on their factual grounding, reasoning abilities, and clarity of responses through a set of expert-written rubrics. The benchmark includes over 2,800 hours of human effort and features 2,500+ detailed rubrics paired with realistic prompts. The study reveals that even top-performing deep research systems struggle to meet the benchmark's standards, indicating a significant need for improved evaluation methods in this field."}, 'zh': {'title': 'è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„æ ‡å‡†åŒ–åŸºå‡†', 'desc': 'ResearchRubricsæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„åŸºå‡†ï¼Œåˆ©ç”¨ä¸“å®¶è¯„åˆ†æ ‡å‡†æ¥è¯„ä¼°å…¶äº‹å®åŸºç¡€ã€æ¨ç†èƒ½åŠ›å’Œæ¸…æ™°åº¦ã€‚æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰æ˜¯ä¸€ä¸ªæ–°å…´çš„ä»£ç†åº”ç”¨ï¼Œä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¤„ç†å¼€æ”¾å¼æŸ¥è¯¢ã€‚è¯„ä¼°DRçš„æŒ‘æˆ˜åœ¨äºå…¶å“åº”å†…å®¹å†—é•¿å¤šæ ·ï¼Œä¸”å­˜åœ¨å¤šç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šå¸¸è¿˜ä¾èµ–åŠ¨æ€ä¿¡æ¯æºã€‚æˆ‘ä»¬æå‡ºçš„ResearchRubricsç»“åˆäº†2800å¤šä¸ªå°æ—¶çš„äººåŠ›åŠ³åŠ¨ï¼Œé…å¤‡2500å¤šä¸ªä¸“å®¶æ’°å†™çš„ç»†è‡´è¯„åˆ†æ ‡å‡†ï¼Œä»¥è¯„ä¼°DRçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09715', 'title': 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control', 'url': 'https://huggingface.co/papers/2511.09715', 'abstract': "SliderEdit enables continuous, fine-grained control over image editing instructions by using low-rank adaptation matrices, improving edit controllability, visual consistency, and user steerability.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '88a7cab83803193d', 'authors': ['Arman Zarei', 'Samyadeep Basu', 'Mobina Pournemat', 'Sayan Nag', 'Ryan Rossi', 'Soheil Feizi'], 'affiliations': ['Adobe Research', 'University of Maryland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09715.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ·ÑƒĞ½ĞºĞ¸', 'desc': 'SliderEdit â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ·ÑƒĞ½Ğ¾Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… FLUX-Kontext Ğ¸ Qwen-Image-Edit.'}, 'en': {'title': 'SliderEdit: Fine-Grained Control for Interactive Image Editing', 'desc': 'SliderEdit is a novel framework that enhances image editing by allowing users to have continuous and fine-grained control over their editing instructions. It utilizes low-rank adaptation matrices to separate and adjust the strength of individual instructions in a multi-part edit prompt. This approach improves edit controllability and visual consistency, enabling users to smoothly manipulate the intensity of edits without needing separate training for each attribute. By integrating SliderEdit with advanced image editing models, the framework significantly boosts user steerability and paves the way for more interactive image manipulation.'}, 'zh': {'title': 'SliderEditï¼šå®ç°å›¾åƒç¼–è¾‘çš„è¿ç»­ç»†ç²’åº¦æ§åˆ¶', 'desc': 'SliderEdit æ˜¯ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·å¯¹ç¼–è¾‘æŒ‡ä»¤è¿›è¡Œè¿ç»­å’Œç»†ç²’åº¦çš„æ§åˆ¶ã€‚é€šè¿‡ä½¿ç”¨ä½ç§©é€‚åº”çŸ©é˜µï¼ŒSliderEdit å¯ä»¥å°†å¤šéƒ¨åˆ†ç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºå¯è°ƒèŠ‚çš„æ»‘å—ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¹³æ»‘åœ°è°ƒæ•´æ¯ä¸ªæŒ‡ä»¤çš„å¼ºåº¦ã€‚ä¸ä»¥å¾€éœ€è¦ä¸ºæ¯ä¸ªå±æ€§å•ç‹¬è®­ç»ƒçš„æ¨¡å‹ä¸åŒï¼ŒSliderEdit é€šè¿‡å­¦ä¹ ä¸€ç»„é€šç”¨çš„ä½ç§©é€‚åº”çŸ©é˜µï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç¼–è¾‘å’Œå±æ€§ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„æ³›åŒ–ã€‚è¯¥æ–¹æ³•åœ¨ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸­åº”ç”¨ï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘çš„å¯æ§æ€§ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·çš„æ“ä½œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10017', 'title': 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2511.10017', 'abstract': "AffordBot, a framework combining Multimodal Large Language Models with chain-of-thought reasoning, achieves state-of-the-art performance in predicting affordance elements' spatial locations, motion types, and axes in 3D scenes based on task instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '57ed63ddf777fd34', 'authors': ['Xinyi Wang', 'Xun Yang', 'Yanlong Xu', 'Yuchen Wu', 'Zhen Li', 'Na Zhao'], 'affiliations': ['Chinese University of Hong Kong, Shenzhen', 'Singapore University of Technology and Design', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10017.jpg', 'data': {'categories': ['#agents', '#3d', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'AffordBot â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² 3D ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° affordance ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ¸Ğ¿ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾ÑÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ 3D Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ 2D-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ MLLM Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ SceneFun3D Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Interaction with AffordBot!', 'desc': 'AffordBot is a new framework that combines Multimodal Large Language Models (MLLMs) with chain-of-thought reasoning to enhance understanding of 3D scenes. It addresses the challenge of predicting the spatial locations, motion types, and axes of affordance elements based on specific task instructions. By using a unique approach that transforms 3D data into 2D images, AffordBot allows for better interaction and reasoning about objects in physical environments. The framework has shown impressive results on the SceneFun3D dataset, outperforming existing methods in fine-grained 3D embodied reasoning.'}, 'zh': {'title': 'AffordBotï¼šæå‡3Dåœºæ™¯ç†è§£çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'AffordBotæ˜¯ä¸€ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé“¾å¼æ€ç»´æ¨ç†çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨3Dåœºæ™¯ä¸­æ ¹æ®ä»»åŠ¡æŒ‡ä»¤é¢„æµ‹å¯æ“ä½œå…ƒç´ çš„ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šç»†ç²’åº¦3Dä½“ç°æ¨ç†ï¼Œè¦æ±‚æ™ºèƒ½ä½“ä¸ºæ¯ä¸ªå‚è€ƒçš„å¯æ“ä½œå…ƒç´ é¢„æµ‹ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰å…ƒç»„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªä»»åŠ¡ï¼ŒAffordBoté€šè¿‡æ¸²æŸ“åœºæ™¯çš„å…¨æ™¯å›¾åƒå¹¶å°†3Då…ƒç´ å€™é€‰æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚ç»è¿‡åœ¨SceneFun3Dæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒAffordBotå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ä»…ä½¿ç”¨3Dç‚¹äº‘è¾“å…¥å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†åŸºç¡€æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10507', 'title': 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following', 'url': 'https://huggingface.co/papers/2511.10507', 'abstract': 'AdvancedIF benchmark and RIFL pipeline improve instruction-following capabilities in large language models by using expert-curated rubrics and reinforcement learning techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'fe35e028d2405a46', 'authors': ['Yun He', 'Wenzhe Li', 'Hejia Zhang', 'Songlin Li', 'Karishma Mandyam', 'Sopan Khosla', 'Yuanhao Xiong', 'Nanshu Wang', 'Xiaoliang Peng', 'Beibin Li', 'Shengjie Bi', 'Shishir G. Patil', 'Qi Qi', 'Shengyu Feng', 'Julian Katz-Samuels', 'Richard Yuanzhe Pang', 'Sujan Gonugondla', 'Hunter Lang', 'Yue Yu', 'Yundi Qian', 'Maryam Fazel-Zarandi', 'Licheng Yu', 'Amine Benhalloum', 'Hany Awadalla', 'Manaal Faruqui'], 'affiliations': ['CMU', 'Meta Superintelligence Labs', 'Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10507.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#rlhf', '#training'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'Ğ ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AdvancedIF Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1600 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RIFL â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6.7% Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ·Ğ°Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞµĞ±Ñ ĞºĞ°Ğº Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Instruction-Following in AI with AdvancedIF and RIFL', 'desc': 'This paper presents the AdvancedIF benchmark and the RIFL pipeline, which enhance the instruction-following abilities of large language models (LLMs). The AdvancedIF benchmark includes over 1,600 expert-curated prompts that evaluate LLMs on complex, multi-turn instructions. The RIFL pipeline utilizes rubric generation and reinforcement learning techniques to improve training effectiveness. Experimental results show that RIFL significantly boosts LLM performance on the AdvancedIF benchmark and other public tests, highlighting the importance of rubrics in AI training and evaluation.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†AdvancedIFåŸºå‡†å’ŒRIFLç®¡é“ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è·Ÿéšèƒ½åŠ›ã€‚AdvancedIFæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼ŒåŒ…å«è¶…è¿‡1600ä¸ªæç¤ºå’Œä¸“å®¶åˆ¶å®šçš„è¯„åˆ†æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚RIFLæ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨è¯„åˆ†æ ‡å‡†ç”Ÿæˆã€å¾®è°ƒçš„è¯„åˆ†éªŒè¯å™¨å’Œå¥–åŠ±å¡‘é€ ï¼Œä¿ƒè¿›æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRIFLæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä¸ºæ›´å¼ºå¤§å’Œå¯é çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10547', 'title': 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation', 'url': 'https://huggingface.co/papers/2511.10547', 'abstract': 'A framework for evaluating diversity in text-to-image models through human assessment and systematic analysis of image embeddings.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.   Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '38f7ef934c414106', 'authors': ['Isabela Albuquerque', 'Ira Ktena', 'Olivia Wiles', 'Ivana KajiÄ‡', 'Amal Rannen-Triki', 'Cristina Vasconcelos', 'Aida Nematzadeh'], 'affiliations': ['Ellison Institute of Technology', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10547.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ: ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image (T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ†Ğ²ĞµÑ‚ ÑĞ±Ğ»Ğ¾ĞºĞ°). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… image embeddings Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ T2I Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Diversity in Text-to-Image Models', 'desc': 'This paper presents a new framework for evaluating the diversity of text-to-image (T2I) models, which often produce similar outputs despite improvements in quality. The framework includes a human evaluation template that allows for detailed assessments of diversity based on specific concepts and their variations. It also features a curated set of prompts that highlight different factors of variation, such as color in images of apples. By comparing various image embeddings and using statistical methods, the study provides a way to rank T2I models based on their diversity, helping to identify areas for improvement.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å¤šæ ·æ€§çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç”Ÿæˆè´¨é‡è™½é«˜ä½†ç¼ºä¹å¤šæ ·æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯„ä¼°ä¸ªåˆ«æ¦‚å¿µåŠå…¶ç›¸å…³å˜å¼‚å› ç´ ï¼Œç³»ç»Ÿåœ°è¯„ä¼°å¤šæ ·æ€§ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€ç§æ–°çš„äººç±»è¯„ä¼°æ¨¡æ¿ç”¨äºç»†è‡´çš„å¤šæ ·æ€§è¯„ä¼°ï¼›ä¸€ä¸ªæ¶µç›–å¤šæ ·åŒ–æ¦‚å¿µåŠå…¶å˜å¼‚å› ç´ çš„æç¤ºé›†ï¼›ä»¥åŠé€šè¿‡äºŒé¡¹æ£€éªŒæ¯”è¾ƒæ¨¡å‹çš„äººç±»æ³¨é‡Šçš„æ–¹æ³•è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸¥æ ¼æ¯”è¾ƒäº†ä¸åŒçš„å›¾åƒåµŒå…¥ä»¥æµ‹é‡å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09067', 'title': 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique', 'url': 'https://huggingface.co/papers/2511.09067', 'abstract': "MM-CRITIC is a benchmark for evaluating the critique abilities of Large Multimodal Models across multiple dimensions and tasks, using expert-informed ground answers for reliable scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'cb7085959f687cf4', 'authors': ['Gailun Zeng', 'Ziyang Luo', 'Hongzhan Lin', 'Yuchen Tian', 'Kaixin Li', 'Ziyang Gong', 'Jianxiong Guo', 'Jing Ma'], 'affiliations': ['Beijing Normal University', 'Beijing Normal-Hong Kong Baptist University', 'Hong Kong Baptist University', 'National University of Singapore', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09067.jpg', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-CRITIC â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ (Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ) Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 500 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 4471 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ GPT-4o Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Evaluating Critique Abilities of Multimodal Models with MM-CRITIC', 'desc': 'MM-CRITIC is a new benchmark designed to assess how well Large Multimodal Models (LMMs) can critique their outputs across various tasks. It evaluates models on three main dimensions: basic critique, correction of errors, and comparison with other outputs. The benchmark includes over 500 tasks and uses expert-informed answers to ensure reliable scoring of model responses. Through extensive testing, MM-CRITIC provides insights into the critique abilities of LMMs and highlights the relationship between the quality of responses and their critique performance.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹æ‰¹è¯„èƒ½åŠ›çš„å…¨æ–°åŸºå‡†', 'desc': 'MM-CRITICæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ‰¹è¯„èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–å¤šä¸ªç»´åº¦å’Œä»»åŠ¡ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸“å®¶æä¾›çš„æ ‡å‡†ç­”æ¡ˆæ¥ç¡®ä¿è¯„åˆ†çš„å¯é æ€§ï¼Œæ¶‰åŠåŸºæœ¬ã€çº æ­£å’Œæ¯”è¾ƒç­‰å¤šä¸ªæ–¹é¢ã€‚MM-CRITICåŒ…å«è¶…è¿‡500ä¸ªä»»åŠ¡ï¼Œæ”¶é›†äº†ä¸åŒæ¨¡å‹å¤§å°çš„LMMsçš„å“åº”ï¼Œæä¾›äº†4471ä¸ªæ ·æœ¬ã€‚å®éªŒç»“æœéªŒè¯äº†MM-CRITICçš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†å“åº”è´¨é‡ä¸æ‰¹è¯„ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠä¸åŒè¯„ä¼°ç»´åº¦çš„æ‰¹è¯„éš¾åº¦å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07790', 'title': 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis', 'url': 'https://huggingface.co/papers/2511.07790', 'abstract': "The CC30k dataset, comprising citation contexts labeled with reproducibility-oriented sentiments, enhances the accuracy of large language models in predicting the reproducibility of machine learning papers.  \t\t\t\t\tAI-generated summary \t\t\t\t Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'e6a8d435a5b3f4a5', 'authors': ['Rochana R. Obadage', 'Sarah M. Rajtmajer', 'Jian Wu'], 'affiliations': ['Old Dominion University', 'The Pennsylvania State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07790.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CC30k, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 30,734 ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ, Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ±Ñ‹Ğ» ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ 94%. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° CC30k Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reproducibility Insights with CC30k Dataset', 'desc': 'The CC30k dataset is designed to improve the prediction of reproducibility-oriented sentiments in machine learning papers. It contains 30,734 citation contexts labeled as Positive, Negative, or Neutral, reflecting the perceived reproducibility of the cited works. This dataset addresses a gap in resources for studying computational reproducibility by providing a robust framework for sentiment analysis. By fine-tuning large language models on this dataset, researchers can enhance their ability to assess the reproducibility of published findings in the machine learning community.'}, 'zh': {'title': 'CC30kæ•°æ®é›†ï¼šæå‡æœºå™¨å­¦ä¹ è®ºæ–‡å¯é‡å¤æ€§é¢„æµ‹çš„åˆ©å™¨', 'desc': 'CC30kæ•°æ®é›†åŒ…å«æ ‡è®°ä¸ºå¯é‡å¤æ€§å¯¼å‘æƒ…æ„Ÿçš„å¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹æœºå™¨å­¦ä¹ è®ºæ–‡å¯é‡å¤æ€§æ–¹é¢çš„å‡†ç¡®æ€§ã€‚è¯¥æ•°æ®é›†å…±åŒ…å«30,734ä¸ªå¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡è¢«æ ‡è®°ä¸ºç§¯æã€æ¶ˆææˆ–ä¸­ç«‹ï¼Œåæ˜ è¢«å¼•ç”¨è®ºæ–‡çš„å¯é‡å¤æ€§ã€‚é€šè¿‡ä¼—åŒ…å’Œæ§åˆ¶ç”Ÿæˆçš„è´Ÿé¢æ ‡ç­¾ï¼ŒCC30kè§£å†³äº†ä¼ ç»Ÿæƒ…æ„Ÿåˆ†ææ•°æ®é›†ä¸­ç¼ºä¹è´Ÿé¢æ ‡ç­¾çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåï¼Œä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯é‡å¤æ€§å¯¼å‘æƒ…æ„Ÿåˆ†ç±»ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œä¸ºæœºå™¨å­¦ä¹ è®ºæ–‡çš„å¯é‡å¤æ€§è¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10047', 'title': 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples', 'url': 'https://huggingface.co/papers/2511.10047', 'abstract': 'MuSc-V2 framework improves zero-shot anomaly detection by leveraging mutual scoring and similarity aggregation in both 2D and 3D data, achieving significant performance gains over existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'df8dad78f3e180de', 'authors': ['Xurui Li', 'Feng Xue', 'Yu Zhou'], 'affiliations': ['School of Computer Science, University of Trento', 'School of Electronic Information and Communications, Huazhong University of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10047.jpg', 'data': {'categories': ['#benchmark', '#3d', '#optimization', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MuSc-V2 Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² (zero-shot) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒÑ ĞºĞ°Ğº Ğ² 2D, Ñ‚Ğ°Ğº Ğ¸ Ğ² 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸, Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ (Mutual Scoring Mechanism) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 2D Ğ¸ 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². MuSc-V2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: +23.7% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° ÑÑ€ĞµĞ´Ğ½ĞµĞ¹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MVTec 3D-AD Ğ¸ +19.3% Ğ½Ğ° Eyecandies, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Zero-Shot Anomaly Detection with MuSc-V2', 'desc': 'The MuSc-V2 framework enhances zero-shot anomaly detection by utilizing mutual scoring and similarity aggregation for both 2D and 3D data. It identifies normal patches that share similarities across products while recognizing that anomalies are often unique and isolated. The framework employs Iterative Point Grouping to improve 3D representations and reduce false positives, and it integrates multi-scale features through Similarity Neighborhood Aggregation. By implementing a Mutual Scoring Mechanism and Cross-modal Anomaly Enhancement, MuSc-V2 achieves significant performance improvements over existing benchmarks, demonstrating its effectiveness in diverse industrial applications.'}, 'zh': {'title': 'MuSc-V2ï¼šæå‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'MuSc-V2æ¡†æ¶é€šè¿‡åˆ©ç”¨äº’è¯„åˆ†æ•°å’Œç›¸ä¼¼æ€§èšåˆï¼Œæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äº2Då’Œ3Dæ•°æ®ã€‚è¯¥æ–¹æ³•æ­ç¤ºäº†æ­£å¸¸å›¾åƒå—ä¸å…¶ä»–ç›¸ä¼¼å—ä¹‹é—´çš„å…³ç³»ï¼Œè€Œå¼‚å¸¸åˆ™é€šå¸¸æ˜¯å¤šæ ·åŒ–å’Œå­¤ç«‹çš„ã€‚MuSc-V2é‡‡ç”¨äº†è¿­ä»£ç‚¹åˆ†ç»„å’Œç›¸ä¼¼æ€§é‚»åŸŸèšåˆç­‰æŠ€æœ¯ï¼Œå¢å¼ºäº†ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬åŸºå‡†ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (6)', '#agi', '#alignment (1)', '#architecture (3)', '#audio (1)', '#benchmark (9)', '#cv (4)', '#data', '#dataset (3)', '#diffusion (2)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (7)', '#open_source (4)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (1)', '#rlhf (1)', '#robotics (1)', '#science (2)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-11-14 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-11-14 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-11-14 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    