{
    "date": {
        "ru": "6 августа",
        "en": "August 6",
        "zh": "8月6日"
    },
    "time_utc": "2025-08-06 06:21",
    "weekday": 2,
    "issue_id": 5201,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.03320",
            "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
            "url": "https://huggingface.co/papers/2508.03320",
            "abstract": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
            "score": 26,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "71dc78f7c773cefd",
            "authors": [
                "Peiyu Wang",
                "Yi Peng",
                "Yimeng Gan",
                "Liang Hu",
                "Tianyidan Xie",
                "Xiaokun Wang",
                "Yichen Wei",
                "Chuanxin Tang",
                "Bo Zhu",
                "Changshi Li",
                "Hongyang Wei",
                "Eric Li",
                "Xuchen Song",
                "Yang Liu",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Multimodality Team, Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания, создания и редактирования изображений",
                    "desc": "Skywork UniPic - это авторегрессионная модель с 1,5 миллиардами параметров, объединяющая понимание изображений, генерацию изображений по тексту и редактирование изображений в единой архитектуре. Модель достигает высоких результатов на различных бенчмарках, включая GenEval, DPG-Bench и GEditBench-EN. Skywork UniPic использует раздельную стратегию кодирования и прогрессивное обучение с увеличением разрешения. Модель демонстрирует, что высококачественная мультимодальная интеграция возможна без чрезмерных вычислительных затрат."
                },
                "en": {
                    "title": "Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic",
                    "desc": "Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources."
                },
                "zh": {
                    "title": "Skywork UniPic：统一多模态AI的高效解决方案",
                    "desc": "Skywork UniPic是一个拥有15亿参数的自回归模型，能够统一图像理解、文本到图像生成和图像编辑。该模型通过一个单一架构消除了对特定任务适配器或模块连接器的需求，展示了紧凑的多模态系统在普通硬件上也能达到最先进的性能。Skywork UniPic在多个基准测试中表现优异，尤其是在图像生成和编辑方面，显示出其高效的训练策略和数据集设计。该模型为高保真多模态AI的实际应用提供了新的范式，且代码和权重已公开。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03694",
            "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
            "url": "https://huggingface.co/papers/2508.03694",
            "abstract": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.",
            "score": 22,
            "issue_id": 5198,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "8c05bd06521b3fb7",
            "authors": [
                "Jianxiong Gao",
                "Zhaoxi Chen",
                "Xian Liu",
                "Jianfeng Feng",
                "Chenyang Si",
                "Yanwei Fu",
                "Yu Qiao",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "NVIDIA",
                "Nanjing University",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03694.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LongVie: прорыв в генерации сверхдлинных видео с сохранением качества",
                    "desc": "LongVie - это новая автореградная модель для генерации сверхдлинных видео. Она решает проблемы временной согласованности и визуальной деградации с помощью унифицированной инициализации шума и глобальной нормализации управляющих сигналов. LongVie использует мультимодальное управление, объединяя плотные и разреженные сигналы. Модель также применяет стратегию обучения с учетом деградации для сохранения визуального качества."
                },
                "en": {
                    "title": "LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality",
                    "desc": "LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing."
                },
                "zh": {
                    "title": "超长视频生成的新突破：LongVie",
                    "desc": "LongVie是一个端到端的自回归框架，旨在解决超长视频生成中的时间一致性和视觉退化问题。它通过统一的噪声初始化、全局控制信号归一化、多模态控制和退化感知训练来实现这些目标。LongVie的核心设计确保了时间一致性，并通过多模态控制框架来减轻视觉退化。实验结果表明，LongVie在长时间可控性、一致性和质量方面达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02193",
            "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
            "url": "https://huggingface.co/papers/2508.02193",
            "abstract": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.",
            "score": 18,
            "issue_id": 5199,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "bec183ec45598da2",
            "authors": [
                "Yuxuan Song",
                "Zheng Zhang",
                "Cheng Luo",
                "Pengyang Gao",
                "Fan Xia",
                "Hao Luo",
                "Zheng Li",
                "Yuehang Yang",
                "Hongli Yu",
                "Xingwei Qu",
                "Yuwei Fu",
                "Jing Su",
                "Ge Zhang",
                "Wenhao Huang",
                "Mingxuan Wang",
                "Lin Yan",
                "Xiaoying Jia",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Yonghui Wu",
                "Hao Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02193.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в скорости генерации текста без потери качества",
                    "desc": "Seed Diffusion Preview - это новая языковая модель, основанная на дискретной диффузии. Она обеспечивает очень быструю генерацию текста благодаря параллельному неупорядоченному декодированию. Модель достигает скорости вывода 2146 токенов в секунду на GPU H20, значительно превосходя аналоги Mercury и Gemini Diffusion. При этом Seed Diffusion Preview сохраняет конкурентоспособное качество на стандартных бенчмарках для оценки кода."
                },
                "en": {
                    "title": "Speed Meets Quality: The Future of Code Generation",
                    "desc": "Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion."
                },
                "zh": {
                    "title": "种子扩散预览：速度与质量的新标杆",
                    "desc": "Seed Diffusion Preview是一种基于离散状态扩散的语言模型，具有极快的推理速度。通过非顺序的并行生成，离散扩散模型显著提高了推理效率，减少了逐个解码的延迟。该模型在H20 GPU上实现了每秒2,146个token的推理速度，同时在标准代码评估基准上保持了竞争力的性能。与当前的Mercury和Gemini Diffusion相比，Seed Diffusion Preview在速度和质量上都设立了新的标杆。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02091",
            "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
            "url": "https://huggingface.co/papers/2508.02091",
            "abstract": "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
            "score": 7,
            "issue_id": 5201,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "10eb53caada711ad",
            "authors": [
                "Xiaoya Li",
                "Xiaofei Sun",
                "Albert Wang",
                "Chris Shum",
                "Jiwei Li"
            ],
            "affiliations": [
                "DeepReinforce Team",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02091.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rag",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "CRINN: Революция в поиске ближайших соседей с помощью ИИ",
                    "desc": "CRINN - это новый подход к оптимизации алгоритмов поиска приближенных ближайших соседей (ANNS), использующий обучение с подкреплением. Он автоматически генерирует все более быстрые реализации ANNS, сохраняя при этом точность. CRINN превзошел современные методы на нескольких эталонных тестах, включая GIST-960-Euclidean и MNIST-784-Euclidean. Успех CRINN показывает, что языковые модели, дополненные обучением с подкреплением, могут эффективно автоматизировать сложные алгоритмические оптимизации."
                },
                "en": {
                    "title": "CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning",
                    "desc": "CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations."
                },
                "zh": {
                    "title": "CRINN：用强化学习加速近似最近邻搜索",
                    "desc": "CRINN是一种基于强化学习的方法，旨在优化近似最近邻搜索算法的速度，同时保持准确性。该方法将近似最近邻搜索的优化视为一个强化学习问题，以执行速度作为奖励信号。通过这种方式，CRINN能够自动生成逐渐更快的近似最近邻搜索实现，并满足准确性约束。实验结果表明，CRINN在多个基准数据集上表现优异，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03050",
            "title": "Multi-human Interactive Talking Dataset",
            "url": "https://huggingface.co/papers/2508.03050",
            "abstract": "MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
            "score": 3,
            "issue_id": 5200,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "01ba126a166568d6",
            "authors": [
                "Zeyu Zhu",
                "Weijia Wu",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03050.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Новый датасет и модель для генерации видео с разговорами нескольких людей",
                    "desc": "Исследователи представили MIT - крупномасштабный набор данных для генерации видео с разговорами нескольких людей. Этот датасет включает детальные аннотации и используется для демонстрации CovOG - базовой модели, объединяющей кодировщик поз нескольких людей и интерактивный аудиодрайвер. MIT содержит 12 часов видео высокого разрешения с 2-4 говорящими и детальными аннотациями поз тела и речевых взаимодействий. CovOG использует агрегацию индивидуальных эмбеддингов поз и модуляцию динамики головы на основе аудиопризнаков каждого говорящего."
                },
                "en": {
                    "title": "MIT: Pioneering Multi-Human Talking Video Generation",
                    "desc": "The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area."
                },
                "zh": {
                    "title": "MIT：多人人对话视频生成的新基准",
                    "desc": "MIT是一个大规模的数据集，专门用于多人的对话视频生成，包含细致的注释信息。现有的研究主要集中在单人独白或孤立的面部动画上，限制了其在真实多人的互动中的应用。我们开发了一个自动化流程，收集和注释多人的对话视频，数据集包含12小时的高分辨率视频，展示了自然的对话动态。为了展示MIT的潜力，我们提出了CovOG模型，结合了多人的姿态编码器和互动音频驱动器，展示了生成真实多人的对话视频的可行性和挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03012",
            "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
            "url": "https://huggingface.co/papers/2508.03012",
            "abstract": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.",
            "score": 3,
            "issue_id": 5199,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "4ab74a355fed1d76",
            "authors": [
                "Zexiong Ma",
                "Chao Peng",
                "Qunhong Zeng",
                "Pengfei Gao",
                "Yanzhen Zou",
                "Bing Xie"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "ByteDance",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03012.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ToolTrain: Эффективная локализация проблем в коде с помощью обученных языковых моделей",
                    "desc": "ToolTrain - это двухэтапная система обучения, объединяющая контролируемую тонкую настройку и обучение с подкреплением для улучшения работы больших языковых моделей в задаче локализации проблем в программном коде. Система интегрирует инструменты поиска по репозиторию, что позволяет преодолеть семантический разрыв между описанием проблемы на естественном языке и проблемным кодом. Экспериментальные результаты показывают, что модели, обученные с помощью ToolTrain, достигают наилучших показателей в этой задаче. Улучшенная производительность в локализации проблем также приводит к лучшим результатам в полном цикле разрешения проблем в программном обеспечении."
                },
                "en": {
                    "title": "ToolTrain: Enhancing LLMs for Superior Issue Localization",
                    "desc": "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."
                },
                "zh": {
                    "title": "ToolTrain：提升问题定位的智能工具训练框架",
                    "desc": "ToolTrain是一种两阶段的训练框架，结合了监督微调和强化学习，旨在提升大型语言模型（LLMs）在问题定位方面的能力。问题定位是识别需要修改的代码位置以解决软件问题的过程，但由于自然语言描述与故障代码之间的语义差距，这一任务非常具有挑战性。ToolTrain通过整合代码库检索工具，帮助LLMs在多步骤推理和导航过程中有效利用这些工具，从而实现了最先进的性能。实验结果表明，ToolTrain训练的模型在功能级定位上超越了Claude-3.7，证明了针对问题定位的训练策略在自动化软件开发中是有效的。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.01780",
            "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
            "url": "https://huggingface.co/papers/2508.01780",
            "abstract": "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
            "score": 3,
            "issue_id": 5199,
            "pub_date": "2025-08-03",
            "pub_date_card": {
                "ru": "3 августа",
                "en": "August 3",
                "zh": "8月3日"
            },
            "hash": "b9c09b0ce4e2dad3",
            "authors": [
                "Guozhao Mo",
                "Wenliang Zhong",
                "Jiawei Chen",
                "Xuanang Chen",
                "Yaojie Lu",
                "Hongyu Lin",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.01780.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#survey",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LiveMCPBench: Новый стандарт оценки LLM-агентов в реальных MCP-средах",
                    "desc": "LiveMCPBench представляет собой комплексный бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в реальных задачах экосистемы MCP. Он включает 95 задач и использует масштабируемый конвейер оценки с адаптивной системой судейства. Бенчмарк содержит LiveMCPTool - набор из 70 MCP-серверов и 527 инструментов, а также LiveMCPEval - фреймворк для автоматизированной оценки с использованием LLM в качестве судьи. Результаты тестирования 10 ведущих моделей показали, что лучшая модель (Claude-Sonnet-4) достигла 78.95% успешности выполнения задач."
                },
                "en": {
                    "title": "Revolutionizing LLM Evaluation in Dynamic MCP Environments",
                    "desc": "LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments."
                },
                "zh": {
                    "title": "全面评估LLM代理的基准测试平台",
                    "desc": "LiveMCPBench是一个全面的基准测试平台，旨在评估大型语言模型（LLM）代理在多样化的真实世界任务中的表现。它解决了现有基准测试仅限于单一服务器设置的问题，提供了95个基于模型上下文协议（MCP）生态系统的真实任务。通过LiveMCPTool，研究人员可以使用70个MCP服务器和527个工具，支持可扩展和可重复的评估流程。此外，LiveMCPEval框架实现了自动化和自适应评估，确保在动态任务环境中与人类评审者的高一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02079",
            "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
            "url": "https://huggingface.co/papers/2508.02079",
            "abstract": "AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.",
            "score": 2,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "25a555fc0f91f562",
            "authors": [
                "Amitava Das",
                "Abhilekh Borah",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "BITS Goa, India",
                "Manipal University, India",
                "Meta AI, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Сохранение безопасности при дообучении языковых моделей",
                    "desc": "AlignGuard-LoRA (AGL) - это фреймворк для сохранения выравнивания при дообучении больших языковых моделей. Он вводит методы регуляризации и диагностический бенчмарк для снижения дрейфа выравнивания. AGL включает несколько ключевых компонентов, таких как регуляризация на основе матрицы Фишера и регуляризация с учетом коллизий. Эмпирические оценки показывают, что AGL снижает дрейф выравнивания до 50% на критически важных для безопасности бенчмарках без ухудшения производительности на целевых задачах."
                },
                "en": {
                    "title": "Preserving Alignment in Fine-Tuning with AGL",
                    "desc": "AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks."
                },
                "zh": {
                    "title": "AlignGuard-LoRA：保持对齐，确保安全",
                    "desc": "AlignGuard-LoRA (AGL) 是一个框架，旨在通过引入正则化技术和诊断基准来保持大型语言模型在微调过程中的对齐性。该框架解决了低秩适应（LoRA）在更新过程中可能导致的对齐漂移问题，从而增强安全性和行为约束。AGL 采用了多种关键组件，包括基于费舍尔信息矩阵的正则化和任务特定的正则化，以稳定新知识的整合。实验证明，AGL 能够在不降低下游任务性能的情况下，将对齐漂移减少多达 50%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.03686",
            "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
            "url": "https://huggingface.co/papers/2508.03686",
            "abstract": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.",
            "score": 1,
            "issue_id": 5201,
            "pub_date": "2025-08-05",
            "pub_date_card": {
                "ru": "5 августа",
                "en": "August 5",
                "zh": "8月5日"
            },
            "hash": "dddc5da46c921b94",
            "authors": [
                "Shudong Liu",
                "Hongwei Liu",
                "Junnan Liu",
                "Linchen Xiao",
                "Songyang Gao",
                "Chengqi Lyu",
                "Yuzhe Gu",
                "Wenwei Zhang",
                "Derek F. Wong",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "NLP2CT Lab",
                "Shanghai AI Laboratory",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.03686.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#interpretability",
                    "#optimization"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "CompassVerifier: Надежная проверка ответов LLM во многих областях",
                    "desc": "CompassVerifier - это легковесная модель для проверки выходных данных больших языковых моделей (LLM) в различных областях. Она поддерживается VerifierBench - комплексным набором данных для оценки. CompassVerifier демонстрирует компетентность в различных областях, включая математику, знания и разнообразные задачи на рассуждение. Модель способна обрабатывать различные типы ответов, включая многозадачные проблемы, формулы и последовательные ответы, эффективно идентифицируя аномальные и недействительные ответы."
                },
                "en": {
                    "title": "Revolutionizing LLM Output Verification with CompassVerifier",
                    "desc": "CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses."
                },
                "zh": {
                    "title": "CompassVerifier：多领域答案验证的轻量级解决方案",
                    "desc": "CompassVerifier 是一种轻量级且稳健的模型，用于验证大型语言模型（LLM）在不同领域的输出。它通过 VerifierBench 这一全面的基准数据集来支持验证过程。该模型能够处理多种类型的答案，包括多子问题、公式和序列答案，并有效识别异常或无效的响应。我们希望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.02063",
            "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
            "url": "https://huggingface.co/papers/2508.02063",
            "abstract": "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7",
            "score": 0,
            "issue_id": 5198,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 августа",
                "en": "August 4",
                "zh": "8月4日"
            },
            "hash": "0b136776fbb19b26",
            "authors": [
                "Amitava Das",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon GenAI",
                "BITS Pilani Goa",
                "Meta AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.02063.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#rlhf",
                    "#alignment",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "TraceAlign: отслеживание и устранение дрейфа выравнивания в больших языковых моделях",
                    "desc": "TraceAlign - это фреймворк для выявления и снижения дрейфа выравнивания в больших языковых моделях. Он отслеживает небезопасные завершения до их источников в обучающих данных и применяет интервенции для уменьшения дрейфа при сохранении полезности модели. Ключевым элементом является Индекс Конфликта Убеждений (BCI), количественно оценивающий семантическое несоответствие между сгенерированными фрагментами и заданными политиками. Фреймворк предлагает три дополняющих друг друга метода защиты: фильтр безопасности TraceShield, контрастивную функцию потерь и стратегию декодирования Prov-Decode."
                },
                "en": {
                    "title": "TraceAlign: Bridging the Gap in LLM Alignment",
                    "desc": "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."
                },
                "zh": {
                    "title": "TraceAlign：减轻大型语言模型对齐漂移的创新框架",
                    "desc": "TraceAlign是一个框架，用于识别和减轻大型语言模型（LLMs）中的对齐漂移。它通过追踪不安全的生成结果到其训练来源，并应用干预措施来减少漂移，同时保持模型的实用性。该框架引入了信念冲突指数（BCI），量化生成内容与对齐政策之间的语义不一致性。通过三种互补的干预措施，TraceAlign能够在保持任务性能的同时，显著降低对齐漂移的发生率。"
                }
            }
        }
    ],
    "link_prev": "2025-08-05.html",
    "link_next": "2025-08-07.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "05.08",
        "en": "08/05",
        "zh": "8月5日"
    },
    "short_date_next": {
        "ru": "07.08",
        "en": "08/07",
        "zh": "8月7日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 8,
        "#agents": 2,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}