{
    "date": {
        "ru": "22 октября",
        "en": "October 22",
        "zh": "10月22日"
    },
    "time_utc": "2025-10-22 02:26",
    "weekday": 2,
    "issue_id": 6544,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.18866",
            "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
            "url": "https://huggingface.co/papers/2510.18866",
            "abstract": "LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.",
            "score": 18,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "cbff2ef26dc05177",
            "authors": [
                "Jizhan Fang",
                "Xinle Deng",
                "Haoming Xu",
                "Ziyan Jiang",
                "Yuqi Tang",
                "Ziwen Xu",
                "Shumin Deng",
                "Yunzhi Yao",
                "Mengru Wang",
                "Shuofei Qiao",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18866.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#data",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Человеческая память для AI: быстрее, точнее, эффективнее",
                    "desc": "LightMem — это система памяти для LLM, вдохновлённая моделью человеческой памяти Аткинсона-Шиффрина. Система организует память в три этапа: сенсорная память быстро фильтрует нерелевантную информацию, кратковременная память группирует данные по темам, а долговременная память консолидирует информацию в офлайн-режиме. Эксперименты показывают улучшение точности до 10.9% при одновременном сокращении использования токенов в 117 раз и времени выполнения более чем в 12 раз. LightMem решает проблему эффективного использования исторической информации из взаимодействий без существенных вычислительных затрат."
                },
                "en": {
                    "title": "LightMem: Enhancing LLMs with Efficient Memory Management",
                    "desc": "LightMem is a novel memory system designed to enhance Large Language Models (LLMs) by efficiently managing historical interaction data. It incorporates a three-stage memory architecture inspired by human memory, which includes sensory memory for filtering information, short-term memory for organizing topics, and long-term memory for offline updates. This approach allows LLMs to utilize past interactions more effectively, leading to improved accuracy and reduced computational costs. Experimental results demonstrate that LightMem significantly outperforms existing memory systems, achieving notable gains in accuracy while drastically lowering resource usage."
                },
                "zh": {
                    "title": "LightMem：高效的记忆系统提升LLMs性能",
                    "desc": "LightMem是一种受人类记忆启发的内存系统，旨在提高大型语言模型（LLMs）的性能。它通过有效管理历史交互信息，帮助模型在动态复杂的环境中更好地利用这些信息。LightMem将内存组织为三个互补的阶段：感知记忆、短期记忆和长期记忆，从而实现信息的快速过滤、组织和总结。实验结果表明，LightMem在准确性上优于强基线，同时显著减少了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18855",
            "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
            "url": "https://huggingface.co/papers/2510.18855",
            "abstract": "Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.",
            "score": 8,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "8c4e98c6e6663cb4",
            "authors": [
                "Ling Team",
                "Anqi Shen",
                "Baihui Li",
                "Bin Hu",
                "Bin Jing",
                "Cai Chen",
                "Chao Huang",
                "Chao Zhang",
                "Chaokun Yang",
                "Cheng Lin",
                "Chengyao Wen",
                "Congqi Li",
                "Deng Zhao",
                "Dingbo Yuan",
                "Donghai You",
                "Fagui Mao",
                "Fanzhuang Meng",
                "Feng Xu",
                "Guojie Li",
                "Guowei Wang",
                "Hao Dai",
                "Haonan Zheng",
                "Hong Liu",
                "Jia Guo",
                "Jiaming Liu",
                "Jian Liu",
                "Jianhao Fu",
                "Jiannan Shi",
                "Jianwen Wang",
                "Jianxin Lai",
                "Jin Yang",
                "Jun Mei",
                "Jun Zhou",
                "Junbo Zhao",
                "Junping Zhao",
                "Kuan Xu",
                "Le Su",
                "Lei Chen",
                "Li Tang",
                "Liang Jiang",
                "Liangcheng Fu",
                "Lianhao Xu",
                "Linfeng Shi",
                "Lisha Liao",
                "Longfei Zheng",
                "Meng Li",
                "Mingchun Chen",
                "Qi Zuo",
                "Qiang Cheng",
                "Qianggang Cao",
                "Qitao Shi",
                "Quanrui Guo",
                "Senlin Zhu",
                "Shaofei Wang",
                "Shaomian Zheng",
                "Shuaicheng Li",
                "Shuwei Gu",
                "Siba Chen",
                "Tao Wu",
                "Tao Zhang",
                "Tianyu Zhang",
                "Tianyu Zhou",
                "Tiwei Bie",
                "Tongkai Yang",
                "Wang Hong",
                "Wang Ren",
                "Weihua Chen",
                "Wenbo Yu",
                "Wengang Zheng",
                "Xiangchun Wang",
                "Xiaodong Yan",
                "Xiaopei Wan",
                "Xin Zhao",
                "Xinyu Kong",
                "Xinyu Tang",
                "Xudong Han",
                "Xudong Wang",
                "Xuemin Yang",
                "Xueyu Hu",
                "Yalin Zhang",
                "Yan Sun",
                "Yicheng Shan",
                "Yilong Wang",
                "Yingying Xu",
                "Yongkang Liu",
                "Yongzhen Guo",
                "Yuanyuan Wang",
                "Yuchen Yan",
                "Yuefan Wang",
                "Yuhong Guo",
                "Zehuan Li",
                "Zhankai Xu",
                "Zhe Li",
                "Zhenduo Zhang",
                "Zhengke Gui",
                "Zhenxuan Pan",
                "Zhenyu Huang",
                "Zhenzhong Lan",
                "Zhiqiang Ding",
                "Zhiqiang Zhang",
                "Zhixun Li",
                "Zhizhen Liu",
                "Zihao Wang",
                "Zujie Wen"
            ],
            "affiliations": [
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18855.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Триллион параметров для всех: демократизация мощного AI-мышления",
                    "desc": "Ring-1T — это первая открытая thinking-модель с триллионом параметров, которая использует около 50 миллиардов параметров на токен. Для обучения такой масштабной модели разработаны три ключевые технологии: IcePop для стабилизации RL-обучения, C3PO++ для эффективной обработки длинных последовательностей и ASystem — высокопроизводительный фреймворк для reinforcement learning. Модель показывает выдающиеся результаты на сложных бенчмарках, включая математические олимпиады и программирование, достигая серебряной медали на IMO-2025. Полный релиз модели делает продвинутые reasoning-способности доступными для всего исследовательского сообщества."
                },
                "en": {
                    "title": "Democratizing Intelligence with a Trillion-Parameter Model",
                    "desc": "Ring-1T is a groundbreaking open-source thinking model that boasts a trillion parameters, making it one of the largest models available. It tackles significant training challenges through three innovative techniques: IcePop for stabilizing reinforcement learning, C3PO++ for optimizing resource use during long rollouts, and ASystem to eliminate bottlenecks in training. These advancements enable Ring-1T to achieve impressive benchmark scores, demonstrating its superior reasoning abilities. By making this model accessible to the research community, it aims to democratize advanced reasoning intelligence and set a new standard for open-source models."
                },
                "zh": {
                    "title": "一万亿参数模型，推动推理智能的民主化",
                    "desc": "Ring-1T是一个开源的思维模型，拥有一万亿个参数，旨在解决训练中的挑战。它通过三项创新技术IcePop、C3PO++和ASystem，克服了训练推理不一致、资源利用效率低下和系统瓶颈等问题。Ring-1T在多个基准测试中取得了优异的成绩，展示了其卓越的推理能力。通过向社区发布完整的一万亿参数模型，我们为研究人员提供了前沿推理能力的直接访问，推动了大规模推理智能的民主化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18876",
            "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
            "url": "https://huggingface.co/papers/2510.18876",
            "abstract": "Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.",
            "score": 5,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "f12731f1d8090440",
            "authors": [
                "Haochen Wang",
                "Yuhao Wang",
                "Tao Zhang",
                "Yikang Zhou",
                "Yanwei Li",
                "Jiacong Wang",
                "Ye Tian",
                "Jiahao Meng",
                "Zilong Huang",
                "Guangcan Mai",
                "Anran Wang",
                "Yunhai Tong",
                "Zhuochen Wang",
                "Xiangtai Li",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "NLPR, MAIS, CASIA",
                "PKU",
                "UCAS",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18876.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#games",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Понимание любых регионов изображения с учётом глобального контекста",
                    "desc": "Статья представляет модель GAR (Grasp Any Region), которая улучшает понимание отдельных регионов на изображениях, интегрируя глобальный контекст и моделируя взаимодействия между объектами. В отличие от предыдущих подходов, которые анализировали регионы изолированно, GAR использует технику RoI-aligned feature replay для точного восприятия с учётом окружающего контекста. Модель способна не только описывать регионы, но и отвечать на свободные вопросы о них, демонстрируя продвинутое композиционное рассуждение. GAR-1B показывает state-of-the-art результаты в задачах генерации описаний и превосходит даже гораздо более крупные модели в понимании взаимосвязей между несколькими регионами, а также легко переносится на видео."
                },
                "en": {
                    "title": "Grasp Any Region: Enhancing Visual Understanding through Global Contexts and Interactions",
                    "desc": "The paper introduces Grasp Any Region (GAR), a model designed to improve region-level visual understanding by integrating global contexts and modeling interactions between different visual prompts. Unlike previous models that focused on isolated regions, GAR utilizes a RoI-aligned feature replay technique to enhance perception and reasoning capabilities. This allows GAR to answer complex questions about specific regions while considering their relationships with other regions. The model has shown superior performance in tasks like captioning and video reference, demonstrating its effectiveness in both single-region comprehension and multi-region interactions."
                },
                "zh": {
                    "title": "掌握任何区域，提升视觉理解！",
                    "desc": "Grasp Any Region (GAR) 是一种增强区域级视觉理解的方法，它通过整合全局上下文和建模交互来实现更高级的推理。GAR 采用有效的 RoI 对齐特征重放技术，支持精确感知和多提示之间的交互建模。通过这些功能，GAR 能够回答关于任何区域的具体自由形式问题，从被动描述转变为主动对话。实验结果表明，GAR 在图像描述和视频参考任务中均超越了现有模型，展现出强大的理解能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16880",
            "title": "Chem-R: Learning to Reason as a Chemist",
            "url": "https://huggingface.co/papers/2510.16880",
            "abstract": "Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.",
            "score": 4,
            "issue_id": 6544,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 октября",
                "en": "October 19",
                "zh": "10月19日"
            },
            "hash": "1cb1fbcf81423b67",
            "authors": [
                "Weida Wang",
                "Benteng Chen",
                "Di Zhang",
                "Wanhao Liu",
                "Shuchen Pu",
                "Ben Gao",
                "Jin Zeng",
                "Lei Bai",
                "Wanli Ouyang",
                "Xiaoyong Wei",
                "Tianshu Yu",
                "Tianfan Fu",
                "Shuzhou Sun",
                "Jiatong Li",
                "Zifu Wang",
                "Yuqiang Li",
                "Shufei Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Shanghai AI Lab",
                "The Chinese University of Hong Kong, Shenzhen",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16880.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#reasoning",
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "⚗️",
                "ru": {
                    "title": "Chem-R: LLM, которая рассуждает как химик",
                    "desc": "Chem-R - это модель для химического рассуждения, обученная в три этапа для решения задач в химии. Сначала модель получает базовые химические знания, затем учится рассуждать как эксперт-химик через дистилляцию структурированных reasoning траекторий, и наконец оптимизируется для работы с различными задачами через Multi-task Group Relative Policy Optimization. Модель превосходит ведущие LLM, включая Gemini-2.5-Pro и DeepSeek-R1, на 46% в молекулярных задачах и на 66% в задачах с химическими реакциями. Chem-R демонстрирует сильную генерализацию и интерпретируемость, открывая путь к новому поколению AI-систем для химических исследований."
                },
                "en": {
                    "title": "Chem-R: Revolutionizing Chemical Reasoning with Expert Insights",
                    "desc": "Chem-R is a novel Chemical Reasoning model that enhances chemical task performance by integrating essential chemical knowledge and expert reasoning. It employs a three-phase training approach: first, it builds a solid foundation of chemical knowledge; second, it distills expert reasoning protocols to improve problem-solving reliability; and third, it optimizes performance across various tasks through multi-task learning. This structured training allows Chem-R to outperform existing large language models and chemical foundation models significantly. The results demonstrate Chem-R's strong generalization capabilities and its potential to drive advancements in AI-assisted chemical discovery."
                },
                "zh": {
                    "title": "Chem-R：化学推理的新纪元",
                    "desc": "Chem-R是一种经过三阶段训练的化学推理模型，能够在化学任务中表现出色。它通过整合核心知识、专家推理和多任务优化，模拟化学家的思维过程。该模型的训练包括建立化学基础知识、引入结构化的专家推理轨迹以及优化多任务性能。Chem-R在多个基准测试中超越了现有的大型语言模型，展示了其强大的泛化能力和解释性，成为下一代AI驱动化学发现的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18692",
            "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
            "url": "https://huggingface.co/papers/2510.18692",
            "abstract": "Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.",
            "score": 2,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "c596d2b417fc08c8",
            "authors": [
                "Weinan Jia",
                "Yuning Lu",
                "Mengqi Huang",
                "Hualiang Wang",
                "Binyuan Huang",
                "Nan Chen",
                "Mu Liu",
                "Jidong Jiang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "FanqieAI, ByteDance China",
                "Hong Kong University of Science and Technology",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18692.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умное внимание для длинных видео",
                    "desc": "Статья представляет Mixture-of-Groups Attention (MoGA) — новый механизм внимания для эффективной генерации длинных видео с помощью Diffusion Transformers. Вместо квадратичного роста вычислений при увеличении длины последовательности, MoGA использует обучаемый роутер для точного сопоставления токенов без блочной оценки. Метод легко интегрируется с современными техниками вроде FlashAttention и позволяет генерировать минутные видео в разрешении 480p при 24 fps с контекстной длиной около 580 тысяч токенов. Эксперименты подтверждают эффективность подхода для различных задач генерации видео."
                },
                "en": {
                    "title": "Efficient Long Video Generation with MoGA",
                    "desc": "The paper presents Mixture-of-Groups Attention (MoGA), a novel approach to improve long video generation using Diffusion Transformers by solving the quadratic scaling problem of full attention. MoGA reduces redundancy in attention mechanisms by employing a learnable token router that accurately matches tokens, eliminating the need for blockwise estimation. This method allows for effective long-range interactions while maintaining efficiency, integrating well with existing attention frameworks. The results demonstrate that MoGA can generate high-quality, long videos at a significant scale, showcasing its potential in video generation tasks."
                },
                "zh": {
                    "title": "高效长视频生成的新方法",
                    "desc": "Mixture-of-Groups Attention（MoGA）是一种高效的稀疏注意力机制，旨在解决扩散变换器中全注意力的二次扩展问题，从而实现长视频生成。由于注意力机制存在高度冗余，输出主要由少量查询-键对主导，MoGA通过轻量级的可学习令牌路由器精确匹配令牌，避免了块状估计的限制。该方法通过语义感知路由实现了有效的长距离交互，并且作为无核方法，MoGA能够与现代注意力堆栈无缝集成。基于MoGA，我们开发了一种高效的长视频生成模型，能够以24帧每秒的速度生成分钟级、480p的多镜头视频，验证了该方法在各种视频生成任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18775",
            "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
            "url": "https://huggingface.co/papers/2510.18775",
            "abstract": "UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
            "score": 1,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "04f7bc0adba95d76",
            "authors": [
                "Teng Hu",
                "Jiangning Zhang",
                "Zihan Su",
                "Ran Yi"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18775.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Нативная генерация видео в 4K через разделение внимания",
                    "desc": "UltraGen - это новый фреймворк для генерации видео, который впервые позволяет эффективно создавать видео в высоком разрешении (1080P/2K/4K) напрямую, без промежуточных этапов. Основная проблема существующих diffusion transformer моделей - квадратичная вычислительная сложность attention механизма, которая делает генерацию высокого разрешения непрактичной. UltraGen решает эту проблему через иерархическую dual-branch архитектуру, разделяющую полное внимание на локальную ветвь для детальной проработки регионов и глобальную ветвь со сжатием для общей семантической согласованности. Эксперименты показывают, что UltraGen превосходит существующие методы и двухэтапные пайплайны с super-resolution по качеству генерации."
                },
                "en": {
                    "title": "UltraGen: High-Resolution Video Generation Made Efficient",
                    "desc": "UltraGen is a new framework for generating high-resolution videos efficiently. It uses a hierarchical dual-branch attention architecture that separates local and global attention, allowing for detailed regional content and overall coherence. This approach, combined with a spatially compressed global modeling strategy, reduces computational costs while maintaining high-quality outputs. As a result, UltraGen can produce videos at resolutions up to 4K, surpassing previous models in both quality and performance."
                },
                "zh": {
                    "title": "UltraGen：高效生成高分辨率视频的新框架",
                    "desc": "UltraGen是一种新的视频生成框架，能够高效地合成高分辨率视频。它采用了分层双分支注意力架构，结合了全局和局部注意力机制，以提高视频生成的质量和效率。通过空间压缩的全局建模策略，UltraGen能够有效学习全局依赖关系，同时降低计算成本。实验结果表明，UltraGen在生成1080P和4K分辨率视频方面表现优于现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18726",
            "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
            "url": "https://huggingface.co/papers/2510.18726",
            "abstract": "A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.",
            "score": 1,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "02c47b36a2a8b616",
            "authors": [
                "Shihao Li",
                "Yuanxing Zhang",
                "Jiangtao Wu",
                "Zhide Lei",
                "Yiwen He",
                "Runzhe Wen",
                "Chenxi Liao",
                "Chengkang Jiang",
                "An Ping",
                "Shuo Gao",
                "Suhan Wang",
                "Zhaozhou Bian",
                "Zijun Zhou",
                "Jingyi Xie",
                "Jiayi Zhou",
                "Jing Wang",
                "Yifan Yao",
                "Weihao Xie",
                "Yingshui Tan",
                "Yanghai Wang",
                "Qianqian Xie",
                "Zhaoxiang Zhang",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "CASIA",
                "Kuaishou Technology",
                "Nanjing University",
                "Shanghai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18726.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Следование инструкциям важнее полноты описания видео",
                    "desc": "Исследователи представили новый бенчмарк IF-VidCap для оценки способности моделей генерировать описания видео согласно конкретным инструкциям пользователя. Бенчмарк содержит 1400 образцов и оценивает модели по двум критериям: корректность формата и корректность содержания. Тестирование более 20 моделей показало, что лучшие open-source решения почти сравнялись с проприетарными моделями. Модели, специализированные на детальном описании видео, уступают универсальным LLM при работе со сложными инструкциями."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Instruction-Following in Video Captioning",
                    "desc": "The paper introduces IF-VidCap, a new benchmark designed to evaluate video captioning models based on their ability to follow user instructions. Unlike existing benchmarks that focus mainly on descriptive accuracy, IF-VidCap assesses both format correctness and content correctness in captions. The study reveals that while proprietary models still lead in performance, top open-source models are rapidly closing the gap. Additionally, it highlights that models optimized for dense captioning struggle with complex instructions, suggesting a need for future advancements in both descriptive and instruction-following capabilities."
                },
                "zh": {
                    "title": "新基准IF-VidCap：提升视频字幕生成的指令遵循能力",
                    "desc": "本文介绍了一个新的基准测试IF-VidCap，用于评估视频字幕生成模型的指令遵循能力。现有的基准主要关注描述的全面性，而忽视了模型在遵循用户特定指令方面的表现。IF-VidCap包含1400个高质量样本，评估字幕的格式正确性和内容正确性两个维度。研究发现，尽管专有模型仍占主导地位，但顶尖的开源模型正在缩小与专有模型的性能差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17722",
            "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
            "url": "https://huggingface.co/papers/2510.17722",
            "abstract": "MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
            "score": 1,
            "issue_id": 6544,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "4f3ffcd4b03ee354",
            "authors": [
                "Yaning Pan",
                "Zekun Wang",
                "Qianqian Xie",
                "Yongqian Wen",
                "Yuanxing Zhang",
                "Guohui Zhang",
                "Haoxuan Hu",
                "Zhiyu Pan",
                "Yibing Huang",
                "Zhidong Gan",
                "Yonghong Lin",
                "An Ping",
                "Tianhao Peng",
                "Jiaheng Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Kuaishou Technology",
                "Nanjing University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17722.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Многоходовые диалоги: новый рубеж в понимании видео для AI",
                    "desc": "Статья представляет MT-Video-Bench — новый бенчмарк для оценки мультимодальных языковых моделей (MLLM) в многоходовых диалогах о видеоконтенте. Существующие методы оценки ограничиваются односложными вопросами и ответами, игнорируя сложность реальных диалоговых сценариев. Бенчмарк включает 987 тщательно составленных многоходовых диалогов из различных областей и оценивает шесть ключевых компетенций, фокусируясь на восприятии и интерактивности. Тестирование современных open-source и closed-source моделей выявило значительные различия в производительности и ограничения при работе с многоходовыми видео-диалогами."
                },
                "en": {
                    "title": "Evaluating AI's Dialogue Skills in Video Contexts",
                    "desc": "The paper introduces MT-Video-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in the context of multi-turn video dialogues. Unlike previous benchmarks that focused on single-turn interactions, MT-Video-Bench assesses the models' abilities to perceive and interact across complex dialogues. It includes 987 carefully curated dialogues from various domains, ensuring relevance to real-world applications like sports analysis and intelligent tutoring. The evaluation reveals notable performance differences among MLLMs, highlighting their strengths and weaknesses in handling multi-turn interactions in video content."
                },
                "zh": {
                    "title": "多轮视频对话的评估新标准",
                    "desc": "MT-Video-Bench 是一个评估多模态大型语言模型（MLLMs）在多轮视频对话中的能力的基准。它关注感知能力和互动性，涵盖了来自不同领域的987个精心策划的多轮对话。该基准旨在填补现有评估工具在多轮对话复杂性方面的空白，特别是在实际应用中，如互动体育分析和基于视频的智能辅导。通过对多种最先进的 MLLMs 进行评估，MT-Video-Bench 揭示了它们在处理多轮视频对话时的显著性能差异和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18701",
            "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2510.18701",
            "abstract": "UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.",
            "score": 0,
            "issue_id": 6544,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "a88009f51e46a1e5",
            "authors": [
                "Yibin Wang",
                "Zhimin Li",
                "Yuhang Zang",
                "Jiazi Bu",
                "Yujie Zhou",
                "Yi Xin",
                "Junjun He",
                "Chunyu Wang",
                "Qinglin Lu",
                "Cheng Jin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Hunyuan, Tencent",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18701.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multilingual",
                    "#multimodal",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Всесторонняя оценка генерации изображений по тексту",
                    "desc": "UniGenBench++ — это комплексный бенчмарк для оценки text-to-image генерации, который проверяет семантическую согласованность моделей в различных сценариях и на разных языках. Он включает 600 промптов, организованных иерархически по 5 основным темам и 20 подтемам, покрывая 10 первичных и 27 дополнительных критериев оценки. Для проверки робастности модели промпты представлены на английском и китайском языках в короткой и длинной форме. Для оценки используется мультимодальная LLM Gemini-2.5-Pro, а также обученная модель для офлайн-анализа результатов text-to-image моделей."
                },
                "en": {
                    "title": "UniGenBench++: Elevating Text-to-Image Evaluation with Semantic Precision",
                    "desc": "UniGenBench++ is a new benchmark designed to evaluate text-to-image (T2I) generation models by measuring how well the generated images match the meanings of their corresponding text prompts. It addresses previous benchmarks' shortcomings by including a wide variety of scenarios and supporting multiple languages, specifically English and Chinese. The benchmark features a hierarchical structure with 600 prompts that cover 5 main themes and 20 subthemes, allowing for detailed assessments across 10 primary and 27 sub-evaluation criteria. Additionally, it utilizes a Multi-modal Large Language Model to create a reliable evaluation pipeline, enabling thorough analysis of both open- and closed-source T2I models."
                },
                "zh": {
                    "title": "UniGenBench++：文本到图像生成的全面评估基准",
                    "desc": "UniGenBench++ 是一个全面的文本到图像生成基准，旨在评估生成图像与文本提示之间的语义一致性。该基准采用分层提示结构，涵盖多种场景和语言，解决了现有基准缺乏多样性和细粒度评估的问题。它包含600个提示，组织成5个主要主题和20个子主题，能够全面探测T2I模型在10个主要和27个子评估标准上的表现。通过使用多模态大型语言模型的能力，UniGenBench++ 提供了一个有效的评估管道，帮助社区更好地评估和比较不同的T2I模型。"
                }
            }
        }
    ],
    "link_prev": "2025-10-21.html",
    "link_next": "2025-10-23.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10月21日"
    },
    "short_date_next": {
        "ru": "23.10",
        "en": "10/23",
        "zh": "10月23日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    }
}