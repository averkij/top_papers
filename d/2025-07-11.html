
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. July 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 июля</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-10.html">⬅️ <span id="prev-date">10.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-14.html">➡️ <span id="next-date">14.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'};
        let feedDateNext = {'ru': '14.07', 'en': '07/14', 'zh': '7月14日'};
        let feedDatePrev = {'ru': '10.07', 'en': '07/10', 'zh': '7月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.07966', 'title': 'Scaling RL to Long Videos', 'url': 'https://huggingface.co/papers/2507.07966', 'abstract': 'A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).', 'score': 82, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '4ab23da398f0e8d8', 'authors': ['Yukang Chen', 'Wei Huang', 'Baifeng Shi', 'Qinghao Hu', 'Hanrong Ye', 'Ligeng Zhu', 'Zhijian Liu', 'Pavlo Molchanov', 'Jan Kautz', 'Xiaojuan Qi', 'Sifei Liu', 'Hongxu Yin', 'Yao Lu', 'Song Han'], 'affiliations': ['HKU', 'MIT', 'NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.07966.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#multimodal', '#rl', '#open_source', '#training', '#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'Революция в понимании длинных видео с помощью ИИ', 'desc': 'Представлена полноценная система для масштабирования моделей визуально-языкового понимания на длинные видео с использованием обучения с подкреплением. Система включает большой датасет LongVideo-Reason с 52 тысячами пар вопрос-ответ по длинным видео, двухэтапный процесс обучения с цепочкой рассуждений и RL, а также специализированную инфраструктуру обучения MR-SP. Модель LongVILA-R1-7B демонстрирует высокую производительность на различных задачах рассуждения по длинным видео. Предложенный подход позволяет эффективно масштабировать визуально-языковые модели для работы с длинными видео.'}, 'en': {'title': 'Scaling Vision-Language Models for Long Video Reasoning', 'desc': "This paper presents a framework designed to enhance vision-language models (VLMs) for reasoning tasks involving long videos. It introduces a large dataset called LongVideo-Reason, which contains 52,000 question-answer pairs related to long videos, facilitating high-quality reasoning across various domains. The framework employs a two-stage training process that combines chain-of-thought supervised fine-tuning with reinforcement learning, optimizing the model's performance. Additionally, it features a specialized training infrastructure, Multi-modal Reinforcement Sequence Parallelism, which significantly accelerates the training process for long video reasoning tasks."}, 'zh': {'title': '长视频推理的新突破', 'desc': '本文提出了一种框架，用于通过强化学习将视觉-语言模型（VLMs）扩展到长视频推理。我们整合了三个关键组件：一个包含52K长视频问答对的大规模数据集LongVideo-Reason，一个两阶段的训练流程，以及一个名为多模态强化序列并行（MR-SP）的训练基础设施。实验结果表明，LongVILA-R1-7B在长视频问答基准上表现优异，并在时间推理、目标和目的推理、空间推理等方面超越了其他模型。我们的系统在长视频强化学习训练中实现了高达2.1倍的加速，标志着在VLMs中进行长视频推理的坚实一步。'}}}, {'id': 'https://huggingface.co/papers/2507.05964', 'title': 'T-LoRA: Single Image Diffusion Model Customization Without Overfitting', 'url': 'https://huggingface.co/papers/2507.05964', 'abstract': 'T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.', 'score': 80, 'issue_id': 4766, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '8df9f241b664bc80', 'authors': ['Vera Soboleva', 'Aibek Alanov', 'Andrey Kuznetsov', 'Konstantin Sobolev'], 'affiliations': ['AIRI, HSE University', 'AIRI, MSU', 'AIRI, Sber, Innopolis', 'HSE University, AIRI'], 'pdf_title_img': 'assets/pdf/title_img/2507.05964.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#low_resource'], 'emoji': '🎨', 'ru': {'title': 'T-LoRA: Точная персонализация диффузионных моделей на одном изображении', 'desc': 'T-LoRA - это новая техника для персонализации диффузионных моделей, использующая адаптацию низкого ранга, зависящую от временного шага. Она решает проблему переобучения при ограниченном наборе данных, улучшая точность воспроизведения концепций и соответствие тексту. T-LoRA применяет динамическую стратегию дообучения и ортогональную инициализацию для обеспечения независимости компонентов адаптера. Эксперименты показывают превосходство T-LoRA над стандартными методами персонализации диффузионных моделей в условиях ограниченных данных и ресурсов.'}, 'en': {'title': 'Personalizing Diffusion Models with T-LoRA: Dynamic Fine-Tuning for Better Results', 'desc': "T-LoRA is a framework designed to improve the personalization of diffusion models, particularly when only a limited amount of data is available. It introduces a dynamic fine-tuning strategy that adapts to different diffusion timesteps, addressing the issue of overfitting that often occurs with higher timesteps. Additionally, T-LoRA employs orthogonal initialization to maintain independence among adapter components, enhancing the model's ability to generate accurate outputs. Through extensive testing, T-LoRA demonstrates superior performance in balancing concept fidelity and text alignment compared to traditional methods."}, 'zh': {'title': 'T-LoRA：扩散模型个性化的新突破', 'desc': 'T-LoRA是一种时间步依赖的低秩适应框架，旨在通过动态微调策略和正交初始化来增强扩散模型的个性化。该方法解决了在样本有限的情况下，扩散模型微调常常出现的过拟合问题，从而提高了概念的保真度和文本对齐能力。T-LoRA的创新之处在于其动态微调策略和权重参数化技术，使得适配器组件之间保持独立。实验结果表明，T-LoRA在数据有限和资源受限的情况下，优于标准的LoRA和其他个性化技术。'}}}, {'id': 'https://huggingface.co/papers/2507.07999', 'title': 'Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology', 'url': 'https://huggingface.co/papers/2507.07999', 'abstract': 'TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human "thinking with images". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.', 'score': 35, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'e52c2296896d713c', 'authors': ['Haochen Wang', 'Xiangtai Li', 'Zilong Huang', 'Anran Wang', 'Jiacong Wang', 'Tao Zhang', 'Jiani Zheng', 'Sule Bai', 'Zijian Kang', 'Jiashi Feng', 'Zhuochen Wang', 'Zhaoxiang Zhang'], 'affiliations': ['ByteDance', 'NLPR, MAIS, CASIA', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2507.07999.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#interpretability', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Отслеживаемые доказательства - ключ к улучшению визуального ИИ', 'desc': 'TreeBench - это диагностический бенчмарк для оценки визуального обоснованного рассуждения, основанный на трех принципах: восприятие сложных сцен, отслеживаемые доказательства и рассуждения второго порядка. Он состоит из 405 сложных пар вопросов и ответов по изображениям, с которыми даже самые продвинутые модели справляются с трудом. TreeVGR - это парадигма обучения, использующая обучение с подкреплением для совместного контроля локализации и рассуждений. Инициализированная на основе Qwen2.5-VL-7B, она улучшает результаты на нескольких бенчмарках, доказывая важность отслеживаемости для развития визуально-обоснованных рассуждений.'}, 'en': {'title': 'Enhancing Visual Grounded Reasoning with TreeBench and TreeVGR', 'desc': 'This paper introduces TreeBench, a benchmark designed to evaluate visual grounded reasoning by focusing on subtle target detection, traceable evidence, and second-order reasoning. It highlights the need for a comprehensive assessment tool as existing models struggle with complex visual tasks, achieving less than 60% accuracy on the benchmark. TreeVGR is proposed as an enhancement that uses reinforcement learning to improve joint localization and reasoning, demonstrating significant performance gains over existing models. The research emphasizes the importance of traceability in developing advanced visual reasoning capabilities.'}, 'zh': {'title': '提升视觉推理的可追溯性', 'desc': '本文提出了TreeBench，一个用于评估视觉基础推理的基准，侧重于复杂场景中微妙目标的检测、可追溯证据和二阶推理。TreeVGR则通过强化学习增强了这一过程，实现了定位和推理的联合训练。研究表明，现有的先进模型在TreeBench基准上表现不佳，准确率未超过60%。通过引入可追溯性，TreeVGR显著提升了模型在视觉基础推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.07984', 'title': 'OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding', 'url': 'https://huggingface.co/papers/2507.07984', 'abstract': 'OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/', 'score': 28, 'issue_id': 4765, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '044eb90ac618689f', 'authors': ['JingLi Lin', 'Chenming Zhu', 'Runsen Xu', 'Xiaohan Mao', 'Xihui Liu', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.07984.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#dataset', '#reasoning', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Новый вызов для ИИ: рассуждения в пространстве и времени', 'desc': 'OST-Bench - это новый бенчмарк для оценки мультимодальных больших языковых моделей в задачах онлайн пространственно-временного рассуждения. Он включает 1,4 тыс. сцен и 10 тыс. пар вопросов-ответов, собранных из реальных 3D-сканов помещений. Эксперименты показали, что современные модели испытывают трудности с комплексным пространственным рассуждением и долговременной памятью. Бенчмарк выявляет ключевые проблемы, которые необходимо решить для улучшения воплощенных рассуждений в реальном мире.'}, 'en': {'title': 'Evaluating MLLMs in Real-World Spatio-Temporal Reasoning', 'desc': 'OST-Bench is a new benchmark that tests multimodal large language models (MLLMs) on their ability to understand and reason about space and time while interacting with real-world environments. Unlike traditional benchmarks that use fixed inputs, OST-Bench evaluates models in an online setting where they must process information as they explore scenes. The study reveals that current MLLMs struggle with complex spatial reasoning and long-term memory tasks, especially as the amount of information increases. By identifying common errors, the research highlights key areas for improvement in online embodied perception and reasoning.'}, 'zh': {'title': '在线时空推理的新挑战', 'desc': 'OST-Bench是一个评估多模态大型语言模型（MLLMs）在在线时空推理任务中的基准。它强调了在动态场景中处理复杂空间线索和长期记忆的挑战。通过对1.4千个场景和1万对问答的评估，发现现有模型在复杂时空推理任务中表现不佳，尤其是在探索范围扩大和记忆增长时准确率下降。该基准旨在推动在线具身推理的研究与发展，提供了数据集和代码以供进一步探索。'}}}, {'id': 'https://huggingface.co/papers/2507.07990', 'title': 'Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs', 'url': 'https://huggingface.co/papers/2507.07990', 'abstract': 'A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.', 'score': 23, 'issue_id': 4766, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'eff3cb8ac467d2a7', 'authors': ['Jeongseok Hyun', 'Sukjun Hwang', 'Su Ho Han', 'Taeoh Kim', 'Inwoong Lee', 'Dongyoon Wee', 'Joon-Young Lee', 'Seon Joo Kim', 'Minho Shim'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'NAVER Cloud', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07990.jpg', 'data': {'categories': ['#video', '#optimization', '#training', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение видео-LLM без потери качества', 'desc': 'Метод пространственно-временного объединения токенов (STTM) повышает эффективность видео-LLM, используя избыточность данных. STTM преобразует каждый кадр в многоуровневые пространственные токены с помощью квадродерева, а затем выполняет направленное попарное объединение по временной оси. Этот подход превосходит существующие методы сокращения токенов на шести эталонных тестах видео-QA. STTM достигает двукратного ускорения с минимальной потерей точности при 50% бюджете токенов.'}, 'en': {'title': 'Boosting Video LLM Efficiency with Smart Token Merging', 'desc': 'This paper introduces a new method called Spatio-Temporal Token Merging (STTM) to enhance the efficiency of video large language models (LLMs). The method reduces the number of tokens used in video processing by merging them based on local spatial and temporal redundancies, which helps to maintain performance while speeding up computations. STTM achieves significant speed improvements, allowing for up to three times faster processing with minimal accuracy loss. Additionally, it is designed to be query-agnostic, enabling the reuse of cached information across different questions about the same video.'}, 'zh': {'title': '时空令牌合并，提升视频模型效率！', 'desc': '本文提出了一种时空令牌合并方法（STTM），旨在提高视频大语言模型（LLM）的效率。该方法通过利用视频数据中的局部空间和时间冗余，显著减少计算量，同时保持较高的准确性。STTM首先将每帧图像转化为多粒度的空间令牌，然后在时间维度上进行有针对性的配对合并。实验结果表明，STTM在多个视频问答基准测试中表现优于现有的令牌减少方法，能够实现显著的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2507.07998', 'title': 'PyVision: Agentic Vision with Dynamic Tooling', 'url': 'https://huggingface.co/papers/2507.07998', 'abstract': 'PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.', 'score': 20, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'ab8504b49800fe67', 'authors': ['Shitian Zhao', 'Haoquan Zhang', 'Shaoheng Lin', 'Ming Li', 'Qilong Wu', 'Kaipeng Zhang', 'Chen Wei'], 'affiliations': ['CUHK', 'NUS', 'Rice University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.07998.jpg', 'data': {'categories': ['#reasoning', '#agents', '#interpretability', '#cv', '#benchmark'], 'emoji': '🔧', 'ru': {'title': 'PyVision: LLM создают инструменты для визуального анализа', 'desc': 'PyVision - это интерактивный фреймворк, позволяющий большим языковым моделям (LLM) автономно создавать и совершенствовать инструменты на Python для визуального анализа. Фреймворк использует многоэтапный подход, где модель генерирует, выполняет и улучшает инструменты под конкретную задачу. PyVision значительно повышает производительность LLM на различных бенчмарках, например, улучшая результаты GPT-4.1 на 7.8% в V* и Claude-4.0-Sonnet на 31.1% в VLMsAreBlind-mini. Это представляет собой шаг к более гибкому и интерпретируемому решению задач в области компьютерного зрения.'}, 'en': {'title': 'Empowering LLMs with Dynamic Tool Creation for Visual Reasoning', 'desc': 'PyVision is an innovative framework that empowers large language models (LLMs) to autonomously create and improve Python tools for visual reasoning tasks. Unlike previous methods that relied on fixed workflows, PyVision allows for dynamic tool generation and execution, enhancing flexibility in problem-solving. The framework has been evaluated across various benchmarks, showing significant performance improvements, such as a 7.8% increase for GPT-4.1 and a 31.1% boost for Claude-4.0-Sonnet. This advancement signifies a shift towards more agentic capabilities in visual reasoning, where models can not only utilize existing tools but also invent new ones.'}, 'zh': {'title': '动态工具，智能推理的新纪元', 'desc': 'PyVision是一个交互式框架，允许大型语言模型（LLMs）自主创建和改进基于Python的视觉推理工具。与以往的静态工具集不同，PyVision支持多轮交互，使模型能够根据具体任务灵活生成和执行工具。研究表明，PyVision在多个基准测试中显著提高了性能，例如GPT-4.1在V*上提升了7.8%。这些结果表明，动态工具的使用使模型不仅能够使用工具，还能发明新工具，推动视觉推理的进步。'}}}, {'id': 'https://huggingface.co/papers/2507.07982', 'title': 'Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling', 'url': 'https://huggingface.co/papers/2507.07982', 'abstract': "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.", 'score': 17, 'issue_id': 4762, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'fbe6e1954d8e9c30', 'authors': ['Haoyu Wu', 'Diankun Wu', 'Tianyu He', 'Junliang Guo', 'Yang Ye', 'Yueqi Duan', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07982.jpg', 'data': {'categories': ['#alignment', '#video', '#diffusion', '#3d'], 'emoji': '🧊', 'ru': {'title': 'Geometry Forcing: внедрение 3D-геометрии в видео-диффузионные модели', 'desc': 'Статья представляет метод Geometry Forcing для улучшения видео-диффузионных моделей. Авторы предлагают выравнивать промежуточные представления модели с признаками из предобученной геометрической модели-основы. Метод включает два вида выравнивания: угловое для согласованности направлений и масштабное для сохранения информации о масштабе. Эксперименты показывают, что Geometry Forcing значительно улучшает визуальное качество и 3D-согласованность генерируемых видео.'}, 'en': {'title': 'Bridging 2D Videos to 3D Understanding with Geometry Forcing', 'desc': 'This paper addresses the limitations of video diffusion models that do not effectively capture the 3D structure of the world from 2D video data. The authors introduce a technique called Geometry Forcing, which helps these models learn geometric representations by aligning their intermediate features with those from a pretrained geometric foundation model. They propose two alignment objectives: Angular Alignment, which ensures directional consistency, and Scale Alignment, which maintains scale information. The results show that Geometry Forcing significantly enhances the visual quality and 3D consistency of generated videos compared to existing methods.'}, 'zh': {'title': '提升视频模型的几何感知能力', 'desc': '本论文提出了一种名为几何强制（Geometry Forcing）的方法，旨在改善视频扩散模型在学习表示时对三维几何结构的捕捉能力。我们发现，仅使用原始视频数据训练的模型往往无法有效捕捉到有意义的几何信息。通过将模型的中间表示与预训练的几何基础模型的特征对齐，我们引入了两个互补的对齐目标：角度对齐和尺度对齐，以增强模型的几何感知能力。实验结果表明，几何强制方法在视频生成任务中显著提高了视觉质量和三维一致性。'}}}, {'id': 'https://huggingface.co/papers/2507.07136', 'title': 'LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS', 'url': 'https://huggingface.co/papers/2507.07136', 'abstract': 'LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 times speedup and a 47 times boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.', 'score': 16, 'issue_id': 4763, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'f35bfc7d12aabb90', 'authors': ['Wanhua Li', 'Yujie Zhao', 'Minghan Qin', 'Yang Liu', 'Yuanhao Cai', 'Chuang Gan', 'Hanspeter Pfister'], 'affiliations': ['Harvard University', 'Johns Hopkins University', 'MIT-IBM Watson AI Lab', 'Tsinghua University', 'UMass Amherst', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.07136.jpg', 'data': {'categories': ['#3d', '#optimization', '#inference', '#data'], 'emoji': '🚀', 'ru': {'title': 'LangSplatV2: Революция в скорости и точности 3D текстовых запросов', 'desc': 'LangSplatV2 - это усовершенствованная версия системы для трехмерных текстовых запросов. Она заменяет тяжеловесный декодер на разреженное поле коэффициентов и использует оптимизацию CUDA. Это позволяет достичь скорости 476.2 кадров в секунду для сплаттинга высокоразмерных признаков и 384.6 кадров в секунду для текстовых запросов в трехмерном пространстве. LangSplatV2 демонстрирует значительное ускорение и повышение точности по сравнению с предыдущей версией.'}, 'en': {'title': 'Speeding Up 3D Text Querying with LangSplatV2', 'desc': 'LangSplatV2 is a machine learning model that improves the speed and accuracy of 3D text querying by replacing the traditional heavyweight decoder with a more efficient sparse coefficient field. It achieves impressive performance metrics, processing high-dimensional features at 476.2 frames per second (FPS) and 3D text queries at 384.6 FPS, marking a significant speedup compared to its predecessor, LangSplat. The model utilizes Gaussian Splatting to effectively integrate 2D language features into a 3D context, enhancing the precision of language interactions in complex scenes. Despite these advancements, LangSplatV2 still struggles to reach real-time inference speeds, which limits its practical applications in dynamic environments.'}, 'zh': {'title': 'LangSplatV2：提升 3D 文本查询速度与准确性', 'desc': 'LangSplatV2 是一种新型的 3D 文本查询方法，通过用稀疏系数场替代传统的重型解码器，显著提高了查询速度和准确性。该方法在高分辨率图像上实现了每秒 476.2 帧的高维特征喷溅和每秒 384.6 帧的 3D 开放词汇文本查询，速度提升达 42 倍，准确性也有显著提高。LangSplatV2 利用高斯喷溅技术将 2D CLIP 语言特征嵌入 3D，学习精确的 3D 语言场，适用于复杂场景中的语言交互应用。尽管 LangSplatV2 显著提升了性能，但在实时推理方面仍有待改进。'}}}, {'id': 'https://huggingface.co/papers/2507.07202', 'title': 'A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality', 'url': 'https://huggingface.co/papers/2507.07202', 'abstract': 'Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled "long-form videos". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.', 'score': 14, 'issue_id': 4762, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'c7dc5888e8a06c13', 'authors': ['Mohamed Elmoghany', 'Ryan Rossi', 'Seunghyun Yoon', 'Subhojyoti Mukherjee', 'Eslam Bakr', 'Puneet Mathur', 'Gang Wu', 'Viet Dac Lai', 'Nedim Lipka', 'Ruiyi Zhang', 'Varun Manjunatha', 'Chien Nguyen', 'Daksh Dangi', 'Abel Salinas', 'Mohammad Taesiri', 'Hongjie Chen', 'Xiaolei Huang', 'Joe Barrow', 'Nesreen Ahmed', 'Hoda Eldardiry', 'Namyong Park', 'Yu Wang', 'Jaemin Cho', 'Anh Totti Nguyen', 'Zhengzhong Tu', 'Thien Nguyen', 'Dinesh Manocha', 'Mohamed Elhoseiny', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research', 'Auburn University', 'Cisco', 'Dolby Labs', 'Independent Researcher', 'KAUST', 'Meta AI', 'Pattern Data', 'Texas A&M University', 'UNC Chapel Hill', 'University of Maryland, College Park', 'University of Memphis', 'University of Oregon', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2507.07202.jpg', 'data': {'categories': ['#survey', '#architecture', '#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'Прорыв в генерации длинных видео: анализ ключевых компонентов и стратегий', 'desc': 'Эта статья посвящена анализу современных методов генерации видео с помощью машинного обучения. Авторы изучили 32 научные работы, чтобы выявить ключевые архитектурные компоненты и стратегии обучения, позволяющие создавать длительные видео с несколькими персонажами и связным сюжетом. В статье отмечается, что существующие модели генеративного ИИ способны создавать видео длительностью только 5-16 секунд, при этом более длинные видео страдают от проблем с согласованностью персонажей и сцен. Авторы представляют новую таксономию существующих методов и сравнительные таблицы, классифицирующие работы по их архитектурным особенностям и характеристикам производительности.'}, 'en': {'title': 'Unlocking the Future of Long-Form Video Generation', 'desc': 'This paper reviews the current state of video generative models, highlighting their limitations in producing long-form videos that exceed 16 seconds. It identifies issues such as character consistency and motion coherence, particularly in videos featuring multiple subjects. The authors analyze 32 existing studies to pinpoint effective architectural components and training strategies that enhance video quality. Additionally, they propose a new taxonomy to classify these methods based on their designs and performance metrics, aiming to guide future research in this area.'}, 'zh': {'title': '提升视频生成的连贯性与多样性', 'desc': '尽管视频生成模型取得了显著进展，但现有的最先进方法只能生成持续5到16秒的视频，通常被称为“长视频”。超过16秒的视频在角色外观和场景布局的一致性方面存在困难，尤其是多角色长视频在角色一致性和运动连贯性方面仍然存在问题。虽然一些方法可以生成长达150秒的视频，但它们往往面临帧冗余和时间多样性不足的问题。我们对32篇视频生成论文进行了全面研究，识别出关键的架构组件和训练策略，并构建了一个新的现有方法分类法。'}}}, {'id': 'https://huggingface.co/papers/2507.07996', 'title': 'Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs', 'url': 'https://huggingface.co/papers/2507.07996', 'abstract': 'A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For >60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation.', 'score': 12, 'issue_id': 4765, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'c861c1e4c9288d18', 'authors': ['Ziyue Li', 'Yang Li', 'Tianyi Zhou'], 'affiliations': ['Department of Computer Science, University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2507.07996.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Динамическая адаптация архитектуры нейросети для каждой задачи', 'desc': 'Метод цепочки слоев (CoLa), основанный на предобученной большой языковой модели, позволяет динамически адаптировать архитектуру, повышая эффективность и точность для различных задач. Это достигается путем выборочной манипуляции слоями и оптимизации с помощью поиска Монте-Карло по дереву. CoLa позволяет пропускать, повторять или переупорядочивать слои, создавая оптимальную архитектуру для каждого входного образца. Анализ показал, что для большинства образцов можно найти более короткие или более точные архитектуры по сравнению с исходной моделью.'}, 'en': {'title': 'Dynamic Layer Adaptation for Enhanced Model Efficiency', 'desc': 'This paper introduces a novel method called chain-of-layers (CoLa) that utilizes a pretrained large language model (LLM) to dynamically adapt its architecture for various tasks. By manipulating individual layers, the model can skip, repeat, or rearrange them, allowing for a more efficient and tailored approach to processing different inputs. The authors employ Monte Carlo Tree Search (MCTS) to optimize the selection of layers for each sample, enhancing both accuracy and efficiency. The findings demonstrate that CoLa can significantly improve performance and reduce inference time compared to traditional static models, highlighting the potential for adaptive architectures in machine learning.'}, 'zh': {'title': '动态架构适应，提升模型效率与准确性', 'desc': '本文提出了一种基于预训练大语言模型的层链（CoLa）方法，能够动态调整模型架构以提高效率和准确性。通过选择性地操作层和使用蒙特卡洛树搜索优化，CoLa可以为每个测试样本构建更优的模型。研究表明，预训练模型的层可以作为独立模块进行操作，从而实现更灵活的架构适应不同输入。我们的实验结果显示，CoLa在提高推理效率和性能方面具有显著优势，尤其是在处理不同样本时。'}}}, {'id': 'https://huggingface.co/papers/2507.06543', 'title': 'Token Bottleneck: One Token to Remember Dynamics', 'url': 'https://huggingface.co/papers/2507.06543', 'abstract': 'ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.', 'score': 11, 'issue_id': 4765, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '5dcb21845afc4bb6', 'authors': ['Taekyung Kim', 'Dongyoon Han', 'Byeongho Heo', 'Jeongeun Park', 'Sangdoo Yun'], 'affiliations': ['Korea University', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.06543.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#cv', '#robotics', '#video'], 'emoji': '🤖', 'ru': {'title': 'ToBo: Компактные временные представления для динамического зрения', 'desc': 'ToBo - это метод самоконтролируемого обучения для создания компактных визуальных представлений с учетом временных зависимостей. Он применяется для задач последовательного понимания сцен, таких как визуальное отслеживание и роботизированные манипуляции. ToBo сжимает сцену в компактный токен и предсказывает последующую сцену, используя минимальные фрагменты в качестве подсказок. Метод превосходит базовые подходы как в симулированных, так и в реальных средах.'}, 'en': {'title': 'Compact and Temporal: Revolutionizing Scene Understanding with ToBo', 'desc': 'ToBo is a self-supervised learning method that focuses on creating compact visual representations for understanding dynamic scenes. It works by encoding a scene into a small bottleneck token and then predicting the next scene using minimal visual hints. This approach helps the model learn temporal relationships between scenes, which is crucial for tasks like visual tracking and robotic manipulation. The effectiveness of ToBo is demonstrated through experiments in both simulated and real-world environments, showing its ability to outperform existing methods.'}, 'zh': {'title': 'ToBo：紧凑的时间感知视觉表示', 'desc': 'ToBo是一种自监督学习方法，旨在为顺序场景理解任务创建紧凑且具有时间感知的视觉表示。该方法通过将场景压缩为瓶颈标记，并利用最小的补丁作为提示来预测后续场景，从而提高了视觉跟踪和机器人操作等任务的性能。ToBo的设计鼓励模型捕捉时间动态，使其能够理解场景之间的动态过渡。大量实验表明，ToBo在模拟和真实环境中的表现优于基线方法，证明了其在实际应用中的有效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.07484', 'title': 'Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2507.07484', 'abstract': "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.  \t\t\t\t\tAI-generated summary \t\t\t\t Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.", 'score': 4, 'issue_id': 4763, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'c35c3791aec951d3', 'authors': ['Kaiqu Liang', 'Haimin Hu', 'Xuandong Zhao', 'Dawn Song', 'Thomas L. Griffiths', 'Jaime Fernández Fisac'], 'affiliations': ['Princeton University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.07484.jpg', 'data': {'categories': ['#alignment', '#ethics', '#benchmark', '#hallucinations', '#rlhf', '#training'], 'emoji': '🤖', 'ru': {'title': 'Проблема правдивости в LLM: анализ машинного буллшита', 'desc': 'В статье рассматривается концепция "машинного буллшита", когда LLMs генерируют утверждения без учета их истинности. Исследователи вводят новый индекс буллшита, чтобы количественно оценить безразличие LLMs к правде, и предлагают таксономию, анализирующую четыре формы буллшита: пустая риторика, увиливание, слова-лазейки и непроверенные утверждения. Эксперименты показывают, что обучение с подкреплением от обратной связи человека (RLHF) и использование цепочки рассуждений (CoT) усиливают некоторые формы буллшита. Результаты подчеркивают проблемы в согласовании AI и предлагают новые пути к более правдивому поведению LLM.'}, 'en': {'title': 'Quantifying Machine Bullshit: A New Framework for LLM Truthfulness', 'desc': "This paper introduces the concept of 'machine bullshit' to describe how large language models (LLMs) can generate statements without regard for their truthfulness. It presents a new framework that includes the Bullshit Index, a metric designed to quantify this indifference to truth. The authors analyze four types of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims, and evaluate these through various datasets. The findings indicate that techniques like reinforcement learning from human feedback (RLHF) and chain-of-thought prompting can worsen the generation of certain types of bullshit, particularly in political contexts."}, 'zh': {'title': '揭示机器胡说的真相', 'desc': '本文提出了一个新的框架来量化和分析大型语言模型（LLM）在生成内容时对真相的漠视，称之为“机器胡说”。我们引入了“胡说指数”，这是一个新的指标，用于量化LLM对真相的无动于衷，并分析了四种胡说的定性形式：空洞修辞、模棱两可、狡猾用词和未经验证的声明。研究表明，使用人类反馈的强化学习（RLHF）会显著加剧胡说现象，而推理时的思维链（CoT）提示则特别放大了空洞修辞和模棱两可的表现。我们的发现揭示了AI对齐中的系统性挑战，并为实现更真实的LLM行为提供了新的见解。'}}}, {'id': 'https://huggingface.co/papers/2507.07867', 'title': 'Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders', 'url': 'https://huggingface.co/papers/2507.07867', 'abstract': 'A Re-Bottleneck framework modifies pre-trained autoencoders to enforce specific latent structures, improving performance in diverse downstream applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a "Re-Bottleneck", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework\'s effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.', 'score': 2, 'issue_id': 4774, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '72ee0206287ff7a0', 'authors': ['Dimitrios Bralios', 'Jonah Casebeer', 'Paris Smaragdis'], 'affiliations': ['Adobe Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.07867.jpg', 'data': {'categories': ['#audio', '#training', '#optimization'], 'emoji': '🎛️', 'ru': {'title': 'Гибкая настройка латентного пространства аудиомоделей', 'desc': "Предложен новый подход 'Re-Bottleneck' для модификации предобученных автоэнкодеров с целью улучшения структуры скрытого пространства. Метод вводит внутреннее сжатие, обучаемое исключительно на основе потерь в латентном пространстве. Эксперименты показали эффективность подхода для упорядочивания латентных каналов, выравнивания с семантическими эмбеддингами и введения эквивариантности. Фреймворк позволяет гибко адаптировать представления нейронных аудиомоделей под различные задачи с минимальным дообучением."}, 'en': {'title': 'Enhancing Autoencoders with Re-Bottleneck for Tailored Latent Structures', 'desc': "This paper presents a new framework called 'Re-Bottleneck' that modifies pre-trained autoencoders to enhance their latent structures for better performance in various applications. Traditional autoencoders focus on reconstructing input data accurately but often overlook the specific latent structures needed for different tasks. The Re-Bottleneck approach introduces an additional bottleneck that is trained to optimize latent space representations based on user-defined criteria. Through experiments, the authors show that this method can enforce order in latent channels, align them with semantic meanings, and ensure that changes in input lead to predictable transformations in the latent space."}, 'zh': {'title': 'Re-Bottleneck框架：优化潜在结构以提升性能', 'desc': '本文提出了一种名为“Re-Bottleneck”的框架，旨在通过修改预训练自编码器的瓶颈部分，来增强潜在结构的特定性。这种方法通过在潜在空间损失上进行训练，来引入用户定义的结构，从而提高在不同下游应用中的性能。我们在三个实验中验证了该框架的有效性，包括对潜在通道的排序、与语义嵌入的对齐以及引入等变性。最终，Re-Bottleneck框架为神经音频模型的表示提供了一种灵活高效的定制方式，能够满足不同应用的多样化需求。'}}}, {'id': 'https://huggingface.co/papers/2507.07574', 'title': 'Beyond the Linear Separability Ceiling', 'url': 'https://huggingface.co/papers/2507.07574', 'abstract': 'The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this "linear reasoning bottleneck" by introducing the Linear Separability Ceiling (LSC), the performance of a simple linear classifier on a VLM\'s visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language model\'s reasoning pathways. We demonstrate this is a solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as a methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides a new lens for VLM analysis, showing that robust reasoning is a matter of targeted alignment, not simply improved representation learning.', 'score': 2, 'issue_id': 4767, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'facb529d60e86e84', 'authors': ['Enrico Vompa', 'Tanel Tammet', 'Mohit Vaishnav'], 'affiliations': ['Applied Artificial Intelligence Group, Tallinn University of Technology, Estonia'], 'pdf_title_img': 'assets/pdf/title_img/2507.07574.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#training', '#cv', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Преодоление линейного барьера в рассуждениях визуально-языковых моделей', 'desc': "Исследование выявляет линейное ограничение в рассуждениях визуально-языковых моделей (VLM) и предлагает метрику 'Потолок линейной разделимости' (LSC) для его оценки. Авторы обнаружили, что это ограничение широко распространено и связано не с плохим восприятием, а с недостатками в путях рассуждений языковой модели. Решение проблемы требует целенаправленной настройки, а не просто улучшения качества представлений. Исследование показывает, что VLM содержат мощные, но неактивные пути рассуждений, которые можно активировать для семантических концепций, в то время как сложные реляционные рассуждения требуют адаптации основных весов модели."}, 'en': {'title': 'Unlocking Reasoning in Visual-Language Models', 'desc': "This paper explores a limitation in Visual-Language Models (VLMs) known as the linear reasoning bottleneck, which affects their performance on abstract reasoning tasks. The authors introduce the Linear Separability Ceiling (LSC) as a metric to evaluate how well a simple linear classifier can perform on the visual embeddings produced by VLMs. They find that the bottleneck arises not from the models' perception abilities but from issues in the reasoning pathways of the language model. The study suggests that improving alignment, rather than just enhancing representation learning, is key to overcoming this bottleneck, with different strategies needed for various types of reasoning tasks."}, 'zh': {'title': '解决视觉语言模型的线性推理瓶颈', 'desc': '本研究发现视觉语言模型（VLMs）在抽象推理任务中存在线性推理瓶颈，并提出了线性可分性上限（Linear Separability Ceiling，LSC）作为评估该瓶颈的指标。研究表明，这一瓶颈普遍存在，主要源于语言模型推理路径的失败，而非感知能力的不足。解决这一问题需要针对性地进行对齐，而不是单纯提高表示学习的质量。对于语义概念，激活现有路径即可，而复杂的关系推理则需要调整核心模型权重。'}}}, {'id': 'https://huggingface.co/papers/2507.07129', 'title': 'Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate', 'url': 'https://huggingface.co/papers/2507.07129', 'abstract': 'A novel approach to scaling large language models through modular composition and layer-wise growth using fixed embeddings enhances performance and flexibility.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, a resource-intensive process that lacks flexibility. This paper explores an alternative, constructive approach to model development, built upon the foundation of non-trainable, deterministic input embeddings. In prior [1], we established that high-level semantic reasoning can emerge in Transformers using frozen embeddings derived from the visual structure of Unicode glyphs. Here, we demonstrate that this fixed representational substrate acts as a universal "docking port," enabling two powerful and efficient scaling paradigms: seamless modular composition and progressive layer-wise growth.   First, we show that specialist models trained on disparate datasets (e.g., Russian and Chinese text) can be merged into a single, more capable Mixture-of-Experts (MoE) model, post-training, with zero architectural modification. This is achieved by simply averaging their output logits. The resulting MoE model exhibits immediate performance improvements on reasoning benchmarks like MMLU, surpassing its constituent experts without catastrophic forgetting. Second, we introduce a layer-wise constructive training methodology, where a deep Transformer is "grown" by progressively stacking and training one layer at a time. This method demonstrates stable convergence and a clear correlation between model depth and the emergence of complex reasoning abilities, such as those required for SQuAD.   Our findings suggest a paradigm shift from monolithic optimization towards a more biological or constructive model of AI development, where complexity is built incrementally and modules can be composed freely. This opens new avenues for resource-efficient scaling, continual learning, and a more democratized ecosystem for building powerful AI systems. We release all code and models to facilitate further research.', 'score': 2, 'issue_id': 4773, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'a5414a289d5b5913', 'authors': ['A. Bochkov'], 'affiliations': ['Moscow Institute of Physics and Technology (MIPT), Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.07129.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#optimization', '#training', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Конструктор ИИ: сборка языковых моделей по кирпичикам', 'desc': "Статья предлагает новый подход к масштабированию больших языковых моделей (LLM) с использованием модульной композиции и послойного роста на основе фиксированных эмбеддингов. Авторы демонстрируют, что специализированные модели, обученные на разных наборах данных, могут быть объединены в единую модель Mixture-of-Experts без архитектурных изменений. Также представлена методология послойного конструктивного обучения, где глубокий трансформер 'выращивается' путем последовательного добавления и обучения слоев. Результаты указывают на возможность перехода от монолитной оптимизации к более биологичной или конструктивной модели развития ИИ."}, 'en': {'title': 'Modular Growth: A New Era for Language Models', 'desc': 'This paper presents a new method for scaling large language models (LLMs) that focuses on modular composition and layer-wise growth instead of traditional end-to-end training. By using fixed, non-trainable embeddings, the authors show that different specialized models can be combined into a more powerful Mixture-of-Experts (MoE) model without changing their architecture. Additionally, they introduce a technique for gradually adding layers to a Transformer model, which leads to better performance and stability during training. This approach promotes a more flexible and efficient way to develop AI systems, allowing for continual learning and easier integration of new capabilities.'}, 'zh': {'title': '模块化组合与逐层增长：大型语言模型的新方法', 'desc': '本文提出了一种新方法，通过模块化组合和逐层增长来扩展大型语言模型（LLMs），使用固定的嵌入增强了模型的性能和灵活性。与传统的整体训练方法不同，这种方法利用不可训练的确定性输入嵌入，允许在不修改架构的情况下将不同数据集训练的专家模型合并为一个更强大的混合专家模型。我们还介绍了一种逐层构建的训练方法，通过逐步堆叠和训练每一层，展示了模型深度与复杂推理能力之间的明确关联。我们的研究表明，AI开发可以从单一优化转向更具生物学特征的构建模型，促进资源高效扩展和持续学习。'}}}, {'id': 'https://huggingface.co/papers/2507.05241', 'title': "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?", 'url': 'https://huggingface.co/papers/2507.05241', 'abstract': "The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.", 'score': 1, 'issue_id': 4765, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '6f1b7fc6f47b4db5', 'authors': ['Jingyi Chai', 'Shuo Tang', 'Rui Ye', 'Yuwen Du', 'Xinyu Zhu', 'Mengcheng Zhou', 'Yanfeng Wang', 'Weinan E', 'Yuzhi Zhang', 'Linfeng Zhang', 'Siheng Chen'], 'affiliations': ['DP Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05241.jpg', 'data': {'categories': ['#agents', '#agi', '#training', '#science', '#reasoning', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'X-Master: ИИ-агент нового поколения для научных открытий', 'desc': "Статья представляет X-Master - агента искусственного интеллекта, способного эмулировать работу исследователей-людей. X-Master использует внешние инструменты и библиотеки Python для улучшения процесса рассуждений. Авторы также разработали X-Masters - систему, объединяющую несколько агентов для повышения широты и глубины анализа. X-Masters достиг рекордного результата в 32.1% в тесте Humanity's Last Exam, превзойдя предыдущие достижения OpenAI и Google."}, 'en': {'title': 'Empowering AI for Scientific Breakthroughs with X-Master', 'desc': "This paper discusses the development of a new AI agent called X-Master, which is designed to enhance scientific discovery by mimicking human researchers. X-Master utilizes external tools and Python libraries to improve its reasoning capabilities, allowing it to tackle complex tasks more effectively. The authors introduce a novel workflow called X-Masters, which enhances the agent's reasoning breadth and depth. Their approach has achieved a new record score of 32.1% on the Humanity's Last Exam, outperforming previous benchmarks set by other leading AI models."}, 'zh': {'title': '利用AI加速科学发现的新时代', 'desc': '本研究探讨了如何利用人工智能代理加速科学发现，提出了人类最后考试（HLE）作为评估科学AI代理的标准。我们构建了通用代理的基础架构，并通过X-Master工具增强推理能力，模拟人类研究者的灵活性。X-Master能够与外部工具互动，利用内置的Python库和定制工具来增强推理过程。我们的开源解决方案X-Masters在HLE上取得了32.1%的新纪录，超越了OpenAI和谷歌的深度研究，首次突破30%的门槛。'}}}, {'id': 'https://huggingface.co/papers/2507.04886', 'title': 'Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations', 'url': 'https://huggingface.co/papers/2507.04886', 'abstract': 'Transformer models equipped with fixed, visually derived embeddings outperform those with trainable embeddings on a reasoning benchmark, challenging the traditional role of embeddings in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer\'s compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.', 'score': 1, 'issue_id': 4773, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'fc80c81b93b3402d', 'authors': ['A. Bochkov'], 'affiliations': ['Moscow Institute of Physics and Technology (MIPT), Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.04886.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление роли эмбеддингов в языковых моделях', 'desc': 'Исследование показывает, что модели трансформеров с фиксированными визуальными эмбеддингами превосходят модели с обучаемыми эмбеддингами в задачах рассуждения. Авторы использовали замороженный слой эмбеддингов, основанный на визуальной структуре символов Unicode. Несмотря на отсутствие обучаемых семантических эмбеддингов, модели сходились и генерировали связный текст. Результаты указывают на то, что высокоуровневая семантика является emergent-свойством архитектуры трансформера, а не содержится в самих эмбеддингах.'}, 'en': {'title': 'Rethinking Embeddings: Structure Over Semantics in Transformers', 'desc': "This paper investigates the role of embeddings in large language models (LLMs) by using fixed, visually derived embeddings instead of trainable ones. The authors demonstrate that their Transformer models, which utilize precomputed visual embeddings from Unicode glyphs, outperform traditional models with trainable embeddings on reasoning tasks. They argue that the conventional view of embeddings as essential meaning vectors is flawed, as high-level semantics emerge from the Transformer's architecture and the scale of data rather than from the embeddings themselves. This research suggests a shift in understanding embeddings as structural components rather than semantic containers, paving the way for new approaches in model design."}, 'zh': {'title': '嵌入的角色重塑：从语义容器到结构原语', 'desc': '本论文探讨了在大型语言模型（LLMs）中，嵌入层的传统角色。研究表明，使用固定的视觉派生嵌入的Transformer模型在推理基准测试中表现优于使用可训练嵌入的模型。我们提出的模型在训练过程中嵌入层保持不变，使用来自Unicode字形的视觉结构向量，而非数据生成的向量。结果表明，高级语义并非嵌入的固有特性，而是Transformer的组合架构和数据规模的涌现属性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi (1)', '#alignment (3)', '#architecture (5)', '#audio (1)', '#benchmark (6)', '#cv (5)', '#data (1)', '#dataset (2)', '#diffusion (2)', '#ethics (1)', '#games', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (3)', '#open_source (5)', '#optimization (7)', '#plp', '#rag', '#reasoning (9)', '#rl (2)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (12)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-11 17:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-11 17:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-11 17:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    