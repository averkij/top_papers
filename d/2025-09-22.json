{
    "date": {
        "ru": "22 сентября",
        "en": "September 22",
        "zh": "9月22日"
    },
    "time_utc": "2025-09-22 02:26",
    "weekday": 0,
    "issue_id": 6006,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.16198",
            "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
            "url": "https://huggingface.co/papers/2509.16198",
            "abstract": "A graph-driven framework called ZeroRepo uses the Repository Planning Graph (RPG) to generate complete software repositories from scratch, significantly outperforming existing baselines in terms of code size, functional coverage, and test pass rate.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9times the strongest baseline (Claude Code) and about 64times other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.",
            "score": 2,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "e99fb2da472bbeba",
            "authors": [
                "Jane Luo",
                "Xin Zhang",
                "Steven Liu",
                "Jie Wu",
                "Yiming Huang",
                "Yangyu Huang",
                "Chengyu Yin",
                "Ying Xin",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Qi Chen",
                "Scarlett Li",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Tsinghua University",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16198.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#plp",
                    "#games",
                    "#benchmark",
                    "#agents",
                    "#graphs"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ZeroRepo: революция в автоматической генерации программных репозиториев",
                    "desc": "ZeroRepo - это фреймворк для генерации полноценных программных репозиториев с нуля, использующий граф планирования репозитория (RPG). RPG объединяет планирование на уровне предложений и реализации, кодируя возможности, структуры файлов, потоки данных и функции в одном графе. ZeroRepo значительно превосходит существующие базовые модели по размеру кода, функциональному охвату и проценту прохождения тестов. На бенчмарке RepoCraft ZeroRepo генерирует репозитории со средним объемом почти 36 тысяч строк кода, что в 3.9 раза больше, чем у ближайшего конкурента."
                },
                "en": {
                    "title": "Revolutionizing Software Generation with ZeroRepo and RPG",
                    "desc": "The paper introduces ZeroRepo, a novel framework that utilizes the Repository Planning Graph (RPG) to create complete software repositories from scratch. Unlike traditional methods that rely on ambiguous natural language, RPG provides a clear and structured representation of software components, enabling better planning and implementation. ZeroRepo significantly outperforms existing models in terms of code size, functional coverage, and test pass rates, demonstrating its effectiveness in generating complex software systems. The framework operates in three stages, ensuring coherent planning and validation, which leads to impressive results on the RepoCraft benchmark."
                },
                "zh": {
                    "title": "图驱动的仓库生成新纪元",
                    "desc": "ZeroRepo是一个基于图的框架，利用仓库规划图（RPG）从零开始生成完整的软件仓库。它在代码规模、功能覆盖率和测试通过率等方面显著超越了现有的基准。RPG通过将提案和实现层面的规划统一在一个图中，解决了自然语言在表示复杂软件结构时的模糊性问题。通过这种方式，ZeroRepo能够进行长远规划和可扩展的仓库生成，提升了大语言模型对仓库的理解能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15123",
            "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
            "url": "https://huggingface.co/papers/2509.15123",
            "abstract": "A novel method for camera parameter optimization in dynamic scenes using a single RGB video, incorporating patch-wise tracking filters, outlier-aware joint optimization, and a two-stage optimization strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.",
            "score": 2,
            "issue_id": 6006,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "0f9467c1183af701",
            "authors": [
                "Fang Li",
                "Hao Zhang",
                "Narendra Ahuja"
            ],
            "affiliations": [
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15123.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "📹",
                "ru": {
                    "title": "Эффективная оптимизация камеры для динамических сцен по одному видео",
                    "desc": "Предложен новый метод оптимизации параметров камеры в динамических сценах, использующий только одно RGB-видео. Метод включает в себя фильтры покадрового отслеживания, совместную оптимизацию с учетом выбросов и двухэтапную стратегию оптимизации. Подход позволяет более точно и эффективно оценивать параметры камеры без использования дополнительных данных, таких как маски движения или облака точек. Эксперименты на реальных и синтетических данных показали превосходство метода над существующими подходами."
                },
                "en": {
                    "title": "Optimizing Camera Parameters from Just One RGB Video!",
                    "desc": "This paper presents a new approach for optimizing camera parameters in dynamic scenes using only a single RGB video. It introduces three main components: Patch-wise Tracking Filters for establishing robust relationships in the video, Outlier-aware Joint Optimization to minimize the impact of moving outliers, and a Two-stage Optimization Strategy to improve stability and speed. Unlike traditional methods that require ground truth data, this method operates effectively without such supervision. The results show that this approach yields more accurate and efficient camera parameter estimates across various real-world and synthetic datasets."
                },
                "zh": {
                    "title": "动态场景中的相机参数优化新方法",
                    "desc": "本文提出了一种新方法，用于在动态场景中优化相机参数，仅依赖单个RGB视频。该方法包括三个关键组件：第一，使用补丁跟踪滤波器建立稳健的稀疏关系；第二，采用考虑异常值的联合优化，通过自适应降低移动异常值的权重来提高优化效率；第三，采用两阶段优化策略，通过在Softplus限制和损失的凸最小值之间进行权衡，增强稳定性和优化速度。实验结果表明，该方法在多个真实和合成数据集上，能够更高效和准确地估计相机参数。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16197",
            "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
            "url": "https://huggingface.co/papers/2509.16197",
            "abstract": "Manzano is a unified multimodal LLM framework that integrates image and text processing using a hybrid tokenizer and diffusion decoder, achieving state-of-the-art performance in both understanding and generating visual content.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "d5d4d53d4fa9323b",
            "authors": [
                "Yanghao Li",
                "Rui Qian",
                "Bowen Pan",
                "Haotian Zhang",
                "Haoshuo Huang",
                "Bowen Zhang",
                "Jialing Tong",
                "Haoxuan You",
                "Xianzhi Du",
                "Zhe Gan",
                "Hyunjik Kim",
                "Chao Jia",
                "Zhenbang Wang",
                "Yinfei Yang",
                "Mingfei Gao",
                "Zi-Yi Dou",
                "Wenze Hu",
                "Chang Gao",
                "Dongxu Li",
                "Philipp Dufter",
                "Zirui Wang",
                "Guoli Yin",
                "Zhengdong Zhang",
                "Chen Chen",
                "Yang Zhao",
                "Ruoming Pang",
                "Zhifeng Chen"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16197.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#games",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания и создания визуального контента",
                    "desc": "Manzano - это унифицированная мультимодальная архитектура большой языковой модели, объединяющая обработку изображений и текста с помощью гибридного токенизатора и декодера диффузии. Модель достигает передовых результатов как в понимании, так и в генерации визуального контента. Ключевой особенностью является использование общего энкодера изображений с двумя легковесными адаптерами для создания непрерывных эмбеддингов и дискретных токенов в едином семантическом пространстве. Архитектура и унифицированный рецепт обучения позволяют масштабируемо обучать модель обоим навыкам одновременно."
                },
                "en": {
                    "title": "Manzano: Bridging Text and Image with Unified Learning",
                    "desc": "Manzano is a unified multimodal large language model (LLM) that effectively processes both images and text. It uses a hybrid tokenizer and a diffusion decoder to enhance its ability to understand and generate visual content. By employing a single vision encoder with lightweight adapters, it creates embeddings for both image-to-text and text-to-image tasks within a shared semantic space. This architecture allows for scalable joint learning, leading to state-of-the-art performance in multimodal tasks while minimizing conflicts between different tasks."
                },
                "zh": {
                    "title": "统一多模态模型的创新之路",
                    "desc": "Manzano是一个统一的多模态大语言模型框架，能够同时处理图像和文本。它通过混合标记器和扩散解码器，实现了在理解和生成视觉内容方面的最先进性能。该框架使用共享的视觉编码器和轻量级适配器，能够在共同的语义空间中生成图像到文本的连续嵌入和文本到图像的离散标记。Manzano的设计使得理解和生成能力可以共同学习，且在文本丰富的评估中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15566",
            "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
            "url": "https://huggingface.co/papers/2509.15566",
            "abstract": "A brain-inspired framework, Blink-Think-Link, enhances human-GUI interaction by mimicking cognitive processes and introduces innovations in data generation and reinforcement learning rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "3324822214c20f79",
            "authors": [
                "Shaojie Zhang",
                "Ruoceng Zhang",
                "Pei Fu",
                "Shaokang Wang",
                "Jiahui Yang",
                "Xin Du",
                "Shiqi Cui",
                "Bin Qin",
                "Ying Huang",
                "Zhenbo Luo",
                "Jian Luan"
            ],
            "affiliations": [
                "Xiaomi Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15566.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Человекоподобное взаимодействие ИИ с интерфейсами",
                    "desc": "Статья представляет новый фреймворк для взаимодействия человека с графическим интерфейсом, названный 'Blink-Think-Link' (BTL). Этот подход имитирует когнитивные процессы человека, разделяя взаимодействие на три фазы: быстрое обнаружение важных областей экрана, высокоуровневое мышление и генерацию команд. В работе также предлагаются инновационные методы генерации данных для обучения и новый механизм вознаграждения для обучения с подкреплением. Разработанная на основе этого фреймворка модель BTL-UI демонстрирует передовые результаты в задачах понимания и взаимодействия с графическим интерфейсом."
                },
                "en": {
                    "title": "Mimicking Human Cognition for Smarter GUI Interaction",
                    "desc": "The paper presents a new framework called Blink-Think-Link (BTL) that improves how humans interact with graphical user interfaces (GUIs) by mimicking human cognitive processes. It breaks down interactions into three phases: 'Blink' for quick attention, 'Think' for reasoning, and 'Link' for executing commands, reflecting how humans naturally engage with screens. The framework also introduces innovative techniques like Blink Data Generation for automated annotation and a unique reward system for reinforcement learning. The BTL-UI model built on this framework shows superior performance in both understanding static GUIs and handling dynamic interactions, proving the framework's effectiveness in creating advanced GUI agents."
                },
                "zh": {
                    "title": "模仿人类认知的GUI交互框架",
                    "desc": "本文提出了一种名为“Blink-Think-Link”（BTL）的框架，旨在改善人机图形用户界面（GUI）交互。该框架模仿人类的认知过程，将交互分为三个阶段：Blink（快速检测）、Think（高层推理）和Link（生成可执行命令）。此外，BTL框架引入了两项技术创新：Blink数据生成和BTL奖励机制，以支持强化学习。通过开发BTL-UI模型，研究表明该框架在静态和动态GUI任务中均表现出色，验证了其在高级GUI代理开发中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15496",
            "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
            "url": "https://huggingface.co/papers/2509.15496",
            "abstract": "Lynx, a high-fidelity personalized video synthesis model, uses a Diffusion Transformer with ID-adapter and Ref-adapter to preserve identity and maintain video quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "bac6af5dc791293d",
            "authors": [
                "Shen Sang",
                "Tiancheng Zhi",
                "Tianpei Gu",
                "Jing Liu",
                "Linjie Luo"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15496.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Lynx: Персонализированное видео с сохранением идентичности",
                    "desc": "Lynx - это модель для персонализированного синтеза видео на основе одного входного изображения. Она использует Diffusion Transformer (DiT) с двумя адаптерами: ID-adapter для сохранения идентичности и Ref-adapter для поддержания качества видео. ID-adapter преобразует лицевые эмбеддинги в компактные токены идентичности, а Ref-adapter интегрирует детальные особенности через кросс-внимание. Оценка на 800 тестовых случаях показала превосходное сохранение сходства лиц и высокое качество видео."
                },
                "en": {
                    "title": "Lynx: Revolutionizing Personalized Video Synthesis with Identity Fidelity",
                    "desc": "Lynx is a cutting-edge model designed for creating personalized videos from just one input image. It utilizes a Diffusion Transformer architecture enhanced with two specialized adapters: the ID-adapter for maintaining identity and the Ref-adapter for adding detailed features. The ID-adapter transforms facial embeddings into identity tokens, while the Ref-adapter enriches the video with fine details using cross-attention mechanisms. Evaluations show that Lynx excels in preserving facial likeness, adhering to prompts, and producing high-quality videos, marking a significant advancement in personalized video synthesis."
                },
                "zh": {
                    "title": "Lynx：个性化视频合成的新突破",
                    "desc": "Lynx是一种高保真个性化视频合成模型，基于扩散变换器（Diffusion Transformer）构建。它引入了ID适配器和Ref适配器，以确保身份的保真度和视频质量。ID适配器使用Perceiver Resampler将ArcFace生成的面部嵌入转换为紧凑的身份标记，而Ref适配器则通过交叉注意力将冻结参考路径中的稠密VAE特征注入到所有变换器层中。通过在40个受试者和20个无偏提示的基准测试中评估，Lynx展示了卓越的面部相似性和强大的视频质量，推动了个性化视频生成的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10452",
            "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers",
            "url": "https://huggingface.co/papers/2509.10452",
            "abstract": "WhisTLE, a text-only adaptation method using a variational autoencoder, enhances pretrained ASR models with text-to-latent encoding and optional TTS adaptation, reducing word error rates across multiple datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "30cff8720dc8ac12",
            "authors": [
                "Akshat Pandey",
                "Karun Kumar",
                "Raphael Tang"
            ],
            "affiliations": [
                "Comcast Applied AI",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10452.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#transfer_learning",
                    "#inference"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Адаптация ASR моделей только по тексту: эффективно и без дополнительных затрат",
                    "desc": "WhisTLE - это метод адаптации предобученных моделей автоматического распознавания речи (ASR) без использования речевых данных. Он использует вариационный автоэнкодер для моделирования выходных данных энкодера из текста и дообучает декодер с помощью обученного текстово-латентного энкодера. WhisTLE может быть дополнительно комбинирован с адаптацией text-to-speech (TTS). Метод показывает значительное снижение частоты ошибок распознавания слов на различных наборах данных по сравнению с базовыми подходами."
                },
                "en": {
                    "title": "WhisTLE: Text-Only Adaptation for Enhanced ASR Performance",
                    "desc": "WhisTLE is a novel method that improves pretrained automatic speech recognition (ASR) models by using a variational autoencoder (VAE) for text-only adaptation. This approach allows the models to better understand and transcribe unseen vocabulary without needing additional speech data, which is often hard to collect. By training the VAE on text inputs, WhisTLE fine-tunes the ASR model's decoder, optionally incorporating text-to-speech (TTS) adaptation for further enhancement. The results show significant reductions in word error rates across various datasets, demonstrating WhisTLE's effectiveness in adapting ASR models to new domains."
                },
                "zh": {
                    "title": "WhisTLE：文本适应提升语音识别模型",
                    "desc": "WhisTLE是一种文本-only的适应方法，利用变分自编码器（VAE）来增强预训练的自动语音识别（ASR）模型。该方法通过文本到潜在编码的方式，减少了在多个数据集上的词错误率（WER）。在许多实际应用中，收集语音数据并不现实，因此WhisTLE提供了一种有效的文本-only适应方案。通过深度监督和可选的文本到语音（TTS）适应，WhisTLE在多个场景中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15937",
            "title": "A Vision-Language-Action-Critic Model for Robotic Real-World\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.15937",
            "abstract": "VLAC, a vision-language-action reward model, enhances real-world robotic reinforcement learning by providing dense rewards and enabling one-shot transfer, significantly improving success rates and sample efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
            "score": 0,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "d41842df1d51895c",
            "authors": [
                "Shaopeng Zhai",
                "Qi Zhang",
                "Tianyi Zhang",
                "Fuxian Huang",
                "Haoran Zhang",
                "Ming Zhou",
                "Shengzhe Zhang",
                "Litao Liu",
                "Sixu Lin",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15937.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#transfer_learning",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "VLAC: Революция в обучении роботов с помощью зрения и языка",
                    "desc": "VLAC - это модель вознаграждения для роботов, основанная на зрении, языке и действиях. Она улучшает обучение с подкреплением в реальном мире, предоставляя плотные вознаграждения и позволяя осуществлять трансфер обучения за один шаг. VLAC обучается на крупномасштабных гетерогенных наборах данных и может генерировать как вознаграждения, так и действия. Использование VLAC значительно повышает успешность выполнения задач и эффективность обучения роботов в реальном мире."
                },
                "en": {
                    "title": "VLAC: Revolutionizing Robotic Learning with Dense Rewards and One-Shot Transfer",
                    "desc": "The paper introduces VLAC, a vision-language-action reward model designed to improve robotic reinforcement learning in real-world scenarios. By providing dense rewards and enabling one-shot transfer, VLAC addresses the challenges of sparse rewards and inefficient exploration that typically hinder RL systems. It is trained on diverse datasets to enhance the robot's perception and reasoning, allowing it to generate rewards and actions effectively. The implementation of a human-in-the-loop protocol further boosts exploration and learning efficiency, resulting in significantly higher success rates in various manipulation tasks."
                },
                "zh": {
                    "title": "提升机器人学习效率的视觉-语言-行动模型",
                    "desc": "VLAC是一种视觉-语言-行动奖励模型，旨在提升现实世界中的机器人强化学习。它通过提供密集的奖励信号，消除了稀疏手工奖励的瓶颈，并支持一次性迁移到未见过的任务和环境。VLAC在大规模异构数据集上训练，增强了机器人的感知、对话和推理能力，同时通过构建大量负样本来提高模型的鲁棒性。通过人机协作的协议，VLAC在四个不同的现实世界操作任务中将成功率从约30%提升至约90%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15233",
            "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided\n  Role-playing Agents",
            "url": "https://huggingface.co/papers/2509.15233",
            "abstract": "A framework incorporating dynamic role profiles with video modality enhances role-playing agents by combining adaptive temporal sampling and static role profile representations, improving response generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs.",
            "score": 0,
            "issue_id": 6006,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "06896ac318611f21",
            "authors": [
                "Xueqiao Zhang",
                "Chao Zhang",
                "Jingtao Xu",
                "Yifan Zhu",
                "Xin Shi",
                "Yi Yang",
                "Yawei Luo"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15233.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#agents",
                    "#story_generation"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Динамические ролевые профили с видео улучшают агентов ролевой игры",
                    "desc": "Предложена новая концепция динамических ролевых профилей для агентов ролевой игры, включающая видеомодальность. Создан крупномасштабный датасет Role-playing-Video60k из 60 тысяч видео и 700 тысяч соответствующих диалогов. Разработан комплексный фреймворк, сочетающий адаптивную временную выборку с динамическими и статическими представлениями ролевых профилей. Экспериментальные результаты демонстрируют эффективность предложенного подхода для улучшения генерации ответов агентами."
                },
                "en": {
                    "title": "Dynamic Role Profiles: Enhancing Role-Playing Agents with Video Modality",
                    "desc": "This paper presents a new framework for role-playing agents (RPAs) that enhances their ability to generate responses by using dynamic role profiles and video data. Traditional RPAs rely on static role profiles, which do not capture the fluid and adaptive nature of human interactions. By introducing dynamic role profiles that utilize video modality, the framework allows for adaptive temporal sampling of video frames, improving the contextual understanding of characters. The authors also introduce a large dataset, Role-playing-Video60k, to support this approach and demonstrate its effectiveness through comprehensive evaluations across multiple metrics."
                },
                "zh": {
                    "title": "动态角色档案提升角色扮演代理的响应能力",
                    "desc": "本文提出了一种结合动态角色档案和视频模态的框架，以增强角色扮演代理（RPA）的能力。通过引入视频模态，研究者们能够更好地模拟人类的动态感知能力，而不仅仅依赖静态角色档案。为此，构建了一个名为Role-playing-Video60k的大规模高质量数据集，包含60,000个视频和700,000个对话。实验结果表明，动态角色档案的整合显著提高了RPA的响应生成能力。"
                }
            }
        }
    ],
    "link_prev": "2025-09-19.html",
    "link_next": "2025-09-23.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9月19日"
    },
    "short_date_next": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9月23日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}