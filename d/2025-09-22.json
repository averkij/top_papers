{
    "date": {
        "ru": "22 сентября",
        "en": "September 22",
        "zh": "9月22日"
    },
    "time_utc": "2025-09-22 15:12",
    "weekday": 0,
    "issue_id": 6019,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.16198",
            "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
            "url": "https://huggingface.co/papers/2509.16198",
            "abstract": "A graph-driven framework called ZeroRepo uses the Repository Planning Graph (RPG) to generate complete software repositories from scratch, significantly outperforming existing baselines in terms of code size, functional coverage, and test pass rate.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9times the strongest baseline (Claude Code) and about 64times other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.",
            "score": 83,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "e99fb2da472bbeba",
            "authors": [
                "Jane Luo",
                "Xin Zhang",
                "Steven Liu",
                "Jie Wu",
                "Yiming Huang",
                "Yangyu Huang",
                "Chengyu Yin",
                "Ying Xin",
                "Jianfeng Liu",
                "Yuefeng Zhan",
                "Hao Sun",
                "Qi Chen",
                "Scarlett Li",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft",
                "Tsinghua University",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16198.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#plp",
                    "#games",
                    "#benchmark",
                    "#agents",
                    "#graphs"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ZeroRepo: революция в автоматической генерации программных репозиториев",
                    "desc": "ZeroRepo - это фреймворк для генерации полноценных программных репозиториев с нуля, использующий граф планирования репозитория (RPG). RPG объединяет планирование на уровне предложений и реализации, кодируя возможности, структуры файлов, потоки данных и функции в одном графе. ZeroRepo значительно превосходит существующие базовые модели по размеру кода, функциональному охвату и проценту прохождения тестов. На бенчмарке RepoCraft ZeroRepo генерирует репозитории со средним объемом почти 36 тысяч строк кода, что в 3.9 раза больше, чем у ближайшего конкурента."
                },
                "en": {
                    "title": "Revolutionizing Software Generation with ZeroRepo and RPG",
                    "desc": "The paper introduces ZeroRepo, a novel framework that utilizes the Repository Planning Graph (RPG) to create complete software repositories from scratch. Unlike traditional methods that rely on ambiguous natural language, RPG provides a clear and structured representation of software components, enabling better planning and implementation. ZeroRepo significantly outperforms existing models in terms of code size, functional coverage, and test pass rates, demonstrating its effectiveness in generating complex software systems. The framework operates in three stages, ensuring coherent planning and validation, which leads to impressive results on the RepoCraft benchmark."
                },
                "zh": {
                    "title": "图驱动的仓库生成新纪元",
                    "desc": "ZeroRepo是一个基于图的框架，利用仓库规划图（RPG）从零开始生成完整的软件仓库。它在代码规模、功能覆盖率和测试通过率等方面显著超越了现有的基准。RPG通过将提案和实现层面的规划统一在一个图中，解决了自然语言在表示复杂软件结构时的模糊性问题。通过这种方式，ZeroRepo能够进行长远规划和可扩展的仓库生成，提升了大语言模型对仓库的理解能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16197",
            "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
            "url": "https://huggingface.co/papers/2509.16197",
            "abstract": "Manzano is a unified multimodal LLM framework that integrates image and text processing using a hybrid tokenizer and diffusion decoder, achieving state-of-the-art performance in both understanding and generating visual content.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.",
            "score": 31,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "d5d4d53d4fa9323b",
            "authors": [
                "Yanghao Li",
                "Rui Qian",
                "Bowen Pan",
                "Haotian Zhang",
                "Haoshuo Huang",
                "Bowen Zhang",
                "Jialing Tong",
                "Haoxuan You",
                "Xianzhi Du",
                "Zhe Gan",
                "Hyunjik Kim",
                "Chao Jia",
                "Zhenbang Wang",
                "Yinfei Yang",
                "Mingfei Gao",
                "Zi-Yi Dou",
                "Wenze Hu",
                "Chang Gao",
                "Dongxu Li",
                "Philipp Dufter",
                "Zirui Wang",
                "Guoli Yin",
                "Zhengdong Zhang",
                "Chen Chen",
                "Yang Zhao",
                "Ruoming Pang",
                "Zhifeng Chen"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16197.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#games",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единая модель для понимания и создания визуального контента",
                    "desc": "Manzano - это унифицированная мультимодальная архитектура большой языковой модели, объединяющая обработку изображений и текста с помощью гибридного токенизатора и декодера диффузии. Модель достигает передовых результатов как в понимании, так и в генерации визуального контента. Ключевой особенностью является использование общего энкодера изображений с двумя легковесными адаптерами для создания непрерывных эмбеддингов и дискретных токенов в едином семантическом пространстве. Архитектура и унифицированный рецепт обучения позволяют масштабируемо обучать модель обоим навыкам одновременно."
                },
                "en": {
                    "title": "Manzano: Bridging Text and Image with Unified Learning",
                    "desc": "Manzano is a unified multimodal large language model (LLM) that effectively processes both images and text. It uses a hybrid tokenizer and a diffusion decoder to enhance its ability to understand and generate visual content. By employing a single vision encoder with lightweight adapters, it creates embeddings for both image-to-text and text-to-image tasks within a shared semantic space. This architecture allows for scalable joint learning, leading to state-of-the-art performance in multimodal tasks while minimizing conflicts between different tasks."
                },
                "zh": {
                    "title": "统一多模态模型的创新之路",
                    "desc": "Manzano是一个统一的多模态大语言模型框架，能够同时处理图像和文本。它通过混合标记器和扩散解码器，实现了在理解和生成视觉内容方面的最先进性能。该框架使用共享的视觉编码器和轻量级适配器，能够在共同的语义空间中生成图像到文本的连续嵌入和文本到图像的离散标记。Manzano的设计使得理解和生成能力可以共同学习，且在文本丰富的评估中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15591",
            "title": "Latent Zoning Network: A Unified Principle for Generative Modeling,\n  Representation Learning, and Classification",
            "url": "https://huggingface.co/papers/2509.15591",
            "abstract": "Latent Zoning Network (LZN) unifies generative modeling, representation learning, and classification by creating a shared latent space for diverse data types.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.",
            "score": 22,
            "issue_id": 6009,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "fc6406f54d74c989",
            "authors": [
                "Zinan Lin",
                "Enshu Liu",
                "Xuefei Ning",
                "Junyi Zhu",
                "Wenyu Wang",
                "Sergey Yekhanin"
            ],
            "affiliations": [
                "Microsoft Research Redmond, WA, USA",
                "Redmond, WA, USA",
                "Samsung R&D Institute UK London, UK",
                "Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15591.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#synthetic",
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Единое латентное пространство для разных задач машинного обучения",
                    "desc": "Latent Zoning Network (LZN) - это новый подход в машинном обучении, объединяющий генеративное моделирование, обучение представлениям и классификацию в едином латентном пространстве. LZN использует энкодеры и декодеры для различных типов данных, позволяя решать разнообразные задачи через их композицию. Метод показал улучшение результатов в генерации изображений, обучении без учителя и совместном решении задач генерации и классификации. LZN демонстрирует потенциал для упрощения и повышения эффективности пайплайнов машинного обучения."
                },
                "en": {
                    "title": "Unifying Generative Modeling, Representation Learning, and Classification with LZN",
                    "desc": "The Latent Zoning Network (LZN) introduces a unified approach to generative modeling, representation learning, and classification by establishing a shared latent space for various data types. This framework allows different tasks to be represented as combinations of encoders and decoders, facilitating seamless transitions between tasks like image generation and classification. LZN demonstrates its effectiveness by enhancing existing models, achieving superior performance in unsupervised representation learning, and successfully executing multiple tasks simultaneously. Overall, LZN aims to simplify machine learning workflows and improve synergy across diverse applications."
                },
                "zh": {
                    "title": "统一生成建模、表示学习与分类的潜在区域网络",
                    "desc": "潜在区域网络（LZN）通过创建一个共享的潜在空间，将生成建模、表示学习和分类统一起来。该方法为不同类型的数据（如图像、文本和标签）提供了编码器和解码器，使得各种机器学习任务可以通过组合这些组件来实现。LZN在多个复杂场景中表现出色，包括提升现有模型的性能、独立解决表示学习任务以及同时进行生成和分类任务。通过这种统一的方法，LZN简化了机器学习流程，并促进了任务之间的协同作用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16127",
            "title": "BaseReward: A Strong Baseline for Multimodal Reward Model",
            "url": "https://huggingface.co/papers/2509.16127",
            "abstract": "The paper provides a comprehensive guide and introduces BaseReward, a state-of-the-art multimodal reward model, which outperforms existing models across various benchmarks and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including reward modeling paradigms (e.g., Naive-RM, Critic-based RM, and Generative RM), reward head architecture, training strategies, data curation (covering over ten multimodal and text-only preference datasets), backbone model and model scale, and ensemble methods.   Based on these experimental insights, we introduce BaseReward, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.",
            "score": 16,
            "issue_id": 6007,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "d131848ac9ef1c25",
            "authors": [
                "Yi-Fan Zhang",
                "Haihua Yang",
                "Huanyu Zhang",
                "Yang Shi",
                "Zezhou Chen",
                "Haochen Tian",
                "Chaoyou Fu",
                "Haotian Wang",
                "Kai Wu",
                "Bo Cui",
                "Xu Wang",
                "Jianfei Pan",
                "Haotian Wang",
                "Zhang Zhang",
                "Liang Wang"
            ],
            "affiliations": [
                "ByteDance",
                "CASIA",
                "NJU",
                "PKU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16127.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#alignment",
                    "#rlhf",
                    "#multimodal",
                    "#rag",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "BaseReward: новый стандарт в мультимодальном моделировании вознаграждений для ИИ",
                    "desc": "Статья представляет собой подробное руководство по созданию мультимодальных моделей вознаграждения (MRM) для больших языковых моделей. Авторы провели систематический анализ всех ключевых компонентов в процессе разработки MRM, включая парадигмы моделирования вознаграждения, архитектуру головы вознаграждения и стратегии обучения. На основе этого анализа они представили BaseReward - передовую мультимодальную модель вознаграждения, превосходящую существующие модели по различным бенчмаркам. BaseReward использует простую, но эффективную архитектуру на основе Qwen2.5-VL и обучается на тщательно отобранных мультимодальных и текстовых данных о предпочтениях."
                },
                "en": {
                    "title": "BaseReward: The Future of Multimodal Reward Modeling",
                    "desc": "This paper introduces BaseReward, a cutting-edge multimodal reward model designed to align Multimodal Large Language Models (MLLMs) with human preferences. It provides a systematic guide for constructing high-performance Multimodal Reward Models (MRMs), detailing essential components such as reward modeling paradigms, architecture, training strategies, and data curation. Through extensive experiments, BaseReward demonstrates superior performance on key benchmarks, establishing a new state-of-the-art in the field. Additionally, it showcases practical applications by enhancing MLLM capabilities in real-world tasks like perception and reasoning."
                },
                "zh": {
                    "title": "构建高性能多模态奖励模型的指南",
                    "desc": "本文介绍了一种先进的多模态奖励模型BaseReward，旨在解决多模态大型语言模型与人类偏好对齐的挑战。通过系统的实验分析，作者提供了构建高性能多模态奖励模型的详细指南，涵盖了奖励建模范式、奖励头架构、训练策略等关键组件。BaseReward在多个基准测试中表现优异，超越了现有模型，并成功应用于实际的强化学习管道中，提升了多模态大型语言模型的性能。该研究不仅提供了顶尖的多模态奖励模型，还为社区开发下一代奖励模型提供了实证支持的清晰指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14981",
            "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
            "url": "https://huggingface.co/papers/2509.14981",
            "abstract": "SpatialGen, a multi-view multi-modal diffusion model, generates realistic and semantically consistent 3D indoor scenes using a large synthetic dataset, outperforming previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.",
            "score": 13,
            "issue_id": 6007,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "5c98a31485e969a0",
            "authors": [
                "Chuan Fang",
                "Heng Li",
                "Yixun Liang",
                "Jia Zheng",
                "Yongsen Mao",
                "Yuan Liu",
                "Rui Tang",
                "Zihan Zhou",
                "Ping Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Manycore Tech Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14981.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#diffusion",
                    "#multimodal",
                    "#3d",
                    "#open_source"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "Реалистичная генерация 3D интерьеров с помощью искусственного интеллекта",
                    "desc": "SpatialGen - это новая мультимодальная диффузионная модель для генерации реалистичных 3D сцен интерьеров. Она использует крупный синтетический датасет из 12,328 аннотированных сцен и 4,7 миллиона фотореалистичных 2D рендеров. Модель генерирует изображение, карту глубины и семантическую сегментацию с произвольных ракурсов на основе 3D планировки и референсного изображения. SpatialGen превосходит предыдущие методы в экспериментах по качеству и согласованности генерируемых сцен."
                },
                "en": {
                    "title": "Revolutionizing 3D Indoor Scene Generation with SpatialGen",
                    "desc": "SpatialGen is a cutting-edge multi-view multi-modal diffusion model designed to create realistic 3D indoor scenes. It utilizes a large synthetic dataset containing over 12,000 annotated scenes and millions of photorealistic images to enhance the quality of generated outputs. The model effectively synthesizes various aspects of a scene, including appearance, geometry, and semantic information, while maintaining spatial consistency across different views. By outperforming existing methods, SpatialGen aims to facilitate advancements in applications like design, virtual reality, and robotics."
                },
                "zh": {
                    "title": "SpatialGen：生成逼真3D室内场景的新方法",
                    "desc": "SpatialGen是一种多视角多模态扩散模型，能够生成逼真且语义一致的3D室内场景。该模型利用一个包含12,328个结构化标注场景的大型合成数据集，克服了现有方法在视觉质量和语义一致性方面的挑战。通过输入3D布局和参考图像，SpatialGen可以从任意视角合成外观、几何和语义信息，同时保持空间一致性。实验结果表明，SpatialGen在生成效果上优于之前的方法，推动了室内场景理解和生成的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15937",
            "title": "A Vision-Language-Action-Critic Model for Robotic Real-World\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.15937",
            "abstract": "VLAC, a vision-language-action reward model, enhances real-world robotic reinforcement learning by providing dense rewards and enabling one-shot transfer, significantly improving success rates and sample efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
            "score": 6,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "d41842df1d51895c",
            "authors": [
                "Shaopeng Zhai",
                "Qi Zhang",
                "Tianyi Zhang",
                "Fuxian Huang",
                "Haoran Zhang",
                "Ming Zhou",
                "Shengzhe Zhang",
                "Litao Liu",
                "Sixu Lin",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15937.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#transfer_learning",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "VLAC: Революция в обучении роботов с помощью зрения и языка",
                    "desc": "VLAC - это модель вознаграждения для роботов, основанная на зрении, языке и действиях. Она улучшает обучение с подкреплением в реальном мире, предоставляя плотные вознаграждения и позволяя осуществлять трансфер обучения за один шаг. VLAC обучается на крупномасштабных гетерогенных наборах данных и может генерировать как вознаграждения, так и действия. Использование VLAC значительно повышает успешность выполнения задач и эффективность обучения роботов в реальном мире."
                },
                "en": {
                    "title": "VLAC: Revolutionizing Robotic Learning with Dense Rewards and One-Shot Transfer",
                    "desc": "The paper introduces VLAC, a vision-language-action reward model designed to improve robotic reinforcement learning in real-world scenarios. By providing dense rewards and enabling one-shot transfer, VLAC addresses the challenges of sparse rewards and inefficient exploration that typically hinder RL systems. It is trained on diverse datasets to enhance the robot's perception and reasoning, allowing it to generate rewards and actions effectively. The implementation of a human-in-the-loop protocol further boosts exploration and learning efficiency, resulting in significantly higher success rates in various manipulation tasks."
                },
                "zh": {
                    "title": "提升机器人学习效率的视觉-语言-行动模型",
                    "desc": "VLAC是一种视觉-语言-行动奖励模型，旨在提升现实世界中的机器人强化学习。它通过提供密集的奖励信号，消除了稀疏手工奖励的瓶颈，并支持一次性迁移到未见过的任务和环境。VLAC在大规模异构数据集上训练，增强了机器人的感知、对话和推理能力，同时通过构建大量负样本来提高模型的鲁棒性。通过人机协作的协议，VLAC在四个不同的现实世界操作任务中将成功率从约30%提升至约90%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15496",
            "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
            "url": "https://huggingface.co/papers/2509.15496",
            "abstract": "Lynx, a high-fidelity personalized video synthesis model, uses a Diffusion Transformer with ID-adapter and Ref-adapter to preserve identity and maintain video quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.",
            "score": 6,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "bac6af5dc791293d",
            "authors": [
                "Shen Sang",
                "Tiancheng Zhi",
                "Tianpei Gu",
                "Jing Liu",
                "Linjie Luo"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15496.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Lynx: Персонализированное видео с сохранением идентичности",
                    "desc": "Lynx - это модель для персонализированного синтеза видео на основе одного входного изображения. Она использует Diffusion Transformer (DiT) с двумя адаптерами: ID-adapter для сохранения идентичности и Ref-adapter для поддержания качества видео. ID-adapter преобразует лицевые эмбеддинги в компактные токены идентичности, а Ref-adapter интегрирует детальные особенности через кросс-внимание. Оценка на 800 тестовых случаях показала превосходное сохранение сходства лиц и высокое качество видео."
                },
                "en": {
                    "title": "Lynx: Revolutionizing Personalized Video Synthesis with Identity Fidelity",
                    "desc": "Lynx is a cutting-edge model designed for creating personalized videos from just one input image. It utilizes a Diffusion Transformer architecture enhanced with two specialized adapters: the ID-adapter for maintaining identity and the Ref-adapter for adding detailed features. The ID-adapter transforms facial embeddings into identity tokens, while the Ref-adapter enriches the video with fine details using cross-attention mechanisms. Evaluations show that Lynx excels in preserving facial likeness, adhering to prompts, and producing high-quality videos, marking a significant advancement in personalized video synthesis."
                },
                "zh": {
                    "title": "Lynx：个性化视频合成的新突破",
                    "desc": "Lynx是一种高保真个性化视频合成模型，基于扩散变换器（Diffusion Transformer）构建。它引入了ID适配器和Ref适配器，以确保身份的保真度和视频质量。ID适配器使用Perceiver Resampler将ArcFace生成的面部嵌入转换为紧凑的身份标记，而Ref适配器则通过交叉注意力将冻结参考路径中的稠密VAE特征注入到所有变换器层中。通过在40个受试者和20个无偏提示的基准测试中评估，Lynx展示了卓越的面部相似性和强大的视频质量，推动了个性化视频生成的进步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15566",
            "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
            "url": "https://huggingface.co/papers/2509.15566",
            "abstract": "A brain-inspired framework, Blink-Think-Link, enhances human-GUI interaction by mimicking cognitive processes and introduces innovations in data generation and reinforcement learning rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
            "score": 5,
            "issue_id": 6006,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "3324822214c20f79",
            "authors": [
                "Shaojie Zhang",
                "Ruoceng Zhang",
                "Pei Fu",
                "Shaokang Wang",
                "Jiahui Yang",
                "Xin Du",
                "Shiqi Cui",
                "Bin Qin",
                "Ying Huang",
                "Zhenbo Luo",
                "Jian Luan"
            ],
            "affiliations": [
                "Xiaomi Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15566.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Человекоподобное взаимодействие ИИ с интерфейсами",
                    "desc": "Статья представляет новый фреймворк для взаимодействия человека с графическим интерфейсом, названный 'Blink-Think-Link' (BTL). Этот подход имитирует когнитивные процессы человека, разделяя взаимодействие на три фазы: быстрое обнаружение важных областей экрана, высокоуровневое мышление и генерацию команд. В работе также предлагаются инновационные методы генерации данных для обучения и новый механизм вознаграждения для обучения с подкреплением. Разработанная на основе этого фреймворка модель BTL-UI демонстрирует передовые результаты в задачах понимания и взаимодействия с графическим интерфейсом."
                },
                "en": {
                    "title": "Mimicking Human Cognition for Smarter GUI Interaction",
                    "desc": "The paper presents a new framework called Blink-Think-Link (BTL) that improves how humans interact with graphical user interfaces (GUIs) by mimicking human cognitive processes. It breaks down interactions into three phases: 'Blink' for quick attention, 'Think' for reasoning, and 'Link' for executing commands, reflecting how humans naturally engage with screens. The framework also introduces innovative techniques like Blink Data Generation for automated annotation and a unique reward system for reinforcement learning. The BTL-UI model built on this framework shows superior performance in both understanding static GUIs and handling dynamic interactions, proving the framework's effectiveness in creating advanced GUI agents."
                },
                "zh": {
                    "title": "模仿人类认知的GUI交互框架",
                    "desc": "本文提出了一种名为“Blink-Think-Link”（BTL）的框架，旨在改善人机图形用户界面（GUI）交互。该框架模仿人类的认知过程，将交互分为三个阶段：Blink（快速检测）、Think（高层推理）和Link（生成可执行命令）。此外，BTL框架引入了两项技术创新：Blink数据生成和BTL奖励机制，以支持强化学习。通过开发BTL-UI模型，研究表明该框架在静态和动态GUI任务中均表现出色，验证了其在高级GUI代理开发中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15123",
            "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
            "url": "https://huggingface.co/papers/2509.15123",
            "abstract": "A novel method for camera parameter optimization in dynamic scenes using a single RGB video, incorporating patch-wise tracking filters, outlier-aware joint optimization, and a two-stage optimization strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.",
            "score": 2,
            "issue_id": 6006,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "0f9467c1183af701",
            "authors": [
                "Fang Li",
                "Hao Zhang",
                "Narendra Ahuja"
            ],
            "affiliations": [
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15123.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "📹",
                "ru": {
                    "title": "Эффективная оптимизация камеры для динамических сцен по одному видео",
                    "desc": "Предложен новый метод оптимизации параметров камеры в динамических сценах, использующий только одно RGB-видео. Метод включает в себя фильтры покадрового отслеживания, совместную оптимизацию с учетом выбросов и двухэтапную стратегию оптимизации. Подход позволяет более точно и эффективно оценивать параметры камеры без использования дополнительных данных, таких как маски движения или облака точек. Эксперименты на реальных и синтетических данных показали превосходство метода над существующими подходами."
                },
                "en": {
                    "title": "Optimizing Camera Parameters from Just One RGB Video!",
                    "desc": "This paper presents a new approach for optimizing camera parameters in dynamic scenes using only a single RGB video. It introduces three main components: Patch-wise Tracking Filters for establishing robust relationships in the video, Outlier-aware Joint Optimization to minimize the impact of moving outliers, and a Two-stage Optimization Strategy to improve stability and speed. Unlike traditional methods that require ground truth data, this method operates effectively without such supervision. The results show that this approach yields more accurate and efficient camera parameter estimates across various real-world and synthetic datasets."
                },
                "zh": {
                    "title": "动态场景中的相机参数优化新方法",
                    "desc": "本文提出了一种新方法，用于在动态场景中优化相机参数，仅依赖单个RGB视频。该方法包括三个关键组件：第一，使用补丁跟踪滤波器建立稳健的稀疏关系；第二，采用考虑异常值的联合优化，通过自适应降低移动异常值的权重来提高优化效率；第三，采用两阶段优化策略，通过在Softplus限制和损失的凸最小值之间进行权衡，增强稳定性和优化速度。实验结果表明，该方法在多个真实和合成数据集上，能够更高效和准确地估计相机参数。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13989",
            "title": "Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in\n  Instruction-Guided Expressive Text-To-Speech Systems",
            "url": "https://huggingface.co/papers/2509.13989",
            "abstract": "Research on ITTS reveals gaps between user instructions and listener perception, highlighting challenges in fine-grained control and voice attribute interpretation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided text-to-speech (ITTS) enables users to control speech generation through natural language prompts, offering a more intuitive interface than traditional TTS. However, the alignment between user style instructions and listener perception remains largely unexplored. This work first presents a perceptual analysis of ITTS controllability across two expressive dimensions (adverbs of degree and graded emotion intensity) and collects human ratings on speaker age and word-level emphasis attributes. To comprehensively reveal the instruction-perception gap, we provide a data collection with large-scale human evaluations, named Expressive VOice Control (E-VOC) corpus. Furthermore, we reveal that (1) gpt-4o-mini-tts is the most reliable ITTS model with great alignment between instruction and generated utterances across acoustic dimensions. (2) The 5 analyzed ITTS systems tend to generate Adult voices even when the instructions ask to use child or Elderly voices. (3) Fine-grained control remains a major challenge, indicating that most ITTS systems have substantial room for improvement in interpreting slightly different attribute instructions.",
            "score": 2,
            "issue_id": 6010,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "08133cbacc29a4b3",
            "authors": [
                "Yi-Cheng Lin",
                "Huang-Cheng Chou",
                "Tzu-Chieh Wei",
                "Kuan-Yu Chen",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University",
                "University of Michigan",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13989.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Разрыв между инструкциями и восприятием в системах ITTS: вызовы точного контроля",
                    "desc": "Исследование инструктивного синтеза речи (ITTS) выявило несоответствия между инструкциями пользователей и восприятием слушателей. Анализ проводился по двум экспрессивным измерениям и включал оценку возраста говорящего и акцентирования слов. Создан корпус E-VOC с масштабными человеческими оценками для выявления разрыва между инструкциями и восприятием. Результаты показывают, что gpt-4o-mini-tts наиболее надежна, но точный контроль и интерпретация атрибутов голоса остаются проблемой для большинства систем ITTS."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing ITTS Control and Perception",
                    "desc": "This paper investigates the effectiveness of instruction-guided text-to-speech (ITTS) systems in aligning user commands with listener perceptions. It highlights the challenges in achieving fine-grained control over voice attributes, such as age and emotional intensity, which are crucial for generating expressive speech. The authors introduce the Expressive VOice Control (E-VOC) corpus, a large-scale dataset for evaluating ITTS systems based on human feedback. The findings reveal that while some models perform well, there is a significant gap in accurately interpreting nuanced user instructions, particularly regarding voice characteristics."
                },
                "zh": {
                    "title": "指令与感知的桥梁：提升ITTS系统的控制能力",
                    "desc": "本研究探讨了指令引导的文本到语音（ITTS）系统中用户指令与听众感知之间的差距，揭示了在细粒度控制和声音属性解释方面的挑战。研究首先对ITTS的可控性进行了感知分析，涉及程度副词和情感强度两个表现维度，并收集了关于说话者年龄和单词强调属性的人类评分。为了全面揭示指令与感知之间的差距，我们提供了一个名为E-VOC的大规模人类评估数据集。此外，研究发现现有ITTS系统在生成声音时，往往偏向成人声音，即使用户指令要求使用儿童或老年声音，细粒度控制仍然是一个主要挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15233",
            "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided\n  Role-playing Agents",
            "url": "https://huggingface.co/papers/2509.15233",
            "abstract": "A framework incorporating dynamic role profiles with video modality enhances role-playing agents by combining adaptive temporal sampling and static role profile representations, improving response generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "06896ac318611f21",
            "authors": [
                "Xueqiao Zhang",
                "Chao Zhang",
                "Jingtao Xu",
                "Yifan Zhu",
                "Xin Shi",
                "Yi Yang",
                "Yawei Luo"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15233.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#multimodal",
                    "#benchmark",
                    "#agents",
                    "#story_generation"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Динамические ролевые профили с видео улучшают агентов ролевой игры",
                    "desc": "Предложена новая концепция динамических ролевых профилей для агентов ролевой игры, включающая видеомодальность. Создан крупномасштабный датасет Role-playing-Video60k из 60 тысяч видео и 700 тысяч соответствующих диалогов. Разработан комплексный фреймворк, сочетающий адаптивную временную выборку с динамическими и статическими представлениями ролевых профилей. Экспериментальные результаты демонстрируют эффективность предложенного подхода для улучшения генерации ответов агентами."
                },
                "en": {
                    "title": "Dynamic Role Profiles: Enhancing Role-Playing Agents with Video Modality",
                    "desc": "This paper presents a new framework for role-playing agents (RPAs) that enhances their ability to generate responses by using dynamic role profiles and video data. Traditional RPAs rely on static role profiles, which do not capture the fluid and adaptive nature of human interactions. By introducing dynamic role profiles that utilize video modality, the framework allows for adaptive temporal sampling of video frames, improving the contextual understanding of characters. The authors also introduce a large dataset, Role-playing-Video60k, to support this approach and demonstrate its effectiveness through comprehensive evaluations across multiple metrics."
                },
                "zh": {
                    "title": "动态角色档案提升角色扮演代理的响应能力",
                    "desc": "本文提出了一种结合动态角色档案和视频模态的框架，以增强角色扮演代理（RPA）的能力。通过引入视频模态，研究者们能够更好地模拟人类的动态感知能力，而不仅仅依赖静态角色档案。为此，构建了一个名为Role-playing-Video60k的大规模高质量数据集，包含60,000个视频和700,000个对话。实验结果表明，动态角色档案的整合显著提高了RPA的响应生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10452",
            "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers",
            "url": "https://huggingface.co/papers/2509.10452",
            "abstract": "WhisTLE, a text-only adaptation method using a variational autoencoder, enhances pretrained ASR models with text-to-latent encoding and optional TTS adaptation, reducing word error rates across multiple datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.",
            "score": 1,
            "issue_id": 6006,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "30cff8720dc8ac12",
            "authors": [
                "Akshat Pandey",
                "Karun Kumar",
                "Raphael Tang"
            ],
            "affiliations": [
                "Comcast Applied AI",
                "University College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10452.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#audio",
                    "#transfer_learning",
                    "#inference"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Адаптация ASR моделей только по тексту: эффективно и без дополнительных затрат",
                    "desc": "WhisTLE - это метод адаптации предобученных моделей автоматического распознавания речи (ASR) без использования речевых данных. Он использует вариационный автоэнкодер для моделирования выходных данных энкодера из текста и дообучает декодер с помощью обученного текстово-латентного энкодера. WhisTLE может быть дополнительно комбинирован с адаптацией text-to-speech (TTS). Метод показывает значительное снижение частоты ошибок распознавания слов на различных наборах данных по сравнению с базовыми подходами."
                },
                "en": {
                    "title": "WhisTLE: Text-Only Adaptation for Enhanced ASR Performance",
                    "desc": "WhisTLE is a novel method that improves pretrained automatic speech recognition (ASR) models by using a variational autoencoder (VAE) for text-only adaptation. This approach allows the models to better understand and transcribe unseen vocabulary without needing additional speech data, which is often hard to collect. By training the VAE on text inputs, WhisTLE fine-tunes the ASR model's decoder, optionally incorporating text-to-speech (TTS) adaptation for further enhancement. The results show significant reductions in word error rates across various datasets, demonstrating WhisTLE's effectiveness in adapting ASR models to new domains."
                },
                "zh": {
                    "title": "WhisTLE：文本适应提升语音识别模型",
                    "desc": "WhisTLE是一种文本-only的适应方法，利用变分自编码器（VAE）来增强预训练的自动语音识别（ASR）模型。该方法通过文本到潜在编码的方式，减少了在多个数据集上的词错误率（WER）。在许多实际应用中，收集语音数据并不现实，因此WhisTLE提供了一种有效的文本-only适应方案。通过深度监督和可选的文本到语音（TTS）适应，WhisTLE在多个场景中表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15061",
            "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn\n  Dialogue",
            "url": "https://huggingface.co/papers/2509.15061",
            "abstract": "The Ask-to-Clarify framework uses a VLM for collaboration and a diffusion model for action generation, enabling embodied agents to handle ambiguous instructions through multi-turn dialogue and outperform existing VLAs in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents.",
            "score": 0,
            "issue_id": 6007,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "6f2bc6f2c7d24315",
            "authors": [
                "Xingyao Lin",
                "Xinghao Zhu",
                "Tianyi Lu",
                "Sicheng Xie",
                "Hui Zhang",
                "Xipeng Qiu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "College of Computer Science and Artificial Intelligence, Fudan University, Shanghai, China",
                "Mechanical Systems Control Lab, UC Berkeley, California, USA",
                "Shanghai Innovation Institute, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15061.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#agents",
                    "#training",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Ask-to-Clarify: путь к интеллектуальным воплощенным агентам через диалог",
                    "desc": "Статья представляет фреймворк Ask-to-Clarify для воплощенных агентов, использующий визуально-языковую модель (VLM) для взаимодействия и диффузионную модель для генерации действий. Фреймворк позволяет агентам обрабатывать неоднозначные инструкции через многоэтапный диалог, превосходя существующие визуально-языковые агенты (VLA) в реальных задачах. Ask-to-Clarify обучается по двухэтапной стратегии: сначала настраивается компонент взаимодействия на данных диалогов по разрешению неоднозначностей, затем интегрируется компонент действий при замороженном компоненте взаимодействия. Результаты оценки на 8 реальных задачах показывают, что предложенный подход открывает путь к созданию совместных воплощенных агентов."
                },
                "en": {
                    "title": "Empowering Agents Through Clarification and Collaboration",
                    "desc": "The Ask-to-Clarify framework enhances embodied agents by enabling them to engage in multi-turn dialogues to clarify ambiguous instructions before executing actions. It utilizes a Vision-Language Model (VLM) for effective collaboration and a diffusion model for generating precise actions. This two-component system allows agents to adapt their responses based on human feedback, moving beyond traditional one-way instruction execution. The framework has been tested in real-world tasks, demonstrating superior performance compared to existing Vision-Language Agents (VLAs)."
                },
                "zh": {
                    "title": "协作型具身代理的新路径",
                    "desc": "本文提出了一种名为Ask-to-Clarify的框架，旨在解决模糊指令的问题。该框架结合了视觉语言模型（VLM）和扩散模型，通过多轮对话来澄清指令，并生成低级动作。与现有的视觉语言代理（VLA）相比，该框架能够在真实世界任务中表现更好，体现了协作型具身代理的潜力。通过两阶段的知识隔离策略进行训练，确保代理能够在询问问题后再执行动作。"
                }
            }
        }
    ],
    "link_prev": "2025-09-19.html",
    "link_next": "2025-09-23.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9月19日"
    },
    "short_date_next": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9月23日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}