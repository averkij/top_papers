{
    "date": {
        "ru": "13 июня",
        "en": "June 13",
        "zh": "6月13日"
    },
    "time_utc": "2025-06-13 02:43",
    "weekday": 4,
    "issue_id": 4272,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.09993",
            "title": "Text-Aware Image Restoration with Diffusion Models",
            "url": "https://huggingface.co/papers/2506.09993",
            "abstract": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
            "score": 11,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "91f1bb97ef062632",
            "authors": [
                "Jaewon Min",
                "Jin Hyeon Kim",
                "Paul Hyunbin Cho",
                "Jaeeun Lee",
                "Jihye Park",
                "Minkyu Park",
                "Sangpil Kim",
                "Hyunhee Park",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "Samsung Electronics",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09993.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#hallucinations"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "Восстановление изображений с сохранением текстовой информации",
                    "desc": "Предложенная система восстановления изображений с учетом текста (TAIR) объединяет многозадачную диффузионную модель с модулем распознавания текста для улучшения как восстановления изображения, так и точности текста. Система превосходит существующие методы на основе диффузии, которые часто создают правдоподобные, но неверные текстоподобные паттерны. Авторы представили SA-Text - крупномасштабный набор данных из 100 тысяч высококачественных изображений с разнообразными текстовыми аннотациями. Предложенная модель TeReDiff интегрирует внутренние признаки диффузионных моделей в модуль распознавания текста, что позволяет извлекать богатые текстовые представления для последующих шагов шумоподавления."
                },
                "en": {
                    "title": "Restoring Images with Textual Precision",
                    "desc": "The Text-Aware Image Restoration (TAIR) system addresses the challenge of restoring images while maintaining the accuracy of textual information. Traditional diffusion-based methods often produce incorrect text patterns, leading to what is known as text-image hallucination. TAIR introduces a multi-task diffusion framework, TeReDiff, which combines image restoration with a text-spotting module to improve both visual and textual fidelity. By leveraging a large-scale dataset of annotated images, TAIR significantly enhances text recognition accuracy compared to existing methods."
                },
                "zh": {
                    "title": "文本感知图像修复：提升图像与文本的双重恢复",
                    "desc": "本文提出了一种名为文本感知图像修复（TAIR）的系统，旨在同时恢复图像内容和文本的准确性。现有的扩散基础修复方法在自然图像修复方面表现良好，但在处理图像中的文本区域时常常出现错误的文本模式。TAIR系统结合了多任务扩散框架和文本检测模块，通过联合训练提高了文本识别的准确性。实验结果表明，TAIR在图像修复和文本保真度方面均优于现有的修复方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10857",
            "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
            "url": "https://huggingface.co/papers/2506.10857",
            "abstract": "VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.",
            "score": 9,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "7eabd83f69c00df7",
            "authors": [
                "Jiashuo Yu",
                "Yue Wu",
                "Meng Chu",
                "Zhifei Ren",
                "Zizheng Huang",
                "Pei Chu",
                "Ruijie Zhang",
                "Yinan He",
                "Qirui Li",
                "Songze Li",
                "Zhenxiang Li",
                "Zhongying Tu",
                "Conghui He",
                "Yu Qiao",
                "Yali Wang",
                "Yi Wang",
                "Limin Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10857.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#reasoning",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VRBench: Оценка глубокого понимания видео через многоступенчатые рассуждения",
                    "desc": "VRBench - это новый бенчмарк для оценки способностей моделей машинного обучения к многоступенчатым рассуждениям на основе длинных видео. Он включает в себя 1010 длинных видео со средней продолжительностью 1,6 часа, а также 9468 пар вопросов-ответов и 30292 шага рассуждений с временными метками, размеченных людьми. VRBench оценивает модели как на уровне конечных результатов, так и на уровне процесса рассуждений, используя многофазный конвейер оценки. Бенчмарк был протестирован на 12 языковых моделях (LLM) и 16 визуально-языковых моделях (VLM), предоставив ценные выводы для развития области многоступенчатых рассуждений."
                },
                "en": {
                    "title": "VRBench: Advancing Multi-Step Reasoning in Long Video Understanding",
                    "desc": "VRBench is a new benchmark designed to evaluate how well large models understand long videos through multi-step reasoning. It includes 1,010 long videos and thousands of human-labeled question-answering pairs, focusing on both temporal reasoning and procedural validity. The framework allows for the generation of coherent reasoning chains that require multiple steps, assessing models not just on final answers but also on the reasoning process. By testing various large language models (LLMs) and vision-language models (VLMs), VRBench aims to provide insights that enhance the understanding of multi-step reasoning in video comprehension."
                },
                "zh": {
                    "title": "VRBench：长视频理解的新基准",
                    "desc": "VRBench是一个用于评估长视频理解的基准，专注于多步骤推理能力，特别是时间推理和程序有效性。该基准包含1010个长视频，平均时长为1.6小时，以及9468个人工标注的多步骤问答对和30292个带时间戳的推理步骤。通过多阶段筛选过程，确保视频情节连贯性，并开发了一个人机协作框架，生成需要多个时间基础步骤的连贯推理链。VRBench设计了一个多阶段评估流程，综合评估模型的结果和过程，推动了多步骤推理领域的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08060",
            "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
            "url": "https://huggingface.co/papers/2506.08060",
            "abstract": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.",
            "score": 3,
            "issue_id": 4272,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "5c9cde8c4bcbdc6e",
            "authors": [
                "Asankhaya Sharma"
            ],
            "affiliations": [
                "Patched Codes, Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08060.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#rag",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Трансформеры: обучение в контексте как альтернатива тонкой настройке",
                    "desc": "Статья исследует способность трансформеров аппроксимировать возможности обучения с учителем без изменения параметров модели, используя обучение в контексте. Авторы предоставляют теоретические границы и практические методы для этого подхода. Исследование охватывает сценарии с ограниченной длиной контекста и частичным доступом к набору данных. Результаты обосновывают эффективное развертывание больших языковых моделей и связывают теорию с практическими приложениями."
                },
                "en": {
                    "title": "Transformers: Fine-Tuning Efficiency through In-Context Learning",
                    "desc": "This paper explores how transformers can mimic the performance of supervised fine-tuning (SFT) through a method called in-context learning (ICL) without changing the model's parameters. It provides theoretical proofs that under certain ideal conditions, a base transformer can achieve results similar to those obtained through SFT. The authors extend their findings to practical situations, showing that smaller datasets can still approximate fine-tuned behavior effectively. This research highlights the potential for more efficient use of large language models in real-world applications by leveraging retrieval-augmented generation techniques."
                },
                "zh": {
                    "title": "变换器模型：高效近似监督微调的未来",
                    "desc": "本论文探讨了变换器模型如何通过上下文学习（ICL）在不改变模型参数的情况下，近似监督微调（SFT）的能力。研究表明，在理想条件下，变换器模型可以利用推理时的技术来模拟SFT的效果。我们还扩展了这些结果到实际场景，考虑有限的上下文长度和部分数据集访问。通过理论证明，这为大语言模型的资源高效部署提供了基础，结合检索增强生成等实用技术，将理论与实际应用相结合。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10890",
            "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
            "url": "https://huggingface.co/papers/2506.10890",
            "abstract": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter",
            "score": 2,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "22fffd0088d280e0",
            "authors": [
                "Zhao Zhang",
                "Yutao Cheng",
                "Dexiang Hong",
                "Maoke Yang",
                "Gonglei Shi",
                "Lei Ma",
                "Hui Zhang",
                "Jie Shao",
                "Xinglong Wu"
            ],
            "affiliations": [
                "ByteDance, Fudan University",
                "ByteDance, Intelligent Creation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10890.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "CreatiPoster: ИИ-революция в графическом дизайне",
                    "desc": "CreatiPoster - это новая система искусственного интеллекта для генерации высококачественных графических композиций на основе текста или готовых ресурсов. Она использует модель протокола для создания JSON-спецификации каждого слоя и условную модель фона для синтеза согласованного фона. CreatiPoster превосходит существующие инструменты и шаблоны, обеспечивая редактируемость и профессиональный визуальный appeal. Система поддерживает различные приложения, включая редактирование холста, наложение текста и адаптивное изменение размера."
                },
                "en": {
                    "title": "Revolutionizing Graphic Design with AI-Generated Custom Compositions",
                    "desc": "CreatiPoster is a novel framework that generates high-quality, editable graphic designs from user inputs like text or images. It utilizes a protocol model to create a detailed JSON specification for each design layer, ensuring precise layout and style. A conditional background model then generates a cohesive background that complements the foreground elements. This approach not only enhances the editability and visual appeal of designs but also outperforms existing tools and templates in the market."
                },
                "zh": {
                    "title": "CreatiPoster：让图形设计更简单",
                    "desc": "CreatiPoster 是一个生成高质量、可编辑和可定制图形作品的框架，能够从文本或资产中创建多层次的图形设计。与现有工具相比，它在用户提供的资产整合、可编辑性和视觉吸引力方面表现更佳。该框架使用协议模型生成详细的 JSON 规范，描述每一层的布局、层次、内容和风格。通过提供一个无版权的 100,000 个多层设计的语料库，CreatiPoster 促进了 AI 辅助图形设计的进一步研究和应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09513",
            "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.09513",
            "abstract": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
            "score": 2,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "7c6aa342a51b1d59",
            "authors": [
                "Yu Sun",
                "Xingyu Qian",
                "Weiwen Xu",
                "Hao Zhang",
                "Chenghao Xiao",
                "Long Li",
                "Yu Rong",
                "Wenbing Huang",
                "Qifeng Bai",
                "Tingyang Xu"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "Beĳing Key Laboratory of Research on Large Models",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation",
                "Gaoling School of",
                "Hupan Lab",
                "Renmin University of China",
                "School of Basic Medical Sciences, Lanzhou University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09513.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#benchmark",
                    "#healthcare",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "ReasonMed: Прорыв в медицинских вопросно-ответных системах на основе ИИ",
                    "desc": "ReasonMed - это крупнейший набор данных для медицинских рассуждений, состоящий из 370 тысяч высококачественных примеров. Он был создан с помощью многоагентного процесса верификации и уточнения, включающего специальный Error Refiner для улучшения цепочек рассуждений. Исследование показало, что комбинация подробных рассуждений по методу Chain-of-Thought с краткими сводками ответов является наиболее эффективной стратегией для обучения моделей медицинских рассуждений. На основе этой стратегии была обучена модель ReasonMed-7B, которая превзошла предыдущие модели на 4.17% и даже превзошла LLaMA3.1-70B на 4.60% в задаче PubMedQA."
                },
                "en": {
                    "title": "Enhancing Medical AI with ReasonMed: A New Benchmark in Reasoning",
                    "desc": "ReasonMed is a comprehensive medical reasoning dataset designed to improve the performance of medical question answering models. It consists of 370,000 high-quality examples derived from 1.7 million initial reasoning paths created by various large language models (LLMs). The dataset is refined through a multi-agent process that includes an Error Refiner to correct mistakes in reasoning paths. By combining detailed Chain-of-Thought reasoning with concise summaries, ReasonMed-7B achieves superior results, surpassing previous benchmarks for smaller models and even outperforming larger models on specific tasks."
                },
                "zh": {
                    "title": "ReasonMed：提升医学问答模型的新基准",
                    "desc": "ReasonMed是一个大型医学推理数据集，旨在提高医学问答模型的准确性。它结合了详细的推理路径和简洁的总结，创造了新的模型性能基准。该数据集包含370,000个高质量示例，经过多代理验证和精炼过程构建而成。通过结合详细的思维链推理和简洁的答案总结，ReasonMed-7B模型在医学问答任务中表现优异，超越了之前的最佳模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10954",
            "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
            "url": "https://huggingface.co/papers/2506.10954",
            "abstract": "An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "dfb4cf3e253468bd",
            "authors": [
                "Lianghong Guo",
                "Yanlin Wang",
                "Caihua Li",
                "Pengyu Yang",
                "Jiachi Chen",
                "Wei Tao",
                "Yingtian Zou",
                "Duyu Tang",
                "Zibin Zheng"
            ],
            "affiliations": [
                "Huawei",
                "Independent Researcher",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10954.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#dataset",
                    "#agents",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🏭",
                "ru": {
                    "title": "SWE-Factory: автоматизация создания датасетов для LLM в разработке ПО",
                    "desc": "SWE-Factory - это автоматизированный конвейер для создания масштабных датасетов для оценки и обучения больших языковых моделей (LLM) в задачах разрешения проблем на GitHub. Он включает в себя SWE-Builder - мультиагентную систему для автоматизированного построения сред оценки, стандартизированный метод оценки на основе кодов выхода и автоматизированную валидацию fail2pass. Эксперименты показали эффективность конвейера в создании валидных экземпляров задач, точность оценки и высокую производительность валидации. Этот подход призван ускорить сбор масштабных, качественных датасетов для обучения и оценки LLM в задачах разработки программного обеспечения."
                },
                "en": {
                    "title": "Automating Dataset Creation for LLMs in GitHub Issue Resolution",
                    "desc": "The paper presents SWE-Factory, an automated pipeline designed to streamline the creation of large-scale datasets for training and evaluating Large Language Models (LLMs) in GitHub issue resolution tasks. It addresses the challenges of environment setup, grading, and validation by integrating three automated components: SWE-Builder for environment construction, a standardized exit-code-based grading system, and an automated fail2pass validation process. Experiments demonstrate that SWE-Factory can efficiently generate valid task instances at a low cost while achieving high accuracy in grading and validation. This innovation aims to enhance the quality and speed of dataset collection for LLM training and evaluation."
                },
                "zh": {
                    "title": "自动化管道加速GitHub问题解决数据集构建",
                    "desc": "本文介绍了一种名为SWE-Factory的自动化管道，旨在简化大规模数据集的创建，以评估和训练大型语言模型在GitHub问题解决任务中的表现。传统的数据集构建过程繁琐且耗时，尤其是在环境搭建、结果评分和任务验证阶段。SWE-Factory通过集成三个核心自动化组件来解决这些问题，包括自动化环境构建的多代理系统SWE-Builder、基于退出代码的标准化评分方法，以及自动化的fail2pass验证过程。实验结果表明，该管道能够有效构建有效的任务实例，并在评分和验证方面表现出高准确率和高精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10357",
            "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
            "url": "https://huggingface.co/papers/2506.10357",
            "abstract": "Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "145045d8e634c76b",
            "authors": [
                "Zaijing Li",
                "Yuquan Xie",
                "Rui Shao",
                "Gongwei Chen",
                "Weili Guan",
                "Dongmei Jiang",
                "Liqiang Nie"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Peng Cheng Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10357.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#agents",
                    "#rag",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Optimus-3: Универсальный ИИ-агент покоряет Minecraft",
                    "desc": "Статья представляет Optimus-3 - интеллектуального агента для игры Minecraft, использующего усовершенствованные методы машинного обучения. Агент применяет генерацию данных с использованием знаний, маршрутизацию на основе смеси экспертов и обучение с подкреплением, дополненное мультимодальными рассуждениями. Optimus-3 демонстрирует превосходную производительность в различных задачах в открытом мире Minecraft. Это достижение преодолевает ключевые проблемы, такие как недостаток специфических данных, интерференция между разнородными задачами и визуальное разнообразие в открытых средах."
                },
                "en": {
                    "title": "Optimus-3: Mastering Minecraft with Advanced AI Techniques",
                    "desc": "Optimus-3 is a general-purpose agent designed for the open-world environment of Minecraft, utilizing advanced techniques in machine learning. It incorporates a knowledge-enhanced data generation pipeline to create high-quality training data, addressing the challenge of insufficient domain-specific data. The agent employs a Mixture-of-Experts (MoE) architecture to effectively manage interference among diverse tasks, allowing for better performance. Additionally, it uses Multimodal Reasoning-Augmented Reinforcement Learning to improve its reasoning capabilities, enabling it to handle the visual diversity present in Minecraft."
                },
                "zh": {
                    "title": "Optimus-3：在Minecraft中超越极限的智能体",
                    "desc": "本文介绍了Optimus-3，一个利用知识增强数据生成、专家混合路由和多模态推理增强强化学习的智能体。该智能体在Minecraft等开放世界环境中表现出色，解决了领域特定数据不足、异构任务干扰和视觉多样性等挑战。我们提出了一种知识增强的数据生成管道，以提供可扩展的高质量训练数据，并引入了任务级路由的专家混合架构来减轻任务间的干扰。此外，我们开发了多模态推理增强的强化学习方法，以提升智能体在视觉多样性方面的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09942",
            "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
            "url": "https://huggingface.co/papers/2506.09942",
            "abstract": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "5430c32ec46dccaa",
            "authors": [
                "Hao Peng",
                "Yunjia Qi",
                "Xiaozhi Wang",
                "Bin Xu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09942.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "VerIF: Гибридная верификация для улучшения RL в следовании инструкциям",
                    "desc": "Статья представляет VerIF - гибридный метод верификации, сочетающий подходы на основе правил и больших языковых моделей (LLM) для улучшения обучения с подкреплением (RL) в задаче следования инструкциям. Авторы создали набор данных VerInstruct с около 22 000 примеров и сигналами верификации. Применение RL с VerIF к двум моделям показало значительное улучшение производительности на нескольких эталонных тестах и хорошую обобщаемость. Метод может быть интегрирован в существующие рецепты RL для повышения общей эффективности моделей."
                },
                "en": {
                    "title": "VerIF: Boosting Instruction-Following RL with Hybrid Verification",
                    "desc": "This paper introduces VerIF, a novel hybrid verification method that merges rule-based and large language model (LLM) approaches to improve reinforcement learning (RL) in instruction-following tasks. The authors highlight the importance of verification engineering in enhancing LLMs through reinforcement learning with verifiable rewards (RLVR). They present a new dataset, VerInstruct, which contains around 22,000 instruction-following instances with verification signals to support their method. The results show that models trained with VerIF achieve state-of-the-art performance and maintain strong generalization capabilities, indicating that this approach can effectively enhance existing RL frameworks."
                },
                "zh": {
                    "title": "VerIF：提升指令跟随的强化学习新方法",
                    "desc": "本文提出了一种名为VerIF的混合验证方法，结合了基于规则的验证和基于大型语言模型（LLM）的验证，显著提升了指令跟随的强化学习（RL）性能和泛化能力。我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其验证信号，以支持这一方法。通过使用VerIF进行RL训练，我们在多个代表性的指令跟随基准上取得了显著的性能提升，训练后的模型在同类模型中达到了最先进的表现，并且对未见约束具有良好的泛化能力。我们的研究表明，VerIF可以与现有的RL方法结合，进一步增强模型的整体性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06561",
            "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
            "url": "https://huggingface.co/papers/2506.06561",
            "abstract": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "12942392f309be51",
            "authors": [
                "Ho Yin 'Sam' Ng",
                "Ting-Yao Hsu",
                "Aashish Anantha Ramakrishnan",
                "Branislav Kveton",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Dongwon Lee",
                "Tong Yu",
                "Sungchul Kim",
                "Ryan A. Rossi",
                "Ting-Hao 'Kenneth' Huang"
            ],
            "affiliations": [
                "Adobe Research",
                "Pennsylvania State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06561.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#multimodal",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Персонализированные подписи к изображениям: мультимодальный подход",
                    "desc": "LaMP-Cap представляет датасет для персонализированной генерации подписей к изображениям с использованием мультимодальных профилей. Эксперименты показали, что использование профильной информации помогает генерировать подписи, более близкие к оригинальным авторским. Исследование выявило, что изображения в профиле более полезны, чем текстовые параграфы, упоминающие рисунки. Это подчеркивает преимущество использования мультимодальных профилей по сравнению с чисто текстовыми."
                },
                "en": {
                    "title": "Personalized Captions Through Multimodal Contexts",
                    "desc": "LaMP-Cap is a new dataset designed to enhance the generation of personalized figure captions by utilizing multimodal profiles. It provides not only the target figure images but also additional contextual figures and their associated captions and paragraphs. This approach allows AI models to create captions that better reflect the author's style and the specific domain. Experiments demonstrate that incorporating profile information, especially images, significantly improves the quality of AI-generated captions compared to traditional text-only methods."
                },
                "zh": {
                    "title": "个性化图形标题生成的新突破",
                    "desc": "LaMP-Cap是一个用于个性化图形标题生成的数据集，旨在通过多模态资料提高AI生成标题的质量。图形标题对于帮助读者理解和记住图形的关键信息至关重要。尽管已有许多模型可以生成这些标题，但作者通常需要修改通用的AI生成标题以匹配他们的写作风格。LaMP-Cap提供了图像和相关图形的上下文资料，实验表明，使用这些多模态资料可以生成更接近作者原始写作的标题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05982",
            "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
            "url": "https://huggingface.co/papers/2506.05982",
            "abstract": "MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.",
            "score": 1,
            "issue_id": 4272,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "6cf7938ff751b2ff",
            "authors": [
                "Zonglin Wu",
                "Yule Xue",
                "Xin Wei",
                "Yiren Song"
            ],
            "affiliations": [
                "National University of Singapore",
                "Southwest University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05982.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#security",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Единый мультимодальный бенчмарк для оценки безопасности CAPTCHA",
                    "desc": "MCA-Bench - это комплексный набор инструментов для оценки безопасности CAPTCHA, использующий мультимодальный подход. Он объединяет различные типы CAPTCHA в единый протокол оценки, используя общую основу модели машинного зрения и обработки естественного языка. MCA-Bench позволяет дообучать специализированных агентов для взлома каждой категории CAPTCHA, обеспечивая последовательную оценку между различными модальностями. Эксперименты показывают, что MCA-Bench эффективно отображает спектр уязвимостей современных CAPTCHA и предлагает количественный анализ взаимосвязи между сложностью задачи, глубиной взаимодействия и способностью модели решать задачи."
                },
                "en": {
                    "title": "Strengthening CAPTCHA Security with MCA-Bench",
                    "desc": "MCA-Bench is a new tool designed to evaluate the security of different types of CAPTCHAs against automated attacks. It combines various CAPTCHA formats, such as text, images, and interactive puzzles, into one comprehensive testing framework. By using a shared vision-language model, it fine-tunes specific agents to crack each type of CAPTCHA, allowing for consistent comparisons across different modalities. The results provide insights into how the complexity and interaction of CAPTCHAs affect their vulnerability, helping to improve their design and security."
                },
                "zh": {
                    "title": "MCA-Bench：CAPTCHA安全评估的新基准",
                    "desc": "MCA-Bench是一个多模态基准测试套件，用于评估CAPTCHA的安全性。它通过共享的视觉-语言模型微调专门的破解代理，以便对不同类型的CAPTCHA进行一致的评估。该研究填补了现有CAPTCHA评估中缺乏统一大规模基准的空白，提供了对现代CAPTCHA设计脆弱性的定量分析。基于实验结果，提出了三个可行的设计原则，并识别了关键的开放挑战，为CAPTCHA的系统性强化和公平基准测试奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08373",
            "title": "Draft-based Approximate Inference for LLMs",
            "url": "https://huggingface.co/papers/2506.08373",
            "abstract": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
            "score": 0,
            "issue_id": 4272,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "02a9a3f798ba1509",
            "authors": [
                "Kevin Galim",
                "Ethan Ewer",
                "Wonjun Kang",
                "Minjae Lee",
                "Hyung Il Koo",
                "Kangwook Lee"
            ],
            "affiliations": [
                "Ajou University",
                "FuriosaAI",
                "Seoul National University",
                "UW-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08373.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение вывода ИИ с помощью умных черновиков",
                    "desc": "Предложена новая система для приближенного вывода в больших языковых моделях с длинным контекстом, использующая вспомогательные модели-черновики. Система точнее предсказывает важность токенов и пар ключ-значение, что повышает точность при сохранении эффективности использования памяти и вычислений. Представлены два варианта реализации: SpecKV для более эффективного отбрасывания кэша ключ-значение и SpecPC для идентификации и удаления неважных токенов запроса. Эксперименты показывают, что предложенные методы превосходят существующие базовые подходы по точности при сохранении преимуществ в использовании памяти, задержке и пропускной способности."
                },
                "en": {
                    "title": "Enhancing LLM Inference with Draft Models for Efficiency and Accuracy",
                    "desc": "This paper introduces a new framework that uses draft models to enhance approximate inference in long-context Large Language Models (LLMs). By accurately predicting the importance of tokens and key-value (KV) pairs, the framework improves the accuracy of LLMs while keeping memory and computational efficiency in check. The authors present two specific implementations: SpecKV for effective KV cache dropping and SpecPC for identifying unimportant prompt tokens. Their experiments demonstrate that this approach outperforms existing methods in accuracy while maintaining low resource usage."
                },
                "zh": {
                    "title": "利用草稿模型提升长上下文LLM推理效率",
                    "desc": "本文提出了一种新的框架，利用草稿模型来增强长上下文大语言模型（LLM）的近似推理能力。通过更准确地预测令牌和键值对的重要性，该方法提高了推理的准确性，同时保持了内存和计算效率。我们介绍了两种具体实现：SpecKV和SpecPC，分别用于优化键值缓存和提示令牌的选择。实验结果表明，该方法在准确性、内存使用、延迟和吞吐量方面均优于现有基线。"
                }
            }
        }
    ],
    "link_prev": "2025-06-12.html",
    "link_next": "2025-06-16.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6月12日"
    },
    "short_date_next": {
        "ru": "16.06",
        "en": "06/16",
        "zh": "6月16日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 3,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了 Seedance 1.0，一个高性能的视频生成模型。它结合了先进的数据整理、高效的架构设计、训练后优化和模型加速。Seedance 1.0 能够在1080p分辨率下生成5秒的视频，只需41.4秒。与其他顶尖的视频生成模型相比，Seedance 1.0 在质量和速度上表现出色，具有优越的时空流畅性和结构稳定性。",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "pinyin": "这篇文章介绍了 Seedance 1.0，一个高性能的视频生成模型。它结合了先进的数据整理、高效的架构设计、训练后优化和模型加速。Seedance 1.0 能够在1080p分辨率下生成5秒的视频，只需41.4秒。与其他顶尖的视频生成模型相比，Seedance 1.0 在质量和速度上表现出色，具有优越的时空流畅性和结构稳定性。\n\nZhè piān wénzhāng jièshào le Seedance 1.0, yīgè gāo xìngnéng de shìpín shēngchéng móxíng. Tā jiéhé le xiānjìn de shùjù zhěnglǐ, gāoxiào de jiàgòu shèjì, xùnliàn hòu yōuhuà hé móxíng jiāsù. Seedance 1.0 nénggòu zài 1080p fēnbiānlǜ xià shēngchéng 5 miǎo de shìpín, zhǐ xū 41.4 miǎo. Yǔ qítā dǐngjiān de shìpín shēngchéng móxíng xiāngbǐ, Seedance 1.0 zài zhìliàng hé sùdù shàng biǎoxiàn chūsè, jùyǒu yōuyuè de shíkōng liúchàngxìng hé jiégòu wěndìngxìng.",
        "vocab": "[\n    {\"word\": \"Seedance\", \"pinyin\": \"Sīdànsì\", \"trans\": \"Seedance\"},\n    {\"word\": \"高性能\", \"pinyin\": \"gāo xìngnéng\", \"trans\": \"high performance\"},\n    {\"word\": \"视频生成模型\", \"pinyin\": \"shìpín shēngchéng móxíng\", \"trans\": \"video generation model\"},\n    {\"word\": \"结合\", \"pinyin\": \"jiéhé\", \"trans\": \"combine\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"数据整理\", \"pinyin\": \"shùjù zhěnglǐ\", \"trans\": \"data processing\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāoxiào\", \"trans\": \"efficient\"},\n    {\"word\": \"架构设计\", \"pinyin\": \"jiàgòu shèjì\", \"trans\": \"architecture design\"},\n    {\"word\": \"训练后优化\", \"pinyin\": \"xùnliàn hòu yōuhuà\", \"trans\": \"post-training optimization\"},\n    {\"word\": \"模型加速\", \"pinyin\": \"móxíng jiāsù\", \"trans\": \"model acceleration\"},\n    {\"word\": \"1080p分辨率\", \"pinyin\": \"1080p fēnbiānlǜ\", \"trans\": \"1080p resolution\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"只需\", \"pinyin\": \"zhǐ xū\", \"trans\": \"only need\"},\n    {\"word\": \"顶尖\", \"pinyin\": \"dǐngjiān\", \"trans\": \"top-notch\"},\n    {\"word\": \"质量\", \"pinyin\": \"zhìliàng\", \"trans\": \"quality\"},\n    {\"word\": \"速度\", \"pinyin\": \"sùdù\", \"trans\": \"speed\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"优越\", \"pinyin\": \"yōuyuè\", \"trans\": \"superior\"},\n    {\"word\": \"时空流畅性\", \"pinyin\": \"shíkōng liúchàngxìng\", \"trans\": \"spatiotemporal smoothness\"},\n    {\"word\": \"结构稳定性\", \"pinyin\": \"jiégòu wěndìngxìng\", \"trans\": \"structural stability\"}\n]",
        "trans": "This article introduces Seedance 1.0, a high-performance video generation model. It combines advanced data curation, efficient architectural design, post-training optimization, and model acceleration. Seedance 1.0 can generate a 5-second video at 1080p resolution in just 41.4 seconds. Compared to other leading video generation models, Seedance 1.0 excels in both quality and speed, demonstrating superior spatiotemporal smoothness and structural stability.",
        "update_ts": "2025-06-12 11:10"
    }
}