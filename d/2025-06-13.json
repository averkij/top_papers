{
    "date": {
        "ru": "12 Ğ¸ÑĞ½Ñ",
        "en": "June 12",
        "zh": "6æœˆ12æ—¥"
    },
    "time_utc": "2025-06-12 23:12",
    "weekday": 3,
    "issue_id": 4270,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.06395",
            "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
            "url": "https://huggingface.co/papers/2506.06395",
            "abstract": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.",
            "score": 70,
            "issue_id": 4258,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "0bd4f12f6c85c81f",
            "authors": [
                "Pengyi Li",
                "Matvey Skripkin",
                "Alexander Zubrey",
                "Andrey Kuznetsov",
                "Ivan Oseledets"
            ],
            "affiliations": [
                "AIRI, Skoltech Moscow"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06395.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Reinforcement Learning via Self-Confidence (RLSC) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RLSC ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ RLSC Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-7B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°."
                },
                "en": {
                    "title": "Boosting Model Accuracy with Self-Confidence Rewards",
                    "desc": "Reinforcement Learning via Self-Confidence (RLSC) is a novel approach that enhances the accuracy of large language models (LLMs) by utilizing the model's own confidence as a reward signal. This method eliminates the reliance on human labels or complex reward engineering, making it more efficient and scalable. RLSC has been tested on the Qwen2.5-Math-7B model, showing significant accuracy improvements across various math benchmarks with minimal training data. By requiring only a few samples and no labeled data, RLSC offers a straightforward solution for post-training alignment of LLMs with task objectives."
                },
                "zh": {
                    "title": "è‡ªä¿¡é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹å‡†ç¡®æ€§ï¼",
                    "desc": "å¼ºåŒ–å­¦ä¹ é€šè¿‡è‡ªä¿¡ï¼ˆRLSCï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è‡ªä¿¡åº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•ä¸å†ä¾èµ–äºäººå·¥æ ‡ç­¾æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚RLSCåœ¨å¤šä¸ªæ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•ç®€å•ä¸”å¯æ‰©å±•ï¼Œåªéœ€å°‘é‡æ ·æœ¬å’Œæ— æ ‡ç­¾çš„ç›‘ç£å³å¯å®ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09113",
            "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "url": "https://huggingface.co/papers/2506.09113",
            "abstract": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
            "score": 47,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "d44df125bd718f2f",
            "authors": [
                "Yu Gao",
                "Haoyuan Guo",
                "Tuyen Hoang",
                "Weilin Huang",
                "Lu Jiang",
                "Fangyuan Kong",
                "Huixia Li",
                "Jiashi Li",
                "Liang Li",
                "Xiaojie Li",
                "Xunsong Li",
                "Yifu Li",
                "Shanchuan Lin",
                "Zhijie Lin",
                "Jiawei Liu",
                "Shu Liu",
                "Xiaonan Nie",
                "Zhiwu Qing",
                "Yuxi Ren",
                "Li Sun",
                "Zhi Tian",
                "Rui Wang",
                "Sen Wang",
                "Guoqiang Wei",
                "Guohong Wu",
                "Jie Wu",
                "Ruiqi Xia",
                "Fei Xiao",
                "Xuefeng Xiao",
                "Jiangqiao Yan",
                "Ceyuan Yang",
                "Jianchao Yang",
                "Runkai Yang",
                "Tao Yang",
                "Yihang Yang",
                "Zilyu Ye",
                "Xuejiao Zeng",
                "Yan Zeng",
                "Heng Zhang",
                "Yang Zhao",
                "Xiaozheng Zheng",
                "Peihao Zhu",
                "Jiaxin Zou",
                "Feilong Zuo"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.09113.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#rlhf",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾",
                    "desc": "Seedance 1.0 - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²ÑƒÑ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğµ RLHF. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼, Seedance 1.0 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ~10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Seedance 1.0: Fast and High-Quality Video Generation Revolutionized",
                    "desc": "Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds."
                },
                "zh": {
                    "title": "Seedance 1.0ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "Seedance 1.0 æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€æœ‰æ•ˆçš„æ¶æ„è®¾è®¡ã€åè®­ç»ƒä¼˜åŒ–å’Œæ¨¡å‹åŠ é€ŸæŠ€æœ¯ï¼Œæä¾›äº†å“è¶Šçš„è´¨é‡å’Œé€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šæºæ•°æ®æ•´ç†å’Œç²¾å‡†çš„è§†é¢‘å­—å¹•ï¼Œå¢å¼ºäº†å¯¹å¤šæ ·åœºæ™¯çš„å…¨é¢å­¦ä¹ èƒ½åŠ›ã€‚å®ƒçš„é«˜æ•ˆæ¶æ„æ”¯æŒå¤šé•œå¤´ç”Ÿæˆï¼Œå¹¶åŒæ—¶å­¦ä¹ æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä»»åŠ¡ã€‚Seedance 1.0 é€šè¿‡å¤šé˜¶æ®µè’¸é¦ç­–ç•¥å®ç°äº†çº¦10å€çš„æ¨ç†åŠ é€Ÿï¼Œèƒ½å¤Ÿåœ¨41.4ç§’å†…ç”Ÿæˆ5ç§’çš„1080pè§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09350",
            "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
            "url": "https://huggingface.co/papers/2506.09350",
            "abstract": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2",
            "score": 35,
            "issue_id": 4256,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "f2ebb5225d0061fa",
            "authors": [
                "Shanchuan Lin",
                "Ceyuan Yang",
                "Hao He",
                "Jianwen Jiang",
                "Yuxi Ren",
                "Xin Xia",
                "Yang Zhao",
                "Xuefeng Xiao",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09350.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AAPT",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressivĞ½Ğ¾Ğ³Ğ¾ adversarial post-training (AAPT) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressĞ¸Ğ²Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (1NFE) Ğ½Ğ° ĞºĞ°Ğ´Ñ€. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ 24 ĞºĞ°Ğ´Ñ€Ğ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 1280x720 Ğ½Ğ° 8 GPU H100."
                },
                "en": {
                    "title": "Real-Time Video Generation Made Easy!",
                    "desc": "This paper introduces a method called autoregressive adversarial post-training (AAPT) to enhance pre-trained latent video diffusion models for real-time video generation. The AAPT approach allows the model to generate video frames one at a time, using a single neural function evaluation, which significantly reduces computational demands. By incorporating adversarial training, the model improves its efficiency and reduces errors during the generation of longer videos. The results show that the model can produce high-quality video at 24 frames per second, making it suitable for interactive applications."
                },
                "zh": {
                    "title": "å®æ—¶äº¤äº’è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’å¯¹æŠ—åè®­ç»ƒï¼ˆAAPTï¼‰æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹è½¬å˜ä¸ºå®æ—¶äº¤äº’å¼è§†é¢‘ç”Ÿæˆå™¨ã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªæ½œåœ¨å¸§ï¼Œä½¿ç”¨å•æ¬¡ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ï¼ˆ1NFEï¼‰ï¼Œå®ç°å®æ—¶æµå¼ä¼ è¾“ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯¹æŠ—è®­ç»ƒä½œä¸ºè‡ªå›å½’ç”Ÿæˆçš„æœ‰æ•ˆèŒƒå¼ï¼Œè®¾è®¡å‡ºæ›´é«˜æ•ˆçš„æ¶æ„ï¼Œå‡å°‘é•¿è§†é¢‘ç”Ÿæˆä¸­çš„è¯¯å·®ç§¯ç´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„8Bæ¨¡å‹åœ¨å•ä¸ªH100ä¸Šå®ç°äº†736x416åˆ†è¾¨ç‡çš„å®æ—¶24fpsè§†é¢‘ç”Ÿæˆï¼Œæˆ–åœ¨8ä¸ªH100ä¸Šç”Ÿæˆ1280x720åˆ†è¾¨ç‡çš„è§†é¢‘ï¼Œæœ€é•¿å¯è¾¾ä¸€åˆ†é’Ÿï¼ˆ1440å¸§ï¼‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09790",
            "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
            "url": "https://huggingface.co/papers/2506.09790",
            "abstract": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.",
            "score": 28,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "6fb3fee31c3739a5",
            "authors": [
                "Zhenran Xu",
                "Yiyu Wang",
                "Xue Yang",
                "Longyue Wang",
                "Weihua Luo",
                "Kaifu Zhang",
                "Baotian Hu",
                "Min Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce, China",
                "Harbin Institute of Technology (Shenzhen), China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09790.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#optimization",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ComfyUI-R1: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "ComfyUI-R1 - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ Ğ˜Ğ˜-Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ComfyUI-R1 Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 4000 Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Automating AI Art Workflows with ComfyUI-R1",
                    "desc": "ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities."
                },
                "zh": {
                    "title": "ComfyUI-R1ï¼šè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆçš„æ¨ç†æ¨¡å‹",
                    "desc": "ComfyUI-R1 æ˜¯ä¸€ä¸ªå¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸“æ³¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒé€šè¿‡é•¿é“¾æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¸®åŠ©ç”¨æˆ·åˆ›å»ºå®šåˆ¶åŒ–çš„å·¥ä½œæµï¼Œé™ä½äº†å­¦ä¹ æ›²çº¿ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº† 4K å·¥ä½œæµçš„æ•°æ®é›†ï¼Œç»è¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒï¼Œç¡®ä¿äº†å·¥ä½œæµçš„æ ¼å¼æœ‰æ•ˆæ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComfyUI-R1 åœ¨æ ¼å¼æœ‰æ•ˆæ€§å’Œ F1 åˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†é•¿é“¾æ¨ç†åœ¨ AI è‰ºæœ¯åˆ›ä½œä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09991",
            "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation",
            "url": "https://huggingface.co/papers/2506.09991",
            "abstract": "Multiverse, a parallel generative model incorporating a MapReduce paradigm, achieves performance comparable to autoregressive LLMs while offering superior scaling and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model that enables natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. Starting from sequential reasoning chains, we create Multiverse 1K by converting them into structured training data using an automated LLM-assisted pipeline, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to enable parallel inference. It features a dedicated scheduler that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, engine, supporting tools, as well as complete data curation prompts and detailed training and evaluation recipes.",
            "score": 27,
            "issue_id": 4261,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "3ffc5ce1d43056b5",
            "authors": [
                "Xinyu Yang",
                "Yuwei An",
                "Hongyi Liu",
                "Tianqi Chen",
                "Beidi Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nvidia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09991.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸŒŒ",
                "ru": {
                    "title": "Multiverse: ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multiverse - Ğ½Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ MapReduce Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Map Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Process Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Reduce Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Multiverse-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ. Ğ’ÑÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Multiverse, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ."
                },
                "en": {
                    "title": "Multiverse: Parallel Power in Generative Modeling",
                    "desc": "The paper introduces Multiverse, a novel generative model that leverages a MapReduce paradigm to achieve efficient parallel generation, competing with autoregressive large language models (AR-LLMs). It operates through three stages: Map for task decomposition, Process for executing subtasks in parallel, and Reduce for synthesizing results without loss. The model incorporates Multiverse Attention to maintain compatibility with causal attention while allowing for parallel reasoning steps. After fine-tuning, Multiverse-32B demonstrates comparable performance to leading AR-LLMs, with significant improvements in scaling and speed, making it a promising alternative in the field of generative modeling."
                },
                "zh": {
                    "title": "Multiverseï¼šå¹¶è¡Œç”Ÿæˆçš„æœªæ¥",
                    "desc": "Multiverseæ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨MapReduceèŒƒå¼ï¼Œå®ç°äº†åŸç”Ÿçš„å¹¶è¡Œç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè‡ªåŠ¨ç”Ÿæˆï¼šMapé˜¶æ®µç”¨äºè‡ªé€‚åº”ä»»åŠ¡åˆ†è§£ï¼ŒProcessé˜¶æ®µç”¨äºå¹¶è¡Œå­ä»»åŠ¡æ‰§è¡Œï¼ŒReduceé˜¶æ®µç”¨äºæ— æŸç»“æœåˆæˆã€‚æˆ‘ä»¬è®¾è®¡äº†Multiverse Attentionï¼Œä»¥åˆ†ç¦»å¹¶è¡Œæ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒä¸å› æœæ³¨æ„åŠ›çš„å…¼å®¹æ€§ï¼Œä»è€Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚ç»è¿‡3å°æ—¶çš„å¾®è°ƒï¼ŒMultiverse-32Båœ¨æ€§èƒ½ä¸Šä¸åŒè§„æ¨¡çš„é¢†å…ˆè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨é¢„ç®—æ§åˆ¶å®éªŒä¸­æ˜¾ç¤ºå‡ºæ›´ä¼˜çš„æ‰©å±•æ€§å’Œé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09995",
            "title": "PlayerOne: Egocentric World Simulator",
            "url": "https://huggingface.co/papers/2506.09995",
            "abstract": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.",
            "score": 24,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "e7cba5eb3e340a0f",
            "authors": [
                "Yuanpeng Tu",
                "Hao Luo",
                "Xi Chen",
                "Xiang Bai",
                "Fan Wang",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "HKU",
                "HUST",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09995.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ PlayerOne",
                    "desc": "PlayerOne - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. PlayerOne Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Egocentric World Simulation with PlayerOne",
                    "desc": "PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments."
                },
                "zh": {
                    "title": "å¼€åˆ›è‡ªæˆ‘ä¸­å¿ƒç°å®ä¸–ç•Œæ¨¡æ‹Ÿçš„æ–°çºªå…ƒ",
                    "desc": "PlayerOneæ˜¯ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç°å®ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æ•æ‰çš„å›¾åƒæ„å»ºå’Œç”Ÿæˆè§†é¢‘ã€‚å®ƒé‡‡ç”¨ç²—åˆ°ç»†çš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡çš„è‡ªæˆ‘ä¸­å¿ƒæ–‡æœ¬-è§†é¢‘å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨åŒæ­¥è¿åŠ¨-è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†éƒ¨åˆ†è§£è€¦çš„è¿åŠ¨æ³¨å…¥æ–¹æ¡ˆï¼Œä»¥å®ç°å¯¹éƒ¨åˆ†è¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶é€šè¿‡è”åˆé‡å»ºæ¡†æ¶ç¡®ä¿é•¿è§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlayerOneåœ¨æ§åˆ¶äººç±»è¿åŠ¨å’Œå»ºæ¨¡å¤šæ ·åœºæ™¯æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08570",
            "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
            "url": "https://huggingface.co/papers/2506.08570",
            "abstract": "A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
            "score": 22,
            "issue_id": 4260,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "1ad6cb6752b933a6",
            "authors": [
                "Or Tal",
                "Felix Kreuk",
                "Yossi Adi"
            ],
            "affiliations": [
                "mail.huji.ac.il",
                "meta.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08570.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#synthetic",
                    "#architecture",
                    "#training",
                    "#audio"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼: ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼."
                },
                "en": {
                    "title": "Decoding the Future of Music Generation: A Paradigm Comparison",
                    "desc": "This paper systematically compares two popular modeling paradigms in text-to-music generation: Auto-Regressive decoding and Conditional Flow-Matching. The authors focus on how these paradigms affect performance by conducting controlled experiments with identical datasets and training setups. They evaluate the models on various criteria, including generation quality, robustness, and editing capabilities. The findings reveal the unique strengths and weaknesses of each approach, providing valuable insights for future developments in music generation systems."
                },
                "zh": {
                    "title": "æ¯”è¾ƒè‡ªå›å½’ä¸æ¡ä»¶æµåŒ¹é…ï¼šæ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆçš„æœªæ¥",
                    "desc": "æœ¬æ–‡ç³»ç»Ÿæ¯”è¾ƒäº†è‡ªå›å½’è§£ç å’Œæ¡ä»¶æµåŒ¹é…åœ¨æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºäº†å„è‡ªçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç ”ç©¶é›†ä¸­åœ¨å»ºæ¨¡èŒƒå¼ä¸Šï¼Œé€šè¿‡ç›¸åŒçš„æ•°æ®é›†å’Œè®­ç»ƒé…ç½®å¯¹æ¨¡å‹è¿›è¡Œæ§åˆ¶æ¯”è¾ƒã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ç”Ÿæˆè´¨é‡ã€å¯¹æ¨ç†é…ç½®çš„é²æ£’æ€§ã€å¯æ‰©å±•æ€§ä»¥åŠå¯¹æ–‡æœ¬å’Œæ—¶é—´å¯¹é½æ¡ä»¶çš„éµå¾ªèƒ½åŠ›ã€‚æ­¤ç ”ç©¶ä¸ºæœªæ¥æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆç³»ç»Ÿçš„æ¶æ„å’Œè®­ç»ƒå†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08889",
            "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
            "url": "https://huggingface.co/papers/2506.08889",
            "abstract": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
            "score": 18,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "a6e46d58b91f0fad",
            "authors": [
                "Yizhao Gao",
                "Shuming Guo",
                "Shijie Cao",
                "Yuqing Xia",
                "Yu Cheng",
                "Lei Wang",
                "Lingxiao Ma",
                "Yutao Sun",
                "Tianzhu Ye",
                "Li Dong",
                "Hayden Kwok-Hay So",
                "Yu Hua",
                "Ting Cao",
                "Fan Yang",
                "Mao Yang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Microsoft Research",
                "Peking University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08889.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "SeerAttention-R - ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ´ĞµÑ€ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 0,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ² 4000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ‚ĞµÑÑ‚Ğµ AIME Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ TileLang, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 9 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention-3 Ğ½Ğ° GPU H100 Ğ¿Ñ€Ğ¸ 90% Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. SeerAttention-R Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models",
                    "desc": "SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware."
                },
                "zh": {
                    "title": "SeerAttention-Rï¼šé«˜æ•ˆç¨€ç–æ³¨æ„åŠ›æ¨ç†æ¡†æ¶",
                    "desc": "SeerAttention-Ræ˜¯ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œä¸“ä¸ºæ¨ç†æ¨¡å‹çš„é•¿è§£ç è€Œè®¾è®¡ã€‚å®ƒé€šè¿‡è‡ªè’¸é¦é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§ï¼ŒåŒæ—¶å»é™¤äº†æŸ¥è¯¢æ± åŒ–ï¼Œä»¥é€‚åº”è‡ªå›å½’è§£ç ã€‚è¯¥æ¡†æ¶è½»é‡ä¸”çµæ´»ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒSeerAttention-Råœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ä»¥4Kä»¤ç‰Œé¢„ç®—ä¿æŒæ¥è¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ï¼Œå¹¶åœ¨H100 GPUä¸Šå®ç°äº†é«˜è¾¾9å€çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09003",
            "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
            "url": "https://huggingface.co/papers/2506.09003",
            "abstract": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).",
            "score": 14,
            "issue_id": 4251,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "794ea1a282cfa727",
            "authors": [
                "Lei Zhang",
                "Jiaxi Yang",
                "Min Yang",
                "Jian Yang",
                "Mouxiang Chen",
                "Jiajun Zhang",
                "Zeyu Cui",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",
                "University of Science and Technology of China, Hefei, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09003.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "SWE-Flow: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ TDD Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "SWE-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (TDD). ĞĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (RDG). SWE-Flow Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² TDD-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Automating TDD with SWE-Flow: Smarter Development Schedules",
                    "desc": "SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding."
                },
                "zh": {
                    "title": "SWE-Flowï¼šè‡ªåŠ¨åŒ–æµ‹è¯•é©±åŠ¨å¼€å‘çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "SWE-Flowæ˜¯ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼ŒåŸºäºæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰æ–¹æ³•ã€‚å®ƒé€šè¿‡è‡ªåŠ¨æ¨æ–­å•å…ƒæµ‹è¯•ä¸­çš„å¼€å‘æ­¥éª¤ï¼Œç”Ÿæˆç»“æ„åŒ–çš„å¼€å‘è®¡åˆ’ï¼Œä»è€Œæé«˜äº†åœ¨çœŸå®é¡¹ç›®ä¸Šå¾®è°ƒå¼€æ”¾æ¨¡å‹çš„æ€§èƒ½ã€‚SWE-Flowçš„æ ¸å¿ƒæ˜¯æ„å»ºè¿è¡Œæ—¶ä¾èµ–å›¾ï¼ˆRDGï¼‰ï¼Œå‡†ç¡®æ•æ‰å‡½æ•°ä¹‹é—´çš„äº¤äº’ï¼Œç¡®ä¿æ¯ä¸€æ­¥ç”Ÿæˆéƒ¨åˆ†ä»£ç åº“åŠç›¸åº”çš„å•å…ƒæµ‹è¯•ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä»çœŸå®çš„GitHubé¡¹ç›®ä¸­ç”Ÿæˆäº†å¤§é‡çš„è®­ç»ƒå’Œæµ‹è¯•å®ä¾‹ï¼Œæ˜¾è‘—æå‡äº†åŸºäºTDDçš„ç¼–ç æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09984",
            "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
            "url": "https://huggingface.co/papers/2506.09984",
            "abstract": "A novel framework for end-to-end human animation with multi-modal conditions enables high-quality video generation through explicit layout control and region-specific modality matching.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.",
            "score": 11,
            "issue_id": 4252,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "97a59a9be9ba0dc3",
            "authors": [
                "Zhenzhi Wang",
                "Jiaqi Yang",
                "Jianwen Jiang",
                "Chao Liang",
                "Gaojie Lin",
                "Zerong Zheng",
                "Ceyuan Yang",
                "Dahua Lin"
            ],
            "affiliations": [
                "ByteDance",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09984.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#games",
                    "#diffusion"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Human Animation with Multi-Modal Control",
                    "desc": "This paper presents a new framework for creating human animations that can incorporate multiple types of input, such as text, images, and audio. Unlike previous methods that only animate one subject at a time, this approach allows for complex interactions between multiple characters and objects in a video. The framework uses a mask predictor to accurately match visual elements to specific regions in the video, ensuring that each character's appearance is correctly represented. Additionally, it integrates audio cues in a way that aligns with the visual layout, resulting in high-quality, controllable animations that reflect rich human interactions."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„é«˜è´¨é‡äººç±»åŠ¨ç”»ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯äººç±»åŠ¨ç”»æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šä¸ªæ¦‚å¿µçš„ç²¾ç¡®æ§åˆ¶ï¼Œå…è®¸äººç±»ä¸ç‰©ä½“ä¹‹é—´çš„ä¸°å¯Œäº¤äº’ã€‚é€šè¿‡åŒºåŸŸç‰¹å®šçš„æ¡ä»¶ç»‘å®šï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­å¸ƒå±€ä¿¡æ¯ï¼Œå¹¶ç¡®ä¿ä¸åŒæ¨¡æ€ä¹‹é—´çš„åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„æ˜¾å¼å¸ƒå±€æ§åˆ¶ä¼˜äºç°æœ‰çš„éšå¼æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09501",
            "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.09501",
            "abstract": "The study investigates reproducibility issues in Large Language Models (LLMs) arising from hardware and precision variations, proposing a lightweight inference pipeline to enhance numerical stability while maintaining memory efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
            "score": 10,
            "issue_id": 4264,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "b14433f51b294499",
            "authors": [
                "Jiayi Yuan",
                "Hao Li",
                "Xinheng Ding",
                "Wenya Xie",
                "Yu-Jhe Li",
                "Wentian Zhao",
                "Kun Wan",
                "Jing Shi",
                "Xia Hu",
                "Zirui Liu"
            ],
            "affiliations": [
                "Adobe Inc.",
                "Rice University",
                "University of Minnesota Twin Cities"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09501.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°ĞºĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ñ GPU, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ° Ğ½ĞµĞ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LayerCast, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ğ²ĞµÑĞ° Ğ² 16-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²ÑĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² FP32, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing Reproducibility in LLMs with LayerCast",
                    "desc": "This paper explores the challenges of reproducibility in Large Language Models (LLMs) caused by variations in hardware and numerical precision. It highlights how changes in system configurations, such as GPU type and batch size, can lead to significant differences in model outputs, particularly in reasoning tasks. The authors identify that the non-associative nature of floating-point arithmetic contributes to this variability, which can affect accuracy and response length. To address these issues, they propose a new inference pipeline called LayerCast, which optimizes memory usage while ensuring numerical stability by using mixed precision during computations."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é‡å¤æ€§ä¸ç¨³å®šæ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¡¬ä»¶å’Œç²¾åº¦å˜åŒ–ä¸‹çš„å¯é‡å¤æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ¨ç†ç®¡é“ï¼Œä»¥æé«˜æ•°å€¼ç¨³å®šæ€§ï¼ŒåŒæ—¶ä¿æŒå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMæ€§èƒ½çš„å¯é‡å¤æ€§éå¸¸è„†å¼±ï¼Œç³»ç»Ÿé…ç½®çš„å˜åŒ–ï¼ˆå¦‚è¯„ä¼°æ‰¹é‡å¤§å°ã€GPUæ•°é‡å’Œç‰ˆæœ¬ï¼‰ä¼šæ˜¾è‘—å½±å“ç”Ÿæˆçš„å“åº”ã€‚å°¤å…¶æ˜¯åœ¨æ¨ç†æ¨¡å‹ä¸­ï¼Œæ—©æœŸæ ‡è®°çš„å¾®å°èˆå…¥å·®å¼‚å¯èƒ½å¯¼è‡´æ€ç»´é“¾çš„åˆ†æ­§ï¼Œä»è€Œå½±å“å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†LayerCastæ¨ç†ç®¡é“ï¼Œä½¿ç”¨16ä½ç²¾åº¦å­˜å‚¨æƒé‡ï¼Œä½†åœ¨è®¡ç®—æ—¶ä½¿ç”¨FP32ï¼Œä»¥å¹³è¡¡å†…å­˜æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05309",
            "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
            "url": "https://huggingface.co/papers/2506.05309",
            "abstract": "An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.",
            "score": 9,
            "issue_id": 4260,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "474c2c6a688311bf",
            "authors": [
                "Niv Eckhaus",
                "Uri Berger",
                "Gabriel Stanovsky"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, The Hebrew University of Jerusalem",
                "School of Computing and Information Systems, University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05309.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ĞœĞ°Ñ„Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ›Ğ›Ğœ-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ² ĞœĞ°Ñ„Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ Ğ² ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ². Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Bridging AI and Human Interaction in Asynchronous Settings",
                    "desc": "This paper presents an adaptive asynchronous LLM-agent designed to participate in online Mafia games, showcasing its ability to mimic human players in complex social interactions. Unlike traditional LLMs that operate in synchronous settings, this agent can decide both what to say and when to say it, reflecting the nuances of real-world communication. The evaluation reveals that the agent performs comparably to human participants, effectively blending into the social dynamics of the game. The findings highlight the potential for LLMs to be integrated into various asynchronous environments, enhancing collaborative efforts in educational and professional contexts."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”å¼‚æ­¥ä»£ç†ï¼šè®©AIèå…¥äººç±»ç¤¾äº¤æ¸¸æˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è‡ªé€‚åº”çš„å¼‚æ­¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œå®ƒåœ¨åœ¨çº¿ Mafia æ¸¸æˆä¸­è¡¨ç°å‡ºä¸äººç±»ç©å®¶ç›¸ä¼¼çš„èƒ½åŠ›ã€‚è¿™ç§ä»£ç†ä¸ä»…å†³å®šè¯´ä»€ä¹ˆï¼Œè¿˜å†³å®šä½•æ—¶è¯´ï¼Œè¿™åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„ç¤¾äº¤åœºåˆä¸­è‡³å…³é‡è¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨æ¸¸æˆè¡¨ç°å’Œä¸äººç±»ç©å®¶çš„äº’åŠ¨ä¸­éƒ½è¡¨ç°è‰¯å¥½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèå…¥äººç±»ç¤¾äº¤åŠ¨æ€ã€‚ä½œè€…è¿˜å‘å¸ƒäº†ç›¸å…³æ•°æ®å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¯¹æ›´çœŸå®çš„å¼‚æ­¥æ²Ÿé€šçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09937",
            "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
            "url": "https://huggingface.co/papers/2506.09937",
            "abstract": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  \t\t\t\t\tAI-generated summary \t\t\t\t While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.",
            "score": 7,
            "issue_id": 4251,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "2a59fafef3ba118c",
            "authors": [
                "Qiao Gu",
                "Yuanliang Ju",
                "Shengxiang Sun",
                "Igor Gilitschenski",
                "Haruki Nishimura",
                "Masha Itkina",
                "Florian Shkurti"
            ],
            "affiliations": [
                "Toyota Research Institute (TRI)",
                "University of Toronto (UofT)",
                "UofT Robotics Institute",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09937.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#optimization",
                    "#agents",
                    "#robotics",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "SAFE: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²",
                    "desc": "SAFE - ÑÑ‚Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² VLA Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ± ÑƒÑĞ¿ĞµÑ…Ğµ Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. SAFE Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ°Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SAFE: Generalizing Failure Detection for Vision-Language-Action Models",
                    "desc": "SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods."
                },
                "zh": {
                    "title": "SAFEï¼šæ™ºèƒ½æœºå™¨äººæ•…éšœæ£€æµ‹çš„æ–°æ–¹æ³•",
                    "desc": "SAFEæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æ•…éšœæ£€æµ‹å™¨ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ¨¡å‹çš„é«˜å±‚å†…éƒ¨ç‰¹å¾æ¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚å°½ç®¡è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–°ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†è®©æœºå™¨äººå®‰å…¨åœ°ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¤ŸåŠæ—¶å‘å‡ºè­¦æŠ¥çš„æ•…éšœæ£€æµ‹å™¨ï¼Œä»¥ä¾¿æœºå™¨äººå¯ä»¥åœæ­¢ã€å›æº¯æˆ–è¯·æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬æå‡ºçš„SAFEèƒ½å¤Ÿä»VLAçš„å†…éƒ¨ç‰¹å¾ä¸­å­¦ä¹ ï¼Œå¹¶é¢„æµ‹ä»»åŠ¡å¤±è´¥çš„å¯èƒ½æ€§ï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ•…éšœæ£€æµ‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09736",
            "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\n  Reasoning",
            "url": "https://huggingface.co/papers/2506.09736",
            "abstract": "Visual perturbation framework enhances multimodal models' mathematical reasoning performance without additional training or algorithmic changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.",
            "score": 4,
            "issue_id": 4262,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "f9912b183b6548a4",
            "authors": [
                "Yuting Li",
                "Lai Wei",
                "Kaipeng Zheng",
                "Jingyuan Huang",
                "Linghe Kong",
                "Lichao Sun",
                "Weiran Huang"
            ],
            "affiliations": [
                "Lehigh University",
                "School of Computer Science, Shanghai Jiao Tong University",
                "Shanghai Innovation Institute",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09736.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#math",
                    "#multimodal",
                    "#training",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ›ÑƒÑ‡ÑˆĞµ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ - Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Reasoning with Visual Perturbations",
                    "desc": "This paper introduces a visual perturbation framework that improves the mathematical reasoning abilities of multimodal large language models (MLLMs) without needing extra training or changes to the algorithms. The authors found that language-only models can perform as well as or better than MLLMs when given image captions, indicating a gap in how MLLMs process visual information. The proposed framework includes three types of visual perturbations that enhance the models' robustness and can be easily added to existing post-training processes. Through various experiments, the study shows that these perturbations lead to significant improvements in reasoning performance, emphasizing the importance of effective visual integration in multimodal models."
                },
                "zh": {
                    "title": "è§†è§‰æ‰°åŠ¨æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰æ‰°åŠ¨æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ è®­ç»ƒæˆ–ç®—æ³•ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œæé«˜å¤šæ¨¡æ€æ¨¡å‹çš„æ•°å­¦æ¨ç†æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä½¿ç”¨è¯­è¨€æ¨¡å‹å¹¶ç»“åˆå›¾åƒæè¿°ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„è¡¨ç°ã€‚è¿™è¡¨æ˜å½“å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰æè¿°æ—¶å¯èƒ½å­˜åœ¨æ•´åˆä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸‰ç§ç‰¹å®šçš„æ‰°åŠ¨ç­–ç•¥ï¼Œè®ºæ–‡å±•ç¤ºäº†åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ•°å­¦æ¨ç†æ€§èƒ½çš„ä¸€è‡´æå‡ï¼Œå¼ºè°ƒäº†è§†è§‰æ‰°åŠ¨åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09278",
            "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow",
            "url": "https://huggingface.co/papers/2506.09278",
            "abstract": "A Unified Flow & Matching model (UFM) improves dense image correspondence accuracy and speed by using a transformer architecture for unified data training, outperforming specialized methods for both optical flow and wide-baseline scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.",
            "score": 3,
            "issue_id": 4263,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "3926eda7133e3fbf",
            "authors": [
                "Yuchen Zhang",
                "Nikhil Keetha",
                "Chenwei Lyu",
                "Bhuvan Jhamb",
                "Yutian Chen",
                "Yuheng Qiu",
                "Jay Karhade",
                "Shreyas Jha",
                "Yaoyu Hu",
                "Deva Ramanan",
                "Sebastian Scherer",
                "Wenshan Wang"
            ],
            "affiliations": [
                "Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09278.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Unified Flow & Matching model (UFM) - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. UFM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 28% Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½Ğ° 62% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. UFM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Unified Training for Superior Image Correspondence",
                    "desc": "The Unified Flow & Matching model (UFM) enhances the accuracy and speed of dense image correspondence by employing a transformer architecture for unified training. It addresses the common challenge of matching content between images in both optical flow and wide-baseline scenarios, which have traditionally been treated separately. UFM directly regresses the (u,v) flow, making it simpler to train and more effective for large flows compared to previous methods that relied on coarse-to-fine cost volumes. This model achieves a 28% improvement in accuracy and is significantly faster, paving the way for advancements in multi-modal and real-time correspondence applications."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ï¼šæå‡å›¾åƒå¯¹åº”çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æµä¸åŒ¹é…æ¨¡å‹ï¼ˆUFMï¼‰ï¼Œæ—¨åœ¨æé«˜å¯†é›†å›¾åƒå¯¹åº”çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚UFMé‡‡ç”¨äº†å˜æ¢å™¨æ¶æ„ï¼Œé€šè¿‡ç»Ÿä¸€æ•°æ®è®­ç»ƒï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å…‰æµä¼°è®¡å’Œå®½åŸºçº¿åœºæ™¯çš„åŒ¹é…é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç²—åˆ°ç»†æˆæœ¬ä½“ç§¯æ–¹æ³•ç›¸æ¯”ï¼ŒUFMåœ¨å¤„ç†å¤§æµæ—¶æ›´æ˜“äºè®­ç»ƒä¸”æ›´ä¸ºå‡†ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUFMåœ¨å‡†ç¡®æ€§ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›çš„æµæ–¹æ³•æé«˜äº†28%ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šæ¯”å¯†é›†å®½åŸºçº¿åŒ¹é…å™¨å¿«äº†6.7å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08008",
            "title": "Hidden in plain sight: VLMs overlook their visual representations",
            "url": "https://huggingface.co/papers/2506.08008",
            "abstract": "Vision language models perform poorly on vision-centric tasks compared to their visual encoders, primarily due to ineffective utilization of visual information and inherited language priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.",
            "score": 3,
            "issue_id": 4262,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "4183c96f8f03c207",
            "authors": [
                "Stephanie Fu",
                "Tyler Bonnen",
                "Devin Guillory",
                "Trevor Darrell"
            ],
            "affiliations": [
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08008.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#cv",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "VLM Ğ½Ğµ Ğ´Ğ¾Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ ÑƒĞ·ĞºĞ¸Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ½Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° VLM Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking Visual Potential in Vision Language Models",
                    "desc": "This paper examines the performance of vision language models (VLMs) on tasks that are primarily visual in nature. It finds that VLMs struggle to effectively integrate visual and linguistic information, leading to significantly poorer performance compared to their visual encoders. The research identifies key issues such as the degradation of visual representations and the influence of language model priors on task performance. Ultimately, the study highlights that VLMs fail to utilize available visual information effectively, which hampers their ability to perform well on vision-centric benchmarks."
                },
                "zh": {
                    "title": "è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ä¸ç“¶é¢ˆ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› åœ¨äºå®ƒä»¬æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è§†è§‰ä¿¡æ¯å’Œç»§æ‰¿çš„è¯­è¨€å…ˆéªŒã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒVLMsä¸å…¶è§†è§‰ç¼–ç å™¨çš„ç›´æ¥è¾“å‡ºï¼Œåˆ†æäº†å®ƒä»¬åœ¨è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ•´åˆæ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒVLMsåœ¨è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä½äºè§†è§‰ç¼–ç å™¨ï¼Œæ¥è¿‘éšæœºæ°´å¹³ã€‚æˆ‘ä»¬æŒ‡å‡ºï¼ŒVLMsåœ¨æ‰§è¡Œè¿™äº›ä»»åŠ¡æ—¶çš„ç“¶é¢ˆä¸»è¦åœ¨äºè¯­è¨€æ¨¡å‹æœªèƒ½æœ‰æ•ˆåˆ©ç”¨å¯ç”¨çš„è§†è§‰ä¿¡æ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09229",
            "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\n  Models",
            "url": "https://huggingface.co/papers/2506.09229",
            "abstract": "Cross-frame Representation Alignment improves video diffusion model fine-tuning by enhancing convergence and semantic coherence across frames.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io",
            "score": 2,
            "issue_id": 4262,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "c5fdb98160ab01fe",
            "authors": [
                "Sungwon Hwang",
                "Hyojin Jang",
                "Kinam Kim",
                "Minho Park",
                "Jaegul choo"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09229.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "CREPA: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM) - Cross-frame Representation Alignment (CREPA). CREPA Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ° Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… VDM, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CogVideoX-5B Ğ¸ Hunyuan Video, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CREPA Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ LoRA. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ĞµĞ³Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Aligning Frames for Better Video Generation",
                    "desc": "This paper introduces Cross-frame Representation Alignment (CREPA), a new technique designed to enhance the fine-tuning of Video Diffusion Models (VDMs). CREPA improves the convergence of VDMs by aligning the hidden states of individual frames with features from neighboring frames, which helps maintain semantic coherence across the video. The authors demonstrate that this method not only boosts visual quality but also ensures that the generated videos are more consistent in meaning throughout. Their experiments on various large-scale VDMs show that CREPA is effective and applicable across different datasets, making it a significant advancement in video generation."
                },
                "zh": {
                    "title": "è·¨å¸§è¡¨ç¤ºå¯¹é½æå‡è§†é¢‘ç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç§°ä¸ºè·¨å¸§è¡¨ç¤ºå¯¹é½ï¼ˆCREPAï¼‰ï¼Œç”¨äºæ”¹è¿›è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰çš„å¾®è°ƒè¿‡ç¨‹ã€‚CREPAé€šè¿‡å°†å½“å‰å¸§çš„éšè—çŠ¶æ€ä¸ç›¸é‚»å¸§çš„å¤–éƒ¨ç‰¹å¾å¯¹é½ï¼Œå¢å¼ºäº†å¸§ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCREPAåœ¨è§†è§‰ä¿çœŸåº¦å’Œè·¨å¸§è¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨é«˜æ•ˆå‚æ•°å¾®è°ƒæ–¹æ³•æ—¶ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08900",
            "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
            "url": "https://huggingface.co/papers/2506.08900",
            "abstract": "MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE.",
            "score": 2,
            "issue_id": 4258,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "d3860686fd37639a",
            "authors": [
                "JosÃ© Morano",
                "Botond Fazekas",
                "Emese SÃ¼kei",
                "Ronald Fecso",
                "Taha Emre",
                "Markus Gumpinger",
                "Georg Faustmann",
                "Marzieh Oghbaie",
                "Ursula Schmidt-Erfurth",
                "Hrvoje BogunoviÄ‡"
            ],
            "affiliations": [
                "Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria",
                "Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria",
                "OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08900.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#science",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "MIRAGE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¾Ñ„Ñ‚Ğ°Ğ»ÑŒĞ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MIRAGE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ° (foundation model) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞĞšĞ¢ Ğ¸ Ğ¡Ğ›Ğ Ğ² Ğ¾Ñ„Ñ‚Ğ°Ğ»ÑŒĞ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. MIRAGE Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ MIRAGE."
                },
                "en": {
                    "title": "MIRAGE: Revolutionizing Ophthalmic Image Analysis with Multimodal AI",
                    "desc": "MIRAGE is a multimodal foundation model designed to improve the classification and segmentation of ophthalmic images, specifically optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) images. It addresses the limitations of existing AI models that often require extensive labeled data and struggle with unseen datasets. By leveraging large amounts of unlabeled data, MIRAGE outperforms both general and specialized models in various tasks. The paper also introduces a new evaluation benchmark for OCT and SLO, demonstrating MIRAGE's effectiveness and potential for advancing AI in retinal image analysis."
                },
                "zh": {
                    "title": "MIRAGEï¼šçœ¼ç§‘å›¾åƒåˆ†æçš„æ–°æ ‡æ†",
                    "desc": "MIRAGEæ˜¯ä¸€ç§å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å’Œæ‰«ææ¿€å…‰çœ¼åº•ç…§ç›¸ï¼ˆSLOï¼‰å›¾åƒçš„åˆ†ç±»å’Œåˆ†å‰²ã€‚è¯¥æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„é€šç”¨å’Œä¸“ä¸šæ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨ç‹¬ç«‹æœªè§æ•°æ®ä¸Šçš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚MIRAGEé€šè¿‡åœ¨å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå…‹æœäº†å¯¹å¤§é‡æ ‡æ³¨çš„ä¾èµ–ï¼Œå¹¶ä¸”åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†MIRAGEåœ¨çœ¼åº•OCTå›¾åƒåˆ†æä¸­çš„é€‚ç”¨æ€§å’Œä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08001",
            "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
            "url": "https://huggingface.co/papers/2506.08001",
            "abstract": "A new reParameterized training algorithm named POET uses Orthogonal Equivalence Transformation to optimize neurons, providing stable optimization and improved generalization for training large-scale neural networks including LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
            "score": 2,
            "issue_id": 4255,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "b47d919d8313c30a",
            "authors": [
                "Zeju Qiu",
                "Simon Buchholz",
                "Tim Z. Xiao",
                "Maximilian Dax",
                "Bernhard SchÃ¶lkopf",
                "Weiyang Liu"
            ],
            "affiliations": [
                "Max Planck Institute for Intelligent Systems, TÃ¼bingen",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08001.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "POET: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "POET - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. POET Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ POET Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "POET: Optimizing Neurons for Stable and Scalable Training",
                    "desc": "The paper introduces POET, a new training algorithm designed to enhance the optimization of neurons in large-scale neural networks, particularly large language models (LLMs). POET employs Orthogonal Equivalence Transformation, which reparameterizes neurons using two learnable orthogonal matrices alongside a fixed random weight matrix. This method ensures the stability of the optimization process and improves the generalization capabilities of the models. The authors also present efficient approximations that allow POET to be flexible and scalable, demonstrating its effectiveness through extensive experiments."
                },
                "zh": {
                    "title": "POETï¼šä¼˜åŒ–ç¥ç»å…ƒçš„é‡å‚æ•°åŒ–è®­ç»ƒæ–°ç®—æ³•",
                    "desc": "POETæ˜¯ä¸€ç§æ–°çš„é‡å‚æ•°åŒ–è®­ç»ƒç®—æ³•ï¼Œåˆ©ç”¨æ­£äº¤ç­‰ä»·å˜æ¢æ¥ä¼˜åŒ–ç¥ç»å…ƒã€‚è¯¥ç®—æ³•é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„æ­£äº¤çŸ©é˜µå’Œä¸€ä¸ªå›ºå®šçš„éšæœºæƒé‡çŸ©é˜µï¼Œå¯¹æ¯ä¸ªç¥ç»å…ƒè¿›è¡Œé‡å‚æ•°åŒ–ã€‚POETèƒ½å¤Ÿç¨³å®šåœ°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚å®éªŒç»“æœéªŒè¯äº†POETåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09958",
            "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy",
            "url": "https://huggingface.co/papers/2506.09958",
            "abstract": "Kvasir-VQA-x1, an expanded dataset for gastrointestinal endoscopy, addresses clinical complexity and visual diversity with large-scale question-answer pairs and visual augmentations to enhance multimodal AI system reliability in clinical settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a model's inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: https://github.com/Simula/Kvasir-VQA-x1 and https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
            "score": 1,
            "issue_id": 4263,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "127078517320f2b7",
            "authors": [
                "Sushant Gautam",
                "Michael A. Riegler",
                "PÃ¥l Halvorsen"
            ],
            "affiliations": [
                "Oslo Metropolitan University (OsloMet), Norway",
                "Simula Metropolitan Center for Digital Engineering (SimulaMet), Norway",
                "Simula Research Laboratory, Norway"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09958.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#science",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸: Kvasir-VQA-x1 Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Kvasir-VQA-x1 Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ°ÑÑ‚Ñ€Ğ¾ÑĞ½Ñ‚ĞµÑ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ½Ğ´Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 159,549 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Kvasir-VQA-x1 Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Clinical AI with Kvasir-VQA-x1: A New Benchmark for MedVQA",
                    "desc": "Kvasir-VQA-x1 is a newly developed dataset aimed at improving Medical Visual Question Answering (MedVQA) systems for gastrointestinal endoscopy. It includes 159,549 question-answer pairs that enhance clinical reasoning and assess AI models' inference capabilities. The dataset also features visual augmentations to simulate common imaging artifacts, ensuring models are tested under realistic conditions. By providing a more complex and diverse benchmark, Kvasir-VQA-x1 seeks to foster the development of robust multimodal AI systems in clinical environments."
                },
                "zh": {
                    "title": "Kvasir-VQA-x1ï¼šæå‡ä¸´åºŠå†³ç­–æ”¯æŒçš„å¤šæ¨¡æ€æ•°æ®é›†",
                    "desc": "Kvasir-VQA-x1æ˜¯ä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºèƒƒè‚ å†…çª¥é•œæ£€æŸ¥ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠå¤æ‚æ€§å’Œè§†è§‰å¤šæ ·æ€§çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«159,549ä¸ªæ–°çš„é—®ç­”å¯¹ï¼Œæ—¨åœ¨æµ‹è¯•æ›´æ·±å±‚æ¬¡çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿç”Ÿæˆè¿™äº›é—®é¢˜ï¼Œå¹¶é€šè¿‡è§†è§‰å¢å¼ºæŠ€æœ¯æ¨¡æ‹Ÿå¸¸è§çš„æˆåƒä¼ªå½±ï¼Œä»¥æé«˜æ¨¡å‹åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„å¯é æ€§ã€‚Kvasir-VQA-x1ä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸´åºŠç›¸å…³æ€§çš„åŸºå‡†ï¼Œä¿ƒè¿›äº†æ›´å¯é å’Œæœ‰æ•ˆçš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09669",
            "title": "Query-Level Uncertainty in Large Language Models",
            "url": "https://huggingface.co/papers/2506.09669",
            "abstract": "A method using Query-Level Uncertainty and Internal Confidence enables Large Language Models to determine knowledge boundaries efficiently, improving adaptability and reducing inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called Internal Confidence, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance.",
            "score": 1,
            "issue_id": 4262,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "b1281e93419bc3d5",
            "authors": [
                "Lihu Chen",
                "GaÃ«l Varoquaux"
            ],
            "affiliations": [
                "Imperial College London, UK",
                "Soda, Inria Saclay, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09669.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#math",
                    "#rag",
                    "#inference",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ·Ğ½Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ»Ğ¸ Ğ¾Ğ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ±ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RAG Ğ¸ ĞºĞ°ÑĞºĞ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Knowledge Boundary Awareness",
                    "desc": "This paper presents a method that helps Large Language Models (LLMs) identify the limits of their knowledge using Query-Level Uncertainty and Internal Confidence. By understanding which queries they can answer confidently, LLMs can adapt their responses, engage in deeper reasoning, or choose to abstain from answering when uncertain. The proposed Internal Confidence method evaluates the model's performance across different layers and tokens without requiring additional training. Experimental results show that this approach not only improves the model's adaptability but also reduces inference costs while maintaining high performance in tasks like factual question answering and mathematical reasoning."
                },
                "zh": {
                    "title": "è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œæå‡æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æŸ¥è¯¢çº§ä¸ç¡®å®šæ€§å’Œå†…éƒ¨ä¿¡å¿ƒçš„æ–¹æ³•ï¼Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆè¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚è¿™ç§æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ€§æ¨ç†ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§åœ°è°ƒç”¨ç›¸å…³çŸ¥è¯†æˆ–é‡‡å–ä¿ç•™æœºåˆ¶ï¼Œä»è€Œæé«˜AIçš„æ•ˆç‡å’Œå¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªæˆ‘è¯„ä¼°æ¥æ£€æµ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¤„ç†ç‰¹å®šæŸ¥è¯¢ï¼Œè€Œæ— éœ€ç”Ÿæˆä»»ä½•è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å†…éƒ¨ä¿¡å¿ƒæ–¹æ³•åœ¨äº‹å®é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09007",
            "title": "Branched SchrÃ¶dinger Bridge Matching",
            "url": "https://huggingface.co/papers/2506.09007",
            "abstract": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.",
            "score": 1,
            "issue_id": 4252,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "8f3c4a6be505cd98",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Computer Science, Duke University",
                "Department of Computer and Information Science, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09007.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "BranchSBM: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "BranchSBM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ SchrÃ¶dinger Bridge Matching Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼. BranchSBM Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¾ÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ñ„ÑƒÑ€ĞºĞ°Ñ†Ğ¸Ğ¹ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑƒĞ´ĞµĞ± Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Branching Out: Modeling Multiple Outcomes with BranchSBM",
                    "desc": "BranchSBM is a new framework in generative modeling that enhances the traditional Schr\"odinger Bridge Matching by allowing for branched stochastic paths. Unlike previous methods that only model single paths between two distributions, BranchSBM can represent multiple outcomes from a single starting point. This is achieved by using multiple time-dependent velocity fields and growth processes, which capture the complexity of population-level divergence. The framework is particularly useful for applications like simulating cell fate decisions and navigating multi-path scenarios."
                },
                "zh": {
                    "title": "åˆ†æ”¯è–›å®šè°”æ¡¥åŒ¹é…ï¼šæ•æ‰å¤šè·¯å¾„æ¼”åŒ–çš„ç”Ÿæˆå»ºæ¨¡æ–°æ¡†æ¶",
                    "desc": "BranchSBMæ˜¯ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œæ‰©å±•äº†è–›å®šè°”æ¡¥åŒ¹é…æ–¹æ³•ï¼Œä»¥å»ºæ¨¡ä»å•ä¸€åˆå§‹åˆ†å¸ƒåˆ°å¤šä¸ªç»“æœçš„åˆ†æ”¯éšæœºè·¯å¾„å’Œå¤šè·¯å¾„æ¼”åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å‚æ•°åŒ–å¤šä¸ªæ—¶é—´ä¾èµ–çš„é€Ÿåº¦åœºå’Œç”Ÿé•¿è¿‡ç¨‹ï¼Œèƒ½å¤Ÿè¡¨ç¤ºä»å…±åŒèµ·æºåˆ°å¤šä¸ªä¸åŒç»“æœçš„äººå£çº§åˆ«çš„åˆ†æ­§ã€‚ä¸ç°æœ‰çš„å•æ¨¡æ€è¿‡æ¸¡æ–¹æ³•ç›¸æ¯”ï¼ŒBranchSBMåœ¨å¤„ç†å¤šè·¯å¾„è¡¨é¢å¯¼èˆªã€ç»†èƒå‘½è¿åˆ†å‰å»ºæ¨¡ä»¥åŠæ¨¡æ‹Ÿç»†èƒå¯¹æ‰°åŠ¨çš„ä¸åŒååº”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚æ€»ä¹‹ï¼ŒBranchSBMä¸ºç”Ÿæˆå»ºæ¨¡æä¾›äº†æ›´ä¸°å¯Œçš„å·¥å…·ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ¼”åŒ–è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06020",
            "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
            "url": "https://huggingface.co/papers/2506.06020",
            "abstract": "A lightweight framework integrating token-level self-confidence and an asymmetric debate between agents enhances the robustness of large language models to contextual inconsistencies with minimal computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.",
            "score": 1,
            "issue_id": 4264,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "2899485806a0e65b",
            "authors": [
                "Zeqi Zhou",
                "Fang Wu",
                "Shayan Talaei",
                "Haokai Zhao",
                "Cheng Meixin",
                "Tinson Xu",
                "Amin Saberi",
                "Yejin Choi"
            ],
            "affiliations": [
                "Brown University",
                "Stanford University",
                "University of Chicago",
                "University of New South Wales",
                "Xian University of Electronic Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06020.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#training",
                    "#hallucinations",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SR-DCR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼. ĞĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ±Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SR-DCR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ²Ğ¾Ğ´ÑÑ‰ĞµĞ¼Ñƒ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Debating for Better Contextual Understanding in AI",
                    "desc": "This paper introduces a new framework called Self-Reflective Debate for Contextual Reliability (SR-DCR) that improves the reliability of large language models when faced with conflicting information. It combines token-level self-confidence with a debate between two agents: a defender who uses the context and a critic who does not. A judge model evaluates their arguments to determine the reliability of the context. The results show that SR-DCR enhances the model's ability to handle misleading information while keeping its performance on accurate inputs intact, all with low computational costs."
                },
                "zh": {
                    "title": "è‡ªæˆ‘åæ€è¾©è®ºï¼šæå‡è¯­è¨€æ¨¡å‹é²æ£’æ€§çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè‡ªæˆ‘åæ€è¾©è®ºçš„æ¡†æ¶ï¼ˆSR-DCRï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸Šä¸‹æ–‡ä¸ä¸€è‡´æ—¶çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºæ ‡è®°çš„è‡ªä¿¡åº¦å’Œä¸å¯¹ç§°çš„å¤šæ™ºèƒ½ä½“è¾©è®ºï¼Œé€šè¿‡è®©æ‰¹è¯„è€…å’Œè¾©æŠ¤è€…è¿›è¡Œè¾©è®ºæ¥è§£å†³çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ä¹‹é—´çš„å†²çªã€‚æœ€ç»ˆï¼Œè¯„åˆ¤æ¨¡å‹ä¼šæ ¹æ®è¾©è®ºç»“æœå’Œæ¨¡å‹è‡ªä¿¡åº¦æ¥é€‰æ‹©ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSR-DCRåœ¨å¤„ç†è¯¯å¯¼æ€§ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒåŒæ—¶åœ¨å¯ä¿¡è¾“å…¥ä¸Šä¿æŒå‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—æˆæœ¬æä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09420",
            "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should\n  Precede AI Autonomy",
            "url": "https://huggingface.co/papers/2506.09420",
            "abstract": "The paper advocates for LLM-based Human-Agent Systems over fully autonomous AI agents due to improved reliability, transparency, and understanding of human needs through collaboration in various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest a different approach: LLM-based Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership.",
            "score": 0,
            "issue_id": 4270,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ½Ñ",
                "en": "June 11",
                "zh": "6æœˆ11æ—¥"
            },
            "hash": "4a2d6b859f002d34",
            "authors": [
                "Henry Peng Zou",
                "Wei-Chieh Huang",
                "Yaozu Wu",
                "Chunyu Miao",
                "Dongyuan Li",
                "Aiwei Liu",
                "Yue Zhou",
                "Yankai Chen",
                "Weizhi Zhang",
                "Yangning Li",
                "Liancheng Fang",
                "Renhe Jiang",
                "Philip S. Yu"
            ],
            "affiliations": [
                "Tsinghua University",
                "University of Illinois Chicago",
                "University of Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09420.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#ethics",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ»Ğ° Ğ² ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸: Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸ Ğ˜Ğ˜ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM-HAS) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM-HAS Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ˜Ğ˜ Ğ½Ğµ Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ° Ğ¿Ğ¾ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Humans Through AI Collaboration",
                    "desc": "This paper argues for the use of LLM-based Human-Agent Systems (LLM-HAS) instead of fully autonomous AI agents. It highlights that while autonomous systems have advanced, they still struggle with reliability, transparency, and understanding human needs. By promoting collaboration between AI and humans, LLM-HAS can provide better adaptability and trustworthiness in various fields like healthcare and finance. The authors emphasize that the future of AI should focus on enhancing human capabilities through partnership rather than replacing human roles."
                },
                "zh": {
                    "title": "äººæœºåä½œï¼Œæå‡æ™ºèƒ½æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå€¡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äººå·¥-æ™ºèƒ½ç³»ç»Ÿï¼ˆLLM-HASï¼‰ï¼Œè€Œä¸æ˜¯å®Œå…¨è‡ªä¸»çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚å› ä¸ºåè€…åœ¨å¯é æ€§ã€é€æ˜æ€§å’Œç†è§£äººç±»éœ€æ±‚æ–¹é¢ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚é€šè¿‡è®©äººå·¥æ™ºèƒ½ä¸äººç±»åˆä½œï¼Œè€Œä¸æ˜¯å–ä»£ä»–ä»¬ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥æ›´å¯ä¿¡å’Œé€‚åº”æ€§æ›´å¼ºã€‚æˆ‘ä»¬é€šè¿‡åŒ»ç–—ã€é‡‘èå’Œè½¯ä»¶å¼€å‘ç­‰é¢†åŸŸçš„ä¾‹å­ï¼Œå±•ç¤ºäº†äººæœºåä½œå¦‚ä½•æ¯”å•ç‹¬çš„äººå·¥æ™ºèƒ½æ›´å¥½åœ°å¤„ç†å¤æ‚ä»»åŠ¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-11.html",
    "link_next": "2025-06-13.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "11.06",
        "en": "06/11",
        "zh": "6æœˆ11æ—¥"
    },
    "short_date_next": {
        "ru": "13.06",
        "en": "06/13",
        "zh": "6æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 4,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 9,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 8,
        "#healthcare": 1,
        "#training": 13,
        "#robotics": 1,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 14,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Seedance 1.0ï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®ƒç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€é«˜æ•ˆçš„æ¶æ„è®¾è®¡ã€è®­ç»ƒåä¼˜åŒ–å’Œæ¨¡å‹åŠ é€Ÿã€‚Seedance 1.0 èƒ½å¤Ÿåœ¨1080påˆ†è¾¨ç‡ä¸‹ç”Ÿæˆ5ç§’çš„è§†é¢‘ï¼Œåªéœ€41.4ç§’ã€‚ä¸å…¶ä»–é¡¶å°–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒSeedance 1.0 åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ—¶ç©ºæµç•…æ€§å’Œç»“æ„ç¨³å®šæ€§ã€‚",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº† Seedance 1.0ï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®ƒç»“åˆäº†å…ˆè¿›çš„æ•°æ®æ•´ç†ã€é«˜æ•ˆçš„æ¶æ„è®¾è®¡ã€è®­ç»ƒåä¼˜åŒ–å’Œæ¨¡å‹åŠ é€Ÿã€‚Seedance 1.0 èƒ½å¤Ÿåœ¨1080påˆ†è¾¨ç‡ä¸‹ç”Ÿæˆ5ç§’çš„è§†é¢‘ï¼Œåªéœ€41.4ç§’ã€‚ä¸å…¶ä»–é¡¶å°–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒSeedance 1.0 åœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ—¶ç©ºæµç•…æ€§å’Œç»“æ„ç¨³å®šæ€§ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le Seedance 1.0, yÄ«gÃ¨ gÄo xÃ¬ngnÃ©ng de shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng. TÄ jiÃ©hÃ© le xiÄnjÃ¬n de shÃ¹jÃ¹ zhÄ›nglÇ, gÄoxiÃ o de jiÃ gÃ²u shÃ¨jÃ¬, xÃ¹nliÃ n hÃ²u yÅuhuÃ  hÃ© mÃ³xÃ­ng jiÄsÃ¹. Seedance 1.0 nÃ©nggÃ²u zÃ i 1080p fÄ“nbiÄnlÇœ xiÃ  shÄ“ngchÃ©ng 5 miÇo de shÃ¬pÃ­n, zhÇ xÅ« 41.4 miÇo. YÇ” qÃ­tÄ dÇngjiÄn de shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng xiÄngbÇ, Seedance 1.0 zÃ i zhÃ¬liÃ ng hÃ© sÃ¹dÃ¹ shÃ ng biÇoxiÃ n chÅ«sÃ¨, jÃ¹yÇ’u yÅuyuÃ¨ de shÃ­kÅng liÃºchÃ ngxÃ¬ng hÃ© jiÃ©gÃ²u wÄ›ndÃ¬ngxÃ¬ng.",
        "vocab": "[\n    {\"word\": \"Seedance\", \"pinyin\": \"SÄ«dÃ nsÃ¬\", \"trans\": \"Seedance\"},\n    {\"word\": \"é«˜æ€§èƒ½\", \"pinyin\": \"gÄo xÃ¬ngnÃ©ng\", \"trans\": \"high performance\"},\n    {\"word\": \"è§†é¢‘ç”Ÿæˆæ¨¡å‹\", \"pinyin\": \"shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng\", \"trans\": \"video generation model\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄnjÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"æ•°æ®æ•´ç†\", \"pinyin\": \"shÃ¹jÃ¹ zhÄ›nglÇ\", \"trans\": \"data processing\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ¶æ„è®¾è®¡\", \"pinyin\": \"jiÃ gÃ²u shÃ¨jÃ¬\", \"trans\": \"architecture design\"},\n    {\"word\": \"è®­ç»ƒåä¼˜åŒ–\", \"pinyin\": \"xÃ¹nliÃ n hÃ²u yÅuhuÃ \", \"trans\": \"post-training optimization\"},\n    {\"word\": \"æ¨¡å‹åŠ é€Ÿ\", \"pinyin\": \"mÃ³xÃ­ng jiÄsÃ¹\", \"trans\": \"model acceleration\"},\n    {\"word\": \"1080påˆ†è¾¨ç‡\", \"pinyin\": \"1080p fÄ“nbiÄnlÇœ\", \"trans\": \"1080p resolution\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"åªéœ€\", \"pinyin\": \"zhÇ xÅ«\", \"trans\": \"only need\"},\n    {\"word\": \"é¡¶å°–\", \"pinyin\": \"dÇngjiÄn\", \"trans\": \"top-notch\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"é€Ÿåº¦\", \"pinyin\": \"sÃ¹dÃ¹\", \"trans\": \"speed\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"ä¼˜è¶Š\", \"pinyin\": \"yÅuyuÃ¨\", \"trans\": \"superior\"},\n    {\"word\": \"æ—¶ç©ºæµç•…æ€§\", \"pinyin\": \"shÃ­kÅng liÃºchÃ ngxÃ¬ng\", \"trans\": \"spatiotemporal smoothness\"},\n    {\"word\": \"ç»“æ„ç¨³å®šæ€§\", \"pinyin\": \"jiÃ©gÃ²u wÄ›ndÃ¬ngxÃ¬ng\", \"trans\": \"structural stability\"}\n]",
        "trans": "This article introduces Seedance 1.0, a high-performance video generation model. It combines advanced data curation, efficient architectural design, post-training optimization, and model acceleration. Seedance 1.0 can generate a 5-second video at 1080p resolution in just 41.4 seconds. Compared to other leading video generation models, Seedance 1.0 excels in both quality and speed, demonstrating superior spatiotemporal smoothness and structural stability.",
        "update_ts": "2025-06-12 11:10"
    }
}