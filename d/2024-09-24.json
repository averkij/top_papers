{
    "date": {
        "ru": "24 сентября",
        "en": "September 24",
        "zh": "9月24日"
    },
    "time_utc": "2024-09-24 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-24",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.14674",
            "title": "RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning",
            "url": "https://huggingface.co/papers/2409.14674",
            "abstract": "Developing robust and correctable visuomotor policies for robotic manipulation is challenging due to the lack of self-recovery mechanisms from failures and the limitations of simple language instructions in guiding robot actions. To address these issues, we propose a scalable data generation pipeline that automatically augments expert demonstrations with failure recovery trajectories and fine-grained language annotations for training. We then introduce Rich languAge-guided failure reCovERy (RACER), a supervisor-actor framework, which combines failure recovery data with rich language descriptions to enhance robot control. RACER features a vision-language model (VLM) that acts as an online supervisor, providing detailed language guidance for error correction and task execution, and a language-conditioned visuomotor policy as an actor to predict the next actions. Our experimental results show that RACER outperforms the state-of-the-art Robotic View Transformer (RVT) on RLbench across various evaluation settings, including standard long-horizon tasks, dynamic goal-change tasks and zero-shot unseen tasks, achieving superior performance in both simulated and real world environments. Videos and code are available at: https://rich-language-failure-recovery.github.io.",
            "score": 41,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "1eadbd614c9eb6fa",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#rl",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Языковое обучение роботов с восстановлением после ошибок",
                    "desc": "Статья представляет новый подход RACER для обучения роботов манипуляциям с использованием языковых инструкций и восстановления после ошибок. Авторы разработали масштабируемый конвейер генерации данных, который автоматически дополняет экспертные демонстрации траекториями восстановления после сбоев и подробными языковыми аннотациями. RACER использует модель зрения-языка в качестве онлайн-супервизора для предоставления детальных языковых инструкций по исправлению ошибок и выполнению задач. Экспериментальные результаты показывают превосходство RACER над современными методами в различных сценариях, включая стандартные долгосрочные задачи и задачи с нулевым обучением."
                },
                "en": {
                    "title": "Empowering Robots with Language for Smart Recovery",
                    "desc": "This paper addresses the challenges in robotic manipulation by developing a system that allows robots to recover from failures using language instructions. The authors propose a data generation pipeline that enhances expert demonstrations with recovery trajectories and detailed language annotations. They introduce a framework called RACER, which integrates a vision-language model to guide robots in correcting errors and executing tasks. Experimental results demonstrate that RACER significantly outperforms existing methods in various task settings, showcasing its effectiveness in both simulated and real-world scenarios."
                },
                "zh": {
                    "title": "提升机器人操作的鲁棒性与纠正能力",
                    "desc": "本论文提出了一种新的方法来提高机器人操作的鲁棒性和可纠正性。我们开发了一个可扩展的数据生成管道，自动增强专家演示，加入失败恢复轨迹和细粒度语言注释。引入的RACER框架结合了失败恢复数据和丰富的语言描述，提升了机器人的控制能力。实验结果表明，RACER在多种评估设置下的表现优于现有的最先进方法，展示了其在模拟和真实环境中的优越性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15277",
            "title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?",
            "url": "https://huggingface.co/papers/2409.15277",
            "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future research.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "6eb21c6e0a002821",
            "data": {
                "categories": [
                    "#medicine",
                    "#rl",
                    "#multilingual",
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Новая языковая модель o1: прорыв в медицинском ИИ",
                    "desc": "Статья описывает исследование возможностей новой большой языковой модели OpenAI o1 в области медицины. Авторы оценили модель по трем ключевым аспектам: понимание, рассуждение и многоязычность, используя данные из 37 медицинских наборов данных. Результаты показали, что o1 превосходит предыдущую модель GPT-4 по точности в среднем на 6,2% и 6,6% в различных медицинских задачах. Однако исследователи также выявили некоторые слабости модели, включая галлюцинации и несогласованные многоязычные способности."
                },
                "en": {
                    "title": "Unlocking Medical Insights with o1: A Leap in Language Model Reasoning",
                    "desc": "This paper explores the capabilities of OpenAI's latest large language model, o1, particularly in the medical domain. It utilizes an internalized chain-of-thought technique and reinforcement learning to enhance its reasoning abilities. The evaluation covers six tasks across 37 medical datasets, revealing that o1 outperforms GPT-4 in accuracy while also highlighting its limitations, such as hallucination and inconsistent multilingual performance. The findings suggest that o1's advanced reasoning may improve its understanding of medical instructions and complex clinical scenarios, making it a valuable tool for real-world applications."
                },
                "zh": {
                    "title": "医学领域的语言模型新突破",
                    "desc": "大型语言模型（LLMs）在学习和认知领域展现了卓越的能力。OpenAI最新的o1模型采用了内化的思维链技术，并结合强化学习策略，成为首个实现这一技术的LLM。我们对o1在医学场景中的表现进行了全面评估，重点分析了理解、推理和多语言能力三个方面。结果显示，o1在19个数据集和两个新创建的复杂问答场景中，准确率平均提高了6.2%和6.6%，但也发现了模型能力和评估协议中的一些不足之处。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14713",
            "title": "Phantom of Latent for Large Language and Vision Models",
            "url": "https://huggingface.co/papers/2409.14713",
            "abstract": "The success of visual instruction tuning has accelerated the development of large language and vision models (LLVMs). Following the scaling laws of instruction-tuned large language models (LLMs), LLVMs either have further increased their sizes, reaching 26B, 34B, and even 80B parameters. While this increase in model size has yielded significant performance gains, it demands substantially more hardware resources for both training and inference. Consequently, there naturally exists a strong need for efficient LLVMs that achieve the performance of larger models while being smaller in size. To achieve this need, we present a new efficient LLVM family with model sizes of 0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances learning capabilities within limited structures. By temporarily increasing the latent hidden dimension during multi-head self-attention (MHSA), we make LLVMs prepare to look and understand much more vision-language knowledge on the latent, without substantially increasing physical model sizes. To maximize its advantage, we introduce Phantom Optimization (PO) using both autoregressive supervised fine-tuning (SFT) and direct preference optimization (DPO)-like concept, which effectively follows correct answers while eliminating incorrect and ambiguous ones. Phantom outperforms numerous larger open- and closed-source LLVMs, positioning itself as a leading solution in the landscape of efficient LLVMs.",
            "score": 27,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "b5cf11108e47bf1b",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "👻",
                "ru": {
                    "title": "Phantom: Эффективные мультимодальные модели с улучшенным обучением",
                    "desc": "В статье представлено семейство эффективных мультимодальных моделей Phantom с размерами от 0,5B до 7B параметров. Модели используют увеличенную скрытую размерность в механизме внимания для улучшения обучения без значительного увеличения физического размера. Предложена оптимизация Phantom (PO), сочетающая автореgressивную тонкую настройку и концепцию прямой оптимизации предпочтений. Phantom превосходит по производительности многие большие открытые и закрытые мультимодальные модели, становясь ведущим решением среди эффективных LLVM."
                },
                "en": {
                    "title": "Phantom: Efficient LLVMs for High Performance with Smaller Sizes",
                    "desc": "This paper discusses the development of a new family of efficient large language and vision models (LLVMs) called Phantom, which are designed to perform well while being smaller in size. The authors highlight that while larger models have shown better performance, they require more hardware resources, creating a need for more efficient alternatives. Phantom models, with sizes ranging from 0.5B to 7B parameters, enhance learning capabilities by temporarily increasing the latent hidden dimension during multi-head self-attention. Additionally, the introduction of Phantom Optimization (PO) combines supervised fine-tuning and preference optimization to improve model accuracy by focusing on correct answers and reducing ambiguity."
                },
                "zh": {
                    "title": "高效视觉语言模型Phantom的崛起",
                    "desc": "这篇论文介绍了一种新的高效视觉语言模型（LLVM）家族，名为Phantom，模型参数分别为0.5B、1.8B、3.8B和7B。尽管模型规模较小，但Phantom通过在多头自注意力机制中临时增加潜在隐藏维度，显著提升了学习能力。论文还提出了Phantom优化（PO）方法，结合自回归监督微调和直接偏好优化，能够有效地学习正确答案并排除错误和模糊的选项。最终，Phantom在性能上超越了许多更大规模的LLVM，成为高效LLVM的领先解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15278",
            "title": "PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions",
            "url": "https://huggingface.co/papers/2409.15278",
            "abstract": "This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting/outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding abilities for images with diverse resolutions but also exhibits promising generalization capabilities with unseen tasks and human instructions. The code and related resources are available at https://github.com/AFeng-x/PixWizard",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "09d534216fa37211",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🧙‍♂️",
                "ru": {
                    "title": "PixWizard: универсальный ассистент для работы с изображениями на основе естественного языка",
                    "desc": "PixWizard - это универсальный визуальный ассистент для генерации, манипуляции и трансформации изображений на основе свободных языковых инструкций. Модель объединяет различные задачи компьютерного зрения в единый фреймворк генерации изображений из текста и изображений. PixWizard использует Diffusion Transformers (DiT) в качестве базовой модели и расширяет её возможности механизмом обработки изображений любого разрешения. Эксперименты показывают впечатляющие способности PixWizard в генерации и понимании изображений различных разрешений, а также многообещающую обобщающую способность на новых задачах и инструкциях."
                },
                "en": {
                    "title": "PixWizard: Your Versatile Image Assistant!",
                    "desc": "PixWizard is an advanced image-to-image visual assistant that utilizes a unified framework for various vision tasks, allowing users to generate, manipulate, and translate images using natural language instructions. The paper introduces the Omni Pixel-to-Pixel Instruction-Tuning Dataset, which supports a wide range of tasks including text-to-image generation and image editing. By employing Diffusion Transformers (DiT) and a flexible resolution mechanism, PixWizard can adapt to different image sizes while maintaining high-quality outputs. The model's structure-aware and semantic-aware guidance enhances its ability to integrate information from input images, demonstrating strong performance across diverse tasks and instructions."
                },
                "zh": {
                    "title": "PixWizard：图像生成与处理的智能助手",
                    "desc": "本文介绍了一种多功能的图像生成助手PixWizard，旨在根据自由形式的语言指令进行图像生成、处理和转换。我们将多种视觉任务整合到一个统一的图像-文本-图像生成框架中，并创建了一个全方位的像素到像素指令调优数据集。通过构建详细的自然语言指令模板，我们涵盖了多种视觉任务，如文本到图像生成、图像修复、图像定位、密集图像预测、图像编辑等。此外，我们采用扩散变换器（DiT）作为基础模型，并通过灵活的任意分辨率机制扩展其能力，使模型能够根据输入的宽高比动态处理图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14988",
            "title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs",
            "url": "https://huggingface.co/papers/2409.14988",
            "abstract": "Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ these methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale clinical pretraining dataset of 50 billion tokens and an instruct fine-tuning dataset of 500 million tokens. Our evaluation across various clinical tasks reveals the impact of each technique. While continuous pretraining beyond 250 billion tokens yields marginal improvements on its own, it establishes a strong foundation for instruct fine-tuning. Notably, NEFTune, designed primarily to enhance generation quality, surprisingly demonstrates additional gains on our benchmark. Complex prompt engineering methods further enhance performance. These findings show the importance of tailoring fine-tuning strategies and exploring innovative techniques to optimize LLM performance in the clinical domain.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "516328c723a83df7",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#medicine"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Оптимизация языковых моделей для медицины: от предобучения до инженерии промптов",
                    "desc": "Исследование изучает эффективность четырех методов адаптации больших языковых моделей (LLM) для клинического применения. Авторы используют непрерывное предобучение, инструктивную донастройку, NEFTune и инженерию промптов на моделях Mistral 7B и Mixtral 8x7B. Результаты показывают, что непрерывное предобучение создает основу для инструктивной донастройки, а NEFTune неожиданно улучшает производительность на бенчмарке. Сложные методы инженерии промптов дополнительно повышают эффективность моделей в клинической области."
                },
                "en": {
                    "title": "Optimizing LLMs for Clinical Excellence",
                    "desc": "This paper explores how to improve Large Language Models (LLMs) for clinical applications using four specific techniques: continuous pretraining, instruct fine-tuning, NEFTune, and prompt engineering. The authors tested these methods on two models, Mistral 7B and Mixtral 8x7B, using a vast clinical dataset. They found that while continuous pretraining alone offers limited benefits, it sets a solid groundwork for more effective instruct fine-tuning. Additionally, NEFTune and advanced prompt engineering methods significantly enhance the models' performance in clinical tasks, highlighting the need for customized fine-tuning approaches in healthcare settings."
                },
                "zh": {
                    "title": "优化大型语言模型在临床应用中的表现",
                    "desc": "本研究探讨了四种技术在临床应用中适应大型语言模型（LLMs）的有效性，包括持续预训练、指令微调、NEFTune和提示工程。我们在Mistral 7B和Mixtral 8x7B模型上应用这些方法，利用了一个包含500亿个标记的大规模临床预训练数据集和一个包含5亿个标记的指令微调数据集。评估结果显示，尽管超过2500亿个标记的持续预训练单独带来的改进有限，但为指令微调奠定了坚实基础。NEFTune和复杂的提示工程方法进一步提升了模型在临床任务中的表现，强调了定制微调策略和探索创新技术的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14677",
            "title": "Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections",
            "url": "https://huggingface.co/papers/2409.14677",
            "abstract": "We tackle the problem of generating highly realistic and plausible mirror reflections using diffusion-based generative models. We formulate this problem as an image inpainting task, allowing for more user control over the placement of mirrors during the generation process. To enable this, we create SynMirror, a large-scale dataset of diverse synthetic scenes with objects placed in front of mirrors. SynMirror contains around 198K samples rendered from 66K unique 3D objects, along with their associated depth maps, normal maps and instance-wise segmentation masks, to capture relevant geometric properties of the scene. Using this dataset, we propose a novel depth-conditioned inpainting method called MirrorFusion, which generates high-quality geometrically consistent and photo-realistic mirror reflections given an input image and a mask depicting the mirror region. MirrorFusion outperforms state-of-the-art methods on SynMirror, as demonstrated by extensive quantitative and qualitative analysis. To the best of our knowledge, we are the first to successfully tackle the challenging problem of generating controlled and faithful mirror reflections of an object in a scene using diffusion based models. SynMirror and MirrorFusion open up new avenues for image editing and augmented reality applications for practitioners and researchers alike.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "f546e78c1b915dac",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🪞",
                "ru": {
                    "title": "Реалистичные зеркала в виртуальных мирах: новый уровень генерации изображений",
                    "desc": "Статья представляет новый подход к генерации реалистичных зеркальных отражений с использованием диффузионных генеративных моделей. Авторы создали большой синтетический датасет SynMirror с 3D-объектами перед зеркалами, включающий карты глубины и сегментации. На основе этого датасета разработан метод MirrorFusion для дорисовки зеркальных отражений с учетом геометрии сцены. Метод превосходит существующие решения и открывает новые возможности для редактирования изображений и дополненной реальности."
                },
                "en": {
                    "title": "Revolutionizing Mirror Reflections with MirrorFusion!",
                    "desc": "This paper addresses the challenge of creating realistic mirror reflections using diffusion-based generative models. It introduces a new approach by framing the task as image inpainting, which allows users to specify where mirrors should be placed in the generated images. The authors present SynMirror, a comprehensive dataset containing 198K samples of various 3D objects and their geometric properties, which aids in training their model. The proposed method, MirrorFusion, demonstrates superior performance in generating high-quality mirror reflections compared to existing techniques, paving the way for advancements in image editing and augmented reality."
                },
                "zh": {
                    "title": "生成真实镜面反射的新方法",
                    "desc": "本文研究了使用扩散生成模型生成高度真实和可信的镜面反射的问题。我们将此问题表述为图像修复任务，从而在生成过程中允许用户更好地控制镜子的放置。为此，我们创建了SynMirror，这是一个包含多样合成场景的大规模数据集，包含约198K样本和66K独特3D对象的深度图、法线图和实例分割掩码。我们提出了一种新的深度条件修复方法MirrorFusion，能够根据输入图像和镜面区域的掩码生成高质量的几何一致和照片真实的镜面反射。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15268",
            "title": "Style over Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
            "url": "https://huggingface.co/papers/2409.15268",
            "abstract": "The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM judges. In this work, we attempt to answer the following question -- do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not? We define a concrete metric for alignment, and introduce SOS-Bench, the largest standardized, reproducible LLM meta-benchmark to date. We find that (1) LLM-judgments do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors. Our codebase and complete results can be found at https://github.com/penfever/sos-bench.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "6d1f8608b7c79ac1",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Переосмысление методов оптимизации языковых моделей",
                    "desc": "Статья исследует эффективность методов оптимизации предпочтений (PO) для языковых моделей. Авторы разработали метрику SOS-Bench для оценки выравнивания ИИ с человеческими ценностями. Исследование показало, что оценки языковых моделей не коррелируют с конкретными мерами безопасности и знаний. Выяснилось, что этап дообучения на размеченных данных (SFT) оказывает наибольшее влияние на выравнивание ИИ."
                },
                "en": {
                    "title": "Evaluating LLM Alignment: Beyond Human Preferences",
                    "desc": "This paper investigates the effectiveness of preference optimization (PO) methods in aligning large language models (LLMs) with human values. It introduces SOS-Bench, a comprehensive benchmark designed to evaluate LLM alignment using concrete metrics. The findings reveal that LLM-judge preferences do not reliably correlate with important alignment measures such as safety and factual accuracy. Additionally, the study highlights that the supervised fine-tuning (SFT) phase is more crucial for achieving alignment than the PO phase, emphasizing the importance of data quality and diversity in training."
                },
                "zh": {
                    "title": "LLM评审与对齐性：偏见与影响的探讨",
                    "desc": "本文探讨了大型语言模型（LLM）在偏好优化（PO）方法中的应用，特别是这些方法是否能有效提升模型的对齐性。研究发现，LLM的判断与安全性、世界知识和指令遵循等具体对齐指标并无相关性。LLM评审存在隐性偏见，倾向于风格而非事实和安全性。最后，研究表明，后训练的监督微调阶段对对齐性影响最大，而数据规模和提示多样性是关键因素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15273",
            "title": "MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors",
            "url": "https://huggingface.co/papers/2409.15273",
            "abstract": "Recent works in inverse rendering have shown promise in using multi-view images of an object to recover shape, albedo, and materials. However, the recovered components often fail to render accurately under new lighting conditions due to the intrinsic challenge of disentangling albedo and material properties from input images. To address this challenge, we introduce MaterialFusion, an enhanced conventional 3D inverse rendering pipeline that incorporates a 2D prior on texture and material properties. We present StableMaterial, a 2D diffusion model prior that refines multi-lit data to estimate the most likely albedo and material from given input appearances. This model is trained on albedo, material, and relit image data derived from a curated dataset of approximately ~12K artist-designed synthetic Blender objects called BlenderVault. we incorporate this diffusion prior with an inverse rendering framework where we use score distillation sampling (SDS) to guide the optimization of the albedo and materials, improving relighting performance in comparison with previous work. We validate MaterialFusion's relighting performance on 4 datasets of synthetic and real objects under diverse illumination conditions, showing our diffusion-aided approach significantly improves the appearance of reconstructed objects under novel lighting conditions. We intend to publicly release our BlenderVault dataset to support further research in this field.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 сентября",
                "en": "September 23",
                "zh": "9月23日"
            },
            "hash": "944a42117642f9a1",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение обратного рендеринга с помощью диффузионной модели материалов",
                    "desc": "Статья представляет MaterialFusion - улучшенный метод обратного рендеринга 3D объектов с использованием 2D диффузионной модели StableMaterial в качестве априорной информации. StableMaterial обучена на синтетическом наборе данных BlenderVault и помогает точнее оценивать альбедо и свойства материалов объектов. Метод интегрирует эту модель в процесс оптимизации с помощью техники Score Distillation Sampling. Эксперименты показывают, что MaterialFusion значительно улучшает качество рендеринга реконструированных объектов при новых условиях освещения."
                },
                "en": {
                    "title": "Enhancing Inverse Rendering with MaterialFusion",
                    "desc": "This paper presents MaterialFusion, a new approach to inverse rendering that improves the recovery of shape, albedo, and material properties from multi-view images. The key innovation is the introduction of StableMaterial, a 2D diffusion model that refines the data to better estimate albedo and material characteristics. By using score distillation sampling (SDS) within the inverse rendering framework, the method enhances the relighting performance of reconstructed objects under various lighting conditions. The authors validate their approach using a curated dataset of synthetic objects and plan to release this dataset to aid future research."
                },
                "zh": {
                    "title": "MaterialFusion：提升逆渲染的重光照性能",
                    "desc": "本论文介绍了一种名为MaterialFusion的增强型3D逆渲染管道，旨在解决从多视角图像中恢复物体形状、反照率和材料时遇到的挑战。通过引入StableMaterial，一个基于2D扩散模型的先验，我们能够更准确地估计输入图像的反照率和材料属性。该模型在一个包含约12K个艺术家设计的合成Blender对象的数据集上进行训练，利用得出的数据来优化渲染效果。实验结果表明，MaterialFusion在不同光照条件下的重光照性能显著优于以往的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13910",
            "title": "Zero-shot Cross-lingual Voice Transfer for TTS",
            "url": "https://huggingface.co/papers/2409.13910",
            "abstract": "In this paper, we introduce a zero-shot Voice Transfer (VT) module that can be seamlessly integrated into a multi-lingual Text-to-speech (TTS) system to transfer an individual's voice across languages. Our proposed VT module comprises a speaker-encoder that processes reference speech, a bottleneck layer, and residual adapters, connected to preexisting TTS layers. We compare the performance of various configurations of these components and report Mean Opinion Score (MOS) and Speaker Similarity across languages. Using a single English reference speech per speaker, we achieve an average voice transfer similarity score of 73% across nine target languages. Vocal characteristics contribute significantly to the construction and perception of individual identity. The loss of one's voice, due to physical or neurological conditions, can lead to a profound sense of loss, impacting one's core identity. As a case study, we demonstrate that our approach can not only transfer typical speech but also restore the voices of individuals with dysarthria, even when only atypical speech samples are available - a valuable utility for those who have never had typical speech or banked their voice. Cross-lingual typical audio samples, plus videos demonstrating voice restoration for dysarthric speakers are available here (google.github.io/tacotron/publications/zero_shot_voice_transfer).",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "10f07d23d3491f3d",
            "data": {
                "categories": [
                    "#audio",
                    "#multilingual",
                    "#medicine"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Перенос голоса между языками без дополнительного обучения",
                    "desc": "В статье представлен модуль переноса голоса с нулевым обучением, который можно интегрировать в многоязычную систему преобразования текста в речь. Модуль состоит из кодировщика диктора, слоя узкого горла и остаточных адаптеров, подключенных к существующим слоям TTS. Авторы сравнивают различные конфигурации компонентов и оценивают качество с помощью MOS и сходства дикторов на разных языках. Система достигает 73% сходства при переносе голоса на 9 целевых языков, используя один образец речи на английском."
                },
                "en": {
                    "title": "Seamless Voice Transfer Across Languages and Conditions",
                    "desc": "This paper presents a novel zero-shot Voice Transfer (VT) module designed for multilingual Text-to-Speech (TTS) systems, enabling the transfer of a person's voice across different languages without needing extensive training data. The VT module includes a speaker-encoder that analyzes reference speech, a bottleneck layer for efficient processing, and residual adapters that connect to existing TTS components. The authors evaluate various configurations of the module, reporting metrics like Mean Opinion Score (MOS) and Speaker Similarity, achieving a 73% voice transfer similarity across nine languages using just one English reference sample. Additionally, the approach shows promise in restoring the voices of individuals with dysarthria, highlighting its potential to aid those who have lost their typical speech."
                },
                "zh": {
                    "title": "无缝跨语言语音转移的创新",
                    "desc": "本文介绍了一种零样本语音转移（VT）模块，可以无缝集成到多语言文本到语音（TTS）系统中，实现个体声音在不同语言间的转移。该VT模块包括一个说话人编码器、一个瓶颈层和残差适配器，连接到现有的TTS层。我们比较了这些组件的不同配置的性能，并报告了跨语言的平均意见评分（MOS）和说话人相似度。通过使用每个说话者的单个英语参考语音，我们在九种目标语言中实现了73%的平均语音转移相似度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14393",
            "title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
            "url": "https://huggingface.co/papers/2409.14393",
            "abstract": "Crafting a single, versatile physics-based controller that can breathe life into interactive characters across a wide spectrum of scenarios represents an exciting frontier in character animation. An ideal controller should support diverse control modalities, such as sparse target keyframes, text instructions, and scene information. While previous works have proposed physically simulated, scene-aware control models, these systems have predominantly focused on developing controllers that each specializes in a narrow set of tasks and control modalities. This work presents MaskedMimic, a novel approach that formulates physics-based character control as a general motion inpainting problem. Our key insight is to train a single unified model to synthesize motions from partial (masked) motion descriptions, such as masked keyframes, objects, text descriptions, or any combination thereof. This is achieved by leveraging motion tracking data and designing a scalable training method that can effectively utilize diverse motion descriptions to produce coherent animations. Through this process, our approach learns a physics-based controller that provides an intuitive control interface without requiring tedious reward engineering for all behaviors of interest. The resulting controller supports a wide range of control modalities and enables seamless transitions between disparate tasks. By unifying character control through motion inpainting, MaskedMimic creates versatile virtual characters. These characters can dynamically adapt to complex scenes and compose diverse motions on demand, enabling more interactive and immersive experiences.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "7f4b73c31b49c3a9",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Универсальное управление персонажами через восстановление движения",
                    "desc": "MaskedMimic - это новый подход к физическому управлению персонажами, представляющий его как задачу восстановления движения. Модель обучается синтезировать движения из частичных описаний, таких как замаскированные ключевые кадры, объекты или текстовые инструкции. Этот универсальный контроллер поддерживает широкий спектр способов управления и позволяет плавно переключаться между различными задачами. MaskedMimic создает универсальных виртуальных персонажей, способных адаптироваться к сложным сценам и создавать разнообразные движения по запросу."
                },
                "en": {
                    "title": "Unified Control for Dynamic Character Animation",
                    "desc": "This paper introduces MaskedMimic, a new physics-based controller designed for character animation that can handle various control methods like keyframes and text instructions. Instead of creating separate controllers for different tasks, MaskedMimic uses a single model to fill in missing motion data, treating it as a motion inpainting problem. The model is trained on motion tracking data, allowing it to generate realistic animations from incomplete descriptions. This approach simplifies the control process and allows characters to adapt to different scenarios seamlessly, enhancing interactivity and immersion in virtual environments."
                },
                "zh": {
                    "title": "统一角色控制，创造多样化虚拟角色",
                    "desc": "本文提出了一种名为MaskedMimic的新方法，将基于物理的角色控制视为一种通用的运动修复问题。该方法的核心思想是训练一个统一的模型，从部分运动描述（如遮蔽的关键帧、物体、文本描述等）合成运动。通过利用运动跟踪数据和可扩展的训练方法，MaskedMimic能够有效地利用多样的运动描述，生成连贯的动画。最终，这种控制器支持多种控制方式，使虚拟角色能够在复杂场景中动态适应，提供更具互动性和沉浸感的体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13191",
            "title": "An adapted large language model facilitates multiple medical tasks in diabetes care",
            "url": "https://huggingface.co/papers/2409.13191",
            "abstract": "Diabetes is a chronic disease that poses a significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but their effectiveness across a diverse range of diabetes tasks remains unproven. In this study, we introduced a framework to train and validate diabetes-specific LLMs. We first developed a comprehensive data processing pipeline that includes data collection, filtering, augmentation and refinement. This approach contributes to creating a high-quality, diabetes-specific dataset, and several evaluation benchmarks entirely from scratch. Utilizing the collected training dataset, we fine-tuned a diabetes-specific LLM family that demonstrated state-of-the-art proficiency in understanding and processing various diabetes tasks compared to other LLMs. Furthermore, clinical studies showed the potential applications of our models in diabetes care, including providing personalized healthcare, assisting medical education, and streamlining clinical tasks. In conclusion, our study introduced a framework to develop and evaluate a diabetes-specific LLM family, and highlighted its potential to enhance clinical practice and provide personalized, data-driven support for diabetes support when facing different end users. The code is provided via GitHub at https://github.com/waltonfuture/Diabetica.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "cd510f97e7406702",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#medicine",
                    "#training"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Языковые модели на страже здоровья: революция в управлении диабетом",
                    "desc": "В этом исследовании представлена методология разработки и оценки языковых моделей, специализированных на диабете. Авторы создали комплексный процесс обработки данных для формирования высококачественного набора данных и эталонов оценки. Используя собранные данные, они дообучили семейство языковых моделей, которые продемонстрировали высокую эффективность в различных задачах, связанных с диабетом. Клинические исследования показали потенциал применения этих моделей в персонализированной медицинской помощи, медицинском образовании и оптимизации клинических задач."
                },
                "en": {
                    "title": "Empowering Diabetes Care with Tailored Language Models",
                    "desc": "This paper presents a framework for developing large language models (LLMs) specifically tailored for diabetes management. It details a comprehensive data processing pipeline that includes steps like data collection, filtering, augmentation, and refinement to create a high-quality dataset. The authors fine-tuned a family of diabetes-specific LLMs, which outperformed existing models in various diabetes-related tasks. The study also highlights the potential applications of these models in personalized healthcare, medical education, and clinical task optimization."
                },
                "zh": {
                    "title": "优化糖尿病管理的智能解决方案",
                    "desc": "本研究针对糖尿病管理提出了一种新的框架，旨在训练和验证糖尿病特定的大型语言模型（LLM）。我们首先建立了一个全面的数据处理流程，包括数据收集、过滤、增强和精炼，以创建高质量的糖尿病数据集。通过对收集的数据集进行微调，我们的糖尿病特定LLM在理解和处理各种糖尿病任务方面表现出色。临床研究表明，这些模型在个性化医疗、医学教育和临床任务优化等方面具有潜在应用价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13926",
            "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
            "url": "https://huggingface.co/papers/2409.13926",
            "abstract": "There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today's models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user's physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users' physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "8b5027b3fd8db2b7",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Слияние реального и виртуального: новый подход к созданию VR-пространств",
                    "desc": "SpaceBlender - это новая система, использующая генеративный ИИ для создания виртуальных пространств, объединяющих физическое окружение пользователей. Система преобразует 2D-изображения в 3D-среды с помощью оценки глубины, выравнивания сетки и диффузионного заполнения пространства. В исследовании с 20 участниками SpaceBlender сравнивался с обычной виртуальной средой для совместной работы. Результаты показали, что пользователи оценили знакомость и контекст, но отметили сложности, отвлекающие от задачи."
                },
                "en": {
                    "title": "Blending Reality: Enhancing VR Collaboration with SpaceBlender",
                    "desc": "This paper presents SpaceBlender, a new approach that uses generative AI to create 3D virtual environments that incorporate users' real-world surroundings. The process involves transforming 2D images into 3D spaces through depth estimation, mesh alignment, and diffusion-based completion, all guided by geometric information and adaptive text prompts. A study with participants showed that SpaceBlender's environments were more familiar and contextually relevant for collaborative tasks compared to traditional virtual spaces. However, some users found the complexity of these environments could distract from their tasks, leading to suggestions for future improvements."
                },
                "zh": {
                    "title": "生成AI助力虚拟现实空间的创新设计",
                    "desc": "本论文介绍了一种名为SpaceBlender的新型生成AI管道，用于创建适合虚拟现实（VR）应用的3D空间。该管道通过深度估计、网格对齐和基于扩散的空间补全，将用户提供的2D图像转化为丰富的3D环境。研究表明，SpaceBlender能够增强用户的熟悉感和上下文，但也存在生成环境复杂性可能影响任务专注度的问题。我们根据参与者的反馈提出了改进方向，并讨论了混合空间在不同场景中的价值和设计。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13773",
            "title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
            "url": "https://huggingface.co/papers/2409.13773",
            "abstract": "This paper presents a case study of coding tasks by the latest reasoning models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other frontier models. The o1 models deliver SOTA results for WebApp1K, a single-task benchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling number of tasks and test cases. The new benchmark causes the o1 model performances to decline significantly, falling behind Claude 3.5. Moreover, they consistently fail when confronted with atypical yet correct test cases, a trap non-reasoning models occasionally avoid. We hypothesize that the performance variability is due to instruction comprehension. Specifically, the reasoning mechanism boosts performance when all expectations are captured, meanwhile exacerbates errors when key expectations are missed, potentially impacted by input lengths. As such, we argue that the coding success of reasoning models hinges on the top-notch base model and SFT to ensure meticulous adherence to instructions.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "03c27d2c3daf0927",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Рассуждающие модели в кодировании: сила и слабость в понимании инструкций",
                    "desc": "Статья представляет сравнительный анализ моделей рассуждения OpenAI (o1-preview и o1-mini) с другими передовыми моделями на задачах кодирования. Модели o1 показывают наилучшие результаты на бенчмарке WebApp1K, но их производительность значительно падает на более сложном WebApp1K-Duo. Исследователи предполагают, что вариативность производительности связана с пониманием инструкций моделями. Успех моделей рассуждения в задачах кодирования зависит от качества базовой модели и тонкой настройки для точного следования инструкциям."
                },
                "en": {
                    "title": "Reasoning Models: Success Hinges on Instruction Comprehension",
                    "desc": "This paper analyzes the performance of OpenAI's latest reasoning models, o1-preview and o1-mini, on coding tasks compared to other advanced models. The authors introduce a new benchmark, WebApp1K-Duo, which is more challenging and leads to a noticeable drop in the performance of the o1 models. The study finds that these models struggle with atypical test cases, which suggests that their reasoning capabilities are sensitive to how well they understand instructions. The authors propose that the success of reasoning models in coding tasks depends on the quality of the base model and the fine-tuning process to ensure precise instruction following."
                },
                "zh": {
                    "title": "推理模型的编码成功依赖于指令理解",
                    "desc": "本文研究了OpenAI最新推理模型o1-preview和o1-mini在编码任务中的表现，并与其他前沿模型进行了比较。o1模型在单任务基准WebApp1K上取得了最先进的结果，但在新引入的更难基准WebApp1K-Duo上表现显著下降。我们发现，o1模型在面对不典型但正确的测试用例时，表现不佳，这表明推理模型在理解指令时存在变异性。我们认为，推理模型的编码成功依赖于高质量的基础模型和精细调优，以确保对指令的严格遵循。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.14340",
            "title": "Self-Supervised Audio-Visual Soundscape Stylization",
            "url": "https://huggingface.co/papers/2409.14340",
            "abstract": "Speech sounds convey a great deal of information about the scenes, resulting in a variety of effects ranging from reverberation to additional ambient sounds. In this paper, we manipulate input speech to sound as though it was recorded within a different scene, given an audio-visual conditional example recorded from that scene. Our model learns through self-supervision, taking advantage of the fact that natural video contains recurring sound events and textures. We extract an audio clip from a video and apply speech enhancement. We then train a latent diffusion model to recover the original speech, using another audio-visual clip taken from elsewhere in the video as a conditional hint. Through this process, the model learns to transfer the conditional example's sound properties to the input speech. We show that our model can be successfully trained using unlabeled, in-the-wild videos, and that an additional visual signal can improve its sound prediction abilities. Please see our project webpage for video results: https://tinglok.netlify.app/files/avsoundscape/",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "2d23be501fdc0ca5",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Перенос звуковой обстановки на речь с помощью аудиовизуального обучения",
                    "desc": "Статья представляет модель для манипуляции речью, чтобы она звучала как будто была записана в другой обстановке. Модель обучается с помощью самоконтролируемого обучения на естественных видео, извлекая повторяющиеся звуковые события. Используется латентная диффузионная модель для восстановления исходной речи с условным примером из другого фрагмента видео. Авторы показывают, что модель может успешно обучаться на немаркированных видео из реальной жизни, а дополнительный визуальный сигнал улучшает способности предсказания звука."
                },
                "en": {
                    "title": "Transforming Speech with Scene-Specific Sound Properties",
                    "desc": "This paper presents a method for transforming speech sounds to make them appear as if they were recorded in different environments. The approach utilizes a latent diffusion model that learns from audio-visual data, leveraging self-supervised learning to capture sound characteristics from video clips. By extracting audio from one video and conditioning it on another, the model enhances the speech to reflect the acoustic properties of the target scene. The results demonstrate that using unlabeled videos from the real world can effectively improve sound prediction when additional visual information is provided."
                },
                "zh": {
                    "title": "通过视觉信号提升语音场景转换能力",
                    "desc": "本论文探讨了如何将输入的语音处理成听起来像是在不同场景中录制的声音。我们使用自监督学习的方法，利用自然视频中重复出现的声音事件和纹理来训练模型。通过从视频中提取音频片段并进行语音增强，我们的潜在扩散模型能够恢复原始语音，并将条件示例的声音特性转移到输入语音上。实验表明，使用未标记的自然视频进行训练，并结合额外的视觉信号，可以显著提高模型的声音预测能力。"
                }
            }
        }
    ],
    "link_prev": "2024-09-23.html",
    "link_next": "2024-09-25.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "23.09",
        "en": "09/23",
        "zh": "9月23日"
    },
    "short_date_next": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9月25日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 4,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 2,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 1,
        "#medicine": 4,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#translation": 0
    }
}