{
    "date": {
        "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 18",
        "zh": "3æœˆ18æ—¥"
    },
    "time_utc": "2025-03-18 02:19",
    "weekday": 1,
    "issue_id": 2753,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.11647",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "url": "https://huggingface.co/papers/2503.11647",
            "abstract": "Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/",
            "score": 85,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "7e72838ea84ed904",
            "authors": [
                "Jianhong Bai",
                "Menghan Xia",
                "Xiao Fu",
                "Xintao Wang",
                "Lianrui Mu",
                "Jinwen Cao",
                "Zuozhu Liu",
                "Haoji Hu",
                "Xiang Bai",
                "Pengfei Wan",
                "Di Zhang"
            ],
            "affiliations": [
                "CUHK",
                "HUST",
                "Kuaishou Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11647.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#video",
                    "#games"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ReCamMaster - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-video Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Unreal Engine 5. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ°ÑƒÑ‚Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "ReCamMaster: Mastering Camera Control in Video Generation",
                    "desc": "This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution."
                },
                "zh": {
                    "title": "é‡å¡‘è§†é¢‘åŠ¨æ€ï¼ŒæŒæ§ç›¸æœºè½¨è¿¹",
                    "desc": "æœ¬è®ºæ–‡ç ”ç©¶äº†åœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹ç”Ÿæˆè§†é¢‘æ—¶çš„ç›¸æœºæ§åˆ¶é—®é¢˜ã€‚å°½ç®¡æ”¹å˜è§†é¢‘çš„ç›¸æœºè½¨è¿¹å¾ˆé‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ReCamMasterï¼Œä¸€ä¸ªåŸºäºç”Ÿæˆæ¨¡å‹çš„è§†é¢‘é‡æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ–°çš„ç›¸æœºè½¨è¿¹ä¸‹é‡ç°è¾“å…¥è§†é¢‘çš„åŠ¨æ€åœºæ™¯ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤šç›¸æœºåŒæ­¥è§†é¢‘æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¾“å…¥ä¸‹è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07677",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "url": "https://huggingface.co/papers/2503.07677",
            "abstract": "Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.",
            "score": 69,
            "issue_id": 2730,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "913b88ac595cc8b6",
            "authors": [
                "Kwanyoung Kim",
                "Byeongsu Sim"
            ],
            "affiliations": [
                "Samsung Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07677.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "PLADIS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PLADIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. PLADIS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ»ÑÑ‡ Ğ² ÑĞ»Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion Models with Sparse Attention",
                    "desc": "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."
                },
                "zh": {
                    "title": "PLADISï¼šé«˜æ•ˆæå‡æ‰©æ•£æ¨¡å‹çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡æ¡ä»¶æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸å¼•å¯¼è’¸é¦æ¨¡å‹ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºPLADISï¼Œé€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æ¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚U-Net/Transformerï¼‰ã€‚PLADISåœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„softmaxå’Œç¨€ç–å¯¹åº”ç‰©ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œæ˜¾è‘—æ”¹å–„æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11646",
            "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
            "url": "https://huggingface.co/papers/2503.11646",
            "abstract": "The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.",
            "score": 31,
            "issue_id": 2731,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "efdf1296bc567414",
            "authors": [
                "Siyuan Huang",
                "Yue Liao",
                "Siyuan Feng",
                "Shu Jiang",
                "Si Liu",
                "Hongsheng Li",
                "Maoqing Yao",
                "Guanghui Ren"
            ],
            "affiliations": [
                "Agibot",
                "Beihang University",
                "MMLab, CUHK",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11646.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#open_source",
                    "#agents",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ - Adversarial Data Collection (ADC). ADC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑÑ€ĞµĞ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ADC-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADC-Robotics Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Maximizing Data Efficiency in Robotic Learning with Adversarial Collection",
                    "desc": "This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods."
                },
                "zh": {
                    "title": "å¯¹æŠ—æ€§æ•°æ®æ”¶é›†ï¼šæå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„å…³é”®",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†æ–¹æ³•ï¼Œç§°ä¸ºå¯¹æŠ—æ€§æ•°æ®æ”¶é›†ï¼ˆADCï¼‰ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œçš„æ•ˆç‡ã€‚é€šè¿‡å®æ—¶çš„äººæœºäº¤äº’ï¼ŒADCèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ”¶é›†é«˜ä¿¡æ¯å¯†åº¦çš„æ¼”ç¤ºæ•°æ®ï¼Œä»è€Œå‡å°‘å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ADCè®­ç»ƒçš„æ¨¡å‹åœ¨é¢å¯¹æœªè§ä»»åŠ¡æŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´å¥½çš„ç»„åˆæ³›åŒ–èƒ½åŠ›å’Œå¯¹ç¯å¢ƒå¹²æ‰°çš„é²æ£’æ€§ã€‚æœ€ç»ˆï¼ŒADCæ–¹æ³•æ˜¾è‘—æé«˜äº†æœºå™¨äººå­¦ä¹ çš„å®ç”¨æ€§ï¼Œå±•ç¤ºäº†æˆ˜ç•¥æ€§æ•°æ®è·å–çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11224",
            "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
            "url": "https://huggingface.co/papers/2503.11224",
            "abstract": "State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.",
            "score": 22,
            "issue_id": 2731,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "fb4219d497e59f64",
            "authors": [
                "Xingtai Lv",
                "Youbang Sun",
                "Kaiyan Zhang",
                "Shang Qu",
                "Xuekai Zhu",
                "Yuchen Fan",
                "Yi Wu",
                "Ermo Hua",
                "Xinwei Long",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University",
                "Robotics Institute, Carnegie Mellon University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11224.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#long_context",
                    "#survey",
                    "#math",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SSM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. SSM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ SSM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° SSM: Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, S4) Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Mamba)."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of State Space Models",
                    "desc": "State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications."
                },
                "zh": {
                    "title": "çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼šé«˜æ•ˆå¤„ç†åºåˆ—æ•°æ®çš„æ–°é€‰æ‹©",
                    "desc": "çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€æ¸å—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®æˆ–é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ã€‚ä¸æµè¡Œçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒSSMsåœ¨æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¹Ÿèƒ½ä¸ä¹‹åª²ç¾ã€‚æœ¬æ–‡å¯¹SSMsè¿›è¡Œäº†ç³»ç»Ÿçš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬å…¶ç†è®ºåŠ¨æœºã€æ•°å­¦å…¬å¼ã€ä¸ç°æœ‰æ¨¡å‹çš„æ¯”è¾ƒä»¥åŠå„ç§åº”ç”¨ã€‚æˆ‘ä»¬å°†SSMç³»åˆ—åˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œè¯¦ç»†ä»‹ç»äº†åŸå§‹SSMã€ç»“æ„åŒ–SSMï¼ˆå¦‚S4ï¼‰å’Œé€‰æ‹©æ€§SSMï¼ˆå¦‚Mambaï¼‰ï¼Œå¹¶å¼ºè°ƒäº†æé«˜SSMæœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„å…³é”®æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11069",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "url": "https://huggingface.co/papers/2503.11069",
            "abstract": "Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.",
            "score": 20,
            "issue_id": 2730,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "29e714954ed20978",
            "authors": [
                "Chaoyun Zhang",
                "Shilin He",
                "Liqun Li",
                "Si Qin",
                "Yu Kang",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11069.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#survey",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ API Ğ¸ GUI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM ÑÑ‚Ğ¸Ñ€Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: API and GUI LLM Agents Unite",
                    "desc": "This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents."
                },
                "zh": {
                    "title": "APIä¸GUIä»£ç†çš„æ¯”è¾ƒä¸èåˆä¹‹è·¯",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»ä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå‘å±•åˆ°èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€å‘½ä»¤ç›´æ¥è½¬åŒ–ä¸ºå®é™…æ“ä½œçš„è½¯ä»¶ä»£ç†ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¯”è¾ƒäº†åŸºäºAPIçš„LLMä»£ç†å’ŒåŸºäºGUIçš„LLMä»£ç†ï¼Œåˆ†æäº†å®ƒä»¬åœ¨æ¶æ„å¤æ‚æ€§ã€å¼€å‘å·¥ä½œæµç¨‹å’Œç”¨æˆ·äº¤äº’æ¨¡å‹ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬æ¢è®¨äº†å…³é”®ç»´åº¦ï¼Œå¹¶å¼ºè°ƒäº†æ··åˆæ–¹æ³•åœ¨åˆ©ç”¨ä¸¤è€…äº’è¡¥ä¼˜åŠ¿æ–¹é¢çš„åœºæ™¯ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æŒ‡å‡ºLLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–åˆ›æ–°å°†æ¨¡ç³ŠAPIå’ŒGUIä»£ç†ä¹‹é—´çš„ç•Œé™ï¼Œä¸ºå„ç§å®é™…åº”ç”¨æä¾›æ›´çµæ´»ã€é€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11576",
            "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
            "url": "https://huggingface.co/papers/2503.11576",
            "abstract": "We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.",
            "score": 16,
            "issue_id": 2744,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "5548a7c6526f8753",
            "authors": [
                "Ahmed Nassar",
                "Andres Marafioti",
                "Matteo Omenetti",
                "Maksym Lysak",
                "Nikolaos Livathinos",
                "Christoph Auer",
                "Lucas Morin",
                "Rafael Teixeira de Lima",
                "Yusik Kim",
                "A. Said Gurbuz",
                "Michele Dolfi",
                "Miquel FarrÃ©",
                "Peter W. J. Staar"
            ],
            "affiliations": [
                "HuggingFace",
                "IBM Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11576.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#open_source",
                    "#cv",
                    "#dataset",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "SmolDocling: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "SmolDocling - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº. ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ DocTags, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²ÑĞµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ¸ÑÑ‚Ğ¸Ğ½Ğ³Ğ¸ ĞºĞ¾Ğ´Ğ°, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. SmolDocling ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² 27 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SmolDocling: Compact and Powerful Document Conversion",
                    "desc": "SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code."
                },
                "zh": {
                    "title": "SmolDoclingï¼šé«˜æ•ˆæ–‡æ¡£è½¬æ¢çš„æ–°é€‰æ‹©",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†SmolDoclingï¼Œè¿™æ˜¯ä¸€ç§è¶…ç´§å‡‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ç«¯åˆ°ç«¯çš„æ–‡æ¡£è½¬æ¢ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”ŸæˆDocTagsï¼Œä¸€ç§æ–°çš„é€šç”¨æ ‡è®°æ ¼å¼ï¼Œå…¨é¢å¤„ç†æ•´ä¸ªé¡µé¢ï¼Œæ•æ‰æ‰€æœ‰é¡µé¢å…ƒç´ çš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œä½ç½®ã€‚ä¸ä¾èµ–å¤§å‹åŸºç¡€æ¨¡å‹æˆ–å¤šä¸ªä¸“ç”¨æ¨¡å‹çš„æ‰‹å·¥ç®¡é“çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSmolDoclingæä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è½¬æ¢ï¼Œå‡†ç¡®æ•æ‰æ–‡æ¡£å…ƒç´ çš„å†…å®¹ã€ç»“æ„å’Œç©ºé—´ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmolDoclingåœ¨æ€§èƒ½ä¸Šä¸å…¶ä»–é«˜è¾¾27å€å¤§å°çš„è§†è§‰è¯­è¨€æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10772",
            "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
            "url": "https://huggingface.co/papers/2503.10772",
            "abstract": "Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.",
            "score": 13,
            "issue_id": 2731,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "548255900cd1ec21",
            "authors": [
                "Ju He",
                "Qihang Yu",
                "Qihao Liu",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10772.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "FlowTok: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ñ‹",
                    "desc": "FlowTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½Ğ°Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "FlowTok: Simplifying Cross-Modality Generation with 1D Tokens",
                    "desc": "This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa."
                },
                "zh": {
                    "title": "FlowTokï¼šç®€åŒ–è·¨æ¨¡æ€ç”Ÿæˆçš„é«˜æ•ˆæ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è·¨æ¨¡æ€ç”Ÿæˆä¸­çš„ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¥é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å°†æ–‡æœ¬æ¨¡æ€è§†ä¸ºå¼•å¯¼ä¿¡å·ï¼Œé€æ­¥å¼•å¯¼å»å™ªè¿‡ç¨‹ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡æµåŒ¹é…ç›´æ¥åœ¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´æ¼”å˜ã€‚æˆ‘ä»¬å¼•å…¥äº†FlowTokæ¡†æ¶ï¼Œå°†å›¾åƒç¼–ç ä¸ºç´§å‡‘çš„1Dæ ‡è®°è¡¨ç¤ºï¼Œä»è€Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­æµåŠ¨ï¼Œæ˜¾è‘—å‡å°‘äº†æ½œåœ¨ç©ºé—´çš„å¤§å°ã€‚FlowTokä¸ä»…æé«˜äº†å†…å­˜æ•ˆç‡å’Œé‡‡æ ·é€Ÿåº¦ï¼Œè¿˜åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11514",
            "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
            "url": "https://huggingface.co/papers/2503.11514",
            "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.",
            "score": 13,
            "issue_id": 2730,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "d31bf6f9bd4bc86b",
            "authors": [
                "Pengxin Guo",
                "Runxi Wang",
                "Shuang Zeng",
                "Jinjing Zhu",
                "Haoning Jiang",
                "Yanran Wang",
                "Yuyin Zhou",
                "Feifei Wang",
                "Hui Xiong",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA",
                "Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA",
                "Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China",
                "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
                "Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China",
                "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China",
                "School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China",
                "Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11514.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#benchmark",
                    "#security",
                    "#survey",
                    "#healthcare",
                    "#data"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° (GIA) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ GIA Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ñ‚Ğ°Ğº Ğ² FL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ GIA ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks",
                    "desc": "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."
                },
                "zh": {
                    "title": "æå‡è”é‚¦å­¦ä¹ éšç§ä¿æŠ¤çš„é˜²å¾¡ç­–ç•¥",
                    "desc": "è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§ä¿æŠ¤éšç§çš„åä½œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œä¸éœ€è¦å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å…±äº«æ¢¯åº¦ä¿¡æ¯ï¼Œç§å¯†ä¿¡æ¯ä»ç„¶å¯èƒ½è¢«æ³„éœ²ï¼Œå¹¶å—åˆ°æ¢¯åº¦åæ¼”æ”»å‡»ï¼ˆGIAï¼‰çš„å¨èƒã€‚æœ¬æ–‡å¯¹ç°æœ‰çš„GIAæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„å›é¡¾å’Œåˆ†ç±»ï¼Œå¹¶åˆ†æäº†ä¸‰ç§ç±»å‹çš„GIAåœ¨FLä¸­çš„è¡¨ç°å’Œå±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„é˜²å¾¡æ–¹æ¡ˆï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨è®¾è®¡FLæ¡†æ¶æ—¶æ›´å¥½åœ°ä¿æŠ¤éšç§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10781",
            "title": "Large-scale Pre-training for Grounded Video Caption Generation",
            "url": "https://huggingface.co/papers/2503.10781",
            "abstract": "We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.",
            "score": 11,
            "issue_id": 2737,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "842a44eb006952de",
            "authors": [
                "Evangelos Kazakos",
                "Cordelia Schmid",
                "Josef Sivic"
            ],
            "affiliations": [
                "Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague",
                "Inria, Ecole normale superieure, CNRS, PSL Research University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10781.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#training",
                    "#video",
                    "#data"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HowToGround1M Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GROVE, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… iGround Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ VidSTG Ğ¸ ActivityNet-Entities. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ."
                },
                "en": {
                    "title": "Grounding Video Captions with Precision",
                    "desc": "This paper presents a new method for video captioning and object grounding, where objects mentioned in captions are linked to specific locations in the video using detailed bounding boxes. The authors introduce a large-scale automatic annotation technique that creates consistent bounding box annotations across video frames, resulting in a dataset called HowToGround1M for pre-training. They also develop a model named GROVE, which is trained on this dataset and further fine-tuned on a smaller, high-quality dataset called iGround, containing manually annotated videos. The results show that their approach outperforms existing methods on several benchmark datasets, highlighting the effectiveness of their pre-training and fine-tuning strategy."
                },
                "zh": {
                    "title": "è§†é¢‘å­—å¹•ç”Ÿæˆä¸ç‰©ä½“å®šä½çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘å­—å¹•ç”Ÿæˆå’Œç‰©ä½“å®šä½æ–¹æ³•ï¼Œé€šè¿‡æ—¶é—´å¯†é›†çš„è¾¹ç•Œæ¡†å°†å­—å¹•ä¸­çš„ç‰©ä½“ä¸è§†é¢‘ä¸­çš„å†…å®¹å…³è”èµ·æ¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¤§è§„æ¨¡è‡ªåŠ¨æ³¨é‡Šæ–¹æ³•ï¼Œå°†å•å¸§çš„è¾¹ç•Œæ¡†æ³¨é‡Šèšåˆä¸ºæ—¶é—´ä¸Šå¯†é›†ä¸”ä¸€è‡´çš„è¾¹ç•Œæ¡†æ³¨é‡Šï¼Œå¹¶åœ¨HowTo100Mæ•°æ®é›†ä¸Šæ„å»ºäº†ä¸€ä¸ªåä¸ºHowToGround1Mçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºGROVEçš„åŸºäºè§†é¢‘çš„å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶åœ¨HowToGround1Mä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºiGroundçš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«3500ä¸ªè§†é¢‘åŠå…¶æ‰‹åŠ¨æ³¨é‡Šçš„å­—å¹•å’Œå¯†é›†çš„æ—¶ç©ºè¾¹ç•Œæ¡†ï¼Œä»¥ä¾¿äºè¯„ä¼°æ¨¡å‹çš„è¿›å±•å’Œè¿›è¡Œå¾®è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11651",
            "title": "VGGT: Visual Geometry Grounded Transformer",
            "url": "https://huggingface.co/papers/2503.11651",
            "abstract": "We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.",
            "score": 10,
            "issue_id": 2740,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "701338c26ac42ac7",
            "authors": [
                "Jianyuan Wang",
                "Minghao Chen",
                "Nikita Karaev",
                "Andrea Vedaldi",
                "Christian Rupprecht",
                "David Novotny"
            ],
            "affiliations": [
                "Meta AI",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11651.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "VGGT: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ 3D-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ†ĞµĞ½",
                    "desc": "VGGT - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ 3D-Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞµÑ‘ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. VGGT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ VGGT Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ½ĞµĞ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ñ‚Ğ¾Ñ‡ĞµĞº."
                },
                "en": {
                    "title": "VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency",
                    "desc": "VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis."
                },
                "zh": {
                    "title": "VGGTï¼šé«˜æ•ˆçš„3Dåœºæ™¯æ¨æ–­ç½‘ç»œ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†VGGTï¼Œè¿™æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¯ä»¥ç›´æ¥æ¨æ–­åœºæ™¯çš„æ‰€æœ‰å…³é”®3Då±æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå‚æ•°ã€ç‚¹å›¾ã€æ·±åº¦å›¾å’Œ3Dç‚¹è½¨è¿¹ã€‚è¯¥æ–¹æ³•åœ¨3Dè®¡ç®—æœºè§†è§‰é¢†åŸŸå‘å‰è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå…‹æœäº†ä»¥å¾€æ¨¡å‹ä»…é™äºå•ä¸€ä»»åŠ¡çš„å±€é™æ€§ã€‚VGGTç®€å•é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…é‡å»ºå›¾åƒï¼Œå¹¶ä¸”åœ¨å¤šä¸ª3Dä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºéœ€è¦åå¤„ç†çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„VGGTä½œä¸ºç‰¹å¾éª¨å¹²æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚éåˆšæ€§ç‚¹è·Ÿè¸ªå’Œå‰é¦ˆæ–°è§†å›¾åˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11579",
            "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
            "url": "https://huggingface.co/papers/2503.11579",
            "abstract": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.",
            "score": 10,
            "issue_id": 2744,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "e0a1f364990dbe23",
            "authors": [
                "Weiming Ren",
                "Wentao Ma",
                "Huan Yang",
                "Cong Wei",
                "Ge Zhang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "1.AI",
                "M-A-P",
                "University of Toronto",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11579.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#benchmark",
                    "#architecture",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VAMBA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VAMBA, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´ Mamba Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. VAMBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Mamba-2 Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 1024 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ½Ğ° 50% Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ´Ğ²Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. VAMBA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.3% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LVBench Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "VAMBA: Efficient Video Processing with Linear Complexity",
                    "desc": "This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šVAMBAæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆMamba-Transformeræ¨¡å‹ï¼ˆVAMBAï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘è¾“å…¥æ—¶çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚VAMBAä½¿ç”¨Mamba-2æ¨¡å—ä»¥çº¿æ€§å¤æ‚åº¦ç¼–ç è§†é¢‘æ ‡è®°ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”æ— éœ€å‡å°‘æ ‡è®°æ•°é‡ã€‚ä¸ä¼ ç»Ÿçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒVAMBAåœ¨å•ä¸ªGPUä¸Šèƒ½å¤Ÿç¼–ç è¶…è¿‡1024å¸§çš„è§†é¢‘ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„é€Ÿåº¦ï¼Œå¹¶å‡å°‘äº†GPUå†…å­˜ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAMBAåœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ¯”ä¹‹å‰çš„é«˜æ•ˆè§†é¢‘æ¨¡å‹æé«˜äº†4.3%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10970",
            "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
            "url": "https://huggingface.co/papers/2503.10970",
            "abstract": "Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.",
            "score": 10,
            "issue_id": 2732,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "b7a03e6b34c3c0de",
            "authors": [
                "Shanghua Gao",
                "Richard Zhu",
                "Zhenglun Kong",
                "Ayush Noori",
                "Xiaorui Su",
                "Curtis Ginder",
                "Theodoros Tsiligkaridis",
                "Marinka Zitnik"
            ],
            "affiliations": [
                "Broad Institute of MIT and Harvard, Cambridge, MA",
                "Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA",
                "Department of Biomedical Informatics, Harvard Medical School, Boston, MA",
                "Harvard Data Science Initiative, Cambridge, MA",
                "Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA",
                "MIT Lincoln Laboratory, Lexington, MA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10970.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#healthcare",
                    "#science",
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ’Š",
                "ru": {
                    "title": "TxAgent: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ñ€Ğ¼Ğ°ĞºĞ¾Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸",
                    "desc": "TxAgent - ÑÑ‚Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ², Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 211 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². TxAgent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 3168 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ 456 Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, TxAgent Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°Ğ¼ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning",
                    "desc": "The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–æ²»ç–—çš„æ™ºèƒ½åŠ©æ‰‹TxAgent",
                    "desc": "ç²¾å‡†æ²»ç–—éœ€è¦å¤šæ¨¡æ€è‡ªé€‚åº”æ¨¡å‹æ¥ç”Ÿæˆä¸ªæ€§åŒ–çš„æ²»ç–—å»ºè®®ã€‚æˆ‘ä»¬ä»‹ç»äº†TxAgentï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤šæ­¥æ¨ç†å’Œå®æ—¶ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æ£€ç´¢çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿåˆ†æè¯ç‰©ç›¸äº’ä½œç”¨ã€ç¦å¿Œç—‡å’Œæ‚£è€…ç‰¹å®šçš„æ²»ç–—ç­–ç•¥ã€‚TxAgentåœ¨åˆ†å­ã€è¯ä»£åŠ¨åŠ›å­¦å’Œä¸´åºŠå±‚é¢è¯„ä¼°è¯ç‰©ç›¸äº’ä½œç”¨ï¼Œå¹¶æ ¹æ®æ‚£è€…çš„åˆå¹¶ç—‡å’ŒåŒæ—¶ç”¨è¯è¯†åˆ«ç¦å¿Œç—‡ï¼Œé‡èº«å®šåˆ¶æ²»ç–—ç­–ç•¥ã€‚é€šè¿‡æ•´åˆå¤šæ­¥æ¨ç†ã€å®æ—¶çŸ¥è¯†åŸºç¡€å’Œå·¥å…·è¾…åŠ©å†³ç­–ï¼ŒTxAgentç¡®ä¿æ²»ç–—å»ºè®®ç¬¦åˆæ—¢å®šçš„ä¸´åºŠæŒ‡å—å’Œç°å®ä¸–ç•Œè¯æ®ï¼Œä»è€Œé™ä½ä¸è‰¯äº‹ä»¶çš„é£é™©ï¼Œæ”¹å–„æ²»ç–—å†³ç­–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10632",
            "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
            "url": "https://huggingface.co/papers/2503.10632",
            "abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt",
            "score": 8,
            "issue_id": 2731,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "46504216bbce5b86",
            "authors": [
                "Subhajit Maity",
                "Killian Hitsman",
                "Xin Li",
                "Aritra Dutta"
            ],
            "affiliations": [
                "Department of Computer Science, University of Central Florida, Orlando, FL, USA",
                "Department of Mathematics, University of Central Florida, Orlando, FL, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10632.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´ ÑĞµÑ‚Ğ¸ Ğ² Vision Transformers",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ - ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (KArAt) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision Transformer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ KArAt Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ViT Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞµÑ‚Ğ¸ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Complex Relationships with Learnable Activations in Vision Transformers",
                    "desc": "Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency."
                },
                "zh": {
                    "title": "æ¢ç´¢å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ½œåŠ›",
                    "desc": "Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œèƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚å°½ç®¡KANsåœ¨ä¸€ç»´å‡½æ•°çš„ç¬¦å·è¡¨ç¤ºå’ŒæŒç»­å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰ç­‰å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡é¦–æ¬¡ä¸ºæ™®é€šçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰è®¾è®¡äº†ä¸€ç§é€šç”¨çš„å¯å­¦ä¹ Kolmogorov-Arnoldæ³¨æ„åŠ›ï¼ˆKArAtï¼‰ï¼Œå¹¶æå‡ºäº†æ›´æ¨¡å—åŒ–çš„Fourier-KArAtç‰ˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFourier-KArAtåŠå…¶å˜ä½“åœ¨CIFAR-10ã€CIFAR-100å’ŒImageNet-1Kæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæˆ–ä¸ViTç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06542",
            "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy",
            "url": "https://huggingface.co/papers/2503.06542",
            "abstract": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
            "score": 7,
            "issue_id": 2737,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 9",
                "zh": "3æœˆ9æ—¥"
            },
            "hash": "4b19cfc1e459fb2f",
            "authors": [
                "Jianwen Sun",
                "Yukang Feng",
                "Chuanhao Li",
                "Fanrui Zhang",
                "Zizhen Li",
                "Jiaxin Ai",
                "Sizhuo Zhou",
                "Yu Dai",
                "Shenglin Zhang",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Nankai University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "University of Science and Technology of China",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06542.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#dataset",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ARMOR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ARMOR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ARMOR Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ARMOR: Efficient Multimodal Mastery with Minimal Resources",
                    "desc": "The paper introduces ARMOR, a new framework designed to enhance multimodal understanding and generation in machine learning models. Unlike existing unified models that require extensive computational resources, ARMOR is resource-efficient and fine-tunes large language models to achieve its goals. It employs an asymmetric encoder-decoder architecture to seamlessly integrate text and image data, allowing for natural interleaved generation. Additionally, ARMOR utilizes a carefully curated dataset and a novel training algorithm to improve the multimodal capabilities of existing models while maintaining their understanding abilities."
                },
                "zh": {
                    "title": "ARMORï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºARMORçš„ç»Ÿä¸€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„æ•ˆç‡ã€‚ARMORé€šè¿‡å¾®è°ƒç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå®ç°äº†æ–‡æœ¬å’Œå›¾åƒçš„è‡ªç„¶äº¤ç»‡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸å¯¹ç§°çš„ç¼–ç -è§£ç æ¶æ„ï¼Œå¹¶å¼•å…¥äº†å‰å‘åˆ‡æ¢æœºåˆ¶ï¼Œä»¥å‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARMORèƒ½å¤Ÿåœ¨æœ‰é™çš„è®­ç»ƒèµ„æºä¸‹ï¼Œæ˜¾è‘—æå‡ç°æœ‰æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06553",
            "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
            "url": "https://huggingface.co/papers/2503.06553",
            "abstract": "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.",
            "score": 6,
            "issue_id": 2737,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 9",
                "zh": "3æœˆ9æ—¥"
            },
            "hash": "9546d0d9f897e116",
            "authors": [
                "Jiaxin Ai",
                "Pengfei Zhou",
                "Zhaopan Xu",
                "Ming Li",
                "Fanrui Zhang",
                "Zizhen Li",
                "Jianwen Sun",
                "Yukang Feng",
                "Baojin Huang",
                "Zhongyuan Wang",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HZAU",
                "NKU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "USTC",
                "WHU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06553.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#multimodal",
                    "#science",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-ÑÑƒĞ´ÑŒÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ProJudgeBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2400 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 50 000 Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ProJudge-173k Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ."
                },
                "en": {
                    "title": "Enhancing MLLM Reliability with ProJudgeBench",
                    "desc": "This paper addresses the challenges faced by multi-modal large language models (MLLMs) in solving scientific problems accurately. It introduces ProJudgeBench, a benchmark designed to evaluate the reasoning abilities of MLLM-based judges through a comprehensive set of test cases and detailed annotations. The study highlights a performance gap between open-source and proprietary models, indicating the need for improvement in the evaluation process. To enhance this, the authors propose ProJudge-173k, a dataset for instruction-tuning and a new fine-tuning strategy that promotes better reasoning in problem-solving."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¿‡ç¨‹è¯„ä¼°èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ProJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿‡ç¨‹åˆ¤æ–­èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«2400ä¸ªæµ‹è¯•æ¡ˆä¾‹å’Œ50118ä¸ªæ­¥éª¤çº§æ ‡ç­¾ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦é¢†åŸŸï¼Œå…·æœ‰ä¸åŒçš„éš¾åº¦å’Œå¤šæ¨¡æ€å†…å®¹ã€‚æ¯ä¸ªæ­¥éª¤éƒ½ç”±äººç±»ä¸“å®¶ä»”ç»†æ³¨é‡Šï¼Œä»¥ä¾¿ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹ã€åˆ†ç±»å’Œè¯Šæ–­é”™è¯¯æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨ProJudgeBenchä¸Šçš„è¯„ä¼°ï¼Œå‘ç°å¼€æºæ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¹¶æå‡ºäº†ProJudge-173kæ•°æ®é›†å’ŒåŠ¨æ€åŒé˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜å¼€æºæ¨¡å‹çš„è¿‡ç¨‹è¯„ä¼°èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09279",
            "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
            "url": "https://huggingface.co/papers/2503.09279",
            "abstract": "Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.",
            "score": 5,
            "issue_id": 2730,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "edf6712b564fd37a",
            "authors": [
                "Luozheng Qin",
                "Zhiyu Tan",
                "Mengping Yang",
                "Xiaomeng Yang",
                "Hao Li"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09279.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#synthetic",
                    "#alignment"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "Cockatiel: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ (VDC) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Cockatiel. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cockatiel-13B Ğ¸ ĞµĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Cockatiel-8B. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Cockatiel Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ VDCSCORE Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Bridging Vision and Language with Cockatiel for Enhanced Video Captioning",
                    "desc": "This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘æè¿°çš„æ™ºèƒ½åŒ–ä¸äººæ€§åŒ–",
                    "desc": "è§†é¢‘è¯¦ç»†æè¿°ï¼ˆVDCï¼‰æ˜¯è¿æ¥è§†è§‰å’Œè¯­è¨€çš„é‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¯¹å¤æ‚è§†é¢‘å†…å®¹è¿›è¡Œç»†è‡´çš„æè¿°ã€‚æœ¬æ–‡é¦–å…ˆå¯¹å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶ç³»ç»Ÿåœ°è¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šå¯¹ç‰¹å®šæè¿°æ–¹é¢çš„åè§èƒ½åŠ›å’Œä¸äººç±»åå¥½çš„ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Cockatielï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆäº†åˆæˆå’Œäººç±»å¯¹é½çš„è®­ç»ƒï¼Œä»¥æé«˜VDCæ€§èƒ½ã€‚é€šè¿‡å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VDCSCOREä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨ä¸äººç±»åå¥½çš„æ¯”è¾ƒä¸­å¤§å¹…è¶…è¶Šäº†é¢†å…ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10696",
            "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
            "url": "https://huggingface.co/papers/2503.10696",
            "abstract": "Visual autoregressive models typically adhere to a raster-order ``next-token prediction\" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction\" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.",
            "score": 5,
            "issue_id": 2738,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "5adda6787d6613db",
            "authors": [
                "Yefei He",
                "Yuanyu He",
                "Shaoxuan He",
                "Feng Chen",
                "Hong Zhou",
                "Kaipeng Zhang",
                "Bohan Zhuang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory, China",
                "The University of Adelaide, Australia",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10696.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#games",
                    "#benchmark",
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "NAR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Neighboring Autoregressive Modeling (NAR). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, NAR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑĞµĞ´Ğ°', ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Visual Generation with Neighboring Autoregressive Modeling",
                    "desc": "This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage."
                },
                "zh": {
                    "title": "é‚»è¿‘è‡ªå›å½’å»ºæ¨¡ï¼šæå‡è§†è§‰ç”Ÿæˆæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œç§°ä¸ºé‚»è¿‘è‡ªå›å½’å»ºæ¨¡ï¼ˆNARï¼‰ï¼Œæ—¨åœ¨æ”¹å–„ä¼ ç»Ÿçš„åŸºäºå…‰æ …é¡ºåºçš„â€œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹â€æ–¹æ³•ã€‚NARé€šè¿‡è¿‘é‚»é¢„æµ‹æœºåˆ¶ï¼Œå°†è‡ªå›å½’è§†è§‰ç”Ÿæˆè§†ä¸ºä¸€ç§é€æ­¥æ‰©å±•çš„è¿‡ç¨‹ï¼Œä»åˆå§‹æ ‡è®°å¼€å§‹ï¼ŒæŒ‰æ›¼å“ˆé¡¿è·ç¦»é€æ­¥è§£ç å‰©ä½™æ ‡è®°ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç»„é¢å‘ç»´åº¦çš„è§£ç å¤´ï¼Œå…è®¸åœ¨ç©ºé—´-æ—¶é—´ç©ºé—´ä¸­å¹¶è¡Œé¢„æµ‹å¤šä¸ªç›¸é‚»æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—å‡å°‘ç”Ÿæˆæ‰€éœ€çš„æ¨¡å‹å‰å‘æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNARåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ååé‡å’Œæ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.06674",
            "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
            "url": "https://huggingface.co/papers/2503.06674",
            "abstract": "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
            "score": 5,
            "issue_id": 2732,
            "pub_date": "2025-03-09",
            "pub_date_card": {
                "ru": "9 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 9",
                "zh": "3æœˆ9æ—¥"
            },
            "hash": "1ce5d8eb2086abfc",
            "authors": [
                "Yihong Luo",
                "Tianyang Hu",
                "Jiacheng Sun",
                "Yujun Cai",
                "Jing Tang"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.06674.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "TDM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Trajectory Distribution Matching (TDM). TDM Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. TDM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº SDXL Ğ¸ PixArt-alpha, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Few-Step Diffusion with TDM",
                    "desc": "This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹é‡‡æ ·æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åŠ é€Ÿæ‰©æ•£æ¨¡å‹é‡‡æ ·å¯¹äºé«˜æ•ˆçš„AIGCéƒ¨ç½²è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ å°‘æ­¥æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç§°ä¸ºè½¨è¿¹åˆ†å¸ƒåŒ¹é…ï¼ˆTDMï¼‰ï¼Œå®ƒç»“åˆäº†åˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…çš„ä¼˜ç‚¹ã€‚é€šè¿‡å¼•å…¥æ— æ•°æ®çš„åˆ†æ•°è’¸é¦ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒé‡‡æ ·æ­¥éª¤ä¹‹é—´è§£è€¦å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDMåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡å¹¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10624",
            "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
            "url": "https://huggingface.co/papers/2503.10624",
            "abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.",
            "score": 4,
            "issue_id": 2738,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "c0aca4cafef8e621",
            "authors": [
                "Boqian Li",
                "Haiwen Feng",
                "Zeyu Cai",
                "Michael J. Black",
                "Yuliang Xiu"
            ],
            "affiliations": [
                "Berkeley AI Research (BAIR)",
                "Max Planck Institute for Intelligent Systems",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10624.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸ‘•",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ° 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ»Ğ° Ğº Ğ¾Ğ´ĞµÑ‚Ğ¾Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ETCH Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ»Ğ° Ğº Ğ¾Ğ±Ğ»Ğ°ĞºÑƒ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ´ĞµÑ‚Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ETCH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½ÑƒÑ SE(3)-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ñ‚ĞµĞ»Ğ¾Ğ¼, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ñ‚ĞµĞ»Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğº Ğ¿Ğ¾Ğ·Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚ĞµĞ»Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ETCH Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚ĞµĞ»Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Body Fitting with Equivariant Tightness!",
                    "desc": "The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy."
                },
                "zh": {
                    "title": "ç­‰å˜ç´§è‡´æ‹Ÿåˆï¼šæå‡3Dç©¿è¡£äººç±»æ‹Ÿåˆç²¾åº¦çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç­‰å˜ç´§è‡´æ‹Ÿåˆï¼ˆETCHï¼‰ï¼Œç”¨äºå°†èº«ä½“ä¸3Dç©¿è¡£äººç±»ç‚¹äº‘ç›¸åŒ¹é…ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå¤šé˜¶æ®µä¼˜åŒ–ï¼Œå®¹æ˜“å—åˆ°å§¿åŠ¿åˆå§‹åŒ–çš„å½±å“ï¼Œè€Œå­¦ä¹ å‹æ–¹æ³•åœ¨ä¸åŒå§¿åŠ¿å’Œæœè£…ç±»å‹çš„æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨å›°éš¾ã€‚ETCHé€šè¿‡å±€éƒ¨è¿‘ä¼¼çš„SE(3)ç­‰å˜æ€§æ¥ä¼°è®¡å¸ƒæ–™ä¸èº«ä½“è¡¨é¢çš„æ˜ å°„ï¼Œå¹¶å°†ç´§è‡´åº¦ç¼–ç ä¸ºä»å¸ƒæ–™è¡¨é¢åˆ°èº«ä½“çš„ä½ç§»å‘é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒETCHåœ¨æ¾æ•£è¡£ç‰©çš„èº«ä½“æ‹Ÿåˆç²¾åº¦å’Œå½¢çŠ¶ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10620",
            "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
            "url": "https://huggingface.co/papers/2503.10620",
            "abstract": "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.",
            "score": 3,
            "issue_id": 2739,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "6fc0a5f12205a8bd",
            "authors": [
                "Kshitij Ambilduke",
                "Ben Peters",
                "Sonal Sannigrahi",
                "Anil Keshwani",
                "Tsz Kin Lam",
                "Bruno Martins",
                "Marcely Zanon Boito",
                "AndrÃ© F. T. Martins"
            ],
            "affiliations": [
                "ELLIS Unit Lisbon",
                "INESC-ID",
                "Instituto Superior TÃ©cnico, Universidade de Lisboa",
                "Instituto de TelecomunicaÃ§Ãµes",
                "NAVER LABS Europe",
                "Paris-Saclay University",
                "Sapienza University of Rome",
                "Unbabel",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10620.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#machine_translation",
                    "#open_source",
                    "#multimodal",
                    "#low_resource"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµÑ‡ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€ĞµÑ‡ÑŒÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ²Ğ¾Ğ´ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TOWER. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SPIRE ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºÑƒÑ Ñ€ĞµÑ‡ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ TOWER Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM."
                },
                "en": {
                    "title": "Integrating Speech into Multilingual LLMs for Enhanced Performance",
                    "desc": "This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use."
                },
                "zh": {
                    "title": "å°†è¯­éŸ³èå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è¯­è¨€å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚åˆä¸å¤šæ¨¡æ€ï¼ˆå¦‚å›¾åƒæˆ–è¯­éŸ³ï¼‰ç»“åˆã€‚æœ¬æ–‡å°†ç°æœ‰çš„LLMæ‰©å±•åˆ°è¯­éŸ³æ¨¡æ€ï¼Œé€šè¿‡è¯­éŸ³ç¦»æ•£åŒ–å’ŒæŒç»­é¢„è®­ç»ƒæ¥å®ç°ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¤šè¯­è¨€LLMï¼Œä¾‹å¦‚TOWERï¼Œå› ä¸ºå®ƒçš„é¢„è®­ç»ƒè®¾ç½®å…è®¸æˆ‘ä»¬å°†ç¦»æ•£åŒ–çš„è¯­éŸ³è¾“å…¥è§†ä¸ºé¢å¤–çš„ç¿»è¯‘è¯­è¨€ã€‚æœ€ç»ˆç”Ÿæˆçš„å¼€æºæ¨¡å‹SPIREèƒ½å¤Ÿè½¬å½•å’Œç¿»è¯‘è‹±è¯­è¯­éŸ³è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒTOWERåœ¨ç¿»è¯‘ç›¸å…³ä»»åŠ¡ä¸Šçš„åŸå§‹æ€§èƒ½ï¼Œè¯æ˜äº†åœ¨LLMé€‚åº”è¿‡ç¨‹ä¸­å°†ç¦»æ•£è¯­éŸ³è¾“å…¥ä½œä¸ºé¢å¤–è¯­è¨€çš„å¯è¡Œæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11629",
            "title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing",
            "url": "https://huggingface.co/papers/2503.11629",
            "abstract": "We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.",
            "score": 2,
            "issue_id": 2744,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "5f56461a339a86bb",
            "authors": [
                "Stefan Lionar",
                "Jiabin Liang",
                "Gim Hee Lee"
            ],
            "affiliations": [
                "Garena",
                "National University of Singapore",
                "Sea AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11629.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑĞµÑ‚Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "TreeMeshGPT - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¹ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ² ÑĞµÑ‚ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞµÑ‚ĞºĞµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞµÑ‚ĞºĞ¸. TreeMeshGPT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Mesh Generation with TreeMeshGPT",
                    "desc": "TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques."
                },
                "zh": {
                    "title": "TreeMeshGPTï¼šé«˜è´¨é‡è‰ºæœ¯ç½‘æ ¼ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†TreeMeshGPTï¼Œè¿™æ˜¯ä¸€ç§è‡ªå›å½’Transformerï¼Œæ—¨åœ¨ç”Ÿæˆä¸è¾“å…¥ç‚¹äº‘å¯¹é½çš„é«˜è´¨é‡è‰ºæœ¯ç½‘æ ¼ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’Transformerçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªå›å½’æ ‘åºåˆ—åŒ–æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å¢é•¿çš„æ ‘ç»“æ„æ¥æ£€ç´¢ä¸‹ä¸€ä¸ªè¾“å…¥æ ‡è®°ã€‚æˆ‘ä»¬çš„åºåˆ—åŒ–æ–¹æ³•ä½¿å¾—ç½‘æ ¼èƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥ä»æœ€åç”Ÿæˆçš„ä¸‰è§’é¢å±€éƒ¨æ‰©å±•ï¼Œä»è€Œé™ä½è®­ç»ƒéš¾åº¦å¹¶æé«˜ç½‘æ ¼è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†æ¯ä¸ªä¸‰è§’é¢è¡¨ç¤ºä¸ºä¸¤ä¸ªæ ‡è®°ï¼Œå®ç°äº†çº¦22%çš„å‹ç¼©ç‡ï¼Œç”Ÿæˆçš„ç½‘æ ¼åœ¨ç»†èŠ‚å’Œæ³•çº¿æ–¹å‘ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11207",
            "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?",
            "url": "https://huggingface.co/papers/2503.11207",
            "abstract": "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.",
            "score": 2,
            "issue_id": 2744,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "1910487e8b409ffb",
            "authors": [
                "Giacomo Camposampiero",
                "Michael Hersche",
                "Roger Wattenhofer",
                "Abu Sebastian",
                "Abbas Rahimi"
            ],
            "affiliations": [
                "ETH ZÃ¼rich",
                "IBM Research - Zurich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11207.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ²ÑƒÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) - o3-mini Ğ¾Ñ‚ OpenAI Ğ¸ DeepSeek R1 - Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² IQ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ Ğ°Ğ²ĞµĞ½Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… I-RAVEN Ğ¸ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ I-RAVEN-X, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… I-RAVEN-X, Ğ²Ğ²ĞµĞ´Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LRM Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ARLC Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Evaluating Reasoning Under Uncertainty in Large Models",
                    "desc": "This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions."
                },
                "zh": {
                    "title": "å¤§å‹æ¨ç†æ¨¡å‹åœ¨ç±»æ¯”æ¨ç†ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "æœ¬æ–‡é¦–æ¬¡è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼ŒOpenAIçš„o3-miniå’ŒDeepSeek R1ï¼Œåœ¨ç±»æ¯”æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨åŸºäºRavenæ¸è¿›çŸ©é˜µçš„éè¯­è¨€äººç±»æ™ºå•†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨I-RAVENæ•°æ®é›†åŠå…¶æ›´éš¾çš„æ‰©å±•ç‰ˆæœ¬I-RAVEN-Xè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œåè€…æµ‹è¯•æ¨¡å‹å¯¹æ›´é•¿æ¨ç†è§„åˆ™å’Œå±æ€§å€¼èŒƒå›´çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°è§†è§‰ä¸ç¡®å®šæ€§å¯¹è¿™äº›éè¯­è¨€ç±»æ¯”æ¨ç†æµ‹è¯•çš„å½±å“ï¼Œæˆ‘ä»¬æ‰©å±•äº†I-RAVEN-Xæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤ç§ç­–ç•¥æ¥æ¨¡æ‹Ÿä¸å®Œç¾çš„è§†è§‰æ„ŸçŸ¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o3-miniåœ¨I-RAVEN-Xä¸Šçš„ä»»åŠ¡å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œä»86.6%é™è‡³ä»…17.0%ï¼Œè€ŒARLCæ¨¡å‹åœ¨æ‰€æœ‰è¿™äº›åˆ†å¸ƒå¤–æµ‹è¯•ä¸­ä¿æŒäº†å¼ºå¤§çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10684",
            "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
            "url": "https://huggingface.co/papers/2503.10684",
            "abstract": "Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.",
            "score": 2,
            "issue_id": 2740,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "004932028027606e",
            "authors": [
                "Jingwen Deng",
                "Zihao Wang",
                "Shaofei Cai",
                "Anji Liu",
                "Yitao Liang"
            ],
            "affiliations": [
                "Peking University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10684.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#video",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Skill Boundary Detection (SBD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Minecraft Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ YouTube Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Segmenting Skills for Smarter Agents",
                    "desc": "This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£å­¦ä¹ åŠ©åŠ›æŠ€èƒ½è¾¹ç•Œæ£€æµ‹",
                    "desc": "åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å­¦ä¹ æŠ€èƒ½å¯¹äºå¼€å‘èƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡çš„æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œå¯ä»¥å°†é•¿è§†é¢‘åˆ†å‰²æˆä¸€ç³»åˆ—è¯­ä¹‰æ˜ç¡®ä¸”æŠ€èƒ½ä¸€è‡´çš„ç‰‡æ®µï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æµ‹é¢„æµ‹è¯¯å·®æ¥è¯†åˆ«æŠ€èƒ½è¾¹ç•Œï¼Œå‡è®¾é¢„æµ‹è¯¯å·®çš„æ˜¾è‘—å¢åŠ è¡¨æ˜æŠ€èƒ½çš„è½¬å˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨Minecraftä¸­æ˜¾è‘—æé«˜äº†æ¡ä»¶ç­–ç•¥å’Œå±‚æ¬¡ä»£ç†çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08111",
            "title": "MaRI: Material Retrieval Integration across Domains",
            "url": "https://huggingface.co/papers/2503.08111",
            "abstract": "Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.",
            "score": 2,
            "issue_id": 2738,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 11",
                "zh": "3æœˆ11æ—¥"
            },
            "hash": "f3b97bb0031c6d57",
            "authors": [
                "Jianhui Wang",
                "Zhifei Yang",
                "Yangfan He",
                "Huixiong Zhang",
                "Yuxuan Chen",
                "Jingwei Huang"
            ],
            "affiliations": [
                "Fudan University",
                "Peking University",
                "Tencent Hunyuan3D",
                "University of Electronic Science and Technology of China",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08111.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#optimization",
                    "#dataset",
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "MaRI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ 3D-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MaRI - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MaRI Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap in Material Retrieval with MaRI",
                    "desc": "This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks."
                },
                "zh": {
                    "title": "MaRIï¼šæå‡ææ–™æ£€ç´¢çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "å‡†ç¡®çš„ææ–™æ£€ç´¢å¯¹äºåˆ›å»ºçœŸå®çš„3Dèµ„äº§è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ•æ‰å½¢çŠ¶ä¸å˜å’Œå…‰ç…§å˜åŒ–çš„ææ–™è¡¨ç¤ºçš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ç¨€ç¼ºä¸”é¢ä¸´å¤šæ ·æ€§ä¸è¶³å’Œç°å®ä¸–ç•Œæ³›åŒ–ä¸è‰¯çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†MaRIæ¡†æ¶ï¼Œæ—¨åœ¨å¼¥åˆåˆæˆææ–™å’ŒçœŸå®ææ–™ä¹‹é—´çš„ç‰¹å¾ç©ºé—´å·®è·ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼ŒMaRIæ„å»ºäº†ä¸€ä¸ªå…±äº«çš„åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—ç›¸ä¼¼çš„ææ–™å’Œå›¾åƒåœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´æ¥è¿‘ï¼ŒåŒæ—¶å°†ä¸ç›¸ä¼¼çš„å¯¹åˆ†å¼€ï¼Œä»è€Œæé«˜äº†ææ–™æ£€ç´¢çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.05689",
            "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2503.05689",
            "abstract": "We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.",
            "score": 2,
            "issue_id": 2731,
            "pub_date": "2025-03-07",
            "pub_date_card": {
                "ru": "7 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 7",
                "zh": "3æœˆ7æ—¥"
            },
            "hash": "eef61e2f2b4c0760",
            "authors": [
                "Zebin Xing",
                "Xingyu Zhang",
                "Yang Hu",
                "Bo Jiang",
                "Tong He",
                "Qian Zhang",
                "Xiaoxiao Long",
                "Wei Yin"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Huazhong University of Science & Technology",
                "Nanjing University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.05689.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "GoalFlow: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "GoalFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ²Ğ²Ğ¾Ğ´Ñ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹. GoalFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Flow Matching Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GoalFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "GoalFlow: Driving the Future with High-Quality Multimodal Trajectories",
                    "desc": "GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps."
                },
                "zh": {
                    "title": "GoalFlowï¼šé«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†GoalFlowï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ï¼Œé€šå¸¸æ²¡æœ‰å•ä¸€åˆé€‚çš„è½¨è¿¹ï¼Œæœ€è¿‘çš„æ–¹æ³•è¶Šæ¥è¶Šå…³æ³¨å¤šæ¨¡æ€è½¨è¿¹åˆ†å¸ƒçš„å»ºæ¨¡ã€‚ä¸ºäº†å…‹æœè½¨è¿¹é€‰æ‹©çš„å¤æ‚æ€§å’Œè½¨è¿¹è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼ŒGoalFlowé€šè¿‡å¼•å…¥ç›®æ ‡ç‚¹æ¥æœ‰æ•ˆçº¦æŸç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGoalFlowåœ¨NavsimDauner2024_navsimä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæä¾›äº†ç¨³å¥çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11958",
            "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital\n  Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts",
            "url": "https://huggingface.co/papers/2503.11958",
            "abstract": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor scenes, designed to create house-scale, collision-free, and hierarchically structured indoor digital twins. In contrast to existing methods that directly synthesize the scene layout as a scene graph or object list, CHOrD incorporates a 2D image-based intermediate layout representation, enabling effective prevention of collision artifacts by successfully capturing them as out-of-distribution (OOD) scenarios during generation. Furthermore, unlike existing methods, CHOrD is capable of generating scene layouts that adhere to complex floor plans with multi-modal controls, enabling the creation of coherent, house-wide layouts robust to both geometric and semantic variations in room structures. Additionally, we propose a novel dataset with expanded coverage of household items and room configurations, as well as significantly improved data quality. CHOrD demonstrates state-of-the-art performance on both the 3D-FRONT and our proposed datasets, delivering photorealistic, spatially coherent indoor scene synthesis adaptable to arbitrary floor plan variations.",
            "score": 1,
            "issue_id": 2752,
            "pub_date": "2025-03-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 15",
                "zh": "3æœˆ15æ—¥"
            },
            "hash": "882c78a449cee429",
            "authors": [
                "Chong Su",
                "Yingbin Fu",
                "Zheyuan Hu",
                "Jing Yang",
                "Param Hanji",
                "Shaojun Wang",
                "Xuan Zhao",
                "Cengiz Ã–ztireli",
                "Fangcheng Zhong"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, University of Cambridge",
                "KE Holdings Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11958.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ 3D-Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ°Ğ¼",
                    "desc": "CHOrD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ°Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ»Ğ»Ğ¸Ğ·Ğ¸Ğ¹ Ğ¸ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ 2D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹. CHOrD ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… 3D-FRONT Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ°."
                },
                "en": {
                    "title": "Revolutionizing 3D Indoor Scene Synthesis with CHOrD",
                    "desc": "CHOrD is a new framework that helps create realistic 3D indoor scenes, focusing on making them collision-free and well-structured. It uses a unique approach by first generating a 2D layout from images, which helps avoid errors in the final scene. This method allows for the creation of detailed layouts that can follow complex floor plans and adapt to different room designs. Additionally, CHOrD comes with a new dataset that includes a wide variety of household items and room types, improving the quality of the generated scenes."
                },
                "zh": {
                    "title": "CHOrDï¼šæ™ºèƒ½åˆæˆæ— ç¢°æ’çš„3Då®¤å†…åœºæ™¯",
                    "desc": "CHOrDæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºå¯æ‰©å±•çš„3Då®¤å†…åœºæ™¯åˆæˆï¼Œæ—¨åœ¨åˆ›å»ºæ— ç¢°æ’ä¸”å±‚æ¬¡ç»“æ„åŒ–çš„å®¤å†…æ•°å­—åŒèƒèƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›´æ¥åˆæˆåœºæ™¯å›¾æˆ–ç‰©ä½“åˆ—è¡¨ä¸åŒï¼ŒCHOrDé‡‡ç”¨åŸºäº2Då›¾åƒçš„ä¸­é—´å¸ƒå±€è¡¨ç¤ºï¼Œæœ‰æ•ˆé˜²æ­¢ç¢°æ’ä¼ªå½±ã€‚å®ƒèƒ½å¤Ÿç”Ÿæˆç¬¦åˆå¤æ‚å¹³é¢å›¾çš„åœºæ™¯å¸ƒå±€ï¼Œå¹¶å…·å¤‡å¤šæ¨¡æ€æ§åˆ¶ï¼Œç¡®ä¿æˆ¿é—´ç»“æ„åœ¨å‡ ä½•å’Œè¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œè¦†ç›–äº†æ›´å¤šå®¶åº­ç‰©å“å’Œæˆ¿é—´é…ç½®ï¼Œæ•°æ®è´¨é‡æ˜¾è‘—æé«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09330",
            "title": "Group-robust Machine Unlearning",
            "url": "https://huggingface.co/papers/2503.09330",
            "abstract": "Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.",
            "score": 0,
            "issue_id": 2739,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "64a6c82be0db725d",
            "authors": [
                "Thomas De Min",
                "Subhankar Roy",
                "StÃ©phane LathuiliÃ¨re",
                "Elisa Ricci",
                "Massimiliano Mancini"
            ],
            "affiliations": [
                "Fondazione Bruno Kessler",
                "Inria Grenoble, Univ. Grenoble Alpes",
                "LTCI, Telecom Paris, Institut Polytechnique de Paris",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09330.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#ethics",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MIU, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…. MIU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MIU Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Fair and Effective Machine Unlearning for Diverse Data Groups",
                    "desc": "This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning."
                },
                "zh": {
                    "title": "ç»„é²æ£’æ€§æœºå™¨é—å¿˜ï¼šå…¬å¹³æ€§ä¸æ€§èƒ½çš„å¹³è¡¡",
                    "desc": "æœºå™¨é—å¿˜æ˜¯ä¸€ç§æ–°å…´çš„èŒƒå¼ï¼Œæ—¨åœ¨ä»æ¨¡å‹ä¸­å»é™¤ç‰¹å®šè®­ç»ƒæ•°æ®çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™å…¶å¯¹å…¶ä»–æ•°æ®çš„çŸ¥è¯†ã€‚ä»¥å¾€çš„æ–¹æ³•å‡è®¾é—å¿˜æ•°æ®å‡åŒ€åˆ†å¸ƒï¼Œä½†å¦‚æœè¦é—å¿˜çš„æ•°æ®åœ¨æŸä¸€ç»„ä¸­å ä¸»å¯¼åœ°ä½ï¼Œæ¨¡å‹åœ¨è¯¥ç»„çš„æ€§èƒ½ä¼šä¸‹é™ï¼Œå¯¼è‡´å…¬å¹³æ€§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡æ ·æœ¬åˆ†å¸ƒé‡åŠ æƒæ¥ç¼“è§£ä¸»å¯¼ç»„çš„æ€§èƒ½æŸå¤±ï¼Œè§£å†³äº†éå‡åŒ€åˆ†å¸ƒé—å¿˜é›†çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MIUï¼ˆäº’ä¿¡æ¯æ„ŸçŸ¥æœºå™¨é—å¿˜ï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è¿‘ä¼¼æœºå™¨é—å¿˜çš„ç»„é²æ£’æ€§æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘ä¸»å¯¼ç»„æ€§èƒ½ä¸‹é™çš„åŒæ—¶å®ç°é—å¿˜ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-17.html",
    "link_next": "2025-03-19.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "17.03",
        "en": "03/17",
        "zh": "3æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.03",
        "en": "03/19",
        "zh": "3æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 10,
        "#data": 4,
        "#benchmark": 8,
        "#agents": 5,
        "#cv": 10,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 5,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 13,
        "#robotics": 1,
        "#agi": 0,
        "#games": 5,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 11,
        "#survey": 3,
        "#diffusion": 4,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 8,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¸€ç§åä¸ºReCamMasterçš„è§†é¢‘é‡æ–°æ¸²æŸ“æ¡†æ¶ã€‚å®ƒå¯ä»¥æ”¹å˜ç»™å®šè§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒå¤šå¸§å¤–è§‚å’ŒåŠ¨æ€åŒæ­¥ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡Unreal Engine 5æ„å»ºäº†ä¸€ä¸ªå¤šæ‘„åƒæœºåŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç¨³å®šã€è¶…åˆ†è¾¨ç‡å’Œè§†é¢‘æ‰©å±•æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
        "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä¸€ç§åä¸ºReCamMasterçš„è§†é¢‘é‡æ–°æ¸²æŸ“æ¡†æ¶ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le yÄ« zhÇ’ng mÃ­ngwÃ¨i ReCamMaster de shÃ¬pÃ­n chÃ³ngxÄ«n xuÃ njiÃ n kuÃ ngjiÃ .\n\nå®ƒå¯ä»¥æ”¹å˜ç»™å®šè§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒå¤šå¸§å¤–è§‚å’ŒåŠ¨æ€åŒæ­¥ã€‚\nTÄ kÄ›yÇ gÇibiÃ n gÄ›idÃ¬ng shÃ¬pÃ­n de shÃ¨xiÃ ngjÄ« guÇjÄ«, tÃ³ngshÃ­ bÇochÃ­ duÅzhÄ“n wÃ iguÇn hÃ© dÃ²ngtÃ i tÃ³ngbÃ¹.\n\nè¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡Unreal Engine 5æ„å»ºäº†ä¸€ä¸ªå¤šæ‘„åƒæœºåŒæ­¥è§†é¢‘æ•°æ®é›†ã€‚\nGÇi fÄngfÇ lÃ¬yÃ²ng le yÃ¹ xÃ¹nliÃ n de wÃ©nbÄ›n dÃ o shÃ¬pÃ­n mÃ³xÃ­ng de shÄ“ngchÄ“ng nÃ©nglÃ¬, bÃ¬ng tÅngguÃ² Unreal Engine 5 gÃ²ujiÃ n le yÄ«gÃ¨ duÅ shÃ¨xiÃ ngjÄ« tÃ³ngbÃ¹ shÃ¬pÃ­n shÃ¹jÃ¹jÃ­.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç¨³å®šã€è¶…åˆ†è¾¨ç‡å’Œè§†é¢‘æ‰©å±•æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, zhÃ¨ zhÇ’ng fÄngfÇ zÃ i shÃ¬pÃ­n wÄ›ndÃ¬ng, chÄo fÄ“nbiÄnlÇœ hÃ© shÃ¬pÃ­n kuÃ²zhÇn fÄngmiÃ n biÇoxiÃ n chÅ«sÃ¨.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'åä¸º', 'pinyin': 'mÃ­ng wÃ©i', 'trans': 'named'},\n{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},\n{'word': 'é‡æ–°', 'pinyin': 'chÃ³ng xÄ«n', 'trans': 'again'},\n{'word': 'æ¸²æŸ“', 'pinyin': 'xuÃ n rÃ¡n', 'trans': 'render'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'æ‘„åƒæœº', 'pinyin': 'shÃ¨ xiÃ ng jÄ«', 'trans': 'camera'},\n{'word': 'è½¨è¿¹', 'pinyin': 'guÇ jÃ¬', 'trans': 'trajectory'},\n{'word': 'åŒæ—¶', 'pinyin': 'tÃ³ng shÃ­', 'trans': 'simultaneously'},\n{'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'},\n{'word': 'å¤šå¸§', 'pinyin': 'duÅ zhÄ“n', 'trans': 'multi-frame'},\n{'word': 'å¤–è§‚', 'pinyin': 'wÃ i guÇn', 'trans': 'appearance'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'åŒæ­¥', 'pinyin': 'tÃ³ng bÃ¹', 'trans': 'synchronization'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-trained'},\n{'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'æ„å»º', 'pinyin': 'gÃ²u jiÃ n', 'trans': 'construct'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'display'},\n{'word': 'ç¨³å®š', 'pinyin': 'wÄ›n dÃ¬ng', 'trans': 'stable'},\n{'word': 'è¶…åˆ†è¾¨ç‡', 'pinyin': 'chÄo fÄ“n biÃ n lÇœ', 'trans': 'super-resolution'},\n{'word': 'æ‰©å±•', 'pinyin': 'kuÃ² zhÇn', 'trans': 'extend'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}]",
        "trans": "This article discusses a video re-rendering framework called ReCamMaster. It can alter the camera trajectory of a given video while maintaining multi-frame appearance and dynamic synchronization. The method leverages the generative capabilities of a pre-trained text-to-video model and constructs a multi-camera synchronized video dataset using Unreal Engine 5. Experimental results demonstrate that this method performs excellently in video stabilization, super-resolution, and video extension.",
        "update_ts": "2025-03-17 09:12"
    }
}