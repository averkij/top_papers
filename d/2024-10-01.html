
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. October 1.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">1 октября</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-30.html">⬅️ <span id="prev-date">30.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-02.html">➡️ <span id="next-date">02.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'};
        let feedDateNext = {'ru': '02.10', 'en': '10/02', 'zh': '10月2日'};
        let feedDatePrev = {'ru': '30.09', 'en': '09/30', 'zh': '9月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.20566', 'title': 'MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning', 'url': 'https://huggingface.co/papers/2409.20566', 'abstract': 'We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development.', 'score': 51, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '4ee08e694f7ab69e', 'data': {'categories': ['#science', '#video', '#cv', '#training', '#data', '#optimization', '#benchmark', '#small_models', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное обучение: Сила данных и архитектуры', 'desc': 'MM1.5 - это новое семейство мультимодальных больших языковых моделей (MLLM), разработанных для улучшения понимания изображений с текстом, визуальной референции и обоснования, а также рассуждений по нескольким изображениям. Модель использует подход, ориентированный на данные, систематически исследуя влияние различных наборов данных на протяжении всего жизненного цикла обучения. MM1.5 включает варианты от 1B до 30B параметров, охватывая как плотные, так и MoE-архитектуры, и демонстрирует, что тщательный подбор данных и стратегии обучения могут обеспечить высокую производительность даже при небольших масштабах. Авторы также представляют специализированные варианты для понимания видео и мобильных интерфейсов.'}, 'en': {'title': 'Unlocking Multimodal Understanding with MM1.5', 'desc': 'The paper introduces MM1.5, a new family of multimodal large language models (MLLMs) that improve understanding of images with text, visual referencing, and reasoning across multiple images. It builds on the previous MM1 architecture by using a data-centric approach, focusing on the effects of various data types throughout the training process. The models, which range from 1B to 30B parameters, utilize high-quality OCR data and synthetic captions for continual pre-training, along with optimized data for supervised fine-tuning. Additionally, two specialized versions are presented: MM1.5-Video for video comprehension and MM1.5-UI for mobile user interface understanding, with insights from empirical studies guiding future MLLM research.'}, 'zh': {'title': 'MM1.5：多模态语言模型的新突破', 'desc': '我们介绍了MM1.5，这是一种新型的多模态大型语言模型（MLLM），旨在增强文本丰富的图像理解、视觉指代和基础，以及多图像推理的能力。MM1.5基于MM1架构，采用以数据为中心的训练方法，系统地探索不同数据混合对模型训练生命周期的影响。我们的模型参数范围从10亿到300亿，包括密集型和专家混合（MoE）变体，表明精心的数据策划和训练策略即使在小规模（10亿和30亿）下也能取得强劲的性能。此外，我们还推出了两个专门的变体：MM1.5-Video，专为视频理解设计，以及MM1.5-UI，专为移动用户界面理解量身定制。'}}}, {'id': 'https://huggingface.co/papers/2409.18943', 'title': 'Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models', 'url': 'https://huggingface.co/papers/2409.18943', 'abstract': "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler.", 'score': 26, 'issue_id': 1, 'pub_date': '2024-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '38f8e74fd0ae8f74', 'data': {'categories': ['#dataset', '#training', '#optimization', '#agents', '#benchmark', '#open_source', '#architecture'], 'emoji': '📏', 'ru': {'title': 'Точный контроль длины текста в больших языковых моделях', 'desc': 'Статья представляет новый подход под названием Ruler для улучшения способности больших языковых моделей генерировать ответы заданной длины. Авторы вводят задачу генерации целевой длины (TLG) и метрики точного и гибкого соответствия для оценки производительности моделей. Ruler использует мета-токены длины (MLT) для повышения способности моделей следовать инструкциям с ограничениями по длине. Эксперименты показывают значительное улучшение точности генерации заданной длины при использовании Ruler для различных языковых моделей.'}, 'en': {'title': 'Mastering Response Length with Ruler: A New Approach for LLMs', 'desc': "This paper addresses the challenge that large language models (LLMs) face when generating responses of specific lengths. It introduces the Target Length Generation Task (TLG) to evaluate how well these models can adhere to numerical constraints in their outputs. The authors propose a new method called Ruler, which uses Meta Length Tokens (MLTs) to improve the models' ability to follow length-specific instructions. Experimental results demonstrate that Ruler significantly enhances performance across various LLMs, showing its effectiveness and adaptability in generating responses that meet specified length requirements."}, 'zh': {'title': '提升语言模型的长度控制能力', 'desc': '本文探讨了大型语言模型在生成特定长度响应时的能力。我们提出了目标长度生成任务（TLG），并设计了精确匹配（PM）和灵活匹配（FM）两个指标来评估模型的表现。为了提高模型在长度限制下的指令跟随能力，我们引入了一种新颖的无模型方法Ruler，利用元长度标记（MLT）来增强生成能力。实验结果表明，Ruler在不同大型语言模型上的表现显著提升，验证了其有效性和通用性。'}}}, {'id': 'https://huggingface.co/papers/2409.19606', 'title': 'Hyper-Connections', 'url': 'https://huggingface.co/papers/2409.19606', 'abstract': 'We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.', 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'd4f17cdf9d999616', 'data': {'categories': ['#training', '#optimization', '#architecture', '#cv'], 'emoji': '🔗', 'ru': {'title': 'Гиперсвязи: новый путь к улучшению глубоких нейронных сетей', 'desc': 'Статья представляет гиперсвязи как альтернативу остаточным связям в нейронных сетях. Этот метод решает проблемы, связанные с вариантами остаточных связей, такие как эффект качелей между исчезновением градиента и коллапсом представления. Теоретически, гиперсвязи позволяют сети настраивать силу связей между признаками на разных уровнях и динамически перестраивать слои. Эксперименты показали значительное улучшение производительности при предобучении больших языковых моделей и в задачах компьютерного зрения.'}, 'en': {'title': 'Hyper-Connections: A New Path to Enhanced Neural Network Performance', 'desc': 'This paper introduces hyper-connections as a new method to improve neural networks, particularly as an alternative to traditional residual connections. Hyper-connections help to mitigate issues like gradient vanishing and representation collapse, which can hinder model performance. By allowing dynamic adjustment of feature connections across different layers, this method enhances the learning process. Experiments on large language models and vision tasks show that hyper-connections lead to significant performance gains, suggesting their potential for various AI applications.'}, 'zh': {'title': '超连接：提升深度学习模型性能的新方法', 'desc': '本文提出了一种名为超连接的方法，作为残差连接的有效替代方案。该方法专门解决了残差连接变体中常见的缺点，如梯度消失和表示崩溃之间的摇摆效应。理论上，超连接允许网络调整不同深度特征之间连接的强度，并动态重排层次结构。我们在大型语言模型的预训练实验中发现，超连接在性能上显著优于残差连接，并且在视觉任务中的实验也显示出类似的改进。'}}}, {'id': 'https://huggingface.co/papers/2409.19020', 'title': 'DiaSynth -- Synthetic Dialogue Generation Framework', 'url': 'https://huggingface.co/papers/2409.19020', 'abstract': "The scarcity of domain specific dialogue datasets across various domains, from academic topics to everyday conversations, limits the development of dialogue systems for various applications. Existing research is often constrained either by dialogue datasets that are too general or by niche domain dialogue datasets whose scale does not match the required scale for training dialogue systems. To address this gap, we introduce DiaSynth - a synthetic dialogue generation framework capable of generating high quality, contextually rich dialogues across a wide range of domains. Our approach differs from existing frameworks by dynamically generating dialogues that incorporate simulated personas, subtopics, and diverse conversational characteristics, using a Large Language Model (LLM) with Chain of Thought (CoT) reasoning to create contextually rich, domain-specific dialogues that closely mimic natural human interactions. DiaSynth produces tailored dialogues that emulate realistic conversations. We perform our experiments by generating synthetic data using different LLMs and few-shot examples from DialogSum and SAMSum. The pretrained language models fine-tuned on the synthetic data outperform the base models by 16.47%, while the comparison between models fine-tuned on in-domain data and synthetic data shows that the synthetic data is able to capture 90.48% of the distribution of the in-domain data. The quality of the data generated also scales with the size of LLMs. These results validate DiaSynth's potential as a robust alternative to traditional data collection methods.", 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'cb6f2fff11f70974', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#training', '#data', '#dialogue', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'DiaSynth: революция в создании диалоговых датасетов', 'desc': 'Статья представляет DiaSynth - фреймворк для синтетической генерации диалогов в различных предметных областях. Используя большую языковую модель (LLM) с рассуждениями по цепочке мыслей (CoT), DiaSynth создает контекстно-богатые диалоги, имитирующие естественное человеческое общение. Эксперименты показывают, что модели, дообученные на синтетических данных DiaSynth, превосходят базовые модели на 16.47%, а качество генерируемых данных улучшается с увеличением размера LLM. Результаты подтверждают потенциал DiaSynth как альтернативы традиционным методам сбора диалоговых данных.'}, 'en': {'title': 'DiaSynth: Bridging the Gap in Dialogue Data Generation', 'desc': "This paper presents DiaSynth, a framework for generating synthetic dialogue data to overcome the lack of domain-specific dialogue datasets. It utilizes a Large Language Model (LLM) with Chain of Thought (CoT) reasoning to create contextually rich dialogues that reflect natural human interactions. The generated dialogues incorporate simulated personas and diverse conversational characteristics, making them suitable for various applications. Experiments show that models fine-tuned on synthetic data outperform those trained on base models, demonstrating DiaSynth's effectiveness in producing high-quality dialogue data."}, 'zh': {'title': 'DiaSynth：合成对话生成的新方法', 'desc': '本论文介绍了一种名为DiaSynth的合成对话生成框架，旨在解决特定领域对话数据集稀缺的问题。该框架利用大型语言模型（LLM）和思维链（CoT）推理，动态生成高质量、上下文丰富的对话，涵盖多个领域。通过模拟角色、子主题和多样化的对话特征，DiaSynth能够生成更接近自然人类互动的对话。实验结果表明，使用合成数据微调的预训练语言模型在性能上优于基础模型，显示出DiaSynth作为传统数据收集方法的有力替代方案的潜力。'}}}, {'id': 'https://huggingface.co/papers/2409.18747', 'title': 'Cottention: Linear Transformers With Cosine Attention', 'url': 'https://huggingface.co/papers/2409.18747', 'abstract': 'Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '2df6bf8a5fefb135', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Cottention: Эффективное внимание для длинных последовательностей', 'desc': 'Представлен новый механизм внимания Cottention, заменяющий софтмакс косинусным сходством. Cottention имеет линейную сложность по памяти относительно длины последовательности, что делает его более эффективным, чем стандартное внимание. Его можно переформулировать как рекуррентную нейронную сеть с конечным скрытым состоянием, что позволяет использовать постоянный объем памяти при инференсе. Cottention показывает сопоставимую производительность с софтмакс-вниманием на задачах BERT и GPT при значительном снижении требований к памяти.'}, 'en': {'title': 'Cottention: Efficient Attention for Long Sequences', 'desc': 'This paper presents Cottention, a new attention mechanism that replaces the traditional softmax operation with cosine similarity to address the memory challenges of processing long sequences in transformer models. By doing so, Cottention achieves linear memory complexity, making it more efficient than softmax attention, which has quadratic complexity. The authors show that Cottention can be viewed as a recurrent neural network (RNN) with a fixed hidden state, allowing for constant memory usage during inference. Evaluations on BERT and GPT tasks indicate that Cottention performs comparably to softmax attention while significantly reducing memory requirements, supported by a custom CUDA kernel for efficient computation.'}, 'zh': {'title': 'Cottention：高效处理长序列的新选择', 'desc': '本文介绍了一种新的注意力机制Cottention，它用余弦相似度替代了传统的softmax操作。Cottention的内存复杂度与序列长度呈线性关系，这使得它在处理长序列时更加高效。我们将Cottention重新构造为具有有限隐藏状态的递归神经网络（RNN），在推理过程中实现了恒定的内存使用。实验结果表明，Cottention在性能上与softmax注意力相当，但显著降低了内存需求，适合处理更长的序列。'}}}, {'id': 'https://huggingface.co/papers/2409.20551', 'title': 'UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models', 'url': 'https://huggingface.co/papers/2409.20551', 'abstract': 'Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in a unified formulation. Specifically, we constructed a dataset labeled with manipulation-related key attributes, comprising 900 articulated objects from 19 categories and 600 tools from 12 categories. Furthermore, we leverage MLLMs to infer object-centric representations for manipulation tasks, including affordance recognition and reasoning about 3D motion constraints. Comprehensive experiments in both simulation and real-world settings indicate that UniAff significantly improves the generalization of robotic manipulation for tools and articulated objects. We hope that UniAff will serve as a general baseline for unified robotic manipulation tasks in the future. Images, videos, dataset, and code are published on the project website at:https://sites.google.com/view/uni-aff/home', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '57856f760b2f14ce', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#graphs', '#open_source', '#robotics', '#3d'], 'emoji': '🤖', 'ru': {'title': 'UniAff: Единый подход к роботизированной манипуляции объектами', 'desc': 'UniAff - это новый подход к роботизированной манипуляции, объединяющий понимание 3D-ограничений движения и возможностей объектов. Исследователи создали датасет из 900 шарнирных объектов и 600 инструментов с ключевыми атрибутами для манипуляции. Используя мультимодальные языковые модели, система выводит объектно-ориентированные представления для задач манипуляции. Эксперименты показали значительное улучшение обобщающей способности роботов при работе с инструментами и шарнирными объектами.'}, 'en': {'title': 'UniAff: Unifying 3D Manipulation and Task Understanding for Robots', 'desc': 'This paper introduces a new framework called UniAff, which aims to enhance robotic manipulation by integrating 3D object understanding with task comprehension. The authors created a detailed dataset that includes 900 articulated objects and 600 tools, each labeled with key manipulation attributes. They utilize Multi-Label Learning Models (MLLMs) to derive object-centric representations that help robots recognize affordances and understand 3D motion constraints. Experimental results show that UniAff improves the ability of robots to generalize their manipulation skills across different tools and objects, establishing a new baseline for future research in this area.'}, 'zh': {'title': '统一机器人操作的新范式：UniAff', 'desc': '本研究提出了一种新的机器人操作范式，称为UniAff，旨在整合3D物体中心的操作和任务理解。我们构建了一个包含900个关节物体和600个工具的数据集，标注了与操作相关的关键属性。通过利用多模态大语言模型（MLLMs），我们能够推断出与操作任务相关的物体中心表示，包括可用性识别和3D运动约束推理。实验结果表明，UniAff显著提高了机器人在工具和关节物体操作中的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2409.20537', 'title': 'Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers', 'url': 'https://huggingface.co/papers/2409.20537', 'abstract': 'One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'e6dbd0e5f73f5c95', 'data': {'categories': ['#dataset', '#agi', '#training', '#optimization', '#transfer_learning', '#open_source', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов через гетерогенные трансформеры', 'desc': 'Эта статья представляет новый подход к обучению роботов - Heterogeneous Pre-trained Transformers (HPT). HPT предварительно обучает большую общую часть нейронной сети политики для создания универсального представления, независимого от конкретных задач и воплощений робота. Архитектура согласует разнородные входные данные от различных роботов в короткую последовательность токенов. Эксперименты показывают, что HPT превосходит базовые модели и улучшает производительность дообученной политики более чем на 20% на новых задачах.'}, 'en': {'title': 'Unlocking Versatility in Robotics with Heterogeneous Pre-training', 'desc': 'This paper addresses the challenge of training robotic models that can perform well across different types of robots and tasks, a problem known as heterogeneity. The authors introduce Heterogeneous Pre-trained Transformers (HPT), a method that pre-trains a shared neural network to create a flexible representation that works for various robot embodiments and tasks. By using a large amount of diverse data from real-world robots, simulations, and human videos, HPT learns to effectively map sensory inputs to control commands. The results show that HPT significantly improves the performance of fine-tuned policies on new tasks, outperforming existing methods by over 20%.'}, 'zh': {'title': '异质预训练，提升机器人学习能力！', 'desc': '本研究解决了训练通用机器人模型时的异质性问题。以往的机器人学习方法通常只针对特定任务和特定机器人收集数据，导致成本高且容易过拟合。我们提出了异质预训练变换器（HPT），通过在不同机器人和任务的数据上进行预训练，学习任务和机器人无关的共享表示。实验结果表明，HPT在多个基准测试中，针对未见任务的微调策略性能提升超过20%。'}}}, {'id': 'https://huggingface.co/papers/2409.19952', 'title': 'Image Copy Detection for Diffusion Models', 'url': 'https://huggingface.co/papers/2409.19952', 'abstract': 'Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'f2c605d982348d94', 'data': {'categories': ['#dataset', '#cv', '#data', '#benchmark', '#open_source', '#diffusion', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'ICDiff: Инновационный подход к обнаружению копий изображений в эпоху диффузионных моделей', 'desc': 'Статья представляет ICDiff - первую модель обнаружения копий изображений, специализированную для диффузионных моделей. Авторы создали датасет Diffusion-Replication (D-Rep) из 40 000 пар изображение-реплика, сгенерированных с помощью Stable Diffusion V1.5. Предложен новый метод глубокого встраивания PDF-Embedding, преобразующий уровень репликации в функцию плотности вероятности. Эксперименты показали, что PDF-Embedding превосходит другие методы на тестовом наборе D-Rep, а уровень репликации известных диффузионных моделей составляет 10-20%.'}, 'en': {'title': 'Detecting Replication in Diffusion-Generated Images', 'desc': 'This paper introduces ICDiff, a specialized Image Copy Detection (ICD) model designed to address the unique challenges posed by images generated through diffusion models. The authors create a new dataset called Diffusion-Replication (D-Rep), which includes 40,000 image-replica pairs annotated with varying levels of replication. They propose a novel deep embedding method, PDF-Embedding, which uses probability density functions to represent the replication levels, ensuring a smooth transition between levels. Experimental results demonstrate that PDF-Embedding outperforms existing methods, revealing significant replication rates of diffusion models in comparison to a public image gallery.'}, 'zh': {'title': '检测扩散模型生成图像的复制新方法', 'desc': '本论文介绍了一种新的图像复制检测模型ICDiff，专门针对扩散模型生成的图像。我们构建了一个名为D-Rep的数据集，包含40,000对图像复制样本，并将其标注为六个复制级别。我们提出的PDF-Embedding方法将每对图像复制的级别转化为概率密度函数，以此作为监督信号。实验结果表明，PDF-Embedding在检测扩散模型生成的图像复制方面表现优于传统方法。'}}}, {'id': 'https://huggingface.co/papers/2409.19715', 'title': 'Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code', 'url': 'https://huggingface.co/papers/2409.19715', 'abstract': "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and machine-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available.", 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '01303362c974f480', 'data': {'categories': ['#dataset', '#training', '#rl', '#plp', '#benchmark', '#games', '#open_source', '#rlhf'], 'emoji': '☕', 'ru': {'title': 'Coffee-Gym: Революция в обучении моделей обратной связи для редактирования кода', 'desc': 'Coffee-Gym - это новая среда для обучения с подкреплением моделей, предоставляющих обратную связь по редактированию кода. Она включает в себя набор данных Coffee с историей редактирования кода людьми и автоматически сгенерированными отзывами, а также функцию вознаграждения CoffeeEval для оценки полезности обратной связи. Coffee-Gym решает проблему нехватки качественных данных для обучения моделей обратной связи с помощью RL и обеспечивает более точные вознаграждения, чем современные модели. Применение Coffee-Gym позволило создать модели обратной связи, превосходящие базовые в улучшении редактирования кода открытыми языковыми моделями.'}, 'en': {'title': 'Enhancing Code Editing Feedback with Coffee-Gym', 'desc': 'This paper introduces Coffee-Gym, a new reinforcement learning (RL) environment designed to improve code editing feedback models. It consists of two key parts: a dataset of human code edits and machine-generated feedback, and a reward function that evaluates the quality of this feedback based on unit test performance. Coffee-Gym aims to fill the gap in high-quality datasets for training feedback models, offering more precise rewards compared to existing models like GPT-4. The results show that models trained with Coffee-Gym significantly enhance the code editing capabilities of open-source language models, making them competitive with proprietary models.'}, 'zh': {'title': 'Coffee-Gym：提升代码编辑反馈的强化学习环境', 'desc': '本文介绍了Coffee-Gym，这是一个用于训练代码编辑反馈模型的综合强化学习环境。Coffee-Gym包含两个主要部分：Coffee数据集和CoffeeEval奖励函数。Coffee数据集包含人类的代码编辑轨迹和机器生成的反馈，而CoffeeEval则通过评估修订后代码在单元测试中的表现来反映反馈的有效性。通过使用Coffee-Gym，我们能够训练出在开源代码编辑方面优于基线模型的反馈模型，使其与闭源模型相媲美，并且我们将数据集和模型检查点公开。'}}}, {'id': 'https://huggingface.co/papers/2409.19808', 'title': 'Can Models Learn Skill Composition from Examples?', 'url': 'https://huggingface.co/papers/2409.19808', 'abstract': 'As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6.   In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '63faa63623faf67c', 'data': {'categories': ['#reasoning', '#agi', '#training', '#benchmark', '#alignment', '#small_models', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Обучение языковых моделей композиционной генерализации на примерах', 'desc': 'Исследование посвящено композиционной генерализации в больших языковых моделях (LLM). Авторы предложили метод обучения моделей комбинированию различных языковых навыков на основе примеров, сгенерированных GPT-4. Эксперименты показали, что модели, обученные на комбинациях 2-3 навыков, могут успешно составлять тексты с 4-5 навыками. Также наблюдалось улучшение способности комбинировать ранее не встречавшиеся навыки.'}, 'en': {'title': 'Unlocking Compositional Generalization in Smaller Language Models', 'desc': 'This paper investigates how smaller language models can learn to combine different language skills to create coherent text, a process known as compositional generalization. The authors use a method called SKILL-MIX, where models are trained on combinations of language skills and then tested on their ability to use new, unseen skills. Results show that even smaller models can improve their performance on complex tasks by training on simpler combinations of skills. The findings suggest that using diverse and skill-rich training data can significantly boost the compositional abilities of these models.'}, 'zh': {'title': '提升模型组合能力的关键在于技能训练', 'desc': '随着大型语言模型（LLMs）的不断进步，它们在组合泛化方面的能力引起了广泛关注。组合泛化是指模型能够以新颖的方式结合学习到的技能，尤其是在训练数据之外的场景中。研究表明，通过使用多种语言技能的组合进行训练，可以显著提高模型在未见过的技能组合上的文本生成能力。该研究还建议，将富含技能的文本纳入训练中，可以大幅提升模型的组合能力。'}}}, {'id': 'https://huggingface.co/papers/2409.19339', 'title': 'Visual Question Decomposition on Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2409.19339', 'abstract': "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce a systematic evaluation framework including a dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce high-quality sub-questions. To address this limitation, we propose a specific finetuning dataset, DecoVQA+, for enhancing the model's question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and a training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets.", 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'a6b68fe261496ab7', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#optimization', '#benchmark', '#synthetic', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Улучшение декомпозиции визуальных вопросов в мультимодальных ИИ', 'desc': 'Эта статья исследует возможности декомпозиции визуальных вопросов в мультимодальных больших языковых моделях (MLLM). Авторы представляют систематическую систему оценки, включающую набор данных и критерии для оценки качества декомпозированных подвопросов. Они предлагают специальный набор данных DecoVQA+ для улучшения способности модели к декомпозиции вопросов. Исследование также включает эффективный процесс дообучения, который значительно улучшает качество подвопросов и политику выборочной декомпозиции вопросов.'}, 'en': {'title': 'Enhancing Question Decomposition in Multimodal Models', 'desc': "This paper investigates how Multimodal Large Language Models (MLLMs) can break down complex questions into simpler sub-questions, a process known as question decomposition. The authors create a new evaluation framework and dataset, DecoVQA+, to measure the effectiveness of this decomposition in MLLMs, revealing that current models often produce low-quality sub-questions. To improve this, they propose a finetuning pipeline that enhances the models' ability to selectively decompose questions. The results show that finetuned MLLMs significantly improve the quality of sub-questions and perform better on visual question answering tasks."}, 'zh': {'title': '提升多模态模型的问题分解能力', 'desc': '本论文探讨了多模态大型语言模型（MLLMs）在视觉问题分解方面的能力。我们提出了一个系统的评估框架，包括数据集和评估标准，以评估分解子问题的质量。研究发现，现有的MLLMs在生成高质量子问题方面存在困难。为了解决这个问题，我们提出了一个特定的微调数据集DecoVQA+，并设计了一个高效的微调流程，以提高模型的问题分解能力。'}}}, {'id': 'https://huggingface.co/papers/2409.19627', 'title': 'IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding', 'url': 'https://huggingface.co/papers/2409.19627', 'abstract': 'The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design a dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '40af324f96dd481e', 'data': {'categories': ['#audio', '#security', '#training', '#optimization', '#architecture'], 'emoji': '🎵', 'ru': {'title': 'Революция в аудио-водяных знаках: глубокое обучение на страже звука', 'desc': 'Эта статья представляет новый метод водяных знаков для аудио, использующий глубокое обучение. Авторы предлагают модель с двойным встраиванием для эффективного определения местоположения водяных знаков. Они также улучшают устойчивость модели, рассматривая влияние атакующего слоя на обратимую нейронную сеть. Эксперименты показывают, что предложенная модель IDEAW превосходит существующие методы по устойчивости к атакам, емкости и способности определения местоположения водяных знаков.'}, 'en': {'title': 'IDEAW: Enhancing Audio Watermarking with Dual-Embedding and Robustness', 'desc': 'This paper presents a novel dual-embedding watermarking model called IDEAW for audio watermarking using deep learning techniques. The model improves upon traditional methods by enhancing robustness against various attacks while also addressing the critical issue of watermark locating. By incorporating an attack layer into the training of an invertible neural network, the model achieves better stability and capacity for embedding messages. Experimental results demonstrate that IDEAW outperforms existing neural audio watermarking methods in terms of robustness, capacity, and locating efficiency.'}, 'zh': {'title': '双重嵌入模型提升音频水印的鲁棒性与定位能力', 'desc': '本文介绍了一种音频水印技术，能够将信息嵌入音频中并准确提取。传统方法依赖专家经验开发算法，而深度学习的出现使得神经音频水印技术得以发展。与传统算法相比，神经音频水印在训练过程中考虑了多种攻击，从而实现了更好的鲁棒性。然而，现有方法在容量和隐蔽性方面存在不足，且水印定位问题尚未得到充分研究。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi (2)', '#alignment (1)', '#architecture (7)', '#audio (1)', '#benchmark (6)', '#cv (4)', '#data (3)', '#dataset (7)', '#dialogue', '#diffusion (1)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (2)', '#open_source (5)', '#optimization (7)', '#plp (1)', '#rag', '#reasoning (4)', '#rl (1)', '#rlhf (1)', '#robotics (2)', '#science (1)', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (4)', '#training (10)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-01 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-01 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-01 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    