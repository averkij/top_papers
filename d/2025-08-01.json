{
    "date": {
        "ru": "1 августа",
        "en": "August 1",
        "zh": "8月1日"
    },
    "time_utc": "2025-08-01 03:49",
    "weekday": 4,
    "issue_id": 5124,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.23726",
            "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2507.23726",
            "abstract": "Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.",
            "score": 34,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "ab5bfbdad68eb6bf",
            "authors": [
                "Luoxin Chen",
                "Jinming Gu",
                "Liankai Huang",
                "Wenhao Huang",
                "Zhicheng Jiang",
                "Allan Jie",
                "Xiaoran Jin",
                "Xing Jin",
                "Chenggang Li",
                "Kaijing Ma",
                "Cheng Ren",
                "Jiawei Shen",
                "Wenlei Shi",
                "Tong Sun",
                "He Sun",
                "Jiahui Wang",
                "Siran Wang",
                "Zhihong Wang",
                "Chenrui Wei",
                "Shufa Wei",
                "Yonghui Wu",
                "Yuchen Wu",
                "Yihang Xia",
                "Huajian Xin",
                "Fan Yang",
                "Huaiyuan Ying",
                "Hongyi Yuan",
                "Zheng Yuan",
                "Tianyang Zhan",
                "Chi Zhang",
                "Yue Zhang",
                "Ge Zhang",
                "Tianyun Zhao",
                "Jianqiu Zhao",
                "Yichi Zhou",
                "Thomas Hanwen Zhu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23726.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в автоматическом доказательстве теорем с помощью ИИ",
                    "desc": "Seed-Prover - это модель для автоматического доказательства теорем, использующая язык Lean. Она применяет итеративное уточнение доказательств и специализированную поддержку геометрии. Модель достигает высокой производительности в формальном доказательстве теорем и автоматизированных математических рассуждениях. Seed-Prover превосходит предыдущие системы на нескольких эталонных наборах задач, включая формализованные задачи Международной математической олимпиады."
                },
                "en": {
                    "title": "Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement",
                    "desc": "The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning."
                },
                "zh": {
                    "title": "Seed-Prover：自动化数学推理的新突破",
                    "desc": "Seed-Prover是一种基于Lean的引理风格推理模型，能够在形式定理证明和自动数学推理中实现高性能。该模型通过迭代优化和专门的几何支持，克服了传统自然语言推理的局限性。Seed-Prover利用Lean的反馈和自我总结来不断改进其证明过程，并设计了三种推理策略以应对国际数学奥林匹克（IMO）级别的问题。通过引入Seed-Geometry几何推理引擎，Seed-Prover在几何问题上也取得了显著进展，展示了形式验证与长链推理的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22879",
            "title": "RecGPT Technical Report",
            "url": "https://huggingface.co/papers/2507.22879",
            "abstract": "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.",
            "score": 9,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "2bd5536810f1694b",
            "authors": [
                "Chao Yi",
                "Dian Chen",
                "Gaoyang Guo",
                "Jiakai Tang",
                "Jian Wu",
                "Jing Yu",
                "Mao Zhang",
                "Sunhao Dai",
                "Wen Chen",
                "Wenjun Yang",
                "Yuning Jiang",
                "Zhujin Gao",
                "Bo Zheng",
                "Chi Li",
                "Dimin Wang",
                "Dixuan Wang",
                "Fan Li",
                "Fan Zhang",
                "Haibin Chen",
                "Haozhuang Liu",
                "Jialin Zhu",
                "Jiamang Wang",
                "Jiawei Wu",
                "Jin Cui",
                "Ju Huang",
                "Kai Zhang",
                "Kan Liu",
                "Lang Tian",
                "Liang Rao",
                "Longbin Li",
                "Lulu Zhao",
                "Na He",
                "Peiyang Wang",
                "Qiqi Huang",
                "Tao Luo",
                "Wenbo Su",
                "Xiaoxiao He",
                "Xin Tong",
                "Xu Chen",
                "Xunke Xi",
                "Yang Li",
                "Yaxuan Wu",
                "Yeqiu Yang",
                "Yi Hu",
                "Yinnan Song",
                "Yuchen Li",
                "Yujie Luo",
                "Yujin Yuan",
                "Yuliang Yan",
                "Zhengyang Wang",
                "Zhibo Xiao",
                "Zhixin Ma",
                "Zile Zhou",
                "Ziqi Zhang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.22879.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "RecGPT: Рекомендации, ориентированные на намерения пользователей",
                    "desc": "RecGPT - это новая система рекомендаций, интегрирующая большие языковые модели (LLM) для фокусировки на намерениях пользователей. Она улучшает разнообразие контента и удовлетворенность пользователей, а также повышает эффективность для продавцов и платформы. RecGPT использует многоэтапную парадигму обучения, включающую предварительное выравнивание с усиленным рассуждением и эволюцию самообучения. Система уже развернута в приложении Taobao и показывает стабильный рост производительности для всех заинтересованных сторон."
                },
                "en": {
                    "title": "Empowering Recommendations with User Intent",
                    "desc": "RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem."
                },
                "zh": {
                    "title": "以用户意图为中心的推荐系统新范式",
                    "desc": "RecGPT 是一种将大型语言模型整合到推荐系统中的新框架，旨在关注用户意图。通过重新设计推荐流程，RecGPT 使推荐过程从单纯依赖历史数据转变为以用户意图为中心。该系统通过多阶段训练方法，结合推理增强的预对齐和自我训练，提升了推荐的准确性和多样性。实验结果表明，RecGPT 在用户满意度、商家曝光率和平台转化率等方面均取得了显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23682",
            "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models",
            "url": "https://huggingface.co/papers/2507.23682",
            "abstract": "The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.",
            "score": 7,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "baa73e4730b01a97",
            "authors": [
                "Xiaoyu Chen",
                "Hangxing Wei",
                "Pushi Zhang",
                "Chuheng Zhang",
                "Kaixin Wang",
                "Yanjiang Guo",
                "Rushuai Yang",
                "Yucen Wang",
                "Xinquan Xiao",
                "Li Zhao",
                "Jianyu Chen",
                "Jiang Bian"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Microsoft Research",
                "Nanjing University",
                "Tsinghua University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23682.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#games",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ViLLA: Улучшение роботизированных манипуляций с помощью латентных действий",
                    "desc": "Фреймворк ViLLA улучшает модели визуально-языкового действия (VLA) путем включения латентных действий. Это позволяет повысить производительность как в симулированных, так и в реальных задачах роботизированных манипуляций. Предложенный подход villa-X совершенствует как обучение латентным действиям, так и их интеграцию в предобучение VLA. Модель демонстрирует превосходные результаты в симуляторах SIMPLER и LIBERO, а также на реальных роботах с захватами и ловкими руками."
                },
                "en": {
                    "title": "Enhancing Robot Manipulation with Latent Actions in ViLLA Framework",
                    "desc": "The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research."
                },
                "zh": {
                    "title": "ViLLA框架：提升机器人操作的潜力",
                    "desc": "ViLLA框架通过引入潜在动作来增强视觉-语言-动作（VLA）模型，从而提高机器人操作任务的性能。潜在动作是一种抽象表示，能够捕捉两个帧之间的视觉变化。本文介绍的villa-X是一个新颖的视觉-语言-潜在动作框架，旨在改进潜在动作建模，以学习可推广的机器人操作策略。我们的研究表明，villa-X在模拟环境和真实机器人设置中均表现出色，展示了ViLLA范式的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22968",
            "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations",
            "url": "https://huggingface.co/papers/2507.22968",
            "abstract": "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.",
            "score": 5,
            "issue_id": 5124,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "3a2f5273d610d5d6",
            "authors": [
                "Chengqian Ma",
                "Wei Tao",
                "Yiwen Guo"
            ],
            "affiliations": [
                "Independent Researcher",
                "LIGHTSPEED",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22968.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#long_context",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Бенчмарк для оценки разговорных ИИ-моделей в реальных условиях",
                    "desc": "В статье представлен набор данных для оценки разговорных диалоговых моделей на английском и китайском языках. Этот бенчмарк позволяет оценить способность моделей понимать и имитировать человеческие разговоры, учитывая такие сложности как неоднозначность и контекстная зависимость. Набор данных содержит 1079 примеров и сопровождается методом оценки на основе больших языковых моделей, который хорошо коррелирует с человеческими оценками. Исследование направлено на комплексное изучение эффективности разговорных диалоговых моделей в решении практических задач."
                },
                "en": {
                    "title": "Benchmarking Spoken Dialogue Models for Real-World Conversations",
                    "desc": "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."
                },
                "zh": {
                    "title": "提升口语对话模型的评估标准",
                    "desc": "本文提出了一个用于评估口语对话模型（SDMs）性能的基准数据集，涵盖英语和中文，旨在理解和模拟人类口语对话。口语对话的复杂性体现在歧义性和上下文依赖性等挑战上，这些因素使得与文本基础的大型语言模型（LLMs）相比，SDMs的研究相对较少。数据集中包含1079个实例，并配备了一种基于LLM的评估方法，以更好地与人类判断相一致。通过这个数据集，研究者可以全面探讨SDMs在应对实际对话挑战中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23632",
            "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective",
            "url": "https://huggingface.co/papers/2507.23632",
            "abstract": "Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.",
            "score": 0,
            "issue_id": 5124,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "b07ddf6cb6b8bee8",
            "authors": [
                "Gabriel Mongaras",
                "Eric C. Larson"
            ],
            "affiliations": [
                "Lyle School of Engineering Southern Methodist University Dallas, TX 75205"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23632.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Раскрывая силу софтмакс-внимания через призму RNN",
                    "desc": "Статья исследует различия между софтмакс-вниманием и линейным вниманием в нейронных сетях. Авторы показывают, что софтмакс-внимание можно представить в рекуррентной форме, аналогичной рекуррентным нейронным сетям (RNN). Это позволяет провести анализ компонентов софтмакс-внимания и объяснить его большую выразительность по сравнению с линейными аналогами. Работа помогает понять, почему софтмакс-внимание остается основой современных трансформерных архитектур, несмотря на квадратичную сложность."
                },
                "en": {
                    "title": "Unlocking the Power of Softmax Attention",
                    "desc": "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."
                },
                "zh": {
                    "title": "软max注意力的优势解析",
                    "desc": "本文探讨了softmax注意力与线性注意力的区别。softmax注意力因其表达能力强而成为现代变换器架构的基础，但其在序列长度上的计算复杂度和内存需求是一个主要缺点。通过将softmax非线性替换为线性注意力，研究者们试图解决这一瓶颈。本文表明，线性注意力实际上是softmax注意力的一种近似，并通过递归神经网络的语言来描述softmax注意力的各个部分，从而揭示其更强表达能力的原因。"
                }
            }
        }
    ],
    "link_prev": "2025-07-31.html",
    "link_next": "2025-08-04.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "31.07",
        "en": "07/31",
        "zh": "7月31日"
    },
    "short_date_next": {
        "ru": "04.08",
        "en": "08/04",
        "zh": "8月4日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}